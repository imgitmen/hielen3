{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"CHANGELOG v2.0.6 27 Gennaio 2021 Inserita la validazione dei campi delle action 25 Gennaio 2021 Inserito il campo \"hint\" nella risposta di GET /actionschemata[/{feature}[/{action}]] Implementata astrazione di DELETE /actions/{feature}/{action}/ . Prima della rimozione della specifica azione dal db, vengono chimati i metodi preposti alla gestione della rimozione delle informazioni in cache, che si presuppone siano implemetati dai relativi moduli. I moduli possono implementare questi metodi solo se necessario. Sintassi: Invocando il metodo deleteActionValues(self,action,timestamp) della superclasse Hielen2.Source, essa tenter\u00e0 di utilizzare il metodo della sottoclasse il cui nome \u00e8 cotruito dal'unione della parola \"delete\" + il nome dell'azione con la prima lettera maiuscola: es: \"deleteConfig\". la superclasse passer\u00e0 sempre un timestamp per individuare l'azione specifica Implementato il metodo \"deleteConfig\" della classe Hielen2.ext.source_PhotoMonitoring Corretto bug minore di gestione delle azioni in caso esse producano errori non preventivati. 22 Gennaio 2021 Ristrutturata la pagina di TODO , inserita categorizzazione e valutazione delle tempistica delle attivit\u00e0 20 Gennaio 2021 modulo hielen2.ext.source_PhotoMonitoring : rimodellato sulla base del modulo hielen2.source . definite le classi schema per le azioni: ## action 'config' (Completamente Funzionante) class ConfigSchema ( ActionSchema ): master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Str ( required = False , default = \"8\" ) window_size_change = fields . Str ( required = False , default = \"0\" ) geo_reference_file = LocalFile ( required = False , default = None ) crs = fields . Str ( required = False , default = None ) ## action 'feed' class FeedSchema ( ActionSchema ): reference_time = fields . Str ( required = False , allow_none = False ) NS_displacement = LocalFile ( required = False , allow_none = False ) EW_displacement = LocalFile ( required = False , allow_none = False ) Coer = LocalFile ( required = False , allow_none = False ) 18 Gennaio 2021 Revisione concettuale delle API e modifiche: GET /parametes : lo schema di ritorno \u00e8 il seguente (semplicemente \"param\" al posto di \"name\" ): { ..., \"data\": { \"ARCCE01\": [ { \"series\": \"ARCCE01_Rotazione_X\", \"param\": \"Rotazione X\", \"unit\": \"mm/m\"}, ..... } ] } actionSchemata \u00e8 l'api che fornir\u00e0 gli schemi per le azioni e va a sostituire quella che era \"prototypes\". Questa esiste ancora e mantiene il legame tra prototipo e modulo ma pi\u00f9 che altro le informazioni che stanno nella relativa tabella mi servono per il back-end GET ../actionschemata[/{prototypes}[/{actions}]] action come prima, \u00e8 l'api che gestisce le azioni: La versione POST nella sostanza non \u00e8 cambiata a parte il fatto che un'azione dichiarer\u00e0 sempre un timestamp per default. Ma questa cosa al front-end non interessa dal momento che le info le recupera da actionSchemanta. E' invece importante nella scrittura dei plugin perch\u00e9 in questo modo le azioni possono essere gestite temporalmente. La versione GET , invece cambia sostanzialmente: non fornir\u00e0 pi\u00f9 i default per la post MA potr\u00e0 fornire una serie temporale di azioni associate a dei valori di elaborazione che danno informazioni all'utente. in questo formato: GET ../actions[/{feature}[/{action}]] ritorna: [ { \"timestamp\":....,\"value\":.... }, { \"timestamp\":...., \"value\":.... }, .... ] esempio: GET .. / actions / featurecode / config { \"meta\" : { \"response\" : \"ok\" , \"message\" : \"\" , \"data_type\" : \"GET /actions/ciaociaociao4/config\" } , \"data\" : [ [ { \"timestamp\" : \"2020-12-30 01:00:05\" , \"value\" : { \"master_image\" : \"TIFF image data, little-endian, direntries=14, height=1842, bps=16, compression=none, PhotometricIntepretation=BlackIsZero, width=3545\" , \"step_size\" : \"35\" , \"window_size_change\" : \"10\" , \"transform\" : [ 15 . 0 , 0 . 0 , 464947 . 5 , 0 . 0 , - 15 . 0 , 7977067 . 5 ], \"cache\" : \"20201230010005\" , \"crs\" : null } } , { \"timestamp\" : \"2020-12-30 01:00:07\" , \"value\" : { \"master_image\" : \"TIFF image data, little-endian, direntries=16, height=1842, bps=16, compression=none, PhotometricIntepretation=BlackIsZero, width=3545\" , \"timestamp\" : \"2020-12-30 01:00:07\" , \"step_size\" : \"35\" , \"window_size_change\" : \"10\" , \"transform\" : [ 15 . 0 , 0 . 0 , 464947 . 5 , 0 . 0 , - 15 . 0 , 7977067 . 5 ], \"cache\" : \"20201230010007\" , \"crs\" : \"EPSG:32622\" } } ] ] } 15 Gennaio 2021 modulo hielene2.source : Implementato il metodo sourceFactory per la generazione degli ogetti HeielenSource in base ai prototipi che sfrutta il cariacmanto dinamico dei moduli specifici (metodo loadModule ) Implementati i metodi e le classi per la gestione agnostica delle azioni ed il recupero degli schemi: getActionSchema , moduleAction , HielenSource.execAction , HielenSource.getActionValues Implementata la gestione dell'ambiente di cache dedicato alle singole istanze di HielenSource: HielenSource.makeCachePath , HielenSource.getRelativePath Definita la classe primitiva per i modelli di schema per le azioni che impone la definizione della marcatura temporale: class ActionSchema(Schema): timestamp = fields.Str(required=True, allow_none=False) 13 Gennaio 2021 rimodellato il db: dalla tabella \"features\" sono state eliminate le colonne \"a priori\" delle azioni. Queste ultime sono state inserite in una nuova tabella \"actions\" con chiave multipla (\"feature\",\"action\",\"timestamp\"). Rivista l'interfaccia db per permettere l'interrogazione su chiave multipla 10 Gennaio 2021 Progettazione della gestione temporale delle azioni e separazione del concetto di form da quello di risultato della azione: ogni azione ha uno schema di input e dei risultati in output con uno schema non necessariamente coincidente. Quello che viene fornito alle form sono i dati necessari ad intraprendere un'azione. I risultati dell'azione devono essere registrati con una marcatura temporale. In questo modo ogni azione \u00e8 univocamente determinata e gestibile con un modello del tipo (\"feature\",\"action\",\"timestamp\"), con una cardinalit\u00e0 1-a-molti tra features e azioni Portata a termine la migrazione della gestione delle azioni che vengono ora completamente affidate ai singoli moduli. L'iterfaccia di alto livello \u00e8 ora in grado di gestire agonsticamente le chiamate ad azioni arbitrarie purch\u00e8 ben definite all'interno dei moduli. In questo modo cade il vincolo di definizione do azione \"a priori\" 30 Dicembre 2020 sviluppo (non completo) di config hielen2.ext.PhotoMonitoring: Implementato il metodo di recupero e settaggio delle informazioni geometriche/geografiche dell'immagine in ingresso Aggancio del codice originale per la gesgione del netcdf (in debug) 22 Dicembre 20202 Delineata la gestione di mappa delle immagini prodotte: Ogni immagine prodotta sar\u00e0 sempre associata al suo crs e la matrice di trasformazione affine, anche nele caso in cui queste informazioni non dovessero essere passate in configurazione. In questo caso si assume un piano cartesiano con udm in m e una matrice identit\u00e0 per le trasformazioni affini. Sar\u00e0 dunque sempre possibile gestire le immagini come mappe (slippy maps) e sfruttare la tassellazione, il cacheing dei tasselli. 20 Dicembre 2020 Modificata l'api POST /actions/{feature}/{form} in modo da interrogare la Source (per ora solo PhotoMonitoring) sulla definizione delle azioni: Implementate le classi di Schema per config e feed per il modulo hielen2.ext.PhotoMonitoring . ATTENZIONE per config : introdotto il campo \"timestamp\", eliminati i campi espliciti relativi al word_file ( word_file mantenuto), modificato il campo epsg in csr . 15 Dicembre 2020 Delineato il modello di scrittura dei Source plugin secondo un template univoco. Ogni plugin potr\u00e0 essere un modulo python definito come segue: deve definire tante classi marshmallow.Schema quante sono le azioni che vengono prese in carico dal Template. Marsmallow \u00e8 un serializzatore di oggetti python. Lo schema definito servir\u00e0 per definire i campi in ingresso per ogni azione e fare i check dei valori in ingresso. Il nome delle classi Schema deve seguire questa sintassi: class {Action}Schema(marshmallow.Schema) dove {Action} \u00e8 il nome dell'azione (es.: config, feed, ..) con l'iniziale maiuscola . Nella classe vengono definiti i tipi dei campi ( marshmallow.fields cfr. https://marshmallow.readthedocs.io/en/stable/ ). ATTENZIONE: in caso fosse necessario l'ingresso di file o comunque oggetti blob dovr\u00e0 essere utilizzato come field la classe hielen2.utils.LocalFile . In questo modo il sistema risolver\u00e0 la chiamata API salvando in locale lo stream dei dati associato a quel determinato field, il quale sar\u00e0 accessibile al template attraverso un path che verr\u00e0 fornito insieme agli altri campi al momento della chiamata del metodo di gestione dell'azione (vedi sotto). deve implementare una classe Source(hielen2.datalink.HielenSource) che esponga tanti metodi quante sono le dichiarazioni di Schema seguendo questa sintassi: il metodo di gestione dell'azione deve chiamarsi come l'azione stessa ( tutto in minuscolo ). Le classi estese sfrutteranno il metodo __init__ della superclasse in modo da avere a disposizione tutto quello di cui necessitano. Questo modello permette di svincolare i template dalla necessit\u00e0 di conoscere a priori le azioni ammmissibili per il sistema. Infatti, facendo introspezione su un template che segua le regole di sintassi sar\u00e0 sempre possibile conoscere le azioni definite ed esternalizzarle al front-end che in base alle definizioni delle classi di Schema delle azioni, sar\u00e0 sempre in grado di instanziare una form adeguata. v2.0.5 9 Dicembre 2020 Implementata working POST /actions/{feature}/{form} tramite content-type/multipart dinamico definito dal prototipo: L'api \u00e8 collegata ai moduli reali delle tipologie definiti come templates, con la funzionalit\u00e0 minima di salvare i parametri in ingresso. I moduli sono in fase di sviluppo e man mano che vengono implementati le funzionalit\u00e0 aumenteranno. Implementato Loading dinamico dei moduli di elaborazione definiti come estensioni di hielen2.datalink.HielenSource Implementata working GET /actions/{feature}[/{form}] : Per ogni form richiesta, risponde con tutti i parametri definiti nel relativo prototipo, riempiti con i valori definiti tramite la POST della stessa api. I valori non precedentemente forniti vengono impostati a null Riveduta e corretta GET prototypes/{prototype}/forms[/form] : ATTENZIONE adesso risponde con TUTTI i campi dentro il dizionario \"args\" e comunica i campi obbligatori attraverso l'array \"mandatory\". Questa struttura \u00e8 pi\u00f9 versatile in quanto, una volta definito il set completo degli argomenti, \u00e8 possibile definire un numero arbitrario di sottoinsiemi predicativi non necessariamente distiniti: Oltre al sottoinsieme \"mandatory\" si potrebbe, ad esempio, definire un sottoinsieme di immutabili. Qui sotto una struttura di esempio: { \"data\": { \"args\": { \"epsg\": \"string\", \"master_image\": \"file\", \"negative_pixel_y_size\": \"string\", \"pixel_x_size\": \"string\", \"rotation_about_the_x_axis\": \"string\", \"rotation_about_the_y_axis\": \"string\", \"step_size\": \"string\", \"window_size_change\": \"string\", \"world_file\": \"file\", \"x_coordinate_of_upper_left_pixel_center\": \"string\", \"y_coordinate_of_upper_left_pixel_center\": \"string\" }, \"mandatory\": [ \"master_image\", \"step_size\", \"window_size_change\" ] }, \"meta\": { \"data_type\": \"GET /prototypes/PhotoMonitoring/forms/config\", \"message\": \"\", \"response\": \"ok\" } } 7 Dicembre 2020 Rimodellato il feature db per contenere gli argomenti delle actions Riveduto il feature_proto db: Inserito il modulo di riferimento tra le info del prototipo (il modulo contenete la classe estesa di hielen2.datalink.HielenSource ) Definita la superclasse hielen2.datalink.HielenSource con definizione univoca di __init__ con questo footprint: (self,featureobject,environment) . La classe definisce inotre i metodi astratti che vengono utilizzati dal sistema che ogni estensione di questa dovr\u00e0 implementare. 2 Dicembre 2020 Struttura definitiva delle features: { \"properties\":\"...\" \"parameters\":\"...\" \"geometry\":\"...\" } dove: properties mantiene tutte le info della feature. Quelle di base: uid , type , classification , location , description e quelle definite per le specifiche azioni definite per la tipologia. In particolare quella di configurzione. parameters mantiene la struttura di accesso alle info e ai dati dei parametri definiti per la feature. geometry fornisce le informazioni geometriche della feature. Rivedute le api /actions , /parameters , /features ( /data da rivedere) 24 Novembre 2020 Implementate dummy /actions/{feature}/ e /actions/{feature}/{form} 23 Novembre 2020 Riorganizzato il db delle features per permettere una gestione pi\u00f9 razionale 19 Novembre 2020 riorganizzata la struttura per la gestione delle classi estese che necessitano di dynamic loading: nel modulo himada2.ext (cartella) vengono raccoliti per comodit\u00e0 gli oggetti che saranno implementati man mano come estensione di superclassi astratte appositamente definite: per ora hielen2.datalink.Source e hielen2.datalink.DB e hielen2.datalink.DataCache. Oltre alle classi in hielen2.ext, il sitema potr\u00e0 utilizzare moduli esterni che estendano le superclassi elencate. inserito 'timestamp' nello schema json accettato da POST /feature e PUT /feature . risolto bug minore di incoerenza su GET /data/{feature} e /data/{feature}/{parameter} . Quest'ultima continua ad accettare uno tra i nomi dei parametri della feature. Entrambe rispondo intestando le colonne in uscita con lo uid della serie, come GET /data/ . 17 Novembre 2020 Implementata dummy POST /actions/{feature}/{form} : v2.0.4 16 Novembre 2020 per coerenza rivisti i parametri di POST /feature : uid:<string> prototype:<string> properties:<json schema Properties> geometry:<json schema GeoJson> analogo discorso per PUT /feature/{uid} : properties:<json schema Properties> geometry:<json schema GeoJson> sistemata la risposta di GET /feature , modificando il livello di \"geometry\" implementata api PUT /features/{uid} . Accetta il paramentro properties con uno schema analogo al parmetro feature di POST /features con queste differenze: nello schema della PUT, uid e prototype NON vengono accettati perch\u00e8 sono campi chiave della feature e non possono essere modificati . lo uid della feature deve essere specificato come url e non come parametro. introduzione dello Schema GeoJson per la validazione modificata POST /features/ per accettare un GeoJson nell'attibuto geometry del Json principale feature 13 Novembre 2020 rinominazione DELETE /elements -> DELETE /features . eliminazione degli alias GET /features/{context} e /features/{context}/{uid} a causa del conflitto l'entry point DELETE /features . Il passaggio del context sar\u00e0 esclusivmante attraverso il parametro cntxt ( nota : questo nome \u00e8 dovuto alla collisione del nome con il campo 'context' dell'oggetto request). In caso lo possiamo cambiare. introduzione dell'alias /features/{uid} per il recupero delle info della specifica Feature. 12 Novembre 2020 ovunque nel mondo il parmetro 'uuid' (universal unique id) diventa 'uid'. rinominazione POST /elements -> POST /features . rinominazione GET /elements -> GET /parameters e modifica uscita in questo schema: { < feature1_UID > :[ { \"series\" : < feature1_param1_series_UID > , \"param\" : < feature1_param1_name > , \"um\" : < feature1_param1_measurement_unit > } , ... { \"series\" : < feature1_paramN_series_UID , \"param\" : < feature1_paramN_name > , \"um\" : < feature1_paramN_meaurement_unit > } ], ... < featureX_UID > :[ { \"series\" : < featureX_param1_series_UID > , \"param\" : < featureX_param1_name > , \"um\" : < featureX_param1_measurement_unit > } , ... { \"series\" : < featureX_paramM_series_UID , \"param\" : < featureX_paramM_name > , \"um\" : < featureX_paramM_meaurement_unit > } ] } introduzione api /features con lo schema usato da Daniele e SimoneD: GET /features GET /features/{context}/ GET /features/{context}/{feature} uscita : nota 1: NON viene introdotto \"context\" , come invece preventivato nota 2: \"cod\" diventa \"label\" . nota 3: \"date\" diventa \"timestamp\" nota 3: dalle properties vengono elminate \"z\" e \"mslm\" . nota 4: \"state\" viene mantenuto ma per ora \u00e8 inutilizzato { \"features\" : [ { \"type\" : \"Feature\" , \"properties\" : { \"uid\" : ..., \"label\" : ..., \"context\" :..., \"date\" : ..., \"type\" : ..., \"style\" : ..., \"state\" : ... } , \"geometry\" : < GeoJson Validable > } , ... { \"type\" : \"Feature\" , \"properties\" : { \"uid\" : ..., \"label\" : ..., \"context\" : ..., \"date\" : ..., \"type\" : ..., \"style\" : ..., \"state\" : ... } , \"geometry\" : < GeoJson Validable > } ] } v2.0.3 11 Novembre 2020 Modificata api POST /elements : la variabile element \u00e8 descritta dalla Classe hielen2.api.data.ElementSchema e validata. In paricolare \u00e8 stato introdotto l'attibuto context Modifcata api GET /data : la variabile datamap \u00e8 descritta dalla Classe hielen2.api.data.DataMapSchema e validata. 9 Novembre 2020 Introdotta la classe hielen2.utils.JsonValidable, per la validazione e documentazione automatica dei parametri delle api (JSON Schema descrition) corretti bug minori in hielen2.datalink 6 Novembre 2020 L'interfaccia DB \u00e8 ora thread safe!!! (almeno per il dummy json db) v2.0.2 4 Novembre 2020 Implementata la documentazione automatica delle api Implementate le api POST ../elements e DELETE ../elements L'uscita per tutte le api element (e per tutte le api con risposta json in generale), seguir\u00e0 questo schema: { \"meta\": { \"data_type\": \"DELETE /elements/ciao\", \"response\": \"ok\" \"message\": \"\", }, \"data\":{ ... } } L'api /series diventa /data e cambia il suo comportamento: la variabile di tipo json datamap si aspetta il campo series invece di parameters . In questo campo devono essere inseriti i codici delle serie e non pi\u00f9 il costrutto \"codice_elemento:parametro_elemento\". I codici delle serie si possono recuperarare dall'api /elements (vedi Nota successiva) L'api /elements cambia la sua risposta e per ogni parametro nella lista parameters degli elementi viene agiunto il codice della serie di riferimento che pu\u00f2 essere fornito senza modifiche a /data : { \"series\":<seriescode>, \"name\":<seriesname>, \"um\":<seriesunit> } GET /series GET /series/{el} GET /series/{el}/{param} GET /prototypes GET /prototypes/{type} GET /prototypes/{type}/forms GET /prototypes/{type}/forms/{form} POST /elements GET /elements GET /elements/{el} DELETE /elements/{el}","title":"Home"},{"location":"#changelog","text":"","title":"CHANGELOG"},{"location":"#v206","text":"","title":"v2.0.6"},{"location":"#27-gennaio-2021","text":"Inserita la validazione dei campi delle action","title":"27 Gennaio 2021"},{"location":"#25-gennaio-2021","text":"Inserito il campo \"hint\" nella risposta di GET /actionschemata[/{feature}[/{action}]] Implementata astrazione di DELETE /actions/{feature}/{action}/ . Prima della rimozione della specifica azione dal db, vengono chimati i metodi preposti alla gestione della rimozione delle informazioni in cache, che si presuppone siano implemetati dai relativi moduli. I moduli possono implementare questi metodi solo se necessario. Sintassi: Invocando il metodo deleteActionValues(self,action,timestamp) della superclasse Hielen2.Source, essa tenter\u00e0 di utilizzare il metodo della sottoclasse il cui nome \u00e8 cotruito dal'unione della parola \"delete\" + il nome dell'azione con la prima lettera maiuscola: es: \"deleteConfig\". la superclasse passer\u00e0 sempre un timestamp per individuare l'azione specifica Implementato il metodo \"deleteConfig\" della classe Hielen2.ext.source_PhotoMonitoring Corretto bug minore di gestione delle azioni in caso esse producano errori non preventivati.","title":"25 Gennaio 2021"},{"location":"#22-gennaio-2021","text":"Ristrutturata la pagina di TODO , inserita categorizzazione e valutazione delle tempistica delle attivit\u00e0","title":"22 Gennaio 2021"},{"location":"#20-gennaio-2021","text":"modulo hielen2.ext.source_PhotoMonitoring : rimodellato sulla base del modulo hielen2.source . definite le classi schema per le azioni: ## action 'config' (Completamente Funzionante) class ConfigSchema ( ActionSchema ): master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Str ( required = False , default = \"8\" ) window_size_change = fields . Str ( required = False , default = \"0\" ) geo_reference_file = LocalFile ( required = False , default = None ) crs = fields . Str ( required = False , default = None ) ## action 'feed' class FeedSchema ( ActionSchema ): reference_time = fields . Str ( required = False , allow_none = False ) NS_displacement = LocalFile ( required = False , allow_none = False ) EW_displacement = LocalFile ( required = False , allow_none = False ) Coer = LocalFile ( required = False , allow_none = False )","title":"20 Gennaio 2021"},{"location":"#18-gennaio-2021","text":"Revisione concettuale delle API e modifiche: GET /parametes : lo schema di ritorno \u00e8 il seguente (semplicemente \"param\" al posto di \"name\" ): { ..., \"data\": { \"ARCCE01\": [ { \"series\": \"ARCCE01_Rotazione_X\", \"param\": \"Rotazione X\", \"unit\": \"mm/m\"}, ..... } ] } actionSchemata \u00e8 l'api che fornir\u00e0 gli schemi per le azioni e va a sostituire quella che era \"prototypes\". Questa esiste ancora e mantiene il legame tra prototipo e modulo ma pi\u00f9 che altro le informazioni che stanno nella relativa tabella mi servono per il back-end GET ../actionschemata[/{prototypes}[/{actions}]] action come prima, \u00e8 l'api che gestisce le azioni: La versione POST nella sostanza non \u00e8 cambiata a parte il fatto che un'azione dichiarer\u00e0 sempre un timestamp per default. Ma questa cosa al front-end non interessa dal momento che le info le recupera da actionSchemanta. E' invece importante nella scrittura dei plugin perch\u00e9 in questo modo le azioni possono essere gestite temporalmente. La versione GET , invece cambia sostanzialmente: non fornir\u00e0 pi\u00f9 i default per la post MA potr\u00e0 fornire una serie temporale di azioni associate a dei valori di elaborazione che danno informazioni all'utente. in questo formato: GET ../actions[/{feature}[/{action}]] ritorna: [ { \"timestamp\":....,\"value\":.... }, { \"timestamp\":...., \"value\":.... }, .... ] esempio: GET .. / actions / featurecode / config { \"meta\" : { \"response\" : \"ok\" , \"message\" : \"\" , \"data_type\" : \"GET /actions/ciaociaociao4/config\" } , \"data\" : [ [ { \"timestamp\" : \"2020-12-30 01:00:05\" , \"value\" : { \"master_image\" : \"TIFF image data, little-endian, direntries=14, height=1842, bps=16, compression=none, PhotometricIntepretation=BlackIsZero, width=3545\" , \"step_size\" : \"35\" , \"window_size_change\" : \"10\" , \"transform\" : [ 15 . 0 , 0 . 0 , 464947 . 5 , 0 . 0 , - 15 . 0 , 7977067 . 5 ], \"cache\" : \"20201230010005\" , \"crs\" : null } } , { \"timestamp\" : \"2020-12-30 01:00:07\" , \"value\" : { \"master_image\" : \"TIFF image data, little-endian, direntries=16, height=1842, bps=16, compression=none, PhotometricIntepretation=BlackIsZero, width=3545\" , \"timestamp\" : \"2020-12-30 01:00:07\" , \"step_size\" : \"35\" , \"window_size_change\" : \"10\" , \"transform\" : [ 15 . 0 , 0 . 0 , 464947 . 5 , 0 . 0 , - 15 . 0 , 7977067 . 5 ], \"cache\" : \"20201230010007\" , \"crs\" : \"EPSG:32622\" } } ] ] }","title":"18 Gennaio 2021"},{"location":"#15-gennaio-2021","text":"modulo hielene2.source : Implementato il metodo sourceFactory per la generazione degli ogetti HeielenSource in base ai prototipi che sfrutta il cariacmanto dinamico dei moduli specifici (metodo loadModule ) Implementati i metodi e le classi per la gestione agnostica delle azioni ed il recupero degli schemi: getActionSchema , moduleAction , HielenSource.execAction , HielenSource.getActionValues Implementata la gestione dell'ambiente di cache dedicato alle singole istanze di HielenSource: HielenSource.makeCachePath , HielenSource.getRelativePath Definita la classe primitiva per i modelli di schema per le azioni che impone la definizione della marcatura temporale: class ActionSchema(Schema): timestamp = fields.Str(required=True, allow_none=False)","title":"15 Gennaio 2021"},{"location":"#13-gennaio-2021","text":"rimodellato il db: dalla tabella \"features\" sono state eliminate le colonne \"a priori\" delle azioni. Queste ultime sono state inserite in una nuova tabella \"actions\" con chiave multipla (\"feature\",\"action\",\"timestamp\"). Rivista l'interfaccia db per permettere l'interrogazione su chiave multipla","title":"13 Gennaio 2021"},{"location":"#10-gennaio-2021","text":"Progettazione della gestione temporale delle azioni e separazione del concetto di form da quello di risultato della azione: ogni azione ha uno schema di input e dei risultati in output con uno schema non necessariamente coincidente. Quello che viene fornito alle form sono i dati necessari ad intraprendere un'azione. I risultati dell'azione devono essere registrati con una marcatura temporale. In questo modo ogni azione \u00e8 univocamente determinata e gestibile con un modello del tipo (\"feature\",\"action\",\"timestamp\"), con una cardinalit\u00e0 1-a-molti tra features e azioni Portata a termine la migrazione della gestione delle azioni che vengono ora completamente affidate ai singoli moduli. L'iterfaccia di alto livello \u00e8 ora in grado di gestire agonsticamente le chiamate ad azioni arbitrarie purch\u00e8 ben definite all'interno dei moduli. In questo modo cade il vincolo di definizione do azione \"a priori\"","title":"10 Gennaio 2021"},{"location":"#30-dicembre-2020","text":"sviluppo (non completo) di config hielen2.ext.PhotoMonitoring: Implementato il metodo di recupero e settaggio delle informazioni geometriche/geografiche dell'immagine in ingresso Aggancio del codice originale per la gesgione del netcdf (in debug)","title":"30 Dicembre 2020"},{"location":"#22-dicembre-20202","text":"Delineata la gestione di mappa delle immagini prodotte: Ogni immagine prodotta sar\u00e0 sempre associata al suo crs e la matrice di trasformazione affine, anche nele caso in cui queste informazioni non dovessero essere passate in configurazione. In questo caso si assume un piano cartesiano con udm in m e una matrice identit\u00e0 per le trasformazioni affini. Sar\u00e0 dunque sempre possibile gestire le immagini come mappe (slippy maps) e sfruttare la tassellazione, il cacheing dei tasselli.","title":"22 Dicembre 20202"},{"location":"#20-dicembre-2020","text":"Modificata l'api POST /actions/{feature}/{form} in modo da interrogare la Source (per ora solo PhotoMonitoring) sulla definizione delle azioni: Implementate le classi di Schema per config e feed per il modulo hielen2.ext.PhotoMonitoring . ATTENZIONE per config : introdotto il campo \"timestamp\", eliminati i campi espliciti relativi al word_file ( word_file mantenuto), modificato il campo epsg in csr .","title":"20 Dicembre 2020"},{"location":"#15-dicembre-2020","text":"Delineato il modello di scrittura dei Source plugin secondo un template univoco. Ogni plugin potr\u00e0 essere un modulo python definito come segue: deve definire tante classi marshmallow.Schema quante sono le azioni che vengono prese in carico dal Template. Marsmallow \u00e8 un serializzatore di oggetti python. Lo schema definito servir\u00e0 per definire i campi in ingresso per ogni azione e fare i check dei valori in ingresso. Il nome delle classi Schema deve seguire questa sintassi: class {Action}Schema(marshmallow.Schema) dove {Action} \u00e8 il nome dell'azione (es.: config, feed, ..) con l'iniziale maiuscola . Nella classe vengono definiti i tipi dei campi ( marshmallow.fields cfr. https://marshmallow.readthedocs.io/en/stable/ ). ATTENZIONE: in caso fosse necessario l'ingresso di file o comunque oggetti blob dovr\u00e0 essere utilizzato come field la classe hielen2.utils.LocalFile . In questo modo il sistema risolver\u00e0 la chiamata API salvando in locale lo stream dei dati associato a quel determinato field, il quale sar\u00e0 accessibile al template attraverso un path che verr\u00e0 fornito insieme agli altri campi al momento della chiamata del metodo di gestione dell'azione (vedi sotto). deve implementare una classe Source(hielen2.datalink.HielenSource) che esponga tanti metodi quante sono le dichiarazioni di Schema seguendo questa sintassi: il metodo di gestione dell'azione deve chiamarsi come l'azione stessa ( tutto in minuscolo ). Le classi estese sfrutteranno il metodo __init__ della superclasse in modo da avere a disposizione tutto quello di cui necessitano. Questo modello permette di svincolare i template dalla necessit\u00e0 di conoscere a priori le azioni ammmissibili per il sistema. Infatti, facendo introspezione su un template che segua le regole di sintassi sar\u00e0 sempre possibile conoscere le azioni definite ed esternalizzarle al front-end che in base alle definizioni delle classi di Schema delle azioni, sar\u00e0 sempre in grado di instanziare una form adeguata.","title":"15 Dicembre 2020"},{"location":"#v205","text":"","title":"v2.0.5"},{"location":"#9-dicembre-2020","text":"Implementata working POST /actions/{feature}/{form} tramite content-type/multipart dinamico definito dal prototipo: L'api \u00e8 collegata ai moduli reali delle tipologie definiti come templates, con la funzionalit\u00e0 minima di salvare i parametri in ingresso. I moduli sono in fase di sviluppo e man mano che vengono implementati le funzionalit\u00e0 aumenteranno. Implementato Loading dinamico dei moduli di elaborazione definiti come estensioni di hielen2.datalink.HielenSource Implementata working GET /actions/{feature}[/{form}] : Per ogni form richiesta, risponde con tutti i parametri definiti nel relativo prototipo, riempiti con i valori definiti tramite la POST della stessa api. I valori non precedentemente forniti vengono impostati a null Riveduta e corretta GET prototypes/{prototype}/forms[/form] : ATTENZIONE adesso risponde con TUTTI i campi dentro il dizionario \"args\" e comunica i campi obbligatori attraverso l'array \"mandatory\". Questa struttura \u00e8 pi\u00f9 versatile in quanto, una volta definito il set completo degli argomenti, \u00e8 possibile definire un numero arbitrario di sottoinsiemi predicativi non necessariamente distiniti: Oltre al sottoinsieme \"mandatory\" si potrebbe, ad esempio, definire un sottoinsieme di immutabili. Qui sotto una struttura di esempio: { \"data\": { \"args\": { \"epsg\": \"string\", \"master_image\": \"file\", \"negative_pixel_y_size\": \"string\", \"pixel_x_size\": \"string\", \"rotation_about_the_x_axis\": \"string\", \"rotation_about_the_y_axis\": \"string\", \"step_size\": \"string\", \"window_size_change\": \"string\", \"world_file\": \"file\", \"x_coordinate_of_upper_left_pixel_center\": \"string\", \"y_coordinate_of_upper_left_pixel_center\": \"string\" }, \"mandatory\": [ \"master_image\", \"step_size\", \"window_size_change\" ] }, \"meta\": { \"data_type\": \"GET /prototypes/PhotoMonitoring/forms/config\", \"message\": \"\", \"response\": \"ok\" } }","title":"9 Dicembre 2020"},{"location":"#7-dicembre-2020","text":"Rimodellato il feature db per contenere gli argomenti delle actions Riveduto il feature_proto db: Inserito il modulo di riferimento tra le info del prototipo (il modulo contenete la classe estesa di hielen2.datalink.HielenSource ) Definita la superclasse hielen2.datalink.HielenSource con definizione univoca di __init__ con questo footprint: (self,featureobject,environment) . La classe definisce inotre i metodi astratti che vengono utilizzati dal sistema che ogni estensione di questa dovr\u00e0 implementare.","title":"7 Dicembre 2020"},{"location":"#2-dicembre-2020","text":"Struttura definitiva delle features: { \"properties\":\"...\" \"parameters\":\"...\" \"geometry\":\"...\" } dove: properties mantiene tutte le info della feature. Quelle di base: uid , type , classification , location , description e quelle definite per le specifiche azioni definite per la tipologia. In particolare quella di configurzione. parameters mantiene la struttura di accesso alle info e ai dati dei parametri definiti per la feature. geometry fornisce le informazioni geometriche della feature. Rivedute le api /actions , /parameters , /features ( /data da rivedere)","title":"2 Dicembre 2020"},{"location":"#24-novembre-2020","text":"Implementate dummy /actions/{feature}/ e /actions/{feature}/{form}","title":"24 Novembre 2020"},{"location":"#23-novembre-2020","text":"Riorganizzato il db delle features per permettere una gestione pi\u00f9 razionale","title":"23 Novembre 2020"},{"location":"#19-novembre-2020","text":"riorganizzata la struttura per la gestione delle classi estese che necessitano di dynamic loading: nel modulo himada2.ext (cartella) vengono raccoliti per comodit\u00e0 gli oggetti che saranno implementati man mano come estensione di superclassi astratte appositamente definite: per ora hielen2.datalink.Source e hielen2.datalink.DB e hielen2.datalink.DataCache. Oltre alle classi in hielen2.ext, il sitema potr\u00e0 utilizzare moduli esterni che estendano le superclassi elencate. inserito 'timestamp' nello schema json accettato da POST /feature e PUT /feature . risolto bug minore di incoerenza su GET /data/{feature} e /data/{feature}/{parameter} . Quest'ultima continua ad accettare uno tra i nomi dei parametri della feature. Entrambe rispondo intestando le colonne in uscita con lo uid della serie, come GET /data/ .","title":"19 Novembre 2020"},{"location":"#17-novembre-2020","text":"Implementata dummy POST /actions/{feature}/{form} :","title":"17 Novembre 2020"},{"location":"#v204","text":"","title":"v2.0.4"},{"location":"#16-novembre-2020","text":"per coerenza rivisti i parametri di POST /feature : uid:<string> prototype:<string> properties:<json schema Properties> geometry:<json schema GeoJson> analogo discorso per PUT /feature/{uid} : properties:<json schema Properties> geometry:<json schema GeoJson> sistemata la risposta di GET /feature , modificando il livello di \"geometry\" implementata api PUT /features/{uid} . Accetta il paramentro properties con uno schema analogo al parmetro feature di POST /features con queste differenze: nello schema della PUT, uid e prototype NON vengono accettati perch\u00e8 sono campi chiave della feature e non possono essere modificati . lo uid della feature deve essere specificato come url e non come parametro. introduzione dello Schema GeoJson per la validazione modificata POST /features/ per accettare un GeoJson nell'attibuto geometry del Json principale feature","title":"16 Novembre 2020"},{"location":"#13-novembre-2020","text":"rinominazione DELETE /elements -> DELETE /features . eliminazione degli alias GET /features/{context} e /features/{context}/{uid} a causa del conflitto l'entry point DELETE /features . Il passaggio del context sar\u00e0 esclusivmante attraverso il parametro cntxt ( nota : questo nome \u00e8 dovuto alla collisione del nome con il campo 'context' dell'oggetto request). In caso lo possiamo cambiare. introduzione dell'alias /features/{uid} per il recupero delle info della specifica Feature.","title":"13 Novembre 2020"},{"location":"#12-novembre-2020","text":"ovunque nel mondo il parmetro 'uuid' (universal unique id) diventa 'uid'. rinominazione POST /elements -> POST /features . rinominazione GET /elements -> GET /parameters e modifica uscita in questo schema: { < feature1_UID > :[ { \"series\" : < feature1_param1_series_UID > , \"param\" : < feature1_param1_name > , \"um\" : < feature1_param1_measurement_unit > } , ... { \"series\" : < feature1_paramN_series_UID , \"param\" : < feature1_paramN_name > , \"um\" : < feature1_paramN_meaurement_unit > } ], ... < featureX_UID > :[ { \"series\" : < featureX_param1_series_UID > , \"param\" : < featureX_param1_name > , \"um\" : < featureX_param1_measurement_unit > } , ... { \"series\" : < featureX_paramM_series_UID , \"param\" : < featureX_paramM_name > , \"um\" : < featureX_paramM_meaurement_unit > } ] } introduzione api /features con lo schema usato da Daniele e SimoneD: GET /features GET /features/{context}/ GET /features/{context}/{feature} uscita : nota 1: NON viene introdotto \"context\" , come invece preventivato nota 2: \"cod\" diventa \"label\" . nota 3: \"date\" diventa \"timestamp\" nota 3: dalle properties vengono elminate \"z\" e \"mslm\" . nota 4: \"state\" viene mantenuto ma per ora \u00e8 inutilizzato { \"features\" : [ { \"type\" : \"Feature\" , \"properties\" : { \"uid\" : ..., \"label\" : ..., \"context\" :..., \"date\" : ..., \"type\" : ..., \"style\" : ..., \"state\" : ... } , \"geometry\" : < GeoJson Validable > } , ... { \"type\" : \"Feature\" , \"properties\" : { \"uid\" : ..., \"label\" : ..., \"context\" : ..., \"date\" : ..., \"type\" : ..., \"style\" : ..., \"state\" : ... } , \"geometry\" : < GeoJson Validable > } ] }","title":"12 Novembre 2020"},{"location":"#v203","text":"","title":"v2.0.3"},{"location":"#11-novembre-2020","text":"Modificata api POST /elements : la variabile element \u00e8 descritta dalla Classe hielen2.api.data.ElementSchema e validata. In paricolare \u00e8 stato introdotto l'attibuto context Modifcata api GET /data : la variabile datamap \u00e8 descritta dalla Classe hielen2.api.data.DataMapSchema e validata.","title":"11 Novembre 2020"},{"location":"#9-novembre-2020","text":"Introdotta la classe hielen2.utils.JsonValidable, per la validazione e documentazione automatica dei parametri delle api (JSON Schema descrition) corretti bug minori in hielen2.datalink","title":"9 Novembre 2020"},{"location":"#6-novembre-2020","text":"L'interfaccia DB \u00e8 ora thread safe!!! (almeno per il dummy json db)","title":"6 Novembre 2020"},{"location":"#v202","text":"","title":"v2.0.2"},{"location":"#4-novembre-2020","text":"Implementata la documentazione automatica delle api Implementate le api POST ../elements e DELETE ../elements L'uscita per tutte le api element (e per tutte le api con risposta json in generale), seguir\u00e0 questo schema: { \"meta\": { \"data_type\": \"DELETE /elements/ciao\", \"response\": \"ok\" \"message\": \"\", }, \"data\":{ ... } } L'api /series diventa /data e cambia il suo comportamento: la variabile di tipo json datamap si aspetta il campo series invece di parameters . In questo campo devono essere inseriti i codici delle serie e non pi\u00f9 il costrutto \"codice_elemento:parametro_elemento\". I codici delle serie si possono recuperarare dall'api /elements (vedi Nota successiva) L'api /elements cambia la sua risposta e per ogni parametro nella lista parameters degli elementi viene agiunto il codice della serie di riferimento che pu\u00f2 essere fornito senza modifiche a /data : { \"series\":<seriescode>, \"name\":<seriesname>, \"um\":<seriesunit> } GET /series GET /series/{el} GET /series/{el}/{param} GET /prototypes GET /prototypes/{type} GET /prototypes/{type}/forms GET /prototypes/{type}/forms/{form} POST /elements GET /elements GET /elements/{el} DELETE /elements/{el}","title":"4 Novembre 2020"},{"location":"CHANGELOG/","text":"CHANGELOG v2.0.6 27 Gennaio 2021 Inserita la validazione dei campi delle action 25 Gennaio 2021 Inserito il campo \"hint\" nella risposta di GET /actionschemata[/{feature}[/{action}]] Implementata astrazione di DELETE /actions/{feature}/{action}/ . Prima della rimozione della specifica azione dal db, vengono chimati i metodi preposti alla gestione della rimozione delle informazioni in cache, che si presuppone siano implemetati dai relativi moduli. I moduli possono implementare questi metodi solo se necessario. Sintassi: Invocando il metodo deleteActionValues(self,action,timestamp) della superclasse Hielen2.Source, essa tenter\u00e0 di utilizzare il metodo della sottoclasse il cui nome \u00e8 cotruito dal'unione della parola \"delete\" + il nome dell'azione con la prima lettera maiuscola: es: \"deleteConfig\". la superclasse passer\u00e0 sempre un timestamp per individuare l'azione specifica Implementato il metodo \"deleteConfig\" della classe Hielen2.ext.source_PhotoMonitoring Corretto bug minore di gestione delle azioni in caso esse producano errori non preventivati. 22 Gennaio 2021 Ristrutturata la pagina di TODO , inserita categorizzazione e valutazione delle tempistica delle attivit\u00e0 20 Gennaio 2021 modulo hielen2.ext.source_PhotoMonitoring : rimodellato sulla base del modulo hielen2.source . definite le classi schema per le azioni: ## action 'config' (Completamente Funzionante) class ConfigSchema ( ActionSchema ): master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Str ( required = False , default = \"8\" ) window_size_change = fields . Str ( required = False , default = \"0\" ) geo_reference_file = LocalFile ( required = False , default = None ) crs = fields . Str ( required = False , default = None ) ## action 'feed' class FeedSchema ( ActionSchema ): reference_time = fields . Str ( required = False , allow_none = False ) NS_displacement = LocalFile ( required = False , allow_none = False ) EW_displacement = LocalFile ( required = False , allow_none = False ) Coer = LocalFile ( required = False , allow_none = False ) 18 Gennaio 2021 Revisione concettuale delle API e modifiche: GET /parametes : lo schema di ritorno \u00e8 il seguente (semplicemente \"param\" al posto di \"name\" ): { ..., \"data\": { \"ARCCE01\": [ { \"series\": \"ARCCE01_Rotazione_X\", \"param\": \"Rotazione X\", \"unit\": \"mm/m\"}, ..... } ] } actionSchemata \u00e8 l'api che fornir\u00e0 gli schemi per le azioni e va a sostituire quella che era \"prototypes\". Questa esiste ancora e mantiene il legame tra prototipo e modulo ma pi\u00f9 che altro le informazioni che stanno nella relativa tabella mi servono per il back-end GET ../actionschemata[/{prototypes}[/{actions}]] action come prima, \u00e8 l'api che gestisce le azioni: La versione POST nella sostanza non \u00e8 cambiata a parte il fatto che un'azione dichiarer\u00e0 sempre un timestamp per default. Ma questa cosa al front-end non interessa dal momento che le info le recupera da actionSchemanta. E' invece importante nella scrittura dei plugin perch\u00e9 in questo modo le azioni possono essere gestite temporalmente. La versione GET , invece cambia sostanzialmente: non fornir\u00e0 pi\u00f9 i default per la post MA potr\u00e0 fornire una serie temporale di azioni associate a dei valori di elaborazione che danno informazioni all'utente. in questo formato: GET ../actions[/{feature}[/{action}]] ritorna: [ { \"timestamp\":....,\"value\":.... }, { \"timestamp\":...., \"value\":.... }, .... ] esempio: GET .. / actions / featurecode / config { \"meta\" : { \"response\" : \"ok\" , \"message\" : \"\" , \"data_type\" : \"GET /actions/ciaociaociao4/config\" } , \"data\" : [ [ { \"timestamp\" : \"2020-12-30 01:00:05\" , \"value\" : { \"master_image\" : \"TIFF image data, little-endian, direntries=14, height=1842, bps=16, compression=none, PhotometricIntepretation=BlackIsZero, width=3545\" , \"step_size\" : \"35\" , \"window_size_change\" : \"10\" , \"transform\" : [ 15 . 0 , 0 . 0 , 464947 . 5 , 0 . 0 , - 15 . 0 , 7977067 . 5 ], \"cache\" : \"20201230010005\" , \"crs\" : null } } , { \"timestamp\" : \"2020-12-30 01:00:07\" , \"value\" : { \"master_image\" : \"TIFF image data, little-endian, direntries=16, height=1842, bps=16, compression=none, PhotometricIntepretation=BlackIsZero, width=3545\" , \"timestamp\" : \"2020-12-30 01:00:07\" , \"step_size\" : \"35\" , \"window_size_change\" : \"10\" , \"transform\" : [ 15 . 0 , 0 . 0 , 464947 . 5 , 0 . 0 , - 15 . 0 , 7977067 . 5 ], \"cache\" : \"20201230010007\" , \"crs\" : \"EPSG:32622\" } } ] ] } 15 Gennaio 2021 modulo hielene2.source : Implementato il metodo sourceFactory per la generazione degli ogetti HeielenSource in base ai prototipi che sfrutta il cariacmanto dinamico dei moduli specifici (metodo loadModule ) Implementati i metodi e le classi per la gestione agnostica delle azioni ed il recupero degli schemi: getActionSchema , moduleAction , HielenSource.execAction , HielenSource.getActionValues Implementata la gestione dell'ambiente di cache dedicato alle singole istanze di HielenSource: HielenSource.makeCachePath , HielenSource.getRelativePath Definita la classe primitiva per i modelli di schema per le azioni che impone la definizione della marcatura temporale: class ActionSchema(Schema): timestamp = fields.Str(required=True, allow_none=False) 13 Gennaio 2021 rimodellato il db: dalla tabella \"features\" sono state eliminate le colonne \"a priori\" delle azioni. Queste ultime sono state inserite in una nuova tabella \"actions\" con chiave multipla (\"feature\",\"action\",\"timestamp\"). Rivista l'interfaccia db per permettere l'interrogazione su chiave multipla 10 Gennaio 2021 Progettazione della gestione temporale delle azioni e separazione del concetto di form da quello di risultato della azione: ogni azione ha uno schema di input e dei risultati in output con uno schema non necessariamente coincidente. Quello che viene fornito alle form sono i dati necessari ad intraprendere un'azione. I risultati dell'azione devono essere registrati con una marcatura temporale. In questo modo ogni azione \u00e8 univocamente determinata e gestibile con un modello del tipo (\"feature\",\"action\",\"timestamp\"), con una cardinalit\u00e0 1-a-molti tra features e azioni Portata a termine la migrazione della gestione delle azioni che vengono ora completamente affidate ai singoli moduli. L'iterfaccia di alto livello \u00e8 ora in grado di gestire agonsticamente le chiamate ad azioni arbitrarie purch\u00e8 ben definite all'interno dei moduli. In questo modo cade il vincolo di definizione do azione \"a priori\" 30 Dicembre 2020 sviluppo (non completo) di config hielen2.ext.PhotoMonitoring: Implementato il metodo di recupero e settaggio delle informazioni geometriche/geografiche dell'immagine in ingresso Aggancio del codice originale per la gesgione del netcdf (in debug) 22 Dicembre 20202 Delineata la gestione di mappa delle immagini prodotte: Ogni immagine prodotta sar\u00e0 sempre associata al suo crs e la matrice di trasformazione affine, anche nele caso in cui queste informazioni non dovessero essere passate in configurazione. In questo caso si assume un piano cartesiano con udm in m e una matrice identit\u00e0 per le trasformazioni affini. Sar\u00e0 dunque sempre possibile gestire le immagini come mappe (slippy maps) e sfruttare la tassellazione, il cacheing dei tasselli. 20 Dicembre 2020 Modificata l'api POST /actions/{feature}/{form} in modo da interrogare la Source (per ora solo PhotoMonitoring) sulla definizione delle azioni: Implementate le classi di Schema per config e feed per il modulo hielen2.ext.PhotoMonitoring . ATTENZIONE per config : introdotto il campo \"timestamp\", eliminati i campi espliciti relativi al word_file ( word_file mantenuto), modificato il campo epsg in csr . 15 Dicembre 2020 Delineato il modello di scrittura dei Source plugin secondo un template univoco. Ogni plugin potr\u00e0 essere un modulo python definito come segue: deve definire tante classi marshmallow.Schema quante sono le azioni che vengono prese in carico dal Template. Marsmallow \u00e8 un serializzatore di oggetti python. Lo schema definito servir\u00e0 per definire i campi in ingresso per ogni azione e fare i check dei valori in ingresso. Il nome delle classi Schema deve seguire questa sintassi: class {Action}Schema(marshmallow.Schema) dove {Action} \u00e8 il nome dell'azione (es.: config, feed, ..) con l'iniziale maiuscola . Nella classe vengono definiti i tipi dei campi ( marshmallow.fields cfr. https://marshmallow.readthedocs.io/en/stable/ ). ATTENZIONE: in caso fosse necessario l'ingresso di file o comunque oggetti blob dovr\u00e0 essere utilizzato come field la classe hielen2.utils.LocalFile . In questo modo il sistema risolver\u00e0 la chiamata API salvando in locale lo stream dei dati associato a quel determinato field, il quale sar\u00e0 accessibile al template attraverso un path che verr\u00e0 fornito insieme agli altri campi al momento della chiamata del metodo di gestione dell'azione (vedi sotto). deve implementare una classe Source(hielen2.datalink.HielenSource) che esponga tanti metodi quante sono le dichiarazioni di Schema seguendo questa sintassi: il metodo di gestione dell'azione deve chiamarsi come l'azione stessa ( tutto in minuscolo ). Le classi estese sfrutteranno il metodo __init__ della superclasse in modo da avere a disposizione tutto quello di cui necessitano. Questo modello permette di svincolare i template dalla necessit\u00e0 di conoscere a priori le azioni ammmissibili per il sistema. Infatti, facendo introspezione su un template che segua le regole di sintassi sar\u00e0 sempre possibile conoscere le azioni definite ed esternalizzarle al front-end che in base alle definizioni delle classi di Schema delle azioni, sar\u00e0 sempre in grado di instanziare una form adeguata. v2.0.5 9 Dicembre 2020 Implementata working POST /actions/{feature}/{form} tramite content-type/multipart dinamico definito dal prototipo: L'api \u00e8 collegata ai moduli reali delle tipologie definiti come templates, con la funzionalit\u00e0 minima di salvare i parametri in ingresso. I moduli sono in fase di sviluppo e man mano che vengono implementati le funzionalit\u00e0 aumenteranno. Implementato Loading dinamico dei moduli di elaborazione definiti come estensioni di hielen2.datalink.HielenSource Implementata working GET /actions/{feature}[/{form}] : Per ogni form richiesta, risponde con tutti i parametri definiti nel relativo prototipo, riempiti con i valori definiti tramite la POST della stessa api. I valori non precedentemente forniti vengono impostati a null Riveduta e corretta GET prototypes/{prototype}/forms[/form] : ATTENZIONE adesso risponde con TUTTI i campi dentro il dizionario \"args\" e comunica i campi obbligatori attraverso l'array \"mandatory\". Questa struttura \u00e8 pi\u00f9 versatile in quanto, una volta definito il set completo degli argomenti, \u00e8 possibile definire un numero arbitrario di sottoinsiemi predicativi non necessariamente distiniti: Oltre al sottoinsieme \"mandatory\" si potrebbe, ad esempio, definire un sottoinsieme di immutabili. Qui sotto una struttura di esempio: { \"data\": { \"args\": { \"epsg\": \"string\", \"master_image\": \"file\", \"negative_pixel_y_size\": \"string\", \"pixel_x_size\": \"string\", \"rotation_about_the_x_axis\": \"string\", \"rotation_about_the_y_axis\": \"string\", \"step_size\": \"string\", \"window_size_change\": \"string\", \"world_file\": \"file\", \"x_coordinate_of_upper_left_pixel_center\": \"string\", \"y_coordinate_of_upper_left_pixel_center\": \"string\" }, \"mandatory\": [ \"master_image\", \"step_size\", \"window_size_change\" ] }, \"meta\": { \"data_type\": \"GET /prototypes/PhotoMonitoring/forms/config\", \"message\": \"\", \"response\": \"ok\" } } 7 Dicembre 2020 Rimodellato il feature db per contenere gli argomenti delle actions Riveduto il feature_proto db: Inserito il modulo di riferimento tra le info del prototipo (il modulo contenete la classe estesa di hielen2.datalink.HielenSource ) Definita la superclasse hielen2.datalink.HielenSource con definizione univoca di __init__ con questo footprint: (self,featureobject,environment) . La classe definisce inotre i metodi astratti che vengono utilizzati dal sistema che ogni estensione di questa dovr\u00e0 implementare. 2 Dicembre 2020 Struttura definitiva delle features: { \"properties\":\"...\" \"parameters\":\"...\" \"geometry\":\"...\" } dove: properties mantiene tutte le info della feature. Quelle di base: uid , type , classification , location , description e quelle definite per le specifiche azioni definite per la tipologia. In particolare quella di configurzione. parameters mantiene la struttura di accesso alle info e ai dati dei parametri definiti per la feature. geometry fornisce le informazioni geometriche della feature. Rivedute le api /actions , /parameters , /features ( /data da rivedere) 24 Novembre 2020 Implementate dummy /actions/{feature}/ e /actions/{feature}/{form} 23 Novembre 2020 Riorganizzato il db delle features per permettere una gestione pi\u00f9 razionale 19 Novembre 2020 riorganizzata la struttura per la gestione delle classi estese che necessitano di dynamic loading: nel modulo himada2.ext (cartella) vengono raccoliti per comodit\u00e0 gli oggetti che saranno implementati man mano come estensione di superclassi astratte appositamente definite: per ora hielen2.datalink.Source e hielen2.datalink.DB e hielen2.datalink.DataCache. Oltre alle classi in hielen2.ext, il sitema potr\u00e0 utilizzare moduli esterni che estendano le superclassi elencate. inserito 'timestamp' nello schema json accettato da POST /feature e PUT /feature . risolto bug minore di incoerenza su GET /data/{feature} e /data/{feature}/{parameter} . Quest'ultima continua ad accettare uno tra i nomi dei parametri della feature. Entrambe rispondo intestando le colonne in uscita con lo uid della serie, come GET /data/ . 17 Novembre 2020 Implementata dummy POST /actions/{feature}/{form} : v2.0.4 16 Novembre 2020 per coerenza rivisti i parametri di POST /feature : uid:<string> prototype:<string> properties:<json schema Properties> geometry:<json schema GeoJson> analogo discorso per PUT /feature/{uid} : properties:<json schema Properties> geometry:<json schema GeoJson> sistemata la risposta di GET /feature , modificando il livello di \"geometry\" implementata api PUT /features/{uid} . Accetta il paramentro properties con uno schema analogo al parmetro feature di POST /features con queste differenze: nello schema della PUT, uid e prototype NON vengono accettati perch\u00e8 sono campi chiave della feature e non possono essere modificati . lo uid della feature deve essere specificato come url e non come parametro. introduzione dello Schema GeoJson per la validazione modificata POST /features/ per accettare un GeoJson nell'attibuto geometry del Json principale feature 13 Novembre 2020 rinominazione DELETE /elements -> DELETE /features . eliminazione degli alias GET /features/{context} e /features/{context}/{uid} a causa del conflitto l'entry point DELETE /features . Il passaggio del context sar\u00e0 esclusivmante attraverso il parametro cntxt ( nota : questo nome \u00e8 dovuto alla collisione del nome con il campo 'context' dell'oggetto request). In caso lo possiamo cambiare. introduzione dell'alias /features/{uid} per il recupero delle info della specifica Feature. 12 Novembre 2020 ovunque nel mondo il parmetro 'uuid' (universal unique id) diventa 'uid'. rinominazione POST /elements -> POST /features . rinominazione GET /elements -> GET /parameters e modifica uscita in questo schema: { < feature1_UID > :[ { \"series\" : < feature1_param1_series_UID > , \"param\" : < feature1_param1_name > , \"um\" : < feature1_param1_measurement_unit > } , ... { \"series\" : < feature1_paramN_series_UID , \"param\" : < feature1_paramN_name > , \"um\" : < feature1_paramN_meaurement_unit > } ], ... < featureX_UID > :[ { \"series\" : < featureX_param1_series_UID > , \"param\" : < featureX_param1_name > , \"um\" : < featureX_param1_measurement_unit > } , ... { \"series\" : < featureX_paramM_series_UID , \"param\" : < featureX_paramM_name > , \"um\" : < featureX_paramM_meaurement_unit > } ] } introduzione api /features con lo schema usato da Daniele e SimoneD: GET /features GET /features/{context}/ GET /features/{context}/{feature} uscita : nota 1: NON viene introdotto \"context\" , come invece preventivato nota 2: \"cod\" diventa \"label\" . nota 3: \"date\" diventa \"timestamp\" nota 3: dalle properties vengono elminate \"z\" e \"mslm\" . nota 4: \"state\" viene mantenuto ma per ora \u00e8 inutilizzato { \"features\" : [ { \"type\" : \"Feature\" , \"properties\" : { \"uid\" : ..., \"label\" : ..., \"context\" :..., \"date\" : ..., \"type\" : ..., \"style\" : ..., \"state\" : ... } , \"geometry\" : < GeoJson Validable > } , ... { \"type\" : \"Feature\" , \"properties\" : { \"uid\" : ..., \"label\" : ..., \"context\" : ..., \"date\" : ..., \"type\" : ..., \"style\" : ..., \"state\" : ... } , \"geometry\" : < GeoJson Validable > } ] } v2.0.3 11 Novembre 2020 Modificata api POST /elements : la variabile element \u00e8 descritta dalla Classe hielen2.api.data.ElementSchema e validata. In paricolare \u00e8 stato introdotto l'attibuto context Modifcata api GET /data : la variabile datamap \u00e8 descritta dalla Classe hielen2.api.data.DataMapSchema e validata. 9 Novembre 2020 Introdotta la classe hielen2.utils.JsonValidable, per la validazione e documentazione automatica dei parametri delle api (JSON Schema descrition) corretti bug minori in hielen2.datalink 6 Novembre 2020 L'interfaccia DB \u00e8 ora thread safe!!! (almeno per il dummy json db) v2.0.2 4 Novembre 2020 Implementata la documentazione automatica delle api Implementate le api POST ../elements e DELETE ../elements L'uscita per tutte le api element (e per tutte le api con risposta json in generale), seguir\u00e0 questo schema: { \"meta\": { \"data_type\": \"DELETE /elements/ciao\", \"response\": \"ok\" \"message\": \"\", }, \"data\":{ ... } } L'api /series diventa /data e cambia il suo comportamento: la variabile di tipo json datamap si aspetta il campo series invece di parameters . In questo campo devono essere inseriti i codici delle serie e non pi\u00f9 il costrutto \"codice_elemento:parametro_elemento\". I codici delle serie si possono recuperarare dall'api /elements (vedi Nota successiva) L'api /elements cambia la sua risposta e per ogni parametro nella lista parameters degli elementi viene agiunto il codice della serie di riferimento che pu\u00f2 essere fornito senza modifiche a /data : { \"series\":<seriescode>, \"name\":<seriesname>, \"um\":<seriesunit> } GET /series GET /series/{el} GET /series/{el}/{param} GET /prototypes GET /prototypes/{type} GET /prototypes/{type}/forms GET /prototypes/{type}/forms/{form} POST /elements GET /elements GET /elements/{el} DELETE /elements/{el}","title":"Home"},{"location":"CHANGELOG/#changelog","text":"","title":"CHANGELOG"},{"location":"CHANGELOG/#v206","text":"","title":"v2.0.6"},{"location":"CHANGELOG/#27-gennaio-2021","text":"Inserita la validazione dei campi delle action","title":"27 Gennaio 2021"},{"location":"CHANGELOG/#25-gennaio-2021","text":"Inserito il campo \"hint\" nella risposta di GET /actionschemata[/{feature}[/{action}]] Implementata astrazione di DELETE /actions/{feature}/{action}/ . Prima della rimozione della specifica azione dal db, vengono chimati i metodi preposti alla gestione della rimozione delle informazioni in cache, che si presuppone siano implemetati dai relativi moduli. I moduli possono implementare questi metodi solo se necessario. Sintassi: Invocando il metodo deleteActionValues(self,action,timestamp) della superclasse Hielen2.Source, essa tenter\u00e0 di utilizzare il metodo della sottoclasse il cui nome \u00e8 cotruito dal'unione della parola \"delete\" + il nome dell'azione con la prima lettera maiuscola: es: \"deleteConfig\". la superclasse passer\u00e0 sempre un timestamp per individuare l'azione specifica Implementato il metodo \"deleteConfig\" della classe Hielen2.ext.source_PhotoMonitoring Corretto bug minore di gestione delle azioni in caso esse producano errori non preventivati.","title":"25 Gennaio 2021"},{"location":"CHANGELOG/#22-gennaio-2021","text":"Ristrutturata la pagina di TODO , inserita categorizzazione e valutazione delle tempistica delle attivit\u00e0","title":"22 Gennaio 2021"},{"location":"CHANGELOG/#20-gennaio-2021","text":"modulo hielen2.ext.source_PhotoMonitoring : rimodellato sulla base del modulo hielen2.source . definite le classi schema per le azioni: ## action 'config' (Completamente Funzionante) class ConfigSchema ( ActionSchema ): master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Str ( required = False , default = \"8\" ) window_size_change = fields . Str ( required = False , default = \"0\" ) geo_reference_file = LocalFile ( required = False , default = None ) crs = fields . Str ( required = False , default = None ) ## action 'feed' class FeedSchema ( ActionSchema ): reference_time = fields . Str ( required = False , allow_none = False ) NS_displacement = LocalFile ( required = False , allow_none = False ) EW_displacement = LocalFile ( required = False , allow_none = False ) Coer = LocalFile ( required = False , allow_none = False )","title":"20 Gennaio 2021"},{"location":"CHANGELOG/#18-gennaio-2021","text":"Revisione concettuale delle API e modifiche: GET /parametes : lo schema di ritorno \u00e8 il seguente (semplicemente \"param\" al posto di \"name\" ): { ..., \"data\": { \"ARCCE01\": [ { \"series\": \"ARCCE01_Rotazione_X\", \"param\": \"Rotazione X\", \"unit\": \"mm/m\"}, ..... } ] } actionSchemata \u00e8 l'api che fornir\u00e0 gli schemi per le azioni e va a sostituire quella che era \"prototypes\". Questa esiste ancora e mantiene il legame tra prototipo e modulo ma pi\u00f9 che altro le informazioni che stanno nella relativa tabella mi servono per il back-end GET ../actionschemata[/{prototypes}[/{actions}]] action come prima, \u00e8 l'api che gestisce le azioni: La versione POST nella sostanza non \u00e8 cambiata a parte il fatto che un'azione dichiarer\u00e0 sempre un timestamp per default. Ma questa cosa al front-end non interessa dal momento che le info le recupera da actionSchemanta. E' invece importante nella scrittura dei plugin perch\u00e9 in questo modo le azioni possono essere gestite temporalmente. La versione GET , invece cambia sostanzialmente: non fornir\u00e0 pi\u00f9 i default per la post MA potr\u00e0 fornire una serie temporale di azioni associate a dei valori di elaborazione che danno informazioni all'utente. in questo formato: GET ../actions[/{feature}[/{action}]] ritorna: [ { \"timestamp\":....,\"value\":.... }, { \"timestamp\":...., \"value\":.... }, .... ] esempio: GET .. / actions / featurecode / config { \"meta\" : { \"response\" : \"ok\" , \"message\" : \"\" , \"data_type\" : \"GET /actions/ciaociaociao4/config\" } , \"data\" : [ [ { \"timestamp\" : \"2020-12-30 01:00:05\" , \"value\" : { \"master_image\" : \"TIFF image data, little-endian, direntries=14, height=1842, bps=16, compression=none, PhotometricIntepretation=BlackIsZero, width=3545\" , \"step_size\" : \"35\" , \"window_size_change\" : \"10\" , \"transform\" : [ 15 . 0 , 0 . 0 , 464947 . 5 , 0 . 0 , - 15 . 0 , 7977067 . 5 ], \"cache\" : \"20201230010005\" , \"crs\" : null } } , { \"timestamp\" : \"2020-12-30 01:00:07\" , \"value\" : { \"master_image\" : \"TIFF image data, little-endian, direntries=16, height=1842, bps=16, compression=none, PhotometricIntepretation=BlackIsZero, width=3545\" , \"timestamp\" : \"2020-12-30 01:00:07\" , \"step_size\" : \"35\" , \"window_size_change\" : \"10\" , \"transform\" : [ 15 . 0 , 0 . 0 , 464947 . 5 , 0 . 0 , - 15 . 0 , 7977067 . 5 ], \"cache\" : \"20201230010007\" , \"crs\" : \"EPSG:32622\" } } ] ] }","title":"18 Gennaio 2021"},{"location":"CHANGELOG/#15-gennaio-2021","text":"modulo hielene2.source : Implementato il metodo sourceFactory per la generazione degli ogetti HeielenSource in base ai prototipi che sfrutta il cariacmanto dinamico dei moduli specifici (metodo loadModule ) Implementati i metodi e le classi per la gestione agnostica delle azioni ed il recupero degli schemi: getActionSchema , moduleAction , HielenSource.execAction , HielenSource.getActionValues Implementata la gestione dell'ambiente di cache dedicato alle singole istanze di HielenSource: HielenSource.makeCachePath , HielenSource.getRelativePath Definita la classe primitiva per i modelli di schema per le azioni che impone la definizione della marcatura temporale: class ActionSchema(Schema): timestamp = fields.Str(required=True, allow_none=False)","title":"15 Gennaio 2021"},{"location":"CHANGELOG/#13-gennaio-2021","text":"rimodellato il db: dalla tabella \"features\" sono state eliminate le colonne \"a priori\" delle azioni. Queste ultime sono state inserite in una nuova tabella \"actions\" con chiave multipla (\"feature\",\"action\",\"timestamp\"). Rivista l'interfaccia db per permettere l'interrogazione su chiave multipla","title":"13 Gennaio 2021"},{"location":"CHANGELOG/#10-gennaio-2021","text":"Progettazione della gestione temporale delle azioni e separazione del concetto di form da quello di risultato della azione: ogni azione ha uno schema di input e dei risultati in output con uno schema non necessariamente coincidente. Quello che viene fornito alle form sono i dati necessari ad intraprendere un'azione. I risultati dell'azione devono essere registrati con una marcatura temporale. In questo modo ogni azione \u00e8 univocamente determinata e gestibile con un modello del tipo (\"feature\",\"action\",\"timestamp\"), con una cardinalit\u00e0 1-a-molti tra features e azioni Portata a termine la migrazione della gestione delle azioni che vengono ora completamente affidate ai singoli moduli. L'iterfaccia di alto livello \u00e8 ora in grado di gestire agonsticamente le chiamate ad azioni arbitrarie purch\u00e8 ben definite all'interno dei moduli. In questo modo cade il vincolo di definizione do azione \"a priori\"","title":"10 Gennaio 2021"},{"location":"CHANGELOG/#30-dicembre-2020","text":"sviluppo (non completo) di config hielen2.ext.PhotoMonitoring: Implementato il metodo di recupero e settaggio delle informazioni geometriche/geografiche dell'immagine in ingresso Aggancio del codice originale per la gesgione del netcdf (in debug)","title":"30 Dicembre 2020"},{"location":"CHANGELOG/#22-dicembre-20202","text":"Delineata la gestione di mappa delle immagini prodotte: Ogni immagine prodotta sar\u00e0 sempre associata al suo crs e la matrice di trasformazione affine, anche nele caso in cui queste informazioni non dovessero essere passate in configurazione. In questo caso si assume un piano cartesiano con udm in m e una matrice identit\u00e0 per le trasformazioni affini. Sar\u00e0 dunque sempre possibile gestire le immagini come mappe (slippy maps) e sfruttare la tassellazione, il cacheing dei tasselli.","title":"22 Dicembre 20202"},{"location":"CHANGELOG/#20-dicembre-2020","text":"Modificata l'api POST /actions/{feature}/{form} in modo da interrogare la Source (per ora solo PhotoMonitoring) sulla definizione delle azioni: Implementate le classi di Schema per config e feed per il modulo hielen2.ext.PhotoMonitoring . ATTENZIONE per config : introdotto il campo \"timestamp\", eliminati i campi espliciti relativi al word_file ( word_file mantenuto), modificato il campo epsg in csr .","title":"20 Dicembre 2020"},{"location":"CHANGELOG/#15-dicembre-2020","text":"Delineato il modello di scrittura dei Source plugin secondo un template univoco. Ogni plugin potr\u00e0 essere un modulo python definito come segue: deve definire tante classi marshmallow.Schema quante sono le azioni che vengono prese in carico dal Template. Marsmallow \u00e8 un serializzatore di oggetti python. Lo schema definito servir\u00e0 per definire i campi in ingresso per ogni azione e fare i check dei valori in ingresso. Il nome delle classi Schema deve seguire questa sintassi: class {Action}Schema(marshmallow.Schema) dove {Action} \u00e8 il nome dell'azione (es.: config, feed, ..) con l'iniziale maiuscola . Nella classe vengono definiti i tipi dei campi ( marshmallow.fields cfr. https://marshmallow.readthedocs.io/en/stable/ ). ATTENZIONE: in caso fosse necessario l'ingresso di file o comunque oggetti blob dovr\u00e0 essere utilizzato come field la classe hielen2.utils.LocalFile . In questo modo il sistema risolver\u00e0 la chiamata API salvando in locale lo stream dei dati associato a quel determinato field, il quale sar\u00e0 accessibile al template attraverso un path che verr\u00e0 fornito insieme agli altri campi al momento della chiamata del metodo di gestione dell'azione (vedi sotto). deve implementare una classe Source(hielen2.datalink.HielenSource) che esponga tanti metodi quante sono le dichiarazioni di Schema seguendo questa sintassi: il metodo di gestione dell'azione deve chiamarsi come l'azione stessa ( tutto in minuscolo ). Le classi estese sfrutteranno il metodo __init__ della superclasse in modo da avere a disposizione tutto quello di cui necessitano. Questo modello permette di svincolare i template dalla necessit\u00e0 di conoscere a priori le azioni ammmissibili per il sistema. Infatti, facendo introspezione su un template che segua le regole di sintassi sar\u00e0 sempre possibile conoscere le azioni definite ed esternalizzarle al front-end che in base alle definizioni delle classi di Schema delle azioni, sar\u00e0 sempre in grado di instanziare una form adeguata.","title":"15 Dicembre 2020"},{"location":"CHANGELOG/#v205","text":"","title":"v2.0.5"},{"location":"CHANGELOG/#9-dicembre-2020","text":"Implementata working POST /actions/{feature}/{form} tramite content-type/multipart dinamico definito dal prototipo: L'api \u00e8 collegata ai moduli reali delle tipologie definiti come templates, con la funzionalit\u00e0 minima di salvare i parametri in ingresso. I moduli sono in fase di sviluppo e man mano che vengono implementati le funzionalit\u00e0 aumenteranno. Implementato Loading dinamico dei moduli di elaborazione definiti come estensioni di hielen2.datalink.HielenSource Implementata working GET /actions/{feature}[/{form}] : Per ogni form richiesta, risponde con tutti i parametri definiti nel relativo prototipo, riempiti con i valori definiti tramite la POST della stessa api. I valori non precedentemente forniti vengono impostati a null Riveduta e corretta GET prototypes/{prototype}/forms[/form] : ATTENZIONE adesso risponde con TUTTI i campi dentro il dizionario \"args\" e comunica i campi obbligatori attraverso l'array \"mandatory\". Questa struttura \u00e8 pi\u00f9 versatile in quanto, una volta definito il set completo degli argomenti, \u00e8 possibile definire un numero arbitrario di sottoinsiemi predicativi non necessariamente distiniti: Oltre al sottoinsieme \"mandatory\" si potrebbe, ad esempio, definire un sottoinsieme di immutabili. Qui sotto una struttura di esempio: { \"data\": { \"args\": { \"epsg\": \"string\", \"master_image\": \"file\", \"negative_pixel_y_size\": \"string\", \"pixel_x_size\": \"string\", \"rotation_about_the_x_axis\": \"string\", \"rotation_about_the_y_axis\": \"string\", \"step_size\": \"string\", \"window_size_change\": \"string\", \"world_file\": \"file\", \"x_coordinate_of_upper_left_pixel_center\": \"string\", \"y_coordinate_of_upper_left_pixel_center\": \"string\" }, \"mandatory\": [ \"master_image\", \"step_size\", \"window_size_change\" ] }, \"meta\": { \"data_type\": \"GET /prototypes/PhotoMonitoring/forms/config\", \"message\": \"\", \"response\": \"ok\" } }","title":"9 Dicembre 2020"},{"location":"CHANGELOG/#7-dicembre-2020","text":"Rimodellato il feature db per contenere gli argomenti delle actions Riveduto il feature_proto db: Inserito il modulo di riferimento tra le info del prototipo (il modulo contenete la classe estesa di hielen2.datalink.HielenSource ) Definita la superclasse hielen2.datalink.HielenSource con definizione univoca di __init__ con questo footprint: (self,featureobject,environment) . La classe definisce inotre i metodi astratti che vengono utilizzati dal sistema che ogni estensione di questa dovr\u00e0 implementare.","title":"7 Dicembre 2020"},{"location":"CHANGELOG/#2-dicembre-2020","text":"Struttura definitiva delle features: { \"properties\":\"...\" \"parameters\":\"...\" \"geometry\":\"...\" } dove: properties mantiene tutte le info della feature. Quelle di base: uid , type , classification , location , description e quelle definite per le specifiche azioni definite per la tipologia. In particolare quella di configurzione. parameters mantiene la struttura di accesso alle info e ai dati dei parametri definiti per la feature. geometry fornisce le informazioni geometriche della feature. Rivedute le api /actions , /parameters , /features ( /data da rivedere)","title":"2 Dicembre 2020"},{"location":"CHANGELOG/#24-novembre-2020","text":"Implementate dummy /actions/{feature}/ e /actions/{feature}/{form}","title":"24 Novembre 2020"},{"location":"CHANGELOG/#23-novembre-2020","text":"Riorganizzato il db delle features per permettere una gestione pi\u00f9 razionale","title":"23 Novembre 2020"},{"location":"CHANGELOG/#19-novembre-2020","text":"riorganizzata la struttura per la gestione delle classi estese che necessitano di dynamic loading: nel modulo himada2.ext (cartella) vengono raccoliti per comodit\u00e0 gli oggetti che saranno implementati man mano come estensione di superclassi astratte appositamente definite: per ora hielen2.datalink.Source e hielen2.datalink.DB e hielen2.datalink.DataCache. Oltre alle classi in hielen2.ext, il sitema potr\u00e0 utilizzare moduli esterni che estendano le superclassi elencate. inserito 'timestamp' nello schema json accettato da POST /feature e PUT /feature . risolto bug minore di incoerenza su GET /data/{feature} e /data/{feature}/{parameter} . Quest'ultima continua ad accettare uno tra i nomi dei parametri della feature. Entrambe rispondo intestando le colonne in uscita con lo uid della serie, come GET /data/ .","title":"19 Novembre 2020"},{"location":"CHANGELOG/#17-novembre-2020","text":"Implementata dummy POST /actions/{feature}/{form} :","title":"17 Novembre 2020"},{"location":"CHANGELOG/#v204","text":"","title":"v2.0.4"},{"location":"CHANGELOG/#16-novembre-2020","text":"per coerenza rivisti i parametri di POST /feature : uid:<string> prototype:<string> properties:<json schema Properties> geometry:<json schema GeoJson> analogo discorso per PUT /feature/{uid} : properties:<json schema Properties> geometry:<json schema GeoJson> sistemata la risposta di GET /feature , modificando il livello di \"geometry\" implementata api PUT /features/{uid} . Accetta il paramentro properties con uno schema analogo al parmetro feature di POST /features con queste differenze: nello schema della PUT, uid e prototype NON vengono accettati perch\u00e8 sono campi chiave della feature e non possono essere modificati . lo uid della feature deve essere specificato come url e non come parametro. introduzione dello Schema GeoJson per la validazione modificata POST /features/ per accettare un GeoJson nell'attibuto geometry del Json principale feature","title":"16 Novembre 2020"},{"location":"CHANGELOG/#13-novembre-2020","text":"rinominazione DELETE /elements -> DELETE /features . eliminazione degli alias GET /features/{context} e /features/{context}/{uid} a causa del conflitto l'entry point DELETE /features . Il passaggio del context sar\u00e0 esclusivmante attraverso il parametro cntxt ( nota : questo nome \u00e8 dovuto alla collisione del nome con il campo 'context' dell'oggetto request). In caso lo possiamo cambiare. introduzione dell'alias /features/{uid} per il recupero delle info della specifica Feature.","title":"13 Novembre 2020"},{"location":"CHANGELOG/#12-novembre-2020","text":"ovunque nel mondo il parmetro 'uuid' (universal unique id) diventa 'uid'. rinominazione POST /elements -> POST /features . rinominazione GET /elements -> GET /parameters e modifica uscita in questo schema: { < feature1_UID > :[ { \"series\" : < feature1_param1_series_UID > , \"param\" : < feature1_param1_name > , \"um\" : < feature1_param1_measurement_unit > } , ... { \"series\" : < feature1_paramN_series_UID , \"param\" : < feature1_paramN_name > , \"um\" : < feature1_paramN_meaurement_unit > } ], ... < featureX_UID > :[ { \"series\" : < featureX_param1_series_UID > , \"param\" : < featureX_param1_name > , \"um\" : < featureX_param1_measurement_unit > } , ... { \"series\" : < featureX_paramM_series_UID , \"param\" : < featureX_paramM_name > , \"um\" : < featureX_paramM_meaurement_unit > } ] } introduzione api /features con lo schema usato da Daniele e SimoneD: GET /features GET /features/{context}/ GET /features/{context}/{feature} uscita : nota 1: NON viene introdotto \"context\" , come invece preventivato nota 2: \"cod\" diventa \"label\" . nota 3: \"date\" diventa \"timestamp\" nota 3: dalle properties vengono elminate \"z\" e \"mslm\" . nota 4: \"state\" viene mantenuto ma per ora \u00e8 inutilizzato { \"features\" : [ { \"type\" : \"Feature\" , \"properties\" : { \"uid\" : ..., \"label\" : ..., \"context\" :..., \"date\" : ..., \"type\" : ..., \"style\" : ..., \"state\" : ... } , \"geometry\" : < GeoJson Validable > } , ... { \"type\" : \"Feature\" , \"properties\" : { \"uid\" : ..., \"label\" : ..., \"context\" : ..., \"date\" : ..., \"type\" : ..., \"style\" : ..., \"state\" : ... } , \"geometry\" : < GeoJson Validable > } ] }","title":"12 Novembre 2020"},{"location":"CHANGELOG/#v203","text":"","title":"v2.0.3"},{"location":"CHANGELOG/#11-novembre-2020","text":"Modificata api POST /elements : la variabile element \u00e8 descritta dalla Classe hielen2.api.data.ElementSchema e validata. In paricolare \u00e8 stato introdotto l'attibuto context Modifcata api GET /data : la variabile datamap \u00e8 descritta dalla Classe hielen2.api.data.DataMapSchema e validata.","title":"11 Novembre 2020"},{"location":"CHANGELOG/#9-novembre-2020","text":"Introdotta la classe hielen2.utils.JsonValidable, per la validazione e documentazione automatica dei parametri delle api (JSON Schema descrition) corretti bug minori in hielen2.datalink","title":"9 Novembre 2020"},{"location":"CHANGELOG/#6-novembre-2020","text":"L'interfaccia DB \u00e8 ora thread safe!!! (almeno per il dummy json db)","title":"6 Novembre 2020"},{"location":"CHANGELOG/#v202","text":"","title":"v2.0.2"},{"location":"CHANGELOG/#4-novembre-2020","text":"Implementata la documentazione automatica delle api Implementate le api POST ../elements e DELETE ../elements L'uscita per tutte le api element (e per tutte le api con risposta json in generale), seguir\u00e0 questo schema: { \"meta\": { \"data_type\": \"DELETE /elements/ciao\", \"response\": \"ok\" \"message\": \"\", }, \"data\":{ ... } } L'api /series diventa /data e cambia il suo comportamento: la variabile di tipo json datamap si aspetta il campo series invece di parameters . In questo campo devono essere inseriti i codici delle serie e non pi\u00f9 il costrutto \"codice_elemento:parametro_elemento\". I codici delle serie si possono recuperarare dall'api /elements (vedi Nota successiva) L'api /elements cambia la sua risposta e per ogni parametro nella lista parameters degli elementi viene agiunto il codice della serie di riferimento che pu\u00f2 essere fornito senza modifiche a /data : { \"series\":<seriescode>, \"name\":<seriesname>, \"um\":<seriesunit> } GET /series GET /series/{el} GET /series/{el}/{param} GET /prototypes GET /prototypes/{type} GET /prototypes/{type}/forms GET /prototypes/{type}/forms/{form} POST /elements GET /elements GET /elements/{el} DELETE /elements/{el}","title":"4 Novembre 2020"},{"location":"MODELLO/","text":"Con Priorit\u00e0 Structure gestore della configurazione di base: modulo utilizzo stato datalink.py livello di astrazione db, interfacce json completo, integrabile utils.py strumenti accessori completo, integrabile source.py astrazione dei moduli di gestione completo, integrabile modello db: tabella descrizione stato features_proto prototipi features info sui moduli completa features persistenza delle features completa actions persistenza delle azioni completa series_proto prototipi serie dati per configurazione dinamica implementazione series peristenza info di elaborazione serie dati avanzato series_cache persistenza serie dati elaborate runtime json utilizzato per mockup modello api: Configurazione Abstraction layer: Produzione Interrogazione First Header Second Header Content from cell 1 Content from cell 2 Content in the first column Content in the second column Configurazione hielen2.ext.PhotoMonitoring (netCDF) definizione array dimensionali X,Y: 1- creo gli array di dimensione adeguata, 2- applico la matrice di trasformazione affine, 3- applico la proiezione da crs in input a EPSG:3857 salvare file in filecache/{uid}/multidim.nc (dati) definire percorso di salvataggio tiles: filecache/{uid}/{map}/ (tiles mappe) salvare il primo tile a risoluzione adeguata: filecache/{uid}/{map}/base.png salvataggio (stoccaggio) dell'immagine di base in filecache/{uid} (eventualmente compressa) Feed hielen2.ext.PhotoMonitoring analisi dei file csv in ingresso (NS, EW, Correlation se esiste) aggirnamento di filecache/{uid}/multidim.nc Configurazione hielen2.ext.TinSAR analisi della formato della master cloud salvataggio (stoccaggio) della nuvola di base recupero info geografiche in caso non esistano info di proiezione geografica si considera spazio cartesiano con coordinate con adeguate alla nuvola base (da verificare) configurare file netCDF e salvarlo in filecache/{uid}/multidim.nc (dati) definire percorso di salvataggio tiles: filecache/{uid}/{map}/ (tiles mappe) configurare cartella di cache per potree filecache/{uid}/{cloud} (potree) Feed hielen2.ext.TinSAR Analisi file in ingresso aggiornamento filecache/{uid}/multidim.nc aggiornamento filecache/{uid}/{cloud} v2.0.6 Interfacce delle Informazioni con risposta mockup. Intento: agganciare lavoro Daniele GET /bases GET /bases/{feature} GET /timelines GET /timelines/{feature} GET /data/ estensione del modello di datamap per accettare GeoGeson v2.0.7 Rivistazione del modulo PhotMonitoring come \"source\". Intento: agganciare le serie dati prodotte dall'elaborazione Photmonitoring alle interfacce v2.0.8 Implementazione del modulo TinSar come \"source\". Intento: agganciare le serie dati prodotte dall'elaborazione TinSar alle interfacce v2.0.9 Implementazione delle chiamate di mappa GET /maps/[/z/x/y] GET /maps/{feature}/[z/x/y] v2.0.10 Implementazione chiamate cloud GET /cloud/{feature} Senza priorit\u00e0 Moduli HielenSource : attualmente, per comodit\u00e0, vengono sviluppati come sotto moduli di hielen2 ma il modo corretto \u00e8 quello di separare lo sviluppo. Sar\u00e0 sempre possibile farlo dal momento che le strutture vengono sviluppate con l'obiettivo della separazione. ~~ Moduli HielenSource : Definire in backend le form come Marshmallow.Schema in modo da condividere la struttura tra moduli e api~~ Obiettivo: assegnare una timestamp ad ogni informazione: le properties degli ogetti dovranno essere delle serie dati. Concetto di informazione minima. Implementare procedura di testing delle api verificare il default dei campi marshmallow (sembra non prenderlo in considerazione, prob non arriva null ma \"\") POST /prototypes Migliorare l'output dei doc del JsonValidable Gestire i filed Nested nei doc del JsonValidable","title":"MODELLO"},{"location":"MODELLO/#con-priorita","text":"","title":"Con Priorit\u00e0"},{"location":"MODELLO/#structure","text":"gestore della configurazione di base: modulo utilizzo stato datalink.py livello di astrazione db, interfacce json completo, integrabile utils.py strumenti accessori completo, integrabile source.py astrazione dei moduli di gestione completo, integrabile modello db: tabella descrizione stato features_proto prototipi features info sui moduli completa features persistenza delle features completa actions persistenza delle azioni completa series_proto prototipi serie dati per configurazione dinamica implementazione series peristenza info di elaborazione serie dati avanzato series_cache persistenza serie dati elaborate runtime json utilizzato per mockup modello api:","title":"Structure"},{"location":"MODELLO/#configurazione","text":"Abstraction layer:","title":"Configurazione"},{"location":"MODELLO/#produzione","text":"","title":"Produzione"},{"location":"MODELLO/#interrogazione","text":"First Header Second Header Content from cell 1 Content from cell 2 Content in the first column Content in the second column","title":"Interrogazione"},{"location":"MODELLO/#configurazione-hielen2extphotomonitoring-netcdf","text":"definizione array dimensionali X,Y: 1- creo gli array di dimensione adeguata, 2- applico la matrice di trasformazione affine, 3- applico la proiezione da crs in input a EPSG:3857 salvare file in filecache/{uid}/multidim.nc (dati) definire percorso di salvataggio tiles: filecache/{uid}/{map}/ (tiles mappe) salvare il primo tile a risoluzione adeguata: filecache/{uid}/{map}/base.png salvataggio (stoccaggio) dell'immagine di base in filecache/{uid} (eventualmente compressa)","title":"Configurazione hielen2.ext.PhotoMonitoring (netCDF)"},{"location":"MODELLO/#feed-hielen2extphotomonitoring","text":"analisi dei file csv in ingresso (NS, EW, Correlation se esiste) aggirnamento di filecache/{uid}/multidim.nc","title":"Feed hielen2.ext.PhotoMonitoring"},{"location":"MODELLO/#configurazione-hielen2exttinsar","text":"analisi della formato della master cloud salvataggio (stoccaggio) della nuvola di base recupero info geografiche in caso non esistano info di proiezione geografica si considera spazio cartesiano con coordinate con adeguate alla nuvola base (da verificare) configurare file netCDF e salvarlo in filecache/{uid}/multidim.nc (dati) definire percorso di salvataggio tiles: filecache/{uid}/{map}/ (tiles mappe) configurare cartella di cache per potree filecache/{uid}/{cloud} (potree)","title":"Configurazione hielen2.ext.TinSAR"},{"location":"MODELLO/#feed-hielen2exttinsar","text":"Analisi file in ingresso aggiornamento filecache/{uid}/multidim.nc aggiornamento filecache/{uid}/{cloud}","title":"Feed hielen2.ext.TinSAR"},{"location":"MODELLO/#v206-interfacce-delle-informazioni-con-risposta-mockup-intento-agganciare-lavoro-daniele","text":"GET /bases GET /bases/{feature} GET /timelines GET /timelines/{feature} GET /data/ estensione del modello di datamap per accettare GeoGeson","title":"v2.0.6 Interfacce delle Informazioni con risposta mockup. Intento: agganciare lavoro Daniele"},{"location":"MODELLO/#v207-rivistazione-del-modulo-photmonitoring-come-source-intento-agganciare-le-serie-dati-prodotte-dallelaborazione-photmonitoring-alle-interfacce","text":"","title":"v2.0.7 Rivistazione del modulo PhotMonitoring come \"source\". Intento: agganciare le serie dati prodotte dall'elaborazione Photmonitoring alle interfacce"},{"location":"MODELLO/#v208-implementazione-del-modulo-tinsar-come-source-intento-agganciare-le-serie-dati-prodotte-dallelaborazione-tinsar-alle-interfacce","text":"","title":"v2.0.8 Implementazione del modulo TinSar come \"source\". Intento: agganciare le serie dati prodotte dall'elaborazione TinSar alle interfacce"},{"location":"MODELLO/#v209-implementazione-delle-chiamate-di-mappa","text":"GET /maps/[/z/x/y] GET /maps/{feature}/[z/x/y]","title":"v2.0.9 Implementazione delle chiamate di mappa"},{"location":"MODELLO/#v2010-implementazione-chiamate-cloud","text":"GET /cloud/{feature}","title":"v2.0.10 Implementazione chiamate cloud"},{"location":"MODELLO/#senza-priorita","text":"Moduli HielenSource : attualmente, per comodit\u00e0, vengono sviluppati come sotto moduli di hielen2 ma il modo corretto \u00e8 quello di separare lo sviluppo. Sar\u00e0 sempre possibile farlo dal momento che le strutture vengono sviluppate con l'obiettivo della separazione. ~~ Moduli HielenSource : Definire in backend le form come Marshmallow.Schema in modo da condividere la struttura tra moduli e api~~ Obiettivo: assegnare una timestamp ad ogni informazione: le properties degli ogetti dovranno essere delle serie dati. Concetto di informazione minima. Implementare procedura di testing delle api verificare il default dei campi marshmallow (sembra non prenderlo in considerazione, prob non arriva null ma \"\") POST /prototypes Migliorare l'output dei doc del JsonValidable Gestire i filed Nested nei doc del JsonValidable","title":"Senza priorit\u00e0"},{"location":"TODO/","text":"Nota : I tempi dichiarati sono da intendersi come di \"effettivo lavoro\", sono indicativi e potrebbero variare in base agli sviluppi (in particolare del del Modulo Principale). Inoltre i tempi riguradano esclusivamente lo sviluppo di Back-End. Anche se lo sviluppo di Front-End pu\u00f2 essere portato avanti parallelamente, sar\u00e0 necessario tenere adeguatamente in considerazione i realitivi tempi. MODULO PRICIPALE: astrazione setup di configurazione : Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 1 2 bassa bassa Features Prototypes o Interfaccia di modulo: Integrare informazioni capability: [series,map,cloud] e cachable (non sempre \u00e8 vantaggioso usare la cache) 2 2 media media Mapserver: istanza mapserver con workers (vedi Ecoplame) per la gestione delle mappe statiche tassellate (chiamate wms standard) xx - bassa Completo inserire \"suggerimenti\" nei prototipi delle azioni da passare nella nella risposta alla chiamata \"/actionSchemata\" xx - bassa alta Completare la Progettazione/Implementazione per la generazione delle istanze delle serie dati associate alla feature sulla base dei prototipi. Non essenziale per Nhazca xx - bassa Completo API: DELETE /action/{feature}/{config} 2 3 media media Modello di dipendenza azione-azione: field list, inserire default in actionSchemata astrazione interrogazione series : Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 1 2 bassa alta Integrazione dell'interrogazione basata su GeoJeson nell'API 4 4 bassa media Revisoni minori del modello di astrazione e API xx - alta bassa Impelementare nuovo modulo cache dati. Attualmente \u00e8 un mero json che per questioni di performance non \u00e8 possiblie utilizzare in produzione. Non \u00e8 essenziale per Nhazca astrazione interrogazione mappa Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 4 1 bassa bassa Implementazione modello di astrazione e API per moduli con capability \"mappa\": Un metodo astratto di hielene.Source che possa essere richiamato dal layer di astrazione e che fornisca in uscita un'immagine georiferita da inserie un path ben codificato. Contestualmente viene prodotto un mapfile associato da passare a mapserver al momento dell'interrogazione (Integrare nel sistema lo sviluppo di SM) 6 2 media bassa Implementazione di modulo di middelware per gestire la produzione/ cacheing delle immagini e passare le chiamate a mapserver per la tassellazione. astrazione interrogazione cloud Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 4 4 bassa media Implementazione API di interrogazione cloud: Attualmente il \"prodotto\" atteso \u00e8 una pagina html generata in automatico da fornire in front-end. 7 3 media media Potree: installazione e gestione del software, Implementazione modulo wrapper (Integrare nel sistema lo sviluppo di GC) MODULO ESTESO hielen2.ext.source_PhotoMonitoring azione config : Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 xx - media Completo Attivit\u00e0 di configurazione e persistenza dati azione feed : Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 2 1 bassa alta Aggancio del codice gi\u00e0 implementato come prototipo per Tisma + revisione interrogazione dati Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 2 1 bassa alta Agganco del codice gi\u00e0 implementato come prototipo per Tisma 4 3 media media Estrazione dati su interpolazione areale interrogazione mappa Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 3 1 media alta Agganco del codice gi\u00e0 implementato come prototipo per Tisma + revisione MODULO ESTESO hielen2.ext.source_TinSAR azione config : Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 5 3 media media Analisi info master cloud, strutture di persistenza (verificare matrici sparse), potree run (Integrare sviluppo GC) 1 3 bassa alta Salvataggio delle info sul modello di source_PhotoMonitoring azione feed : Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 2 4 media alta Analisi file in ingresso ed elaborazione file in ingresso (parzialmente implementato) 3 4 media media Aggiornamento strutture di persistenza interrogazione series Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 3 5 media bassa Interazione tramite modello di astrazione (interrogazione tramite GeoJeson mutuabile da source_Photomonitoring). Nota : Estrazione puntuale del front-end parzialmente implementata (Integrare sviluppo GC + DD). Estendere con estrazione punti in area e volume (Potree ritorna sempre un set di punti) interrogazione mappa Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 xx - alta bassa Proiezione della nuvola su piano x,y: Da trovare un modello efficiente di proiezione. Una volta proiettata l'immagine il resto rientra nel modello generale. interrogazione cloud Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 6 5 media media Restituire in output il prodotto \"html\" della nuvola di punti. Nota : Produzione html parzialmente implementato da sviluppo di GC. ALTRO SENZA PRIORITA' Gestione degli schemi del db: Definendo gli schemi Marshmallow per le tabelle dei DB \u00e8 possibile utilizzare Dump e Load per aggirare la non seriabilit\u00e0 di datetime Gestione della cache per le azioni: i moduli gestiscono la \"produzione\" la cache deve essere gestita esternamente Moduli HielenSource : attualmente, per comodit\u00e0, vengono sviluppati come sotto moduli di hielen2 ma il modo corretto \u00e8 quello di separare lo sviluppo. Sar\u00e0 sempre possibile farlo dal momento che le strutture vengono sviluppate con l'obiettivo della separazione. Implementare procedura di testing delle api verificare il default dei campi marshmallow (sembra non prenderlo in considerazione, prob non arriva null ma \"\") POST /prototypes Migliorare l'output dei doc del JsonValidable Gestire i filed Nested nei doc del JsonValidable","title":"TODO"},{"location":"TODO/#modulo-pricipale","text":"","title":"MODULO PRICIPALE:"},{"location":"TODO/#astrazione-setup-di-configurazione","text":"Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 1 2 bassa bassa Features Prototypes o Interfaccia di modulo: Integrare informazioni capability: [series,map,cloud] e cachable (non sempre \u00e8 vantaggioso usare la cache) 2 2 media media Mapserver: istanza mapserver con workers (vedi Ecoplame) per la gestione delle mappe statiche tassellate (chiamate wms standard) xx - bassa Completo inserire \"suggerimenti\" nei prototipi delle azioni da passare nella nella risposta alla chiamata \"/actionSchemata\" xx - bassa alta Completare la Progettazione/Implementazione per la generazione delle istanze delle serie dati associate alla feature sulla base dei prototipi. Non essenziale per Nhazca xx - bassa Completo API: DELETE /action/{feature}/{config} 2 3 media media Modello di dipendenza azione-azione: field list, inserire default in actionSchemata","title":"astrazione setup di configurazione:"},{"location":"TODO/#astrazione-interrogazione-series","text":"Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 1 2 bassa alta Integrazione dell'interrogazione basata su GeoJeson nell'API 4 4 bassa media Revisoni minori del modello di astrazione e API xx - alta bassa Impelementare nuovo modulo cache dati. Attualmente \u00e8 un mero json che per questioni di performance non \u00e8 possiblie utilizzare in produzione. Non \u00e8 essenziale per Nhazca","title":"astrazione interrogazione series:"},{"location":"TODO/#astrazione-interrogazione-mappa","text":"Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 4 1 bassa bassa Implementazione modello di astrazione e API per moduli con capability \"mappa\": Un metodo astratto di hielene.Source che possa essere richiamato dal layer di astrazione e che fornisca in uscita un'immagine georiferita da inserie un path ben codificato. Contestualmente viene prodotto un mapfile associato da passare a mapserver al momento dell'interrogazione (Integrare nel sistema lo sviluppo di SM) 6 2 media bassa Implementazione di modulo di middelware per gestire la produzione/ cacheing delle immagini e passare le chiamate a mapserver per la tassellazione.","title":"astrazione interrogazione mappa"},{"location":"TODO/#astrazione-interrogazione-cloud","text":"Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 4 4 bassa media Implementazione API di interrogazione cloud: Attualmente il \"prodotto\" atteso \u00e8 una pagina html generata in automatico da fornire in front-end. 7 3 media media Potree: installazione e gestione del software, Implementazione modulo wrapper (Integrare nel sistema lo sviluppo di GC)","title":"astrazione interrogazione cloud"},{"location":"TODO/#modulo-esteso-hielen2extsource_photomonitoring","text":"","title":"MODULO ESTESO hielen2.ext.source_PhotoMonitoring"},{"location":"TODO/#azione-config","text":"Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 xx - media Completo Attivit\u00e0 di configurazione e persistenza dati","title":"azione config:"},{"location":"TODO/#azione-feed","text":"Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 2 1 bassa alta Aggancio del codice gi\u00e0 implementato come prototipo per Tisma + revisione","title":"azione feed:"},{"location":"TODO/#interrogazione-dati","text":"Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 2 1 bassa alta Agganco del codice gi\u00e0 implementato come prototipo per Tisma 4 3 media media Estrazione dati su interpolazione areale","title":"interrogazione dati"},{"location":"TODO/#interrogazione-mappa","text":"Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 3 1 media alta Agganco del codice gi\u00e0 implementato come prototipo per Tisma + revisione","title":"interrogazione mappa"},{"location":"TODO/#modulo-esteso-hielen2extsource_tinsar","text":"","title":"MODULO ESTESO hielen2.ext.source_TinSAR"},{"location":"TODO/#azione-config_1","text":"Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 5 3 media media Analisi info master cloud, strutture di persistenza (verificare matrici sparse), potree run (Integrare sviluppo GC) 1 3 bassa alta Salvataggio delle info sul modello di source_PhotoMonitoring","title":"azione config:"},{"location":"TODO/#azione-feed_1","text":"Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 2 4 media alta Analisi file in ingresso ed elaborazione file in ingresso (parzialmente implementato) 3 4 media media Aggiornamento strutture di persistenza","title":"azione feed:"},{"location":"TODO/#interrogazione-series","text":"Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 3 5 media bassa Interazione tramite modello di astrazione (interrogazione tramite GeoJeson mutuabile da source_Photomonitoring). Nota : Estrazione puntuale del front-end parzialmente implementata (Integrare sviluppo GC + DD). Estendere con estrazione punti in area e volume (Potree ritorna sempre un set di punti)","title":"interrogazione series"},{"location":"TODO/#interrogazione-mappa_1","text":"Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 xx - alta bassa Proiezione della nuvola su piano x,y: Da trovare un modello efficiente di proiezione. Una volta proiettata l'immagine il resto rientra nel modello generale.","title":"interrogazione mappa"},{"location":"TODO/#interrogazione-cloud","text":"Giorni/Uomo effettivi Priorit\u00e0 Complessit\u00e0 Copertura Attivit\u00e0 6 5 media media Restituire in output il prodotto \"html\" della nuvola di punti. Nota : Produzione html parzialmente implementato da sviluppo di GC.","title":"interrogazione cloud"},{"location":"TODO/#altro-senza-priorita","text":"Gestione degli schemi del db: Definendo gli schemi Marshmallow per le tabelle dei DB \u00e8 possibile utilizzare Dump e Load per aggirare la non seriabilit\u00e0 di datetime Gestione della cache per le azioni: i moduli gestiscono la \"produzione\" la cache deve essere gestita esternamente Moduli HielenSource : attualmente, per comodit\u00e0, vengono sviluppati come sotto moduli di hielen2 ma il modo corretto \u00e8 quello di separare lo sviluppo. Sar\u00e0 sempre possibile farlo dal momento che le strutture vengono sviluppate con l'obiettivo della separazione. Implementare procedura di testing delle api verificare il default dei campi marshmallow (sembra non prenderlo in considerazione, prob non arriva null ma \"\") POST /prototypes Migliorare l'output dei doc del JsonValidable Gestire i filed Nested nei doc del JsonValidable","title":"ALTRO SENZA PRIORITA'"},{"location":"docs/API%20Reference/actions/","text":"Actions /actions/{feature} GET params : feature : Basic text / string value actions : Basic text / string value timestamp : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Recupero dello stato corrente delle azioni effettuate su una feature L'intento di questa api \u00e8 quello di fornire i valori richiesti secondo lo schema dell'azione nota 1 : actions accetta valori multipli separati da virgola nota 2 : A seconda dell'action richiesta, alcuni parametri potrebbero essere utilizzati in fase di input ma non registrati. Il che vuol dire che per quei parametri il valore di ritorno sar\u00e0 null viene restituito una struttura di questo tipo: [ { \"feature\"*:..., \"action_name*\":..., \"timestamp\": ..., \"value\":{...} }, { \"feature\"*:..., \"action_name*\":..., \"timestamp\": ..., \"value\":{...} }, ... ] nota 3 :(*) I campi \"feature\" e \"action\" potrebbero non essere restituiti nella struttura nel caso in cui essi risultino non ambigui. \"timestamp\" e \"value\" vengono sempre restituiti Possibili risposte: 404 Not Found : Nel non venga trovata la feature richiesta o essa abbia un problema di configurazione /actions/{feature}/{action} GET params : feature : Basic text / string value action : Basic text / string value timestamp : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : **Recupero dello stato corrente per una specifica azione di una specifica feature** DELETE params : feature : Basic text / string value action : Basic text / string value timestamp : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : **Eliminazione di una determinata azione di una specifica feature** POST params : feature : Basic text / string value action : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Esecuzione delle azioni Richiede l'esecuzione di una specifica azione su una feature, fornendo tutte le informazioni necessarie attraverso una form dinamica dedicata. Oltre ai due parametri feature e form , timestamp , indicati nella url, accetta un multipart/form-data basato sulla specifica form, selezionata tramite i due parametri espliciti. Tutto il content \u00e8 scaricato attarverso i chunk dello stream ('100 continue') per evitare il timeout dei workers in caso di contenuti di grandi dimensioni. Possibili risposte: 200 OK : Nel caso in cui l'azione vada a buon fine. L'azione richiesta viene presa in carico ma potrebbe avere un tempo di esecuzione arbitrario. L'azione quindi viene splittata su un altro processo. 404 Not Found : Nel caso la feature non esista o non sia definita per essa l'azione richiesta. 500 Internal Server Error : Nel caso pessimo che il modulo dichiarato non esista. 501 Not Implemented' : Nel caso la tipologia non fornisse ancora l'iplementazione di uno o tutti i moduli di gestione E' stato implementato il meccanismo minimo di gestione che prevede il salvataggio delle info fornite che possono essere fornite tali e quali in uscita (vedi metodo GET dell'api). Questo meccanismo permette di svluppare i moduli a partire da un template con risposta di default.","title":"Actions"},{"location":"docs/API%20Reference/actions/#actions","text":"","title":"Actions"},{"location":"docs/API%20Reference/actions/#actionsfeature","text":"","title":"/actions/{feature}"},{"location":"docs/API%20Reference/actions/#get","text":"params : feature : Basic text / string value actions : Basic text / string value timestamp : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Recupero dello stato corrente delle azioni effettuate su una feature L'intento di questa api \u00e8 quello di fornire i valori richiesti secondo lo schema dell'azione nota 1 : actions accetta valori multipli separati da virgola nota 2 : A seconda dell'action richiesta, alcuni parametri potrebbero essere utilizzati in fase di input ma non registrati. Il che vuol dire che per quei parametri il valore di ritorno sar\u00e0 null viene restituito una struttura di questo tipo: [ { \"feature\"*:..., \"action_name*\":..., \"timestamp\": ..., \"value\":{...} }, { \"feature\"*:..., \"action_name*\":..., \"timestamp\": ..., \"value\":{...} }, ... ] nota 3 :(*) I campi \"feature\" e \"action\" potrebbero non essere restituiti nella struttura nel caso in cui essi risultino non ambigui. \"timestamp\" e \"value\" vengono sempre restituiti Possibili risposte: 404 Not Found : Nel non venga trovata la feature richiesta o essa abbia un problema di configurazione","title":"GET"},{"location":"docs/API%20Reference/actions/#actionsfeatureaction","text":"","title":"/actions/{feature}/{action}"},{"location":"docs/API%20Reference/actions/#get_1","text":"params : feature : Basic text / string value action : Basic text / string value timestamp : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : **Recupero dello stato corrente per una specifica azione di una specifica feature**","title":"GET"},{"location":"docs/API%20Reference/actions/#delete","text":"params : feature : Basic text / string value action : Basic text / string value timestamp : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : **Eliminazione di una determinata azione di una specifica feature**","title":"DELETE"},{"location":"docs/API%20Reference/actions/#post","text":"params : feature : Basic text / string value action : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Esecuzione delle azioni Richiede l'esecuzione di una specifica azione su una feature, fornendo tutte le informazioni necessarie attraverso una form dinamica dedicata. Oltre ai due parametri feature e form , timestamp , indicati nella url, accetta un multipart/form-data basato sulla specifica form, selezionata tramite i due parametri espliciti. Tutto il content \u00e8 scaricato attarverso i chunk dello stream ('100 continue') per evitare il timeout dei workers in caso di contenuti di grandi dimensioni. Possibili risposte: 200 OK : Nel caso in cui l'azione vada a buon fine. L'azione richiesta viene presa in carico ma potrebbe avere un tempo di esecuzione arbitrario. L'azione quindi viene splittata su un altro processo. 404 Not Found : Nel caso la feature non esista o non sia definita per essa l'azione richiesta. 500 Internal Server Error : Nel caso pessimo che il modulo dichiarato non esista. 501 Not Implemented' : Nel caso la tipologia non fornisse ancora l'iplementazione di uno o tutti i moduli di gestione E' stato implementato il meccanismo minimo di gestione che prevede il salvataggio delle info fornite che possono essere fornite tali e quali in uscita (vedi metodo GET dell'api). Questo meccanismo permette di svluppare i moduli a partire da un template con risposta di default.","title":"POST"},{"location":"docs/API%20Reference/actionschemata/","text":"Actionschemata /actionschemata/ GET params : prototypes : Basic text / string value actions : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Recupero dello schema dei parametri per inizializare le forms delle azioni ritorna una struttura json di questo tipo: { \"NomePrototipo1\": { \"action1\": { \"args\": { \"arg1.1\": \"type_arg1.1\", \"arg1.2\": \"type_arg1.2\", ... }, \"mandatory\": [ args keys sublist ] }, \"action2\": { \"args\": { \"arg2.1\": \"type_arg2.1\", \"arg2.2\": \"type_arg2.2\", ... }, }, ... }, \"NomePrototipo3\": { ... }, ... }, /actionschemata/{prototype} GET params : prototype : Basic text / string value actions : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Alias per il recupero di tutte le informazioni di uno specifico prototipo /actionschemata/{prototype}/{action} GET params : prototype : Basic text / string value action : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Alias per il recupero di tutte le informazioni delle form di uno specifico prototipo","title":"ActionSchemata"},{"location":"docs/API%20Reference/actionschemata/#actionschemata","text":"","title":"Actionschemata"},{"location":"docs/API%20Reference/actionschemata/#actionschemata_1","text":"","title":"/actionschemata/"},{"location":"docs/API%20Reference/actionschemata/#get","text":"params : prototypes : Basic text / string value actions : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Recupero dello schema dei parametri per inizializare le forms delle azioni ritorna una struttura json di questo tipo: { \"NomePrototipo1\": { \"action1\": { \"args\": { \"arg1.1\": \"type_arg1.1\", \"arg1.2\": \"type_arg1.2\", ... }, \"mandatory\": [ args keys sublist ] }, \"action2\": { \"args\": { \"arg2.1\": \"type_arg2.1\", \"arg2.2\": \"type_arg2.2\", ... }, }, ... }, \"NomePrototipo3\": { ... }, ... },","title":"GET"},{"location":"docs/API%20Reference/actionschemata/#actionschemataprototype","text":"","title":"/actionschemata/{prototype}"},{"location":"docs/API%20Reference/actionschemata/#get_1","text":"params : prototype : Basic text / string value actions : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Alias per il recupero di tutte le informazioni di uno specifico prototipo","title":"GET"},{"location":"docs/API%20Reference/actionschemata/#actionschemataprototypeaction","text":"","title":"/actionschemata/{prototype}/{action}"},{"location":"docs/API%20Reference/actionschemata/#get_2","text":"params : prototype : Basic text / string value action : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Alias per il recupero di tutte le informazioni delle form di uno specifico prototipo","title":"GET"},{"location":"docs/API%20Reference/data/","text":"Data /data/ GET params : datamap : JSON Schema [{ timeto : str|bytes, timefrom : str|bytes, series : [str|bytes]}] content_type : Basic text / string value result : format : Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) content_type : text/plain; charset=utf-8, application/json; charset=utf-8 /data/{feature}/ GET params : feature : Basic text / string value par : Basic text / string value timefrom : Basic text / string value timeto : Basic text / string value content_type : Basic text / string value result : format : Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) content_type : text/plain; charset=utf-8, application/json; charset=utf-8 /data/{feature}/{par} GET params : feature : Basic text / string value par : Basic text / string value timefrom : Basic text / string value timeto : Basic text / string value content_type : Basic text / string value result : format : Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) content_type : text/plain; charset=utf-8, application/json; charset=utf-8","title":"Data"},{"location":"docs/API%20Reference/data/#data","text":"","title":"Data"},{"location":"docs/API%20Reference/data/#data_1","text":"","title":"/data/"},{"location":"docs/API%20Reference/data/#get","text":"params : datamap : JSON Schema [{ timeto : str|bytes, timefrom : str|bytes, series : [str|bytes]}] content_type : Basic text / string value result : format : Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) content_type : text/plain; charset=utf-8, application/json; charset=utf-8","title":"GET"},{"location":"docs/API%20Reference/data/#datafeature","text":"","title":"/data/{feature}/"},{"location":"docs/API%20Reference/data/#get_1","text":"params : feature : Basic text / string value par : Basic text / string value timefrom : Basic text / string value timeto : Basic text / string value content_type : Basic text / string value result : format : Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) content_type : text/plain; charset=utf-8, application/json; charset=utf-8","title":"GET"},{"location":"docs/API%20Reference/data/#datafeaturepar","text":"","title":"/data/{feature}/{par}"},{"location":"docs/API%20Reference/data/#get_2","text":"params : feature : Basic text / string value par : Basic text / string value timefrom : Basic text / string value timeto : Basic text / string value content_type : Basic text / string value result : format : Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) content_type : text/plain; charset=utf-8, application/json; charset=utf-8","title":"GET"},{"location":"docs/API%20Reference/elements/","text":"Elements /elements/ POST params : element : JSON Schema { prototype : str|bytes, style : str|bytes, description : str|bytes, label : str|bytes, uuid : str|bytes, status : str|bytes, geom : str|bytes, context : str|bytes} result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Api di creazione degli elementi. Ogni elemento deve avere il suo codice univoco uuid e il suo prototipo prototype . Il prototipo dell'elemento forisce informazioni per l'inizializazione della struttura. Possibili risposte: 409 Conflict : Nel caso in cui il codice fornito esista gi\u00e0. 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 201 Created : Nel caso in cui l'elemento venga creato correttamente. GET params : elist : Basic text / string value context : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 /elements/{uuid} GET params : uuid : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 DELETE params : uuid : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8","title":"Elements"},{"location":"docs/API%20Reference/elements/#elements","text":"","title":"Elements"},{"location":"docs/API%20Reference/elements/#elements_1","text":"","title":"/elements/"},{"location":"docs/API%20Reference/elements/#post","text":"params : element : JSON Schema { prototype : str|bytes, style : str|bytes, description : str|bytes, label : str|bytes, uuid : str|bytes, status : str|bytes, geom : str|bytes, context : str|bytes} result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Api di creazione degli elementi. Ogni elemento deve avere il suo codice univoco uuid e il suo prototipo prototype . Il prototipo dell'elemento forisce informazioni per l'inizializazione della struttura. Possibili risposte: 409 Conflict : Nel caso in cui il codice fornito esista gi\u00e0. 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 201 Created : Nel caso in cui l'elemento venga creato correttamente.","title":"POST"},{"location":"docs/API%20Reference/elements/#get","text":"params : elist : Basic text / string value context : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8","title":"GET"},{"location":"docs/API%20Reference/elements/#elementsuuid","text":"","title":"/elements/{uuid}"},{"location":"docs/API%20Reference/elements/#get_1","text":"params : uuid : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8","title":"GET"},{"location":"docs/API%20Reference/elements/#delete","text":"params : uuid : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8","title":"DELETE"},{"location":"docs/API%20Reference/features/","text":"Features /features/ POST params : uid : Basic text / string value prototype : Basic text / string value properties : JSON Schema { description : str|bytes, location : str|bytes, label : str|bytes, style : str|bytes, timestamp : str|bytes, status : str|bytes, context : str|bytes} geometry : JSON Schema {} result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Creazione delle Features. Ogni feature deve avere il suo codice univoco uid e il suo prototipo prototype . Questi due campi sono immutabli (vedi PUT /feature/{uid} ). Il prototipo della feature forisce informazioni per l'inizializazione della struttura. Il parametro geometry deve essere un GeoJson Se la feature viene creata correttamente ne restituisce la struttura Possibili risposte: 409 Conflict : Nel caso in cui il uid fornito esista gi\u00e0. 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 201 Created : Nel caso in cui la feature venga creata correttamente. GET params : uids : Basic text / string value cntxt : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Recupero delle informazioni delle features. nota : uids accetta valori multipli separati da virgola viene restituito una struttura di questo tipo: { \"features\": [ { \"type\": \"Feature\", \"properties\": { ... }, \"geometry\": <GeoJson Validable> }, ... ] } nota : Al contrario di quanto detto nel TODO non viene inserito il context a livello \"features\" perch\u00e8 in effetti \u00e8 una informazione sempre conosciuta a priori (se si lavora per commesse). Al contrario se si lavora per uids allora ha senso inserie questa info all' interno delle properties delle singole features. Possibili risposte: 404 Not Found : Nel caso in cui nessuna feature risponda ai criteri /features/{uid} GET params : uid : Basic text / string value cntxt : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : **Alias di recupero informazioni della specifica feature** PUT params : uid : Basic text / string value properties : JSON Schema { description : str|bytes, location : str|bytes, label : str|bytes, style : str|bytes, timestamp : str|bytes, status : str|bytes, context : str|bytes} geometry : JSON Schema {} result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Modifica delle properties di una feature Possibili risposte: 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 202 Accepted : Nel caso in cui la feature venga modificata correttamente. DELETE params : uid : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Cancellazione delle Features Se la feature viene cancellata correttamente ne restituisce la struttura Possibili risposte: 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 202 Accepted : Nel caso in cui la feature venga eliminata correttamente.","title":"Features"},{"location":"docs/API%20Reference/features/#features","text":"","title":"Features"},{"location":"docs/API%20Reference/features/#features_1","text":"","title":"/features/"},{"location":"docs/API%20Reference/features/#post","text":"params : uid : Basic text / string value prototype : Basic text / string value properties : JSON Schema { description : str|bytes, location : str|bytes, label : str|bytes, style : str|bytes, timestamp : str|bytes, status : str|bytes, context : str|bytes} geometry : JSON Schema {} result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Creazione delle Features. Ogni feature deve avere il suo codice univoco uid e il suo prototipo prototype . Questi due campi sono immutabli (vedi PUT /feature/{uid} ). Il prototipo della feature forisce informazioni per l'inizializazione della struttura. Il parametro geometry deve essere un GeoJson Se la feature viene creata correttamente ne restituisce la struttura Possibili risposte: 409 Conflict : Nel caso in cui il uid fornito esista gi\u00e0. 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 201 Created : Nel caso in cui la feature venga creata correttamente.","title":"POST"},{"location":"docs/API%20Reference/features/#get","text":"params : uids : Basic text / string value cntxt : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Recupero delle informazioni delle features. nota : uids accetta valori multipli separati da virgola viene restituito una struttura di questo tipo: { \"features\": [ { \"type\": \"Feature\", \"properties\": { ... }, \"geometry\": <GeoJson Validable> }, ... ] } nota : Al contrario di quanto detto nel TODO non viene inserito il context a livello \"features\" perch\u00e8 in effetti \u00e8 una informazione sempre conosciuta a priori (se si lavora per commesse). Al contrario se si lavora per uids allora ha senso inserie questa info all' interno delle properties delle singole features. Possibili risposte: 404 Not Found : Nel caso in cui nessuna feature risponda ai criteri","title":"GET"},{"location":"docs/API%20Reference/features/#featuresuid","text":"","title":"/features/{uid}"},{"location":"docs/API%20Reference/features/#get_1","text":"params : uid : Basic text / string value cntxt : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : **Alias di recupero informazioni della specifica feature**","title":"GET"},{"location":"docs/API%20Reference/features/#put","text":"params : uid : Basic text / string value properties : JSON Schema { description : str|bytes, location : str|bytes, label : str|bytes, style : str|bytes, timestamp : str|bytes, status : str|bytes, context : str|bytes} geometry : JSON Schema {} result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Modifica delle properties di una feature Possibili risposte: 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 202 Accepted : Nel caso in cui la feature venga modificata correttamente.","title":"PUT"},{"location":"docs/API%20Reference/features/#delete","text":"params : uid : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Cancellazione delle Features Se la feature viene cancellata correttamente ne restituisce la struttura Possibili risposte: 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 202 Accepted : Nel caso in cui la feature venga eliminata correttamente.","title":"DELETE"},{"location":"docs/API%20Reference/mapping/","text":"Mapping /mapping/ GET params : datamap : JSON Schema [{ refresh : bool, timeref : str|bytes, times : , series : [str|bytes]}] content_type : Basic text / string value result : format : Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) content_type : text/plain; charset=utf-8, application/json; charset=utf-8 /mapping/{feature}/ GET params : feature : Basic text / string value par : Basic text / string value times : Basic text / string value timeref : Basic text / string value refresh : Basic text / string value content_type : Basic text / string value result : format : Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) content_type : text/plain; charset=utf-8, application/json; charset=utf-8 /mapping/{feature}/{par} GET params : feature : Basic text / string value par : Basic text / string value times : Basic text / string value timeref : Basic text / string value refresh : Basic text / string value content_type : Basic text / string value result : format : Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) content_type : text/plain; charset=utf-8, application/json; charset=utf-8","title":"Mapping"},{"location":"docs/API%20Reference/mapping/#mapping","text":"","title":"Mapping"},{"location":"docs/API%20Reference/mapping/#mapping_1","text":"","title":"/mapping/"},{"location":"docs/API%20Reference/mapping/#get","text":"params : datamap : JSON Schema [{ refresh : bool, timeref : str|bytes, times : , series : [str|bytes]}] content_type : Basic text / string value result : format : Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) content_type : text/plain; charset=utf-8, application/json; charset=utf-8","title":"GET"},{"location":"docs/API%20Reference/mapping/#mappingfeature","text":"","title":"/mapping/{feature}/"},{"location":"docs/API%20Reference/mapping/#get_1","text":"params : feature : Basic text / string value par : Basic text / string value times : Basic text / string value timeref : Basic text / string value refresh : Basic text / string value content_type : Basic text / string value result : format : Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) content_type : text/plain; charset=utf-8, application/json; charset=utf-8","title":"GET"},{"location":"docs/API%20Reference/mapping/#mappingfeaturepar","text":"","title":"/mapping/{feature}/{par}"},{"location":"docs/API%20Reference/mapping/#get_2","text":"params : feature : Basic text / string value par : Basic text / string value times : Basic text / string value timeref : Basic text / string value refresh : Basic text / string value content_type : Basic text / string value result : format : Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) content_type : text/plain; charset=utf-8, application/json; charset=utf-8","title":"GET"},{"location":"docs/API%20Reference/parameters/","text":"Parameters /parameters/ GET params : cntxt : Basic text / string value uids : Basic text / string value params : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : ** Ricerca dei parametri associati alle features ** . __nota__ : uid accetta valori multipli separati da virgola viene restituita una struttura di questo tipo : { \"<fetUID>\" :[ { \"series\" : \"<series_UID>\" , \"param\" : \"<param_name>\" , \"um\" : \"<mearurement_unit>\" } ... ] ... } Possibili risposte : - _404 Not Found_ : Nel caso in cui nessun parametro risponda ai criteri /parameters/{cntxt} GET params : cntxt : Basic text / string value uids : Basic text / string value params : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : **Alias di ricerca dei Parametri nello lo specifico contesto** /parameters/{cntxt}/{uid} GET params : cntxt : Basic text / string value uid : Basic text / string value params : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : **Alias di ricerca dei Parametri della specifica Feature lo specifico contesto** /parameters/{cntxt}/{uid}/{param} GET params : cntxt : Basic text / string value uid : Basic text / string value param : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : **Alias di ricerca dello specifico Parametro della specifica Feature lo specifico contesto**","title":"Parameters"},{"location":"docs/API%20Reference/parameters/#parameters","text":"","title":"Parameters"},{"location":"docs/API%20Reference/parameters/#parameters_1","text":"","title":"/parameters/"},{"location":"docs/API%20Reference/parameters/#get","text":"params : cntxt : Basic text / string value uids : Basic text / string value params : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : ** Ricerca dei parametri associati alle features ** . __nota__ : uid accetta valori multipli separati da virgola viene restituita una struttura di questo tipo : { \"<fetUID>\" :[ { \"series\" : \"<series_UID>\" , \"param\" : \"<param_name>\" , \"um\" : \"<mearurement_unit>\" } ... ] ... } Possibili risposte : - _404 Not Found_ : Nel caso in cui nessun parametro risponda ai criteri","title":"GET"},{"location":"docs/API%20Reference/parameters/#parameterscntxt","text":"","title":"/parameters/{cntxt}"},{"location":"docs/API%20Reference/parameters/#get_1","text":"params : cntxt : Basic text / string value uids : Basic text / string value params : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : **Alias di ricerca dei Parametri nello lo specifico contesto**","title":"GET"},{"location":"docs/API%20Reference/parameters/#parameterscntxtuid","text":"","title":"/parameters/{cntxt}/{uid}"},{"location":"docs/API%20Reference/parameters/#get_2","text":"params : cntxt : Basic text / string value uid : Basic text / string value params : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : **Alias di ricerca dei Parametri della specifica Feature lo specifico contesto**","title":"GET"},{"location":"docs/API%20Reference/parameters/#parameterscntxtuidparam","text":"","title":"/parameters/{cntxt}/{uid}/{param}"},{"location":"docs/API%20Reference/parameters/#get_3","text":"params : cntxt : Basic text / string value uid : Basic text / string value param : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : **Alias di ricerca dello specifico Parametro della specifica Feature lo specifico contesto**","title":"GET"},{"location":"docs/API%20Reference/prepare/","text":"Prepare /prepare/map/ GET params : features : Basic text / string value timestamp : Basic text / string value paramser : Basic text / string value timeref : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : /prepare/map/{feature} GET params : feature : Basic text / string value timestamp : Basic text / string value paramser : Basic text / string value timeref : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8","title":"Prepare"},{"location":"docs/API%20Reference/prepare/#prepare","text":"","title":"Prepare"},{"location":"docs/API%20Reference/prepare/#preparemap","text":"","title":"/prepare/map/"},{"location":"docs/API%20Reference/prepare/#get","text":"params : features : Basic text / string value timestamp : Basic text / string value paramser : Basic text / string value timeref : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description :","title":"GET"},{"location":"docs/API%20Reference/prepare/#preparemapfeature","text":"","title":"/prepare/map/{feature}"},{"location":"docs/API%20Reference/prepare/#get_1","text":"params : feature : Basic text / string value timestamp : Basic text / string value paramser : Basic text / string value timeref : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8","title":"GET"},{"location":"docs/API%20Reference/prototypes/","text":"Prototypes /prototypes/ POST params : prototype : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Definizione di nuovi prototipi PLACEHOLDER: Non ancora implementato GET result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Recupero di tutte le informazioni dei prototipi ritorna una struttura json di questo tipo: { { \"uid1\": ..., \"module1\": ..., \"struct\": { \"classification\": ..., \"type\": ..., \"parameters\": { \"par1_1\": { \"type\": ..., \"operands\": { \"output\": ... } }, \"...\", \"par1_N\": { \"type\": ..., \"operands\": { \"output\": ... } } } } }, { \"uid2\": ..., \"module2\": ..., \"struct\": { \"classification\": ..., \"type\": ..., \"parameters\": { \"par2_1\": { \"type\": ..., \"operands\": { \"output\": ... } }, \"...\", \"par2_N\": { \"type\": ..., \"operands\": { \"output\": ... } } } } } } /prototypes/{prototype}/struct GET params : prototype : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Alias per il recupero delle info di inizializzazione delle features legate ad uno specifico prototipo","title":"Prototypes"},{"location":"docs/API%20Reference/prototypes/#prototypes","text":"","title":"Prototypes"},{"location":"docs/API%20Reference/prototypes/#prototypes_1","text":"","title":"/prototypes/"},{"location":"docs/API%20Reference/prototypes/#post","text":"params : prototype : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Definizione di nuovi prototipi PLACEHOLDER: Non ancora implementato","title":"POST"},{"location":"docs/API%20Reference/prototypes/#get","text":"result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Recupero di tutte le informazioni dei prototipi ritorna una struttura json di questo tipo: { { \"uid1\": ..., \"module1\": ..., \"struct\": { \"classification\": ..., \"type\": ..., \"parameters\": { \"par1_1\": { \"type\": ..., \"operands\": { \"output\": ... } }, \"...\", \"par1_N\": { \"type\": ..., \"operands\": { \"output\": ... } } } } }, { \"uid2\": ..., \"module2\": ..., \"struct\": { \"classification\": ..., \"type\": ..., \"parameters\": { \"par2_1\": { \"type\": ..., \"operands\": { \"output\": ... } }, \"...\", \"par2_N\": { \"type\": ..., \"operands\": { \"output\": ... } } } } } }","title":"GET"},{"location":"docs/API%20Reference/prototypes/#prototypesprototypestruct","text":"","title":"/prototypes/{prototype}/struct"},{"location":"docs/API%20Reference/prototypes/#get_1","text":"params : prototype : Basic text / string value result : format : JSON (Javascript Serialized Object Notation) content_type : application/json; charset=utf-8 description : Alias per il recupero delle info di inizializzazione delle features legate ad uno specifico prototipo","title":"GET"},{"location":"reference/hielen2/","text":"Module hielen2 View Source # coding=utf-8 __name__ = 'hielen2' __version__ = '2.0.7' __author__ = 'Alessandro Modesti' __email__ = 'it@img-srl.com' __description__ = 'Multidimention Hierarichical Elaboration Engine' __license__ = 'MIT' __uri__ = '' import warnings import json from .datalink import dbinit # , cacheinit def _initconf ( confile , envfile ): env = None with open ( envfile ) as ef : env = json . load ( ef ) with open ( confile ) as cf : confstr = cf . read () for k , w in env . items (): placeholder = \"{{\" + k + \"}}\" confstr = confstr . replace ( placeholder , w ) return json . loads ( confstr ) conf = _initconf ( \"./conf/hielen.json\" , \"./conf/env.json\" ) db = dbinit ( conf ) def int_or_str ( value ): try : return int ( value ) except ValueError : return value VERSION = tuple ( map ( int_or_str , __version__ . split ( \".\" ))) __all__ = [ \"conf\" , \"db\" ] Sub-modules hielen2.api hielen2.data hielen2.datalink hielen2.datalink_prova_df hielen2.ext hielen2.maps hielen2.source hielen2.utils Variables conf db","title":"Index"},{"location":"reference/hielen2/#module-hielen2","text":"View Source # coding=utf-8 __name__ = 'hielen2' __version__ = '2.0.7' __author__ = 'Alessandro Modesti' __email__ = 'it@img-srl.com' __description__ = 'Multidimention Hierarichical Elaboration Engine' __license__ = 'MIT' __uri__ = '' import warnings import json from .datalink import dbinit # , cacheinit def _initconf ( confile , envfile ): env = None with open ( envfile ) as ef : env = json . load ( ef ) with open ( confile ) as cf : confstr = cf . read () for k , w in env . items (): placeholder = \"{{\" + k + \"}}\" confstr = confstr . replace ( placeholder , w ) return json . loads ( confstr ) conf = _initconf ( \"./conf/hielen.json\" , \"./conf/env.json\" ) db = dbinit ( conf ) def int_or_str ( value ): try : return int ( value ) except ValueError : return value VERSION = tuple ( map ( int_or_str , __version__ . split ( \".\" ))) __all__ = [ \"conf\" , \"db\" ]","title":"Module hielen2"},{"location":"reference/hielen2/#sub-modules","text":"hielen2.api hielen2.data hielen2.datalink hielen2.datalink_prova_df hielen2.ext hielen2.maps hielen2.source hielen2.utils","title":"Sub-modules"},{"location":"reference/hielen2/#variables","text":"conf db","title":"Variables"},{"location":"reference/hielen2/datalink/","text":"Module hielen2.datalink View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 #!/usr/bin/env python # coding=utf-8 from pandas import DataFrame , Series , read_json , NaT , read_csv from abc import ABC , abstractmethod from hielen2.utils import loadjsonfile , savejsonfile , newinstanceof , hashfile from filelock import Timeout , FileLock from numpy import nan from pathlib import Path from hashlib import md5 from shutil import rmtree import os def dbinit ( conf ): return { k : newinstanceof ( w . pop ( \"klass\" ), ** w ) for k , w in conf [ \"db\" ] . items () } class DB ( ABC ): @abstractmethod def __init__ ( self , connection ): pass @abstractmethod def __getitem__ ( self , key ): pass @abstractmethod def __setitem__ ( self , key , value ): pass @abstractmethod def pop ( self , key ): pass class JsonDB ( DB ): def __init__ ( self , connection , schema , lock_timeout_seconds = 10 ): self . jsonfile = connection self . lock = FileLock ( f \" { connection } .lock\" , timeout = lock_timeout_seconds ) self . md5file = f \" { connection } .md5\" self . md5 = None self . schema = schema self . __chk_and_reload_jsondb ( force = True ) def __brute_load_jsondb ( self ): try : self . db = read_json ( self . jsonfile , orient = 'table' , convert_dates = False ) except Exception as e : self . db = DataFrame () if self . db . empty : self . db = DataFrame ({}, columns = self . schema [ 'columns' ]) self . db = self . db . set_index ( self . schema [ 'primary_key' ]) def __chk_and_reload_jsondb ( self , force = False ): \"\"\" Needs to check for json-database file changes in a thread safe way!! \"\"\" md5 = None error = None try : self . lock . acquire () try : if force : raise FileNotFoundError () with open ( self . md5file ) as o : md5 = o . read () if not md5 == self . md5 : self . md5 = md5 self . __brute_load_jsondb () except FileNotFoundError as e : ## refershing hash self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) self . __brute_load_jsondb () finally : self . lock . release () except Timeout : pass def save ( self ): try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def __write_jsondb ( self , key , value ): \"\"\" Needs to lock for writing json-database \"\"\" item = None error = None try : self . lock . acquire () try : self . __chk_and_reload_jsondb () if value is None : # Request to remove key, raises KeyError item = self . __getitem__ ( key ) try : self . db = self . db . drop ( key , axis = 0 ) except KeyError : raise KeyError ( f \"key { key } to remove does not exist\" ) else : # Request to insert key, raises ValueError primarykey = self . schema [ 'primary_key' ] if not isinstance ( key ,( list , set , tuple )): key = [ key ] if key . __len__ () < primarykey . __len__ (): raise ValueError ( f \"key { key !r} is not fully determinated\" ) keydict = dict ( zip ( self . schema [ 'primary_key' ], key )) value . update ( keydict ) df = DataFrame ([ value . values ()]) df . columns = value . keys () df = df . set_index ( self . schema [ 'primary_key' ]) try : self . db = self . db . append ( df , verify_integrity = True ) . sort_index () except ValueError : raise ValueError ( f \"key { key } to insert exists\" ) self . db . replace ({ nan : None , NaT : None }, inplace = True ) item = self . __brute_getitem ( key ) self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error return item def __brute_getitem ( self , key = None ): out = None if key is None : out = self . db else : out = self . db . loc [ key ] if isinstance ( out , Series ): out = out . to_frame () . T out . index . names = self . schema [ 'primary_key' ] out = out . reset_index () . to_dict ( orient = 'records' ) if out . __len__ () == 1 : out = out [ 0 ] return out def __getitem__ ( self , key = None ): self . __chk_and_reload_jsondb () if isinstance ( key , list ): try : key = list ( filter ( None , key )) except TypeError : pass return self . __brute_getitem ( key ) def pop ( self , key ): return self . __write_jsondb ( key , None ) def __setitem__ ( self , key = None , value = None ): self . __write_jsondb ( key , value ) ''' class JsonCache(DB): def __init__(self, connection): self.cache = ( read_json(connection, convert_dates=False) .set_index([\"uid\", \"timestamp\"])[\"value\"] .sort_index() ) self.filename = connection def __getitem__(self, key): return self.cache[key] def __setitem__(self, key, value): pass def pop(self, key): pass def save(self): self.cache.reset_index().to_json(self.filename, orient=\"records\") ''' class seriescode (): def __init__ ( self , * args , ** kwargs ): self . h = [ * args ] self . h . extend ( list ( kwargs . values ())) self . h = '' . join ([ str ( a ) for a in self . h ]) self . h = md5 ( f ' { self . h } ' . encode () ) . hexdigest () def __repr__ ( self ): return self . h class fsHielenCache ( JsonDB ): def __init__ ( self , connection , lock_timeout_seconds = 10 ): self . cachepath = connection self . lts = lock_timeout_seconds schema = { \"columns\" :[ \"uid\" , \"info\" ], \"primary_key\" :[ \"uid\" ]} connfile = str ( Path ( connection ) / \"index.json\" ) super () . __init__ ( connfile , schema , self . lts ) def __getitem__ ( self , key ): info = super () . __getitem__ ( key ) return CsvCache ( self . cachepath , key , self . lts ) . get ( force_reload = True ) def __setitem__ ( self , key , value ): if value is not None and not isinstance ( value , Series ): raise ValueError ( \"pandas.Series required\" ) try : assert isinstance ( key , str ) assert key . __len__ () == 32 except AssertionError as e : raise ValueError ( f \"key { key } doesn't seems to match requirement format\" ) #testing existence (stops if exits) if value is not None : super () . __setitem__ ( key ,{}) item = CsvCache ( self . cachepath , key , self . lts ) os . makedirs ( item . cachepath , exist_ok = True ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ] = statistics else : super () . __setitem__ ( key , None ) try : CsvCache ( self . cachepath , key , self . lts ) . drop () except FileNotFoundError as e : pass def update ( self , key , value ): if value is not None and not isinstance ( value , Series ): #if value is not None and not isinstance(value,DataFrame): raise ValueError ( \"pandas.Series required\" ) if value is not None : info = super () . __getitem__ ( key ) item = CsvCache ( self . cachepath , key , self . lts ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ] = statistics class CsvCache (): def __init__ ( self , cachepath , item , lock_timeout_seconds = 10 ): self . cachepath = Path ( cachepath ) / item [ 0 : 8 ] / item [ 8 : 16 ] / item [ 16 : 24 ] / item [ 24 : 32 ] self . db = None self . csv = str ( self . cachepath / f \" { item } .csv\" ) self . lock = FileLock ( f \" { self . csv } .lock\" , timeout = lock_timeout_seconds ) self . md5file = f \" { self . csv } .md5\" self . md5 = None #self.__chk_and_reload_cache(force=True) def __repr__ ( self ): return self . db . __repr__ () def __brute_load_cache ( self ): try : self . db = read_csv ( self . csv , header = None , sep = \";\" , index_col = 0 , parse_dates = True )[ 1 ] except Exception as e : self . db = Series () self . db . name = \"s\" def __chk_and_reload_cache ( self , force = False ): \"\"\" Needs to check for cache file changes in a thread safe way!! \"\"\" md5 = None error = None try : self . lock . acquire () try : if force : raise FileNotFoundError () with open ( self . md5file ) as o : md5 = o . read () if not md5 == self . md5 : self . md5 = md5 self . __brute_load_cache () except FileNotFoundError as e : ## refershing hash try : self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) except FileNotFoundError as e : pass self . __brute_load_cache () finally : self . lock . release () except Timeout : pass def save ( self ): try : self . lock . acquire () try : self . db . to_csv ( self . csv , header = None , sep = \";\" ) self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def drop ( self ): try : self . lock . acquire () try : os . unlink ( self . csv ) os . unlink ( self . md5file ) self . db = None finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def update ( self , value : Series ): \"\"\" Needs to lock for writing json-database \"\"\" error = None try : self . lock . acquire () try : self . __chk_and_reload_cache () self . db . drop ( value . index , errors = 'ignore' , inplace = True ) value . name = 's' self . db = self . db . append ( value ) . sort_index () self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error def get ( self , force_reload = False ): self . __chk_and_reload_cache ( force = force_reload ) return self . db Variables nan Functions dbinit def dbinit ( conf ) View Source def dbinit ( conf ): return { k : newinstanceof ( w . pop ( \"klass\" ), ** w ) for k , w in conf [ \"db\" ]. items () } Classes CsvCache class CsvCache ( cachepath , item , lock_timeout_seconds = 10 ) View Source class CsvCache (): def __init__ ( self , cachepath , item , lock_timeout_seconds = 10 ): self . cachepath = Path ( cachepath ) / item [ 0 : 8 ] / item [ 8 : 16 ] / item [ 16 : 24 ] / item [ 24 : 32 ] self . db = None self . csv = str ( self . cachepath / f \"{item}.csv\" ) self . lock = FileLock ( f \"{self.csv}.lock\" , timeout = lock_timeout_seconds ) self . md5file = f \"{self.csv}.md5\" self . md5 = None #self.__chk_and_reload_cache(force=True) def __repr__ ( self ): return self . db . __repr__ () def __brute_load_cache ( self ): try: self . db = read_csv ( self . csv , header = None , sep = \";\" , index_col = 0 , parse_dates = True )[ 1 ] except Exception as e: self . db = Series () self . db . name = \"s\" def __chk_and_reload_cache ( self , force = False ): \"\"\" Needs to check for cache file changes in a thread safe way!! \"\"\" md5 = None error = None try: self . lock . acquire () try: if force: raise FileNotFoundError () with open ( self . md5file ) as o: md5 = o . read () if not md5 == self . md5: self . md5 = md5 self . __brute_load_cache () except FileNotFoundError as e: ## refershing hash try: self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o: o . write ( self . md5 ) except FileNotFoundError as e: pass self . __brute_load_cache () finally: self . lock . release () except Timeout: pass def save ( self ): try: self . lock . acquire () try: self . db . to_csv ( self . csv , header = None , sep = \";\" ) self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o: o . write ( self . md5 ) finally: self . lock . release () except Timeout as e: # Just to remind Timout error here raise e def drop ( self ): try: self . lock . acquire () try: os . unlink ( self . csv ) os . unlink ( self . md5file ) self . db = None finally: self . lock . release () except Timeout as e: # Just to remind Timout error here raise e def update ( self , value:Series ): \"\"\" Needs to lock for writing json-database \"\"\" error = None try: self . lock . acquire () try: self . __chk_and_reload_cache () self . db . drop ( value . index , errors = 'ignore' , inplace = True ) value . name = 's' self . db = self . db . append ( value ). sort_index () self . save () except Exception as e: error = e finally: self . lock . release () except Timeout as e: error = e if error is not None: raise error def get ( self , force_reload = False ): self . __chk_and_reload_cache ( force = force_reload ) return self . db Methods drop def drop ( self ) View Source def drop ( self ): try : self . lock . acquire () try : os . unlink ( self . csv ) os . unlink ( self . md5file ) self . db = None finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e get def get ( self , force_reload = False ) View Source def get ( self , force_reload = False ): self . __chk_and_reload_cache ( force = force_reload ) return self . db save def save ( self ) View Source def save ( self ): try : self . lock . acquire () try : self . db . to_csv ( self . csv , header = None , sep = \";\" ) self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e update def update ( self , value : pandas . core . series . Series ) Needs to lock for writing json-database View Source def update ( self , value : Series ): \"\"\" Needs to lock for writing json-database \"\"\" error = None try : self . lock . acquire () try : self . __chk_and_reload_cache () self . db . drop ( value . index , errors = 'ignore' , inplace = True ) value . name = 's' self . db = self . db . append ( value ). sort_index () self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error DB class DB ( connection ) Helper class that provides a standard way to create an ABC using inheritance. View Source class DB ( ABC ) : @abstractmethod def __init__ ( self , connection ) : pass @abstractmethod def __getitem__ ( self , key ) : pass @abstractmethod def __setitem__ ( self , key , value ) : pass @abstractmethod def pop ( self , key ) : pass Ancestors (in MRO) abc.ABC Descendants hielen2.datalink.JsonDB Methods pop def pop ( self , key ) View Source @abstractmethod def pop ( self , key ) : pass JsonDB class JsonDB ( connection , schema , lock_timeout_seconds = 10 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class JsonDB ( DB ) : def __init__ ( self , connection , schema , lock_timeout_seconds = 10 ) : self . jsonfile = connection self . lock = FileLock ( f \"{connection}.lock\" , timeout = lock_timeout_seconds ) self . md5file = f \"{connection}.md5\" self . md5 = None self . schema = schema self . __chk_and_reload_jsondb ( force = True ) def __brute_load_jsondb ( self ) : try : self . db = read_json ( self . jsonfile , orient = 'table' , convert_dates = False ) except Exception as e : self . db = DataFrame () if self . db . empty : self . db = DataFrame ( {} , columns = self . schema [ 'columns' ] ) self . db = self . db . set_index ( self . schema [ 'primary_key' ] ) def __chk_and_reload_jsondb ( self , force = False ) : \"\"\" Needs to check for json-database file changes in a thread safe way!! \"\"\" md5 = None error = None try : self . lock . acquire () try : if force : raise FileNotFoundError () with open ( self . md5file ) as o : md5 = o . read () if not md5 == self . md5 : self . md5 = md5 self . __brute_load_jsondb () except FileNotFoundError as e : ## refershing hash self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) self . __brute_load_jsondb () finally : self . lock . release () except Timeout : pass def save ( self ) : try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def __write_jsondb ( self , key , value ) : \"\"\" Needs to lock for writing json-database \"\"\" item = None error = None try : self . lock . acquire () try : self . __chk_and_reload_jsondb () if value is None : # Request to remove key , raises KeyError item = self . __getitem__ ( key ) try : self . db = self . db . drop ( key , axis = 0 ) except KeyError : raise KeyError ( f \"key {key} to remove does not exist\" ) else : # Request to insert key , raises ValueError primarykey = self . schema [ 'primary_key' ] if not isinstance ( key ,( list , set , tuple )) : key =[ key ] if key . __len__ () < primarykey . __len__ () : raise ValueError ( f \"key {key!r} is not fully determinated\" ) keydict = dict ( zip ( self . schema [ 'primary_key' ] , key )) value . update ( keydict ) df = DataFrame ( [ value.values() ] ) df . columns = value . keys () df = df . set_index ( self . schema [ 'primary_key' ] ) try : self . db = self . db . append ( df , verify_integrity = True ). sort_index () except ValueError : raise ValueError ( f \"key {key} to insert exists\" ) self . db . replace ( { nan : None , NaT : None } , inplace = True ) item = self . __brute_getitem ( key ) self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error return item def __brute_getitem ( self , key = None ) : out = None if key is None : out = self . db else : out = self . db . loc [ key ] if isinstance ( out , Series ) : out = out . to_frame (). T out . index . names = self . schema [ 'primary_key' ] out = out . reset_index (). to_dict ( orient = 'records' ) if out . __len__ () == 1 : out = out [ 0 ] return out def __getitem__ ( self , key = None ) : self . __chk_and_reload_jsondb () if isinstance ( key , list ) : try : key = list ( filter ( None , key )) except TypeError : pass return self . __brute_getitem ( key ) def pop ( self , key ) : return self . __write_jsondb ( key , None ) def __setitem__ ( self , key = None , value = None ) : self . __write_jsondb ( key , value ) Ancestors (in MRO) hielen2.datalink.DB abc.ABC Descendants hielen2.datalink.fsHielenCache Methods pop def pop ( self , key ) View Source def pop ( self , key ): return self . __write_jsondb ( key , None ) save def save ( self ) View Source def save ( self ): try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e fsHielenCache class fsHielenCache ( connection , lock_timeout_seconds = 10 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class fsHielenCache ( JsonDB ) : def __init__ ( self , connection , lock_timeout_seconds = 10 ) : self . cachepath = connection self . lts = lock_timeout_seconds schema = { \"columns\" : [ \"uid\",\"info\" ] , \"primary_key\" : [ \"uid\" ] } connfile = str ( Path ( connection ) / \"index.json\" ) super (). __init__ ( connfile , schema , self . lts ) def __getitem__ ( self , key ) : info = super (). __getitem__ ( key ) return CsvCache ( self . cachepath , key , self . lts ). get ( force_reload = True ) def __setitem__ ( self , key , value ) : if value is not None and not isinstance ( value , Series ) : raise ValueError ( \"pandas.Series required\" ) try : assert isinstance ( key , str ) assert key . __len__ () == 32 except AssertionError as e : raise ValueError ( f \"key {key} doesn't seems to match requirement format\" ) #testing existence ( stops if exits ) if value is not None : super (). __setitem__ ( key , {} ) item = CsvCache ( self . cachepath , key , self . lts ) os . makedirs ( item . cachepath , exist_ok = True ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ]= statistics else : super (). __setitem__ ( key , None ) try : CsvCache ( self . cachepath , key , self . lts ). drop () except FileNotFoundError as e : pass def update ( self , key , value ) : if value is not None and not isinstance ( value , Series ) : #if value is not None and not isinstance ( value , DataFrame ) : raise ValueError ( \"pandas.Series required\" ) if value is not None : info = super (). __getitem__ ( key ) item = CsvCache ( self . cachepath , key , self . lts ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ]= statistics Ancestors (in MRO) hielen2.datalink.JsonDB hielen2.datalink.DB abc.ABC Methods pop def pop ( self , key ) View Source def pop ( self , key ): return self . __write_jsondb ( key , None ) save def save ( self ) View Source def save ( self ): try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e update def update ( self , key , value ) View Source def update ( self , key , value ) : if value is not None and not isinstance ( value , Series ) : #if value is not None and not isinstance ( value , DataFrame ) : raise ValueError ( \"pandas.Series required\" ) if value is not None : info = super (). __getitem__ ( key ) item = CsvCache ( self . cachepath , key , self . lts ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ]= statistics seriescode class seriescode ( * args , ** kwargs ) View Source class seriescode (): def __init__ ( self ,* args ,** kwargs ): self . h =[ * args ] self . h . extend ( list ( kwargs . values ())) self . h = '' . join ([ str ( a ) for a in self . h ]) self . h = md5 ( f' { self . h }'. encode () ). hexdigest () def __repr__ ( self ): return self . h","title":"Datalink"},{"location":"reference/hielen2/datalink/#module-hielen2datalink","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 #!/usr/bin/env python # coding=utf-8 from pandas import DataFrame , Series , read_json , NaT , read_csv from abc import ABC , abstractmethod from hielen2.utils import loadjsonfile , savejsonfile , newinstanceof , hashfile from filelock import Timeout , FileLock from numpy import nan from pathlib import Path from hashlib import md5 from shutil import rmtree import os def dbinit ( conf ): return { k : newinstanceof ( w . pop ( \"klass\" ), ** w ) for k , w in conf [ \"db\" ] . items () } class DB ( ABC ): @abstractmethod def __init__ ( self , connection ): pass @abstractmethod def __getitem__ ( self , key ): pass @abstractmethod def __setitem__ ( self , key , value ): pass @abstractmethod def pop ( self , key ): pass class JsonDB ( DB ): def __init__ ( self , connection , schema , lock_timeout_seconds = 10 ): self . jsonfile = connection self . lock = FileLock ( f \" { connection } .lock\" , timeout = lock_timeout_seconds ) self . md5file = f \" { connection } .md5\" self . md5 = None self . schema = schema self . __chk_and_reload_jsondb ( force = True ) def __brute_load_jsondb ( self ): try : self . db = read_json ( self . jsonfile , orient = 'table' , convert_dates = False ) except Exception as e : self . db = DataFrame () if self . db . empty : self . db = DataFrame ({}, columns = self . schema [ 'columns' ]) self . db = self . db . set_index ( self . schema [ 'primary_key' ]) def __chk_and_reload_jsondb ( self , force = False ): \"\"\" Needs to check for json-database file changes in a thread safe way!! \"\"\" md5 = None error = None try : self . lock . acquire () try : if force : raise FileNotFoundError () with open ( self . md5file ) as o : md5 = o . read () if not md5 == self . md5 : self . md5 = md5 self . __brute_load_jsondb () except FileNotFoundError as e : ## refershing hash self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) self . __brute_load_jsondb () finally : self . lock . release () except Timeout : pass def save ( self ): try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def __write_jsondb ( self , key , value ): \"\"\" Needs to lock for writing json-database \"\"\" item = None error = None try : self . lock . acquire () try : self . __chk_and_reload_jsondb () if value is None : # Request to remove key, raises KeyError item = self . __getitem__ ( key ) try : self . db = self . db . drop ( key , axis = 0 ) except KeyError : raise KeyError ( f \"key { key } to remove does not exist\" ) else : # Request to insert key, raises ValueError primarykey = self . schema [ 'primary_key' ] if not isinstance ( key ,( list , set , tuple )): key = [ key ] if key . __len__ () < primarykey . __len__ (): raise ValueError ( f \"key { key !r} is not fully determinated\" ) keydict = dict ( zip ( self . schema [ 'primary_key' ], key )) value . update ( keydict ) df = DataFrame ([ value . values ()]) df . columns = value . keys () df = df . set_index ( self . schema [ 'primary_key' ]) try : self . db = self . db . append ( df , verify_integrity = True ) . sort_index () except ValueError : raise ValueError ( f \"key { key } to insert exists\" ) self . db . replace ({ nan : None , NaT : None }, inplace = True ) item = self . __brute_getitem ( key ) self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error return item def __brute_getitem ( self , key = None ): out = None if key is None : out = self . db else : out = self . db . loc [ key ] if isinstance ( out , Series ): out = out . to_frame () . T out . index . names = self . schema [ 'primary_key' ] out = out . reset_index () . to_dict ( orient = 'records' ) if out . __len__ () == 1 : out = out [ 0 ] return out def __getitem__ ( self , key = None ): self . __chk_and_reload_jsondb () if isinstance ( key , list ): try : key = list ( filter ( None , key )) except TypeError : pass return self . __brute_getitem ( key ) def pop ( self , key ): return self . __write_jsondb ( key , None ) def __setitem__ ( self , key = None , value = None ): self . __write_jsondb ( key , value ) ''' class JsonCache(DB): def __init__(self, connection): self.cache = ( read_json(connection, convert_dates=False) .set_index([\"uid\", \"timestamp\"])[\"value\"] .sort_index() ) self.filename = connection def __getitem__(self, key): return self.cache[key] def __setitem__(self, key, value): pass def pop(self, key): pass def save(self): self.cache.reset_index().to_json(self.filename, orient=\"records\") ''' class seriescode (): def __init__ ( self , * args , ** kwargs ): self . h = [ * args ] self . h . extend ( list ( kwargs . values ())) self . h = '' . join ([ str ( a ) for a in self . h ]) self . h = md5 ( f ' { self . h } ' . encode () ) . hexdigest () def __repr__ ( self ): return self . h class fsHielenCache ( JsonDB ): def __init__ ( self , connection , lock_timeout_seconds = 10 ): self . cachepath = connection self . lts = lock_timeout_seconds schema = { \"columns\" :[ \"uid\" , \"info\" ], \"primary_key\" :[ \"uid\" ]} connfile = str ( Path ( connection ) / \"index.json\" ) super () . __init__ ( connfile , schema , self . lts ) def __getitem__ ( self , key ): info = super () . __getitem__ ( key ) return CsvCache ( self . cachepath , key , self . lts ) . get ( force_reload = True ) def __setitem__ ( self , key , value ): if value is not None and not isinstance ( value , Series ): raise ValueError ( \"pandas.Series required\" ) try : assert isinstance ( key , str ) assert key . __len__ () == 32 except AssertionError as e : raise ValueError ( f \"key { key } doesn't seems to match requirement format\" ) #testing existence (stops if exits) if value is not None : super () . __setitem__ ( key ,{}) item = CsvCache ( self . cachepath , key , self . lts ) os . makedirs ( item . cachepath , exist_ok = True ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ] = statistics else : super () . __setitem__ ( key , None ) try : CsvCache ( self . cachepath , key , self . lts ) . drop () except FileNotFoundError as e : pass def update ( self , key , value ): if value is not None and not isinstance ( value , Series ): #if value is not None and not isinstance(value,DataFrame): raise ValueError ( \"pandas.Series required\" ) if value is not None : info = super () . __getitem__ ( key ) item = CsvCache ( self . cachepath , key , self . lts ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ] = statistics class CsvCache (): def __init__ ( self , cachepath , item , lock_timeout_seconds = 10 ): self . cachepath = Path ( cachepath ) / item [ 0 : 8 ] / item [ 8 : 16 ] / item [ 16 : 24 ] / item [ 24 : 32 ] self . db = None self . csv = str ( self . cachepath / f \" { item } .csv\" ) self . lock = FileLock ( f \" { self . csv } .lock\" , timeout = lock_timeout_seconds ) self . md5file = f \" { self . csv } .md5\" self . md5 = None #self.__chk_and_reload_cache(force=True) def __repr__ ( self ): return self . db . __repr__ () def __brute_load_cache ( self ): try : self . db = read_csv ( self . csv , header = None , sep = \";\" , index_col = 0 , parse_dates = True )[ 1 ] except Exception as e : self . db = Series () self . db . name = \"s\" def __chk_and_reload_cache ( self , force = False ): \"\"\" Needs to check for cache file changes in a thread safe way!! \"\"\" md5 = None error = None try : self . lock . acquire () try : if force : raise FileNotFoundError () with open ( self . md5file ) as o : md5 = o . read () if not md5 == self . md5 : self . md5 = md5 self . __brute_load_cache () except FileNotFoundError as e : ## refershing hash try : self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) except FileNotFoundError as e : pass self . __brute_load_cache () finally : self . lock . release () except Timeout : pass def save ( self ): try : self . lock . acquire () try : self . db . to_csv ( self . csv , header = None , sep = \";\" ) self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def drop ( self ): try : self . lock . acquire () try : os . unlink ( self . csv ) os . unlink ( self . md5file ) self . db = None finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def update ( self , value : Series ): \"\"\" Needs to lock for writing json-database \"\"\" error = None try : self . lock . acquire () try : self . __chk_and_reload_cache () self . db . drop ( value . index , errors = 'ignore' , inplace = True ) value . name = 's' self . db = self . db . append ( value ) . sort_index () self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error def get ( self , force_reload = False ): self . __chk_and_reload_cache ( force = force_reload ) return self . db","title":"Module hielen2.datalink"},{"location":"reference/hielen2/datalink/#variables","text":"nan","title":"Variables"},{"location":"reference/hielen2/datalink/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/datalink/#dbinit","text":"def dbinit ( conf ) View Source def dbinit ( conf ): return { k : newinstanceof ( w . pop ( \"klass\" ), ** w ) for k , w in conf [ \"db\" ]. items () }","title":"dbinit"},{"location":"reference/hielen2/datalink/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/datalink/#csvcache","text":"class CsvCache ( cachepath , item , lock_timeout_seconds = 10 ) View Source class CsvCache (): def __init__ ( self , cachepath , item , lock_timeout_seconds = 10 ): self . cachepath = Path ( cachepath ) / item [ 0 : 8 ] / item [ 8 : 16 ] / item [ 16 : 24 ] / item [ 24 : 32 ] self . db = None self . csv = str ( self . cachepath / f \"{item}.csv\" ) self . lock = FileLock ( f \"{self.csv}.lock\" , timeout = lock_timeout_seconds ) self . md5file = f \"{self.csv}.md5\" self . md5 = None #self.__chk_and_reload_cache(force=True) def __repr__ ( self ): return self . db . __repr__ () def __brute_load_cache ( self ): try: self . db = read_csv ( self . csv , header = None , sep = \";\" , index_col = 0 , parse_dates = True )[ 1 ] except Exception as e: self . db = Series () self . db . name = \"s\" def __chk_and_reload_cache ( self , force = False ): \"\"\" Needs to check for cache file changes in a thread safe way!! \"\"\" md5 = None error = None try: self . lock . acquire () try: if force: raise FileNotFoundError () with open ( self . md5file ) as o: md5 = o . read () if not md5 == self . md5: self . md5 = md5 self . __brute_load_cache () except FileNotFoundError as e: ## refershing hash try: self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o: o . write ( self . md5 ) except FileNotFoundError as e: pass self . __brute_load_cache () finally: self . lock . release () except Timeout: pass def save ( self ): try: self . lock . acquire () try: self . db . to_csv ( self . csv , header = None , sep = \";\" ) self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o: o . write ( self . md5 ) finally: self . lock . release () except Timeout as e: # Just to remind Timout error here raise e def drop ( self ): try: self . lock . acquire () try: os . unlink ( self . csv ) os . unlink ( self . md5file ) self . db = None finally: self . lock . release () except Timeout as e: # Just to remind Timout error here raise e def update ( self , value:Series ): \"\"\" Needs to lock for writing json-database \"\"\" error = None try: self . lock . acquire () try: self . __chk_and_reload_cache () self . db . drop ( value . index , errors = 'ignore' , inplace = True ) value . name = 's' self . db = self . db . append ( value ). sort_index () self . save () except Exception as e: error = e finally: self . lock . release () except Timeout as e: error = e if error is not None: raise error def get ( self , force_reload = False ): self . __chk_and_reload_cache ( force = force_reload ) return self . db","title":"CsvCache"},{"location":"reference/hielen2/datalink/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/datalink/#drop","text":"def drop ( self ) View Source def drop ( self ): try : self . lock . acquire () try : os . unlink ( self . csv ) os . unlink ( self . md5file ) self . db = None finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e","title":"drop"},{"location":"reference/hielen2/datalink/#get","text":"def get ( self , force_reload = False ) View Source def get ( self , force_reload = False ): self . __chk_and_reload_cache ( force = force_reload ) return self . db","title":"get"},{"location":"reference/hielen2/datalink/#save","text":"def save ( self ) View Source def save ( self ): try : self . lock . acquire () try : self . db . to_csv ( self . csv , header = None , sep = \";\" ) self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e","title":"save"},{"location":"reference/hielen2/datalink/#update","text":"def update ( self , value : pandas . core . series . Series ) Needs to lock for writing json-database View Source def update ( self , value : Series ): \"\"\" Needs to lock for writing json-database \"\"\" error = None try : self . lock . acquire () try : self . __chk_and_reload_cache () self . db . drop ( value . index , errors = 'ignore' , inplace = True ) value . name = 's' self . db = self . db . append ( value ). sort_index () self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error","title":"update"},{"location":"reference/hielen2/datalink/#db","text":"class DB ( connection ) Helper class that provides a standard way to create an ABC using inheritance. View Source class DB ( ABC ) : @abstractmethod def __init__ ( self , connection ) : pass @abstractmethod def __getitem__ ( self , key ) : pass @abstractmethod def __setitem__ ( self , key , value ) : pass @abstractmethod def pop ( self , key ) : pass","title":"DB"},{"location":"reference/hielen2/datalink/#ancestors-in-mro","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/datalink/#descendants","text":"hielen2.datalink.JsonDB","title":"Descendants"},{"location":"reference/hielen2/datalink/#methods_1","text":"","title":"Methods"},{"location":"reference/hielen2/datalink/#pop","text":"def pop ( self , key ) View Source @abstractmethod def pop ( self , key ) : pass","title":"pop"},{"location":"reference/hielen2/datalink/#jsondb","text":"class JsonDB ( connection , schema , lock_timeout_seconds = 10 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class JsonDB ( DB ) : def __init__ ( self , connection , schema , lock_timeout_seconds = 10 ) : self . jsonfile = connection self . lock = FileLock ( f \"{connection}.lock\" , timeout = lock_timeout_seconds ) self . md5file = f \"{connection}.md5\" self . md5 = None self . schema = schema self . __chk_and_reload_jsondb ( force = True ) def __brute_load_jsondb ( self ) : try : self . db = read_json ( self . jsonfile , orient = 'table' , convert_dates = False ) except Exception as e : self . db = DataFrame () if self . db . empty : self . db = DataFrame ( {} , columns = self . schema [ 'columns' ] ) self . db = self . db . set_index ( self . schema [ 'primary_key' ] ) def __chk_and_reload_jsondb ( self , force = False ) : \"\"\" Needs to check for json-database file changes in a thread safe way!! \"\"\" md5 = None error = None try : self . lock . acquire () try : if force : raise FileNotFoundError () with open ( self . md5file ) as o : md5 = o . read () if not md5 == self . md5 : self . md5 = md5 self . __brute_load_jsondb () except FileNotFoundError as e : ## refershing hash self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) self . __brute_load_jsondb () finally : self . lock . release () except Timeout : pass def save ( self ) : try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def __write_jsondb ( self , key , value ) : \"\"\" Needs to lock for writing json-database \"\"\" item = None error = None try : self . lock . acquire () try : self . __chk_and_reload_jsondb () if value is None : # Request to remove key , raises KeyError item = self . __getitem__ ( key ) try : self . db = self . db . drop ( key , axis = 0 ) except KeyError : raise KeyError ( f \"key {key} to remove does not exist\" ) else : # Request to insert key , raises ValueError primarykey = self . schema [ 'primary_key' ] if not isinstance ( key ,( list , set , tuple )) : key =[ key ] if key . __len__ () < primarykey . __len__ () : raise ValueError ( f \"key {key!r} is not fully determinated\" ) keydict = dict ( zip ( self . schema [ 'primary_key' ] , key )) value . update ( keydict ) df = DataFrame ( [ value.values() ] ) df . columns = value . keys () df = df . set_index ( self . schema [ 'primary_key' ] ) try : self . db = self . db . append ( df , verify_integrity = True ). sort_index () except ValueError : raise ValueError ( f \"key {key} to insert exists\" ) self . db . replace ( { nan : None , NaT : None } , inplace = True ) item = self . __brute_getitem ( key ) self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error return item def __brute_getitem ( self , key = None ) : out = None if key is None : out = self . db else : out = self . db . loc [ key ] if isinstance ( out , Series ) : out = out . to_frame (). T out . index . names = self . schema [ 'primary_key' ] out = out . reset_index (). to_dict ( orient = 'records' ) if out . __len__ () == 1 : out = out [ 0 ] return out def __getitem__ ( self , key = None ) : self . __chk_and_reload_jsondb () if isinstance ( key , list ) : try : key = list ( filter ( None , key )) except TypeError : pass return self . __brute_getitem ( key ) def pop ( self , key ) : return self . __write_jsondb ( key , None ) def __setitem__ ( self , key = None , value = None ) : self . __write_jsondb ( key , value )","title":"JsonDB"},{"location":"reference/hielen2/datalink/#ancestors-in-mro_1","text":"hielen2.datalink.DB abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/datalink/#descendants_1","text":"hielen2.datalink.fsHielenCache","title":"Descendants"},{"location":"reference/hielen2/datalink/#methods_2","text":"","title":"Methods"},{"location":"reference/hielen2/datalink/#pop_1","text":"def pop ( self , key ) View Source def pop ( self , key ): return self . __write_jsondb ( key , None )","title":"pop"},{"location":"reference/hielen2/datalink/#save_1","text":"def save ( self ) View Source def save ( self ): try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e","title":"save"},{"location":"reference/hielen2/datalink/#fshielencache","text":"class fsHielenCache ( connection , lock_timeout_seconds = 10 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class fsHielenCache ( JsonDB ) : def __init__ ( self , connection , lock_timeout_seconds = 10 ) : self . cachepath = connection self . lts = lock_timeout_seconds schema = { \"columns\" : [ \"uid\",\"info\" ] , \"primary_key\" : [ \"uid\" ] } connfile = str ( Path ( connection ) / \"index.json\" ) super (). __init__ ( connfile , schema , self . lts ) def __getitem__ ( self , key ) : info = super (). __getitem__ ( key ) return CsvCache ( self . cachepath , key , self . lts ). get ( force_reload = True ) def __setitem__ ( self , key , value ) : if value is not None and not isinstance ( value , Series ) : raise ValueError ( \"pandas.Series required\" ) try : assert isinstance ( key , str ) assert key . __len__ () == 32 except AssertionError as e : raise ValueError ( f \"key {key} doesn't seems to match requirement format\" ) #testing existence ( stops if exits ) if value is not None : super (). __setitem__ ( key , {} ) item = CsvCache ( self . cachepath , key , self . lts ) os . makedirs ( item . cachepath , exist_ok = True ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ]= statistics else : super (). __setitem__ ( key , None ) try : CsvCache ( self . cachepath , key , self . lts ). drop () except FileNotFoundError as e : pass def update ( self , key , value ) : if value is not None and not isinstance ( value , Series ) : #if value is not None and not isinstance ( value , DataFrame ) : raise ValueError ( \"pandas.Series required\" ) if value is not None : info = super (). __getitem__ ( key ) item = CsvCache ( self . cachepath , key , self . lts ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ]= statistics","title":"fsHielenCache"},{"location":"reference/hielen2/datalink/#ancestors-in-mro_2","text":"hielen2.datalink.JsonDB hielen2.datalink.DB abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/datalink/#methods_3","text":"","title":"Methods"},{"location":"reference/hielen2/datalink/#pop_2","text":"def pop ( self , key ) View Source def pop ( self , key ): return self . __write_jsondb ( key , None )","title":"pop"},{"location":"reference/hielen2/datalink/#save_2","text":"def save ( self ) View Source def save ( self ): try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e","title":"save"},{"location":"reference/hielen2/datalink/#update_1","text":"def update ( self , key , value ) View Source def update ( self , key , value ) : if value is not None and not isinstance ( value , Series ) : #if value is not None and not isinstance ( value , DataFrame ) : raise ValueError ( \"pandas.Series required\" ) if value is not None : info = super (). __getitem__ ( key ) item = CsvCache ( self . cachepath , key , self . lts ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ]= statistics","title":"update"},{"location":"reference/hielen2/datalink/#seriescode","text":"class seriescode ( * args , ** kwargs ) View Source class seriescode (): def __init__ ( self ,* args ,** kwargs ): self . h =[ * args ] self . h . extend ( list ( kwargs . values ())) self . h = '' . join ([ str ( a ) for a in self . h ]) self . h = md5 ( f' { self . h }'. encode () ). hexdigest () def __repr__ ( self ): return self . h","title":"seriescode"},{"location":"reference/hielen2/datalink_prova_df/","text":"Module hielen2.datalink_prova_df View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 #!/usr/bin/env python # coding=utf-8 from pandas import DataFrame , Series , read_json , NaT , read_csv from abc import ABC , abstractmethod from hielen2.utils import loadjsonfile , savejsonfile , newinstanceof , hashfile from filelock import Timeout , FileLock from numpy import nan from pathlib import Path from hashlib import md5 from shutil import rmtree import os def dbinit ( conf ): return { k : newinstanceof ( w . pop ( \"klass\" ), ** w ) for k , w in conf [ \"db\" ] . items () } class DB ( ABC ): @abstractmethod def __init__ ( self , connection ): pass @abstractmethod def __getitem__ ( self , key ): pass @abstractmethod def __setitem__ ( self , key , value ): pass @abstractmethod def pop ( self , key ): pass class JsonDB ( DB ): def __init__ ( self , connection , schema , lock_timeout_seconds = 10 ): self . jsonfile = connection self . lock = FileLock ( f \" { connection } .lock\" , timeout = lock_timeout_seconds ) self . md5file = f \" { connection } .md5\" self . md5 = None self . schema = schema self . __chk_and_reload_jsondb ( force = True ) def __brute_load_jsondb ( self ): try : self . db = read_json ( self . jsonfile , orient = 'table' , convert_dates = False ) except Exception as e : self . db = DataFrame () if self . db . empty : self . db = DataFrame ({}, columns = self . schema [ 'columns' ]) self . db = self . db . set_index ( self . schema [ 'primary_key' ]) def __chk_and_reload_jsondb ( self , force = False ): \"\"\" Needs to check for json-database file changes in a thread safe way!! \"\"\" md5 = None error = None try : self . lock . acquire () try : if force : raise FileNotFoundError () with open ( self . md5file ) as o : md5 = o . read () if not md5 == self . md5 : self . md5 = md5 self . __brute_load_jsondb () except FileNotFoundError as e : ## refershing hash self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) self . __brute_load_jsondb () finally : self . lock . release () except Timeout : pass def save ( self ): try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def __write_jsondb ( self , key , value ): \"\"\" Needs to lock for writing json-database \"\"\" item = None error = None try : self . lock . acquire () try : self . __chk_and_reload_jsondb () if value is None : # Request to remove key, raises KeyError item = self . __getitem__ ( key ) try : self . db = self . db . drop ( key , axis = 0 ) except KeyError : raise KeyError ( f \"key { key } to remove does not exist\" ) else : # Request to insert key, raises ValueError primarykey = self . schema [ 'primary_key' ] if not isinstance ( key ,( list , set , tuple )): key = [ key ] if key . __len__ () < primarykey . __len__ (): raise ValueError ( f \"key { key !r} is not fully determinated\" ) keydict = dict ( zip ( self . schema [ 'primary_key' ], key )) value . update ( keydict ) df = DataFrame ([ value . values ()]) df . columns = value . keys () df = df . set_index ( self . schema [ 'primary_key' ]) try : self . db = self . db . append ( df , verify_integrity = True ) . sort_index () except ValueError : raise ValueError ( f \"key { key } to insert exists\" ) self . db . replace ({ nan : None , NaT : None }, inplace = True ) item = self . __brute_getitem ( key ) self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error return item def __brute_getitem ( self , key = None ): out = None if key is None : out = self . db else : out = self . db . loc [ key ] if isinstance ( out , Series ): out = out . to_frame () . T out . index . names = self . schema [ 'primary_key' ] out = out . reset_index () . to_dict ( orient = 'records' ) if out . __len__ () == 1 : out = out [ 0 ] return out def __getitem__ ( self , key = None ): self . __chk_and_reload_jsondb () if isinstance ( key , list ): try : key = list ( filter ( None , key )) except TypeError : pass return self . __brute_getitem ( key ) def pop ( self , key ): return self . __write_jsondb ( key , None ) def __setitem__ ( self , key = None , value = None ): self . __write_jsondb ( key , value ) ''' class JsonCache(DB): def __init__(self, connection): self.cache = ( read_json(connection, convert_dates=False) .set_index([\"uid\", \"timestamp\"])[\"value\"] .sort_index() ) self.filename = connection def __getitem__(self, key): return self.cache[key] def __setitem__(self, key, value): pass def pop(self, key): pass def save(self): self.cache.reset_index().to_json(self.filename, orient=\"records\") ''' class seriescode (): def __init__ ( self , * args , ** kwargs ): self . h = [ * args ] self . h . extend ( list ( kwargs . values ())) self . h = '' . join ([ str ( a ) for a in self . h ]) self . h = md5 ( f ' { self . h } ' . encode () ) . hexdigest () def __repr__ ( self ): return self . h class fsHielenCache ( JsonDB ): def __init__ ( self , connection , lock_timeout_seconds = 10 ): self . cachepath = connection self . lts = lock_timeout_seconds schema = { \"columns\" :[ \"uid\" , \"info\" ], \"primary_key\" :[ \"uid\" ]} connfile = str ( Path ( connection ) / \"index.json\" ) super () . __init__ ( connfile , schema , self . lts ) def __getitem__ ( self , key ): info = super () . __getitem__ ( key ) return CsvCache ( self . cachepath , key , self . lts ) . get ( force_reload = True ) def __setitem__ ( self , key , value ): if value is not None and not isinstance ( value ,( DataFrame , Series )): raise ValueError ( \"pandas.DataFrame or pandas.Series required\" ) try : assert isinstance ( key , str ) assert key . __len__ () == 32 except AssertionError as e : raise ValueError ( f \"key { key } doesn't seems to match requirement format\" ) if value is not None : super () . __setitem__ ( key ,{}) item = CsvCache ( self . cachepath , key , self . lts ) os . makedirs ( item . cachepath , exist_ok = True ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ] = statistics else : super () . __setitem__ ( key , None ) try : item = CsvCache ( self . cachepath , key , self . lts ) item . cleanfs () except FileNotFoundError as e : pass def update ( self , key , value ): if value is not None and not isinstance ( value ,( DataFrame , Series )): #if value is not None and not isinstance(value,DataFrame): raise ValueError ( \"pandas.DataFrame or pandas.Series required\" ) if value is not None : info = super () . __getitem__ ( key ) item = CsvCache ( self . cachepath , key , self . lts ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ] = statistics class CsvCache ( DataFrame ): def __init__ ( self , cachepath , item , lock_timeout_seconds = 10 ): super () . __init__ ({}) self . cachepath = Path ( cachepath ) / item [ 0 : 8 ] / item [ 8 : 16 ] / item [ 16 : 24 ] / item [ 24 : 32 ] self . db = None self . csv = str ( self . cachepath / f \" { item } .csv\" ) self . lock = FileLock ( f \" { self . csv } .lock\" , timeout = lock_timeout_seconds ) self . md5file = f \" { self . csv } .md5\" self . md5 = None #self.__chk_and_reload_cache(force=True) def __brute_load_cache ( self ): try : tmpdf = read_csv ( self . csv , header = None , sep = \";\" , parse_dates = True ) super () . __init__ ( tmpdf ) print ( 'STOCAZZO' ) print ( self ) print ( 'CETTESEFREGE' ) except Exception as e : pass def __chk_and_reload_cache ( self , force = False ): \"\"\" Needs to check for cache file changes in a thread safe way!! \"\"\" md5 = None error = None try : self . lock . acquire () try : if force : raise FileNotFoundError () with open ( self . md5file ) as o : md5 = o . read () if not md5 == self . md5 : self . md5 = md5 self . __brute_load_cache () except FileNotFoundError as e : ## refershing hash try : self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) except FileNotFoundError as e : pass self . __brute_load_cache () finally : self . lock . release () except Timeout : pass def save ( self ): try : self . lock . acquire () try : self . to_csv ( self . csv , header = None , sep = \";\" ) self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def cleanfs ( self ): try : self . lock . acquire () try : os . unlink ( self . csv ) os . unlink ( self . md5file ) self . drop ( self . index , axis = 1 , inplace = True ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def update ( self , value ): \"\"\" Needs to lock for writing json-database \"\"\" try : value = value . to_frame () except AttributeError as e : pass value = value . reset_index () value . columns = list ( range ( value . columns . __len__ ())) value = value . set_index ( value . columns [ 0 ]) error = None try : self . lock . acquire () try : self . __chk_and_reload_cache () self . drop ( value . index , axis = 0 , errors = 'ignore' , inplace = True ) self . append ( value , inplace = True ) . sort_index ( inplace = True ) self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error def get ( self , force_reload = False ): self . __chk_and_reload_cache ( force = force_reload ) return self . db Variables nan Functions dbinit def dbinit ( conf ) View Source def dbinit ( conf ): return { k : newinstanceof ( w . pop ( \"klass\" ), ** w ) for k , w in conf [ \"db\" ]. items () } Classes CsvCache class CsvCache ( cachepath , item , lock_timeout_seconds = 10 ) Two-dimensional, size-mutable, potentially heterogeneous tabular data. Data structure also contains labeled axes (rows and columns). Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects. The primary pandas data structure. Parameters data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame Dict can contain Series, arrays, constants, or list-like objects. .. versionchanged :: 0.23.0 If data is a dict, column order follows insertion-order for Python 3.6 and later. .. versionchanged :: 0.25.0 If data is a list of dicts, column order follows insertion-order for Python 3.6 and later. index : Index or array-like Index to use for resulting frame. Will default to RangeIndex if no indexing information part of input data and no index provided. columns : Index or array-like Column labels to use for resulting frame. Will default to RangeIndex (0, 1, 2, ..., n) if no column labels are provided. dtype : dtype, default None Data type to force. Only a single dtype is allowed. If None, infer. copy : bool, default False Copy data from inputs. Only affects DataFrame / 2d ndarray input. See Also DataFrame.from_records : Constructor from tuples, also record arrays. DataFrame.from_dict : From dicts of Series, arrays, or dicts. read_csv : Read a comma-separated values (csv) file into DataFrame. read_table : Read general delimited file into DataFrame. read_clipboard : Read text from clipboard into DataFrame. Examples Constructing DataFrame from a dictionary. d = {'col1': [1, 2], 'col2': [3, 4]} df = pd.DataFrame(data=d) df col1 col2 0 1 3 1 2 4 Notice that the inferred dtype is int64. df.dtypes col1 int64 col2 int64 dtype: object To enforce a single dtype: df = pd.DataFrame(data=d, dtype=np.int8) df.dtypes col1 int8 col2 int8 dtype: object Constructing DataFrame from numpy ndarray: df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), ... columns=['a', 'b', 'c']) df2 a b c 0 1 2 3 1 4 5 6 2 7 8 9 View Source class CsvCache ( DataFrame ): def __init__ ( self , cachepath , item , lock_timeout_seconds = 10 ): super (). __init__ ({}) self . cachepath = Path ( cachepath ) / item [ 0 : 8 ] / item [ 8 : 16 ] / item [ 16 : 24 ] / item [ 24 : 32 ] self . db = None self . csv = str ( self . cachepath / f \"{item}.csv\" ) self . lock = FileLock ( f \"{self.csv}.lock\" , timeout = lock_timeout_seconds ) self . md5file = f \"{self.csv}.md5\" self . md5 = None #self.__chk_and_reload_cache(force=True) def __brute_load_cache ( self ): try: tmpdf = read_csv ( self . csv , header = None , sep = \";\" , parse_dates = True ) super (). __init__ ( tmpdf ) print ( 'STOCAZZO' ) print ( self ) print ( 'CETTESEFREGE' ) except Exception as e: pass def __chk_and_reload_cache ( self , force = False ): \"\"\" Needs to check for cache file changes in a thread safe way!! \"\"\" md5 = None error = None try: self . lock . acquire () try: if force: raise FileNotFoundError () with open ( self . md5file ) as o: md5 = o . read () if not md5 == self . md5: self . md5 = md5 self . __brute_load_cache () except FileNotFoundError as e: ## refershing hash try: self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o: o . write ( self . md5 ) except FileNotFoundError as e: pass self . __brute_load_cache () finally: self . lock . release () except Timeout: pass def save ( self ): try: self . lock . acquire () try: self . to_csv ( self . csv , header = None , sep = \";\" ) self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o: o . write ( self . md5 ) finally: self . lock . release () except Timeout as e: # Just to remind Timout error here raise e def cleanfs ( self ): try: self . lock . acquire () try: os . unlink ( self . csv ) os . unlink ( self . md5file ) self . drop ( self . index , axis = 1 , inplace = True ) finally: self . lock . release () except Timeout as e: # Just to remind Timout error here raise e def update ( self , value ): \"\"\" Needs to lock for writing json-database \"\"\" try: value = value . to_frame () except AttributeError as e: pass value = value . reset_index () value . columns = list ( range ( value . columns . __len__ ())) value = value . set_index ( value . columns [ 0 ]) error = None try: self . lock . acquire () try: self . __chk_and_reload_cache () self . drop ( value . index , axis = 0 , errors = 'ignore' , inplace = True ) self . append ( value , inplace = True ). sort_index ( inplace = True ) self . save () except Exception as e: error = e finally: self . lock . release () except Timeout as e: error = e if error is not None: raise error def get ( self , force_reload = False ): self . __chk_and_reload_cache ( force = force_reload ) return self . db Ancestors (in MRO) pandas.core.frame.DataFrame pandas.core.generic.NDFrame pandas.core.base.PandasObject pandas.core.accessor.DirNamesMixin pandas.core.base.SelectionMixin pandas.core.indexing.IndexingMixin Class variables columns index plot sparse Static methods from_dict def from_dict ( data , orient = 'columns' , dtype = None , columns = None ) -> 'DataFrame' Construct DataFrame from dict of array-like or dicts. Creates DataFrame object from dictionary by columns or by index allowing dtype specification. Parameters data : dict Of the form {field : array-like} or {field : dict}. orient : {'columns', 'index'}, default 'columns' The \"orientation\" of the data. If the keys of the passed dict should be the columns of the resulting DataFrame, pass 'columns' (default). Otherwise if the keys should be rows, pass 'index'. dtype : dtype, default None Data type to force, otherwise infer. columns : list, default None Column labels to use when orient='index' . Raises a ValueError if used with orient='columns' . .. versionadded :: 0.23.0 Returns DataFrame See Also DataFrame.from_records : DataFrame from structured ndarray, sequence of tuples or dicts, or DataFrame. DataFrame : DataFrame object creation using constructor. Examples By default the keys of the dict become the DataFrame columns: data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']} pd.DataFrame.from_dict(data) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Specify orient='index' to create the DataFrame using dictionary keys as rows: data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']} pd.DataFrame.from_dict(data, orient='index') 0 1 2 3 row_1 3 2 1 0 row_2 a b c d When using the 'index' orientation, the column names can be specified manually: pd.DataFrame.from_dict(data, orient='index', ... columns=['A', 'B', 'C', 'D']) A B C D row_1 3 2 1 0 row_2 a b c d View Source @classmethod def from_dict ( cls , data , orient = \"columns\" , dtype = None , columns = None ) -> \"DataFrame\" : \"\"\" Construct DataFrame from dict of array-like or dicts. Creates DataFrame object from dictionary by columns or by index allowing dtype specification. Parameters ---------- data : dict Of the form {field : array-like} or {field : dict}. orient : {'columns', 'index'}, default 'columns' The \" orientation \" of the data. If the keys of the passed dict should be the columns of the resulting DataFrame, pass 'columns' (default). Otherwise if the keys should be rows, pass 'index'. dtype : dtype, default None Data type to force, otherwise infer. columns : list, default None Column labels to use when ``orient='index'``. Raises a ValueError if used with ``orient='columns'``. .. versionadded:: 0.23.0 Returns ------- DataFrame See Also -------- DataFrame.from_records : DataFrame from structured ndarray, sequence of tuples or dicts, or DataFrame. DataFrame : DataFrame object creation using constructor. Examples -------- By default the keys of the dict become the DataFrame columns: >>> data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']} >>> pd.DataFrame.from_dict(data) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Specify ``orient='index'`` to create the DataFrame using dictionary keys as rows: >>> data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']} >>> pd.DataFrame.from_dict(data, orient='index') 0 1 2 3 row_1 3 2 1 0 row_2 a b c d When using the 'index' orientation, the column names can be specified manually: >>> pd.DataFrame.from_dict(data, orient='index', ... columns=['A', 'B', 'C', 'D']) A B C D row_1 3 2 1 0 row_2 a b c d \"\"\" index = None orient = orient . lower () if orient == \"index\" : if len ( data ) > 0 : # TODO speed up Series case if isinstance ( list ( data . values ()) [ 0 ] , ( Series , dict )) : data = _from_nested_dict ( data ) else : data , index = list ( data . values ()), list ( data . keys ()) elif orient == \"columns\" : if columns is not None : raise ValueError ( \"cannot use columns parameter with orient='columns'\" ) else : # pragma : no cover raise ValueError ( \"only recognize index or columns for orient\" ) return cls ( data , index = index , columns = columns , dtype = dtype ) from_records def from_records ( data , index = None , exclude = None , columns = None , coerce_float = False , nrows = None ) -> 'DataFrame' Convert structured or record ndarray to DataFrame. Creates a DataFrame object from a structured ndarray, sequence of tuples or dicts, or DataFrame. Parameters data : structured ndarray, sequence of tuples or dicts, or DataFrame Structured input data. index : str, list of fields, array-like Field of array to use as the index, alternately a specific set of input labels to use. exclude : sequence, default None Columns or fields to exclude. columns : sequence, default None Column names to use. If the passed data do not have names associated with them, this argument provides names for the columns. Otherwise this argument indicates the order of the columns in the result (any names not found in the data will become all-NA columns). coerce_float : bool, default False Attempt to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point, useful for SQL result sets. nrows : int, default None Number of rows to read if data is an iterator. Returns DataFrame See Also DataFrame.from_dict : DataFrame from dict of array-like or dicts. DataFrame : DataFrame object creation using constructor. Examples Data can be provided as a structured ndarray: data = np.array([(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')], ... dtype=[('col_1', 'i4'), ('col_2', 'U1')]) pd.DataFrame.from_records(data) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Data can be provided as a list of dicts: data = [{'col_1': 3, 'col_2': 'a'}, ... {'col_1': 2, 'col_2': 'b'}, ... {'col_1': 1, 'col_2': 'c'}, ... {'col_1': 0, 'col_2': 'd'}] pd.DataFrame.from_records(data) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Data can be provided as a list of tuples with corresponding columns: data = [(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')] pd.DataFrame.from_records(data, columns=['col_1', 'col_2']) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d View Source @classmethod def from_records ( cls , data , index = None , exclude = None , columns = None , coerce_float = False , nrows = None , ) -> \"DataFrame\" : \"\"\" Convert structured or record ndarray to DataFrame. Creates a DataFrame object from a structured ndarray, sequence of tuples or dicts, or DataFrame. Parameters ---------- data : structured ndarray, sequence of tuples or dicts, or DataFrame Structured input data. index : str, list of fields, array-like Field of array to use as the index, alternately a specific set of input labels to use. exclude : sequence, default None Columns or fields to exclude. columns : sequence, default None Column names to use. If the passed data do not have names associated with them, this argument provides names for the columns. Otherwise this argument indicates the order of the columns in the result (any names not found in the data will become all-NA columns). coerce_float : bool, default False Attempt to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point, useful for SQL result sets. nrows : int, default None Number of rows to read if data is an iterator. Returns ------- DataFrame See Also -------- DataFrame.from_dict : DataFrame from dict of array-like or dicts. DataFrame : DataFrame object creation using constructor. Examples -------- Data can be provided as a structured ndarray: >>> data = np.array([(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')], ... dtype=[('col_1', 'i4'), ('col_2', 'U1')]) >>> pd.DataFrame.from_records(data) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Data can be provided as a list of dicts: >>> data = [{'col_1': 3, 'col_2': 'a'}, ... {'col_1': 2, 'col_2': 'b'}, ... {'col_1': 1, 'col_2': 'c'}, ... {'col_1': 0, 'col_2': 'd'}] >>> pd.DataFrame.from_records(data) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Data can be provided as a list of tuples with corresponding columns: >>> data = [(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')] >>> pd.DataFrame.from_records(data, columns=['col_1', 'col_2']) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d \"\"\" # Make a copy of the input columns so we can modify it if columns is not None : columns = ensure_index ( columns ) if is_iterator ( data ) : if nrows == 0 : return cls () try : first_row = next ( data ) except StopIteration : return cls ( index = index , columns = columns ) dtype = None if hasattr ( first_row , \"dtype\" ) and first_row . dtype . names : dtype = first_row . dtype values = [ first_row ] if nrows is None : values += data else : values . extend ( itertools . islice ( data , nrows - 1 )) if dtype is not None : data = np . array ( values , dtype = dtype ) else : data = values if isinstance ( data , dict ) : if columns is None : columns = arr_columns = ensure_index ( sorted ( data )) arrays = [ data[k ] for k in columns ] else : arrays = [] arr_columns = [] for k , v in data . items () : if k in columns : arr_columns . append ( k ) arrays . append ( v ) arrays , arr_columns = reorder_arrays ( arrays , arr_columns , columns ) elif isinstance ( data , ( np . ndarray , DataFrame )) : arrays , columns = to_arrays ( data , columns ) if columns is not None : columns = ensure_index ( columns ) arr_columns = columns else : arrays , arr_columns = to_arrays ( data , columns , coerce_float = coerce_float ) arr_columns = ensure_index ( arr_columns ) if columns is not None : columns = ensure_index ( columns ) else : columns = arr_columns if exclude is None : exclude = set () else : exclude = set ( exclude ) result_index = None if index is not None : if isinstance ( index , str ) or not hasattr ( index , \"__iter__\" ) : i = columns . get_loc ( index ) exclude . add ( index ) if len ( arrays ) > 0 : result_index = Index ( arrays [ i ] , name = index ) else : result_index = Index ( [] , name = index ) else : try : index_data = [ arrays[arr_columns.get_loc(field) ] for field in index ] except ( KeyError , TypeError ) : # raised by get_loc , see GH#29258 result_index = index else : result_index = ensure_index_from_sequences ( index_data , names = index ) exclude . update ( index ) if any ( exclude ) : arr_exclude = [ x for x in exclude if x in arr_columns ] to_remove = [ arr_columns.get_loc(col) for col in arr_exclude ] arrays = [ v for i, v in enumerate(arrays) if i not in to_remove ] arr_columns = arr_columns . drop ( arr_exclude ) columns = columns . drop ( exclude ) mgr = arrays_to_mgr ( arrays , arr_columns , result_index , columns ) return cls ( mgr ) Instance variables T at Access a single value for a row/column label pair. Similar to loc , in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series. Raises KeyError If 'label' does not exist in DataFrame. See Also DataFrame.iat : Access a single value for a row/column pair by integer position. DataFrame.loc : Access a group of rows and columns by label(s). Series.at : Access a single value using a label. Examples df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]], ... index=[4, 5, 6], columns=['A', 'B', 'C']) df A B C 4 0 2 3 5 0 4 1 6 10 20 30 Get value at specified row/column pair df.at[4, 'B'] 2 Set value at specified row/column pair df.at[4, 'B'] = 10 df.at[4, 'B'] 10 Get value within a Series df.loc[5].at['B'] 4 attrs Dictionary of global attributes on this object. .. warning:: attrs is experimental and may change without warning. axes Return a list representing the axes of the DataFrame. It has the row axis labels and column axis labels as the only members. They are returned in that order. Examples df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) df.axes [RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'], dtype='object')] dtypes Return the dtypes in the DataFrame. This returns a Series with the data type of each column. The result's index is the original DataFrame's columns. Columns with mixed types are stored with the object dtype. See :ref: the User Guide <basics.dtypes> for more. Returns pandas.Series The data type of each column. Examples df = pd.DataFrame({'float': [1.0], ... 'int': [1], ... 'datetime': [pd.Timestamp('20180310')], ... 'string': ['foo']}) df.dtypes float float64 int int64 datetime datetime64[ns] string object dtype: object empty Indicator whether DataFrame is empty. True if DataFrame is entirely empty (no items), meaning any of the axes are of length 0. Returns bool If DataFrame is empty, return True, if not return False. See Also Series.dropna : Return series without null values. DataFrame.dropna : Return DataFrame with labels on given axis omitted where (all or any) data are missing. Notes If DataFrame contains only NaNs, it is still not considered empty. See the example below. Examples An example of an actual empty DataFrame. Notice the index is empty: df_empty = pd.DataFrame({'A' : []}) df_empty Empty DataFrame Columns: [A] Index: [] df_empty.empty True If we only have NaNs in our DataFrame, it is not considered empty! We will need to drop the NaNs to make the DataFrame empty: df = pd.DataFrame({'A' : [np.nan]}) df A 0 NaN df.empty False df.dropna().empty True iat Access a single value for a row/column pair by integer position. Similar to iloc , in that both provide integer-based lookups. Use iat if you only need to get or set a single value in a DataFrame or Series. Raises IndexError When integer position is out of bounds. See Also DataFrame.at : Access a single value for a row/column label pair. DataFrame.loc : Access a group of rows and columns by label(s). DataFrame.iloc : Access a group of rows and columns by integer position(s). Examples df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]], ... columns=['A', 'B', 'C']) df A B C 0 0 2 3 1 0 4 1 2 10 20 30 Get value at specified row/column pair df.iat[1, 2] 1 Set value at specified row/column pair df.iat[1, 2] = 10 df.iat[1, 2] 10 Get value within a series df.loc[0].iat[1] 2 iloc Purely integer-location based indexing for selection by position. .iloc[] is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. Allowed inputs are: An integer, e.g. 5 . A list or array of integers, e.g. [4, 3, 0] . A slice object with ints, e.g. 1:7 . A boolean array. A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above). This is useful in method chains, when you don't have a reference to the calling object, but would like to base your selection on some value. .iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing (this conforms with python/numpy slice semantics). See more at :ref: Selection by Position <indexing.integer> . See Also DataFrame.iat : Fast integer location scalar accessor. DataFrame.loc : Purely label-location based indexer for selection by label. Series.iloc : Purely integer-location based indexing for selection by position. Examples mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4}, ... {'a': 100, 'b': 200, 'c': 300, 'd': 400}, ... {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }] df = pd.DataFrame(mydict) df a b c d 0 1 2 3 4 1 100 200 300 400 2 1000 2000 3000 4000 Indexing just the rows With a scalar integer. type(df.iloc[0]) df.iloc[0] a 1 b 2 c 3 d 4 Name: 0, dtype: int64 With a list of integers. df.iloc[[0]] a b c d 0 1 2 3 4 type(df.iloc[[0]]) df.iloc[[0, 1]] a b c d 0 1 2 3 4 1 100 200 300 400 With a slice object. df.iloc[:3] a b c d 0 1 2 3 4 1 100 200 300 400 2 1000 2000 3000 4000 With a boolean mask the same length as the index. df.iloc[[True, False, True]] a b c d 0 1 2 3 4 2 1000 2000 3000 4000 With a callable, useful in method chains. The x passed to the lambda is the DataFrame being sliced. This selects the rows whose index label even. df.iloc[lambda x: x.index % 2 == 0] a b c d 0 1 2 3 4 2 1000 2000 3000 4000 Indexing both axes You can mix the indexer types for the index and columns. Use : to select the entire axis. With scalar integers. df.iloc[0, 1] 2 With lists of integers. df.iloc[[0, 2], [1, 3]] b d 0 2 4 2 2000 4000 With slice objects. df.iloc[1:3, 0:3] a b c 1 100 200 300 2 1000 2000 3000 With a boolean array whose length matches the columns. df.iloc[:, [True, False, True, False]] a c 0 1 3 1 100 300 2 1000 3000 With a callable function that expects the Series or DataFrame. df.iloc[:, lambda df: [0, 2]] a c 0 1 3 1 100 300 2 1000 3000 loc Access a group of rows and columns by label(s) or a boolean array. .loc[] is primarily label based, but may also be used with a boolean array. Allowed inputs are: A single label, e.g. 5 or 'a' , (note that 5 is interpreted as a label of the index, and never as an integer position along the index). A list or array of labels, e.g. ['a', 'b', 'c'] . A slice object with labels, e.g. 'a':'f' . .. warning:: Note that contrary to usual python slices, both the start and the stop are included A boolean array of the same length as the axis being sliced, e.g. [True, False, True] . A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above) See more at :ref: Selection by Label <indexing.label> Raises KeyError If any items are not found. See Also DataFrame.at : Access a single value for a row/column label pair. DataFrame.iloc : Access group of rows and columns by integer position(s). DataFrame.xs : Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. Series.loc : Access group of values using labels. Examples Getting values df = pd.DataFrame([[1, 2], [4, 5], [7, 8]], ... index=['cobra', 'viper', 'sidewinder'], ... columns=['max_speed', 'shield']) df max_speed shield cobra 1 2 viper 4 5 sidewinder 7 8 Single label. Note this returns the row as a Series. df.loc['viper'] max_speed 4 shield 5 Name: viper, dtype: int64 List of labels. Note using [[]] returns a DataFrame. df.loc[['viper', 'sidewinder']] max_speed shield viper 4 5 sidewinder 7 8 Single label for row and column df.loc['cobra', 'shield'] 2 Slice with labels for row and single label for column. As mentioned above, note that both the start and stop of the slice are included. df.loc['cobra':'viper', 'max_speed'] cobra 1 viper 4 Name: max_speed, dtype: int64 Boolean list with the same length as the row axis df.loc[[False, False, True]] max_speed shield sidewinder 7 8 Conditional that returns a boolean Series df.loc[df['shield'] > 6] max_speed shield sidewinder 7 8 Conditional that returns a boolean Series with column labels specified df.loc[df['shield'] > 6, ['max_speed']] max_speed sidewinder 7 Callable that returns a boolean Series df.loc[lambda df: df['shield'] == 8] max_speed shield sidewinder 7 8 Setting values Set value for all items matching the list of labels df.loc[['viper', 'sidewinder'], ['shield']] = 50 df max_speed shield cobra 1 2 viper 4 50 sidewinder 7 50 Set value for an entire row df.loc['cobra'] = 10 df max_speed shield cobra 10 10 viper 4 50 sidewinder 7 50 Set value for an entire column df.loc[:, 'max_speed'] = 30 df max_speed shield cobra 30 10 viper 30 50 sidewinder 30 50 Set value for rows matching callable condition df.loc[df['shield'] > 35] = 0 df max_speed shield cobra 30 10 viper 0 0 sidewinder 0 0 Getting values on a DataFrame with an index that has integer labels Another example using integers for the index df = pd.DataFrame([[1, 2], [4, 5], [7, 8]], ... index=[7, 8, 9], columns=['max_speed', 'shield']) df max_speed shield 7 1 2 8 4 5 9 7 8 Slice with integer labels for rows. As mentioned above, note that both the start and stop of the slice are included. df.loc[7:9] max_speed shield 7 1 2 8 4 5 9 7 8 Getting values with a MultiIndex A number of examples using a DataFrame with a MultiIndex tuples = [ ... ('cobra', 'mark i'), ('cobra', 'mark ii'), ... ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'), ... ('viper', 'mark ii'), ('viper', 'mark iii') ... ] index = pd.MultiIndex.from_tuples(tuples) values = [[12, 2], [0, 4], [10, 20], ... [1, 4], [7, 1], [16, 36]] df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index) df max_speed shield cobra mark i 12 2 mark ii 0 4 sidewinder mark i 10 20 mark ii 1 4 viper mark ii 7 1 mark iii 16 36 Single label. Note this returns a DataFrame with a single index. df.loc['cobra'] max_speed shield mark i 12 2 mark ii 0 4 Single index tuple. Note this returns a Series. df.loc[('cobra', 'mark ii')] max_speed 0 shield 4 Name: (cobra, mark ii), dtype: int64 Single label for row and column. Similar to passing in a tuple, this returns a Series. df.loc['cobra', 'mark i'] max_speed 12 shield 2 Name: (cobra, mark i), dtype: int64 Single tuple. Note using [[]] returns a DataFrame. df.loc[[('cobra', 'mark ii')]] max_speed shield cobra mark ii 0 4 Single tuple for the index with a single label for the column df.loc[('cobra', 'mark i'), 'shield'] 2 Slice from index tuple to single label df.loc[('cobra', 'mark i'):'viper'] max_speed shield cobra mark i 12 2 mark ii 0 4 sidewinder mark i 10 20 mark ii 1 4 viper mark ii 7 1 mark iii 16 36 Slice from index tuple to index tuple df.loc[('cobra', 'mark i'):('viper', 'mark ii')] max_speed shield cobra mark i 12 2 mark ii 0 4 sidewinder mark i 10 20 mark ii 1 4 viper mark ii 7 1 ndim Return an int representing the number of axes / array dimensions. Return 1 if Series. Otherwise return 2 if DataFrame. See Also ndarray.ndim : Number of array dimensions. Examples s = pd.Series({'a': 1, 'b': 2, 'c': 3}) s.ndim 1 df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) df.ndim 2 shape Return a tuple representing the dimensionality of the DataFrame. See Also ndarray.shape Examples df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) df.shape (2, 2) df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4], ... 'col3': [5, 6]}) df.shape (2, 3) size Return an int representing the number of elements in this object. Return the number of rows if Series. Otherwise return the number of rows times number of columns if DataFrame. See Also ndarray.size : Number of elements in the array. Examples s = pd.Series({'a': 1, 'b': 2, 'c': 3}) s.size 3 df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) df.size 4 style Returns a Styler object. Contains methods for building a styled HTML representation of the DataFrame. See Also io.formats.style.Styler : Helps style a DataFrame or Series according to the data with HTML and CSS. values Return a Numpy representation of the DataFrame. .. warning:: We recommend using :meth: DataFrame.to_numpy instead. Only the values in the DataFrame will be returned, the axes labels will be removed. Returns numpy.ndarray The values of the DataFrame. See Also DataFrame.to_numpy : Recommended alternative to this method. DataFrame.index : Retrieve the index labels. DataFrame.columns : Retrieving the column names. Notes The dtype will be a lower-common-denominator dtype (implicit upcasting); that is to say if the dtypes (even of numeric types) are mixed, the one that accommodates all will be chosen. Use this with care if you are not dealing with the blocks. e.g. If the dtypes are float16 and float32, dtype will be upcast to float32. If dtypes are int32 and uint8, dtype will be upcast to int32. By :func: numpy.find_common_type convention, mixing int64 and uint64 will result in a float64 dtype. Examples A DataFrame where all columns are the same type (e.g., int64) results in an array of the same type. df = pd.DataFrame({'age': [ 3, 29], ... 'height': [94, 170], ... 'weight': [31, 115]}) df age height weight 0 3 94 31 1 29 170 115 df.dtypes age int64 height int64 weight int64 dtype: object df.values array([[ 3, 94, 31], [ 29, 170, 115]]) A DataFrame with mixed type columns(e.g., str/object, int64, float32) results in an ndarray of the broadest type that accommodates these mixed types (e.g., object). df2 = pd.DataFrame([('parrot', 24.0, 'second'), ... ('lion', 80.5, 1), ... ('monkey', np.nan, None)], ... columns=('name', 'max_speed', 'rank')) df2.dtypes name object max_speed float64 rank object dtype: object df2.values array([['parrot', 24.0, 'second'], ['lion', 80.5, 1], ['monkey', nan, None]], dtype=object) Methods abs def abs ( self : ~ FrameOrSeries ) -> ~ FrameOrSeries Return a Series/DataFrame with absolute numeric value of each element. This function only applies to elements that are all numeric. Returns abs Series/DataFrame containing the absolute value of each element. See Also numpy.absolute : Calculate the absolute value element-wise. Notes For complex inputs, 1.2 + 1j , the absolute value is :math: \\sqrt{ a^2 + b^2 } . Examples Absolute numeric values in a Series. s = pd.Series([-1.10, 2, -3.33, 4]) s.abs() 0 1.10 1 2.00 2 3.33 3 4.00 dtype: float64 Absolute numeric values in a Series with complex numbers. s = pd.Series([1.2 + 1j]) s.abs() 0 1.56205 dtype: float64 Absolute numeric values in a Series with a Timedelta element. s = pd.Series([pd.Timedelta('1 days')]) s.abs() 0 1 days dtype: timedelta64[ns] Select rows with data closest to certain value using argsort (from StackOverflow <https://stackoverflow.com/a/17758115> __). df = pd.DataFrame({ ... 'a': [4, 5, 6, 7], ... 'b': [10, 20, 30, 40], ... 'c': [100, 50, -30, -50] ... }) df a b c 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 df.loc[(df.c - 43).abs().argsort()] a b c 1 5 20 50 0 4 10 100 2 6 30 -30 3 7 40 -50 View Source def abs ( self : FrameOrSeries ) -> FrameOrSeries : \"\"\" Return a Series/DataFrame with absolute numeric value of each element. This function only applies to elements that are all numeric. Returns ------- abs Series/DataFrame containing the absolute value of each element. See Also -------- numpy.absolute : Calculate the absolute value element-wise. Notes ----- For ``complex`` inputs, ``1.2 + 1j``, the absolute value is :math:`\\\\sqrt{ a^2 + b^2 }`. Examples -------- Absolute numeric values in a Series. >>> s = pd.Series([-1.10, 2, -3.33, 4]) >>> s.abs() 0 1.10 1 2.00 2 3.33 3 4.00 dtype: float64 Absolute numeric values in a Series with complex numbers. >>> s = pd.Series([1.2 + 1j]) >>> s.abs() 0 1.56205 dtype: float64 Absolute numeric values in a Series with a Timedelta element. >>> s = pd.Series([pd.Timedelta('1 days')]) >>> s.abs() 0 1 days dtype: timedelta64[ns] Select rows with data closest to certain value using argsort (from `StackOverflow <https://stackoverflow.com/a/17758115>`__). >>> df = pd.DataFrame({ ... 'a': [4, 5, 6, 7], ... 'b': [10, 20, 30, 40], ... 'c': [100, 50, -30, -50] ... }) >>> df a b c 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 >>> df.loc[(df.c - 43).abs().argsort()] a b c 1 5 20 50 0 4 10 100 2 6 30 -30 3 7 40 -50 \"\"\" return np . abs ( self ) add def add ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Addition of dataframe and other, element-wise (binary operator add ). Equivalent to dataframe + other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, radd . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) add_prefix def add_prefix ( self : ~ FrameOrSeries , prefix : str ) -> ~ FrameOrSeries Prefix labels with string prefix . For Series, the row labels are prefixed. For DataFrame, the column labels are prefixed. Parameters prefix : str The string to add before each label. Returns Series or DataFrame New Series or DataFrame with updated labels. See Also Series.add_suffix: Suffix row labels with string suffix . DataFrame.add_suffix: Suffix column labels with string suffix . Examples s = pd.Series([1, 2, 3, 4]) s 0 1 1 2 2 3 3 4 dtype: int64 s.add_prefix('item_') item_0 1 item_1 2 item_2 3 item_3 4 dtype: int64 df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]}) df A B 0 1 3 1 2 4 2 3 5 3 4 6 df.add_prefix('col_') col_A col_B 0 1 3 1 2 4 2 3 5 3 4 6 View Source def add_prefix ( self : FrameOrSeries , prefix : str ) -> FrameOrSeries : \"\"\" Prefix labels with string `prefix`. For Series, the row labels are prefixed. For DataFrame, the column labels are prefixed. Parameters ---------- prefix : str The string to add before each label. Returns ------- Series or DataFrame New Series or DataFrame with updated labels. See Also -------- Series.add_suffix: Suffix row labels with string `suffix`. DataFrame.add_suffix: Suffix column labels with string `suffix`. Examples -------- >>> s = pd.Series([1, 2, 3, 4]) >>> s 0 1 1 2 2 3 3 4 dtype: int64 >>> s.add_prefix('item_') item_0 1 item_1 2 item_2 3 item_3 4 dtype: int64 >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]}) >>> df A B 0 1 3 1 2 4 2 3 5 3 4 6 >>> df.add_prefix('col_') col_A col_B 0 1 3 1 2 4 2 3 5 3 4 6 \"\"\" f = functools . partial ( \"{prefix}{}\" . format , prefix = prefix ) mapper = { self . _info_axis_name : f } return self . rename ( ** mapper ) # type: ignore add_suffix def add_suffix ( self : ~ FrameOrSeries , suffix : str ) -> ~ FrameOrSeries Suffix labels with string suffix . For Series, the row labels are suffixed. For DataFrame, the column labels are suffixed. Parameters suffix : str The string to add after each label. Returns Series or DataFrame New Series or DataFrame with updated labels. See Also Series.add_prefix: Prefix row labels with string prefix . DataFrame.add_prefix: Prefix column labels with string prefix . Examples s = pd.Series([1, 2, 3, 4]) s 0 1 1 2 2 3 3 4 dtype: int64 s.add_suffix('_item') 0_item 1 1_item 2 2_item 3 3_item 4 dtype: int64 df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]}) df A B 0 1 3 1 2 4 2 3 5 3 4 6 df.add_suffix('_col') A_col B_col 0 1 3 1 2 4 2 3 5 3 4 6 View Source def add_suffix ( self : FrameOrSeries , suffix : str ) -> FrameOrSeries : \"\"\" Suffix labels with string `suffix`. For Series, the row labels are suffixed. For DataFrame, the column labels are suffixed. Parameters ---------- suffix : str The string to add after each label. Returns ------- Series or DataFrame New Series or DataFrame with updated labels. See Also -------- Series.add_prefix: Prefix row labels with string `prefix`. DataFrame.add_prefix: Prefix column labels with string `prefix`. Examples -------- >>> s = pd.Series([1, 2, 3, 4]) >>> s 0 1 1 2 2 3 3 4 dtype: int64 >>> s.add_suffix('_item') 0_item 1 1_item 2 2_item 3 3_item 4 dtype: int64 >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]}) >>> df A B 0 1 3 1 2 4 2 3 5 3 4 6 >>> df.add_suffix('_col') A_col B_col 0 1 3 1 2 4 2 3 5 3 4 6 \"\"\" f = functools . partial ( \"{}{suffix}\" . format , suffix = suffix ) mapper = { self . _info_axis_name : f } return self . rename ( ** mapper ) # type: ignore agg def agg ( self , func = None , axis = 0 , * args , ** kwargs ) Aggregate using one or more operations over the specified axis. .. versionadded:: 0.20.0 Parameters func : function, str, list or dict Function to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are : - function - string function name - list of functions and / or function names , e . g . `` [ np . sum , 'mean' ] `` - dict of axis labels -> functions , function names or list of such . axis : {0 or 'index', 1 or 'columns'}, default 0 If 0 or 'index': apply function to each column. If 1 or 'columns': apply function to each row. args Positional arguments to pass to func . *kwargs Keyword arguments to pass to func . Returns scalar, Series or DataFrame The return can be : * scalar : when Series . agg is called with single function * Series : when DataFrame . agg is called with a single function * DataFrame : when DataFrame . agg is called with several functions Return scalar , Series or DataFrame . The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions ( mean , median , prod , sum , std , var ), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0) . agg is an alias for aggregate . Use the alias. See Also DataFrame.apply : Perform any type of operations. DataFrame.transform : Perform transformation type operations. core.groupby.GroupBy : Perform operations over groups. core.resample.Resampler : Perform operations over resampled bins. core.window.Rolling : Perform operations over rolling window. core.window.Expanding : Perform operations over expanding window. core.window.ExponentialMovingWindow : Perform operation over exponential weighted window. Notes agg is an alias for aggregate . Use the alias. A passed user-defined-function will be passed a Series for evaluation. Examples df = pd.DataFrame([[1, 2, 3], ... [4, 5, 6], ... [7, 8, 9], ... [np.nan, np.nan, np.nan]], ... columns=['A', 'B', 'C']) Aggregate these functions over the rows. df.agg(['sum', 'min']) A B C sum 12.0 15.0 18.0 min 1.0 2.0 3.0 Different aggregations per column. df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']}) A B max NaN 8.0 min 1.0 2.0 sum 12.0 NaN Aggregate over the columns. df.agg(\"mean\", axis=\"columns\") 0 2.0 1 5.0 2 8.0 3 NaN dtype: float64 View Source @doc ( _shared_docs [ \"aggregate\" ] , klass = _shared_doc_kwargs [ \"klass\" ] , axis = _shared_doc_kwargs [ \"axis\" ] , see_also = _agg_summary_and_see_also_doc , examples = _agg_examples_doc , versionadded = \"\\n.. versionadded:: 0.20.0\\n\" , ) def aggregate ( self , func = None , axis = 0 , * args , ** kwargs ) : axis = self . _get_axis_number ( axis ) relabeling , func , columns , order = reconstruct_func ( func , ** kwargs ) result = None try : result , how = self . _aggregate ( func , axis = axis , * args , ** kwargs ) except TypeError as err : exc = TypeError ( \"DataFrame constructor called with \" f \"incompatible data and dtype: {err}\" ) raise exc from err if result is None : return self . apply ( func , axis = axis , args = args , ** kwargs ) if relabeling : # This is to keep the order to columns occurrence unchanged , and also # keep the order of new columns occurrence unchanged result_in_dict = relabel_result ( result , func , columns , order ) result = DataFrame ( result_in_dict , index = columns ) return result aggregate def aggregate ( self , func = None , axis = 0 , * args , ** kwargs ) Aggregate using one or more operations over the specified axis. .. versionadded:: 0.20.0 Parameters func : function, str, list or dict Function to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are : - function - string function name - list of functions and / or function names , e . g . `` [ np . sum , 'mean' ] `` - dict of axis labels -> functions , function names or list of such . axis : {0 or 'index', 1 or 'columns'}, default 0 If 0 or 'index': apply function to each column. If 1 or 'columns': apply function to each row. args Positional arguments to pass to func . *kwargs Keyword arguments to pass to func . Returns scalar, Series or DataFrame The return can be : * scalar : when Series . agg is called with single function * Series : when DataFrame . agg is called with a single function * DataFrame : when DataFrame . agg is called with several functions Return scalar , Series or DataFrame . The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions ( mean , median , prod , sum , std , var ), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0) . agg is an alias for aggregate . Use the alias. See Also DataFrame.apply : Perform any type of operations. DataFrame.transform : Perform transformation type operations. core.groupby.GroupBy : Perform operations over groups. core.resample.Resampler : Perform operations over resampled bins. core.window.Rolling : Perform operations over rolling window. core.window.Expanding : Perform operations over expanding window. core.window.ExponentialMovingWindow : Perform operation over exponential weighted window. Notes agg is an alias for aggregate . Use the alias. A passed user-defined-function will be passed a Series for evaluation. Examples df = pd.DataFrame([[1, 2, 3], ... [4, 5, 6], ... [7, 8, 9], ... [np.nan, np.nan, np.nan]], ... columns=['A', 'B', 'C']) Aggregate these functions over the rows. df.agg(['sum', 'min']) A B C sum 12.0 15.0 18.0 min 1.0 2.0 3.0 Different aggregations per column. df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']}) A B max NaN 8.0 min 1.0 2.0 sum 12.0 NaN Aggregate over the columns. df.agg(\"mean\", axis=\"columns\") 0 2.0 1 5.0 2 8.0 3 NaN dtype: float64 View Source @doc ( _shared_docs [ \"aggregate\" ] , klass = _shared_doc_kwargs [ \"klass\" ] , axis = _shared_doc_kwargs [ \"axis\" ] , see_also = _agg_summary_and_see_also_doc , examples = _agg_examples_doc , versionadded = \"\\n.. versionadded:: 0.20.0\\n\" , ) def aggregate ( self , func = None , axis = 0 , * args , ** kwargs ) : axis = self . _get_axis_number ( axis ) relabeling , func , columns , order = reconstruct_func ( func , ** kwargs ) result = None try : result , how = self . _aggregate ( func , axis = axis , * args , ** kwargs ) except TypeError as err : exc = TypeError ( \"DataFrame constructor called with \" f \"incompatible data and dtype: {err}\" ) raise exc from err if result is None : return self . apply ( func , axis = axis , args = args , ** kwargs ) if relabeling : # This is to keep the order to columns occurrence unchanged , and also # keep the order of new columns occurrence unchanged result_in_dict = relabel_result ( result , func , columns , order ) result = DataFrame ( result_in_dict , index = columns ) return result align def align ( self , other , join = 'outer' , axis = None , level = None , copy = True , fill_value = None , method = None , limit = None , fill_axis = 0 , broadcast_axis = None ) -> 'DataFrame' Align two objects on their axes with the specified join method. Join method is specified for each axis Index. Parameters other : DataFrame or Series join : {'outer', 'inner', 'left', 'right'}, default 'outer' axis : allowed axis of the other object, default None Align on index (0), columns (1), or both (None). level : int or level name, default None Broadcast across a level, matching Index values on the passed MultiIndex level. copy : bool, default True Always returns new objects. If copy=False and no reindexing is required then original objects are returned. fill_value : scalar, default np.NaN Value to use for missing values. Defaults to NaN, but can be any \"compatible\" value. method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None Method to use for filling holes in reindexed Series: - pad / ffill: propagate last valid observation forward to next valid. - backfill / bfill: use NEXT valid observation to fill gap. limit : int, default None If method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. fill_axis : {0 or 'index', 1 or 'columns'}, default 0 Filling axis, method and limit. broadcast_axis : {0 or 'index', 1 or 'columns'}, default None Broadcast values along this axis, if aligning two objects of different dimensions. Returns (left, right) : (DataFrame, type of other) Aligned objects. View Source @doc ( NDFrame . align , ** _shared_doc_kwargs ) def align ( self , other , join = \"outer\" , axis = None , level = None , copy = True , fill_value = None , method = None , limit = None , fill_axis = 0 , broadcast_axis = None , ) -> \"DataFrame\" : return super (). align ( other , join = join , axis = axis , level = level , copy = copy , fill_value = fill_value , method = method , limit = limit , fill_axis = fill_axis , broadcast_axis = broadcast_axis , ) all def all ( self , axis = 0 , bool_only = None , skipna = True , level = None , ** kwargs ) Return whether all elements are True, potentially over an axis. Returns True unless there at least one element within a series or along a Dataframe axis that is False or equivalent (e.g. zero or empty). Parameters axis : {0 or 'index', 1 or 'columns', None}, default 0 Indicate which axis or axes should be reduced. * 0 / 'index' : reduce the index, return a Series whose index is the original column labels. * 1 / 'columns' : reduce the columns, return a Series whose index is the original index. * None : reduce all axes, return a scalar. bool_only : bool, default None Include only boolean columns. If None, will attempt to use everything, then use only boolean data. Not implemented for Series. skipna : bool, default True Exclude NA/null values. If the entire row/column is NA and skipna is True, then the result will be True, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. **kwargs : any, default None Additional keywords have no effect but might be accepted for compatibility with NumPy. Returns Series or DataFrame If level is specified, then, DataFrame is returned; otherwise, Series is returned. See Also Series.all : Return True if all elements are True. DataFrame.any : Return True if one (or more) elements are True. Examples Series pd.Series([True, True]).all() True pd.Series([True, False]).all() False pd.Series([]).all() True pd.Series([np.nan]).all() True pd.Series([np.nan]).all(skipna=False) True DataFrames Create a dataframe from a dictionary. df = pd.DataFrame({'col1': [True, True], 'col2': [True, False]}) df col1 col2 0 True True 1 True False Default behaviour checks if column-wise values all return True. df.all() col1 True col2 False dtype: bool Specify axis='columns' to check if row-wise values all return True. df.all(axis='columns') 0 True 1 False dtype: bool Or axis=None for whether every value is True. df.all(axis=None) False View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , see_also = see_also , examples = examples , empty_value = empty_value , ) @Appender ( _bool_doc ) def logical_func ( self , axis = 0 , bool_only = None , skipna = True , level = None , ** kwargs ) : nv . validate_logical_func ( tuple (), kwargs , fname = name ) if level is not None : if bool_only is not None : raise NotImplementedError ( \"Option bool_only is not implemented with option level.\" ) return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = bool_only , filter_type = \"bool\" , ) any def any ( self , axis = 0 , bool_only = None , skipna = True , level = None , ** kwargs ) Return whether any element is True, potentially over an axis. Returns False unless there at least one element within a series or along a Dataframe axis that is True or equivalent (e.g. non-zero or non-empty). Parameters axis : {0 or 'index', 1 or 'columns', None}, default 0 Indicate which axis or axes should be reduced. * 0 / 'index' : reduce the index, return a Series whose index is the original column labels. * 1 / 'columns' : reduce the columns, return a Series whose index is the original index. * None : reduce all axes, return a scalar. bool_only : bool, default None Include only boolean columns. If None, will attempt to use everything, then use only boolean data. Not implemented for Series. skipna : bool, default True Exclude NA/null values. If the entire row/column is NA and skipna is True, then the result will be False, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. **kwargs : any, default None Additional keywords have no effect but might be accepted for compatibility with NumPy. Returns Series or DataFrame If level is specified, then, DataFrame is returned; otherwise, Series is returned. See Also numpy.any : Numpy version of this method. Series.any : Return whether any element is True. Series.all : Return whether all elements are True. DataFrame.any : Return whether any element is True over requested axis. DataFrame.all : Return whether all elements are True over requested axis. Examples Series For Series input, the output is a scalar indicating whether any element is True. pd.Series([False, False]).any() False pd.Series([True, False]).any() True pd.Series([]).any() False pd.Series([np.nan]).any() False pd.Series([np.nan]).any(skipna=False) True DataFrame Whether each column contains at least one True element (the default). df = pd.DataFrame({\"A\": [1, 2], \"B\": [0, 2], \"C\": [0, 0]}) df A B C 0 1 0 0 1 2 2 0 df.any() A True B True C False dtype: bool Aggregating over the columns. df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 2]}) df A B 0 True 1 1 False 2 df.any(axis='columns') 0 True 1 True dtype: bool df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 0]}) df A B 0 True 1 1 False 0 df.any(axis='columns') 0 True 1 False dtype: bool Aggregating over the entire DataFrame with axis=None . df.any(axis=None) True any for an empty DataFrame is an empty Series. pd.DataFrame([]).any() Series([], dtype: bool) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , see_also = see_also , examples = examples , empty_value = empty_value , ) @Appender ( _bool_doc ) def logical_func ( self , axis = 0 , bool_only = None , skipna = True , level = None , ** kwargs ) : nv . validate_logical_func ( tuple (), kwargs , fname = name ) if level is not None : if bool_only is not None : raise NotImplementedError ( \"Option bool_only is not implemented with option level.\" ) return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = bool_only , filter_type = \"bool\" , ) append def append ( self , other , ignore_index = False , verify_integrity = False , sort = False ) -> 'DataFrame' Append rows of other to the end of caller, returning a new object. Columns in other that are not in the caller are added as new columns. Parameters other : DataFrame or Series/dict-like object, or list of these The data to append. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. verify_integrity : bool, default False If True, raise ValueError on creating index with duplicates. sort : bool, default False Sort columns if the columns of self and other are not aligned. .. versionadded :: 0.23.0 .. versionchanged :: 1.0.0 Changed to not sort by default. Returns DataFrame See Also concat : General function to concatenate DataFrame or Series objects. Notes If a list of dict/series is passed and the keys are all contained in the DataFrame's index, the order of the columns in the resulting DataFrame will be unchanged. Iteratively appending rows to a DataFrame can be more computationally intensive than a single concatenate. A better solution is to append those rows to a list and then concatenate the list with the original DataFrame all at once. Examples df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB')) df A B 0 1 2 1 3 4 df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB')) df.append(df2) A B 0 1 2 1 3 4 0 5 6 1 7 8 With ignore_index set to True: df.append(df2, ignore_index=True) A B 0 1 2 1 3 4 2 5 6 3 7 8 The following, while not recommended methods for generating DataFrames, show two ways to generate a DataFrame from multiple data sources. Less efficient: df = pd.DataFrame(columns=['A']) for i in range(5): ... df = df.append({'A': i}, ignore_index=True) df A 0 0 1 1 2 2 3 3 4 4 More efficient: pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)], ... ignore_index=True) A 0 0 1 1 2 2 3 3 4 4 View Source def append ( self , other , ignore_index = False , verify_integrity = False , sort = False ) -> \"DataFrame\" : \"\"\" Append rows of `other` to the end of caller, returning a new object. Columns in `other` that are not in the caller are added as new columns. Parameters ---------- other : DataFrame or Series/dict-like object, or list of these The data to append. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. verify_integrity : bool, default False If True, raise ValueError on creating index with duplicates. sort : bool, default False Sort columns if the columns of `self` and `other` are not aligned. .. versionadded:: 0.23.0 .. versionchanged:: 1.0.0 Changed to not sort by default. Returns ------- DataFrame See Also -------- concat : General function to concatenate DataFrame or Series objects. Notes ----- If a list of dict/series is passed and the keys are all contained in the DataFrame's index, the order of the columns in the resulting DataFrame will be unchanged. Iteratively appending rows to a DataFrame can be more computationally intensive than a single concatenate. A better solution is to append those rows to a list and then concatenate the list with the original DataFrame all at once. Examples -------- >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB')) >>> df A B 0 1 2 1 3 4 >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB')) >>> df.append(df2) A B 0 1 2 1 3 4 0 5 6 1 7 8 With `ignore_index` set to True: >>> df.append(df2, ignore_index=True) A B 0 1 2 1 3 4 2 5 6 3 7 8 The following, while not recommended methods for generating DataFrames, show two ways to generate a DataFrame from multiple data sources. Less efficient: >>> df = pd.DataFrame(columns=['A']) >>> for i in range(5): ... df = df.append({'A': i}, ignore_index=True) >>> df A 0 0 1 1 2 2 3 3 4 4 More efficient: >>> pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)], ... ignore_index=True) A 0 0 1 1 2 2 3 3 4 4 \"\"\" if isinstance ( other , ( Series , dict )): if isinstance ( other , dict ): if not ignore_index : raise TypeError ( \"Can only append a dict if ignore_index=True\" ) other = Series ( other ) if other . name is None and not ignore_index : raise TypeError ( \"Can only append a Series if ignore_index=True \" \"or if the Series has a name\" ) index = Index ([ other . name ], name = self . index . name ) idx_diff = other . index . difference ( self . columns ) try : combined_columns = self . columns . append ( idx_diff ) except TypeError : combined_columns = self . columns . astype ( object ). append ( idx_diff ) other = ( other . reindex ( combined_columns , copy = False ) . to_frame () . T . infer_objects () . rename_axis ( index . names , copy = False ) ) if not self . columns . equals ( combined_columns ): self = self . reindex ( columns = combined_columns ) elif isinstance ( other , list ): if not other : pass elif not isinstance ( other [ 0 ], DataFrame ): other = DataFrame ( other ) if ( self . columns . get_indexer ( other . columns ) >= 0 ). all (): other = other . reindex ( columns = self . columns ) from pandas . core . reshape . concat import concat if isinstance ( other , ( list , tuple )): to_concat = [ self , * other ] else : to_concat = [ self , other ] return concat ( to_concat , ignore_index = ignore_index , verify_integrity = verify_integrity , sort = sort , ) apply def apply ( self , func , axis = 0 , raw = False , result_type = None , args = (), ** kwds ) Apply a function along an axis of the DataFrame. Objects passed to the function are Series objects whose index is either the DataFrame's index ( axis=0 ) or the DataFrame's columns ( axis=1 ). By default ( result_type=None ), the final return type is inferred from the return type of the applied function. Otherwise, it depends on the result_type argument. Parameters func : function Function to apply to each column or row. axis : {0 or 'index', 1 or 'columns'}, default 0 Axis along which the function is applied: * 0 or 'index': apply function to each column. * 1 or 'columns': apply function to each row. raw : bool, default False Determines if row or column is passed as a Series or ndarray object: * `` False `` : passes each row or column as a Series to the function . * `` True `` : the passed function will receive ndarray objects instead . If you are just applying a NumPy reduction function this will achieve much better performance . result_type : {'expand', 'reduce', 'broadcast', None}, default None These only act when axis=1 (columns): * 'expand' : list - like results will be turned into columns . * 'reduce' : returns a Series if possible rather than expanding list - like results . This is the opposite of 'expand' . * 'broadcast' : results will be broadcast to the original shape of the DataFrame , the original index and columns will be retained . The default behaviour ( None ) depends on the return value of the applied function: list - like results will be returned as a Series of those . However if the apply function returns a Series these are expanded to columns . .. versionadded:: 0.23.0 args : tuple Positional arguments to pass to func in addition to the array/series. **kwds Additional keyword arguments to pass as keywords arguments to func . Returns Series or DataFrame Result of applying func along the given axis of the DataFrame. See Also DataFrame.applymap: For elementwise operations. DataFrame.aggregate: Only perform aggregating type operations. DataFrame.transform: Only perform transforming type operations. Examples df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B']) df A B 0 4 9 1 4 9 2 4 9 Using a numpy universal function (in this case the same as np.sqrt(df) ): df.apply(np.sqrt) A B 0 2.0 3.0 1 2.0 3.0 2 2.0 3.0 Using a reducing function on either axis df.apply(np.sum, axis=0) A 12 B 27 dtype: int64 df.apply(np.sum, axis=1) 0 13 1 13 2 13 dtype: int64 Returning a list-like will result in a Series df.apply(lambda x: [1, 2], axis=1) 0 [1, 2] 1 [1, 2] 2 [1, 2] dtype: object Passing result_type='expand' will expand list-like results to columns of a Dataframe df.apply(lambda x: [1, 2], axis=1, result_type='expand') 0 1 0 1 2 1 1 2 2 1 2 Returning a Series inside the function is similar to passing result_type='expand' . The resulting column names will be the Series index. df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1) foo bar 0 1 2 1 1 2 2 1 2 Passing result_type='broadcast' will ensure the same shape result, whether list-like or scalar is returned by the function, and broadcast it along the axis. The resulting column names will be the originals. df.apply(lambda x: [1, 2], axis=1, result_type='broadcast') A B 0 1 2 1 1 2 2 1 2 View Source def apply ( self , func , axis = 0 , raw = False , result_type = None , args = (), ** kwds ): \"\"\" Apply a function along an axis of the DataFrame. Objects passed to the function are Series objects whose index is either the DataFrame's index (``axis=0``) or the DataFrame's columns (``axis=1``). By default (``result_type=None``), the final return type is inferred from the return type of the applied function. Otherwise, it depends on the `result_type` argument. Parameters ---------- func : function Function to apply to each column or row. axis : {0 or 'index', 1 or 'columns'}, default 0 Axis along which the function is applied: * 0 or 'index': apply function to each column. * 1 or 'columns': apply function to each row. raw : bool, default False Determines if row or column is passed as a Series or ndarray object: * ``False`` : passes each row or column as a Series to the function. * ``True`` : the passed function will receive ndarray objects instead. If you are just applying a NumPy reduction function this will achieve much better performance. result_type : {'expand', 'reduce', 'broadcast', None}, default None These only act when ``axis=1`` (columns): * 'expand' : list-like results will be turned into columns. * 'reduce' : returns a Series if possible rather than expanding list-like results. This is the opposite of 'expand'. * 'broadcast' : results will be broadcast to the original shape of the DataFrame, the original index and columns will be retained. The default behaviour (None) depends on the return value of the applied function: list-like results will be returned as a Series of those. However if the apply function returns a Series these are expanded to columns. .. versionadded:: 0.23.0 args : tuple Positional arguments to pass to `func` in addition to the array/series. **kwds Additional keyword arguments to pass as keywords arguments to `func`. Returns ------- Series or DataFrame Result of applying ``func`` along the given axis of the DataFrame. See Also -------- DataFrame.applymap: For elementwise operations. DataFrame.aggregate: Only perform aggregating type operations. DataFrame.transform: Only perform transforming type operations. Examples -------- >>> df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B']) >>> df A B 0 4 9 1 4 9 2 4 9 Using a numpy universal function (in this case the same as ``np.sqrt(df)``): >>> df.apply(np.sqrt) A B 0 2.0 3.0 1 2.0 3.0 2 2.0 3.0 Using a reducing function on either axis >>> df.apply(np.sum, axis=0) A 12 B 27 dtype: int64 >>> df.apply(np.sum, axis=1) 0 13 1 13 2 13 dtype: int64 Returning a list-like will result in a Series >>> df.apply(lambda x: [1, 2], axis=1) 0 [1, 2] 1 [1, 2] 2 [1, 2] dtype: object Passing ``result_type='expand'`` will expand list-like results to columns of a Dataframe >>> df.apply(lambda x: [1, 2], axis=1, result_type='expand') 0 1 0 1 2 1 1 2 2 1 2 Returning a Series inside the function is similar to passing ``result_type='expand'``. The resulting column names will be the Series index. >>> df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1) foo bar 0 1 2 1 1 2 2 1 2 Passing ``result_type='broadcast'`` will ensure the same shape result, whether list-like or scalar is returned by the function, and broadcast it along the axis. The resulting column names will be the originals. >>> df.apply(lambda x: [1, 2], axis=1, result_type='broadcast') A B 0 1 2 1 1 2 2 1 2 \"\"\" from pandas . core . apply import frame_apply op = frame_apply ( self , func = func , axis = axis , raw = raw , result_type = result_type , args = args , kwds = kwds , ) return op . get_result () applymap def applymap ( self , func ) -> 'DataFrame' Apply a function to a Dataframe elementwise. This method applies a function that accepts and returns a scalar to every element of a DataFrame. Parameters func : callable Python function, returns a single value from a single value. Returns DataFrame Transformed DataFrame. See Also DataFrame.apply : Apply a function along input axis of DataFrame. Examples df = pd.DataFrame([[1, 2.12], [3.356, 4.567]]) df 0 1 0 1.000 2.120 1 3.356 4.567 df.applymap(lambda x: len(str(x))) 0 1 0 3 4 1 5 5 Note that a vectorized version of func often exists, which will be much faster. You could square each number elementwise. df.applymap(lambda x: x**2) 0 1 0 1.000000 4.494400 1 11.262736 20.857489 But it's better to avoid applymap in that case. df ** 2 0 1 0 1.000000 4.494400 1 11.262736 20.857489 View Source def applymap ( self , func ) -> \"DataFrame\" : \"\"\" Apply a function to a Dataframe elementwise. This method applies a function that accepts and returns a scalar to every element of a DataFrame. Parameters ---------- func : callable Python function, returns a single value from a single value. Returns ------- DataFrame Transformed DataFrame. See Also -------- DataFrame.apply : Apply a function along input axis of DataFrame. Examples -------- >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]]) >>> df 0 1 0 1.000 2.120 1 3.356 4.567 >>> df.applymap(lambda x: len(str(x))) 0 1 0 3 4 1 5 5 Note that a vectorized version of `func` often exists, which will be much faster. You could square each number elementwise. >>> df.applymap(lambda x: x**2) 0 1 0 1.000000 4.494400 1 11.262736 20.857489 But it's better to avoid applymap in that case. >>> df ** 2 0 1 0 1.000000 4.494400 1 11.262736 20.857489 \"\"\" # if we have a dtype == 'M8[ns]', provide boxed values def infer ( x ): if x . empty : return lib . map_infer ( x , func ) return lib . map_infer ( x . astype ( object ). _values , func ) return self . apply ( infer ) asfreq def asfreq ( self : ~ FrameOrSeries , freq , method = None , how : Union [ str , NoneType ] = None , normalize : bool = False , fill_value = None ) -> ~ FrameOrSeries Convert TimeSeries to specified frequency. Optionally provide filling method to pad/backfill missing values. Returns the original data conformed to a new index with the specified frequency. resample is more appropriate if an operation, such as summarization, is necessary to represent the data at the new frequency. Parameters freq : DateOffset or str Frequency DateOffset or string. method : {'backfill'/'bfill', 'pad'/'ffill'}, default None Method to use for filling holes in reindexed Series (note this does not fill NaNs that already were present): * 'pad' / 'ffill': propagate last valid observation forward to next valid * 'backfill' / 'bfill': use NEXT valid observation to fill. how : {'start', 'end'}, default end For PeriodIndex only (see PeriodIndex.asfreq). normalize : bool, default False Whether to reset output index to midnight. fill_value : scalar, optional Value to use for missing values, applied during upsampling (note this does not fill NaNs that already were present). Returns Same type as caller Object converted to the specified frequency. See Also reindex : Conform DataFrame to new index with optional filling logic. Notes To learn more about the frequency strings, please see this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases> __. Examples Start by creating a series with 4 one minute timestamps. index = pd.date_range('1/1/2000', periods=4, freq='T') series = pd.Series([0.0, None, 2.0, 3.0], index=index) df = pd.DataFrame({'s':series}) df s 2000-01-01 00:00:00 0.0 2000-01-01 00:01:00 NaN 2000-01-01 00:02:00 2.0 2000-01-01 00:03:00 3.0 Upsample the series into 30 second bins. df.asfreq(freq='30S') s 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 NaN 2000-01-01 00:01:00 NaN 2000-01-01 00:01:30 NaN 2000-01-01 00:02:00 2.0 2000-01-01 00:02:30 NaN 2000-01-01 00:03:00 3.0 Upsample again, providing a fill value . df.asfreq(freq='30S', fill_value=9.0) s 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 9.0 2000-01-01 00:01:00 NaN 2000-01-01 00:01:30 9.0 2000-01-01 00:02:00 2.0 2000-01-01 00:02:30 9.0 2000-01-01 00:03:00 3.0 Upsample again, providing a method . df.asfreq(freq='30S', method='bfill') s 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 NaN 2000-01-01 00:01:00 NaN 2000-01-01 00:01:30 2.0 2000-01-01 00:02:00 2.0 2000-01-01 00:02:30 3.0 2000-01-01 00:03:00 3.0 View Source def asfreq ( self : FrameOrSeries , freq , method = None , how : Optional [ str ] = None , normalize : bool_t = False , fill_value = None , ) -> FrameOrSeries : \"\"\" Convert TimeSeries to specified frequency. Optionally provide filling method to pad/backfill missing values. Returns the original data conformed to a new index with the specified frequency. ``resample`` is more appropriate if an operation, such as summarization, is necessary to represent the data at the new frequency. Parameters ---------- freq : DateOffset or str Frequency DateOffset or string. method : {'backfill'/'bfill', 'pad'/'ffill'}, default None Method to use for filling holes in reindexed Series (note this does not fill NaNs that already were present): * 'pad' / 'ffill': propagate last valid observation forward to next valid * 'backfill' / 'bfill': use NEXT valid observation to fill. how : {'start', 'end'}, default end For PeriodIndex only (see PeriodIndex.asfreq). normalize : bool, default False Whether to reset output index to midnight. fill_value : scalar, optional Value to use for missing values, applied during upsampling (note this does not fill NaNs that already were present). Returns ------- Same type as caller Object converted to the specified frequency. See Also -------- reindex : Conform DataFrame to new index with optional filling logic. Notes ----- To learn more about the frequency strings, please see `this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__. Examples -------- Start by creating a series with 4 one minute timestamps. >>> index = pd.date_range('1/1/2000', periods=4, freq='T') >>> series = pd.Series([0.0, None, 2.0, 3.0], index=index) >>> df = pd.DataFrame({'s':series}) >>> df s 2000-01-01 00:00:00 0.0 2000-01-01 00:01:00 NaN 2000-01-01 00:02:00 2.0 2000-01-01 00:03:00 3.0 Upsample the series into 30 second bins. >>> df.asfreq(freq='30S') s 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 NaN 2000-01-01 00:01:00 NaN 2000-01-01 00:01:30 NaN 2000-01-01 00:02:00 2.0 2000-01-01 00:02:30 NaN 2000-01-01 00:03:00 3.0 Upsample again, providing a ``fill value``. >>> df.asfreq(freq='30S', fill_value=9.0) s 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 9.0 2000-01-01 00:01:00 NaN 2000-01-01 00:01:30 9.0 2000-01-01 00:02:00 2.0 2000-01-01 00:02:30 9.0 2000-01-01 00:03:00 3.0 Upsample again, providing a ``method``. >>> df.asfreq(freq='30S', method='bfill') s 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 NaN 2000-01-01 00:01:00 NaN 2000-01-01 00:01:30 2.0 2000-01-01 00:02:00 2.0 2000-01-01 00:02:30 3.0 2000-01-01 00:03:00 3.0 \"\"\" from pandas . core . resample import asfreq return asfreq ( self , freq , method = method , how = how , normalize = normalize , fill_value = fill_value , ) asof def asof ( self , where , subset = None ) Return the last row(s) without any NaNs before where . The last row (for each element in where , if list) without any NaN is taken. In case of a :class: ~pandas.DataFrame , the last row without NaN considering only the subset of columns (if not None ) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame Parameters where : date or array-like of dates Date(s) before which the last row(s) are returned. subset : str or array-like of str, default None For DataFrame, if not None , only use these columns to check for NaNs. Returns scalar, Series, or DataFrame The return can be : * scalar : when `self` is a Series and `where` is a scalar * Series : when `self` is a Series and `where` is an array - like , or when `self` is a DataFrame and `where` is a scalar * DataFrame : when `self` is a DataFrame and `where` is an array - like Return scalar , Series , or DataFrame . See Also merge_asof : Perform an asof merge. Similar to left join. Notes Dates are assumed to be sorted. Raises if this is not the case. Examples A Series and a scalar where . s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40]) s 10 1.0 20 2.0 30 NaN 40 4.0 dtype: float64 s.asof(20) 2.0 For a sequence where , a Series is returned. The first value is NaN, because the first element of where is before the first index value. s.asof([5, 20]) 5 NaN 20 2.0 dtype: float64 Missing values are not considered. The following is 2.0 , not NaN, even though NaN is at the index location for 30 . s.asof(30) 2.0 Take all columns into consideration df = pd.DataFrame({'a': [10, 20, 30, 40, 50], ... 'b': [None, None, None, None, 500]}, ... index=pd.DatetimeIndex(['2018-02-27 09:01:00', ... '2018-02-27 09:02:00', ... '2018-02-27 09:03:00', ... '2018-02-27 09:04:00', ... '2018-02-27 09:05:00'])) df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30', ... '2018-02-27 09:04:30'])) a b 2018-02-27 09:03:30 NaN NaN 2018-02-27 09:04:30 NaN NaN Take a single column into consideration df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30', ... '2018-02-27 09:04:30']), ... subset=['a']) a b 2018-02-27 09:03:30 30.0 NaN 2018-02-27 09:04:30 40.0 NaN View Source def asof ( self , where , subset = None ): \"\"\" Return the last row(s) without any NaNs before `where`. The last row (for each element in `where`, if list) without any NaN is taken. In case of a :class:`~pandas.DataFrame`, the last row without NaN considering only the subset of columns (if not `None`) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame Parameters ---------- where : date or array-like of dates Date(s) before which the last row(s) are returned. subset : str or array-like of str, default `None` For DataFrame, if not `None`, only use these columns to check for NaNs. Returns ------- scalar, Series, or DataFrame The return can be: * scalar : when `self` is a Series and `where` is a scalar * Series: when `self` is a Series and `where` is an array-like, or when `self` is a DataFrame and `where` is a scalar * DataFrame : when `self` is a DataFrame and `where` is an array-like Return scalar, Series, or DataFrame. See Also -------- merge_asof : Perform an asof merge. Similar to left join. Notes ----- Dates are assumed to be sorted. Raises if this is not the case. Examples -------- A Series and a scalar `where`. >>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40]) >>> s 10 1.0 20 2.0 30 NaN 40 4.0 dtype: float64 >>> s.asof(20) 2.0 For a sequence `where`, a Series is returned. The first value is NaN, because the first element of `where` is before the first index value. >>> s.asof([5, 20]) 5 NaN 20 2.0 dtype: float64 Missing values are not considered. The following is ``2.0``, not NaN, even though NaN is at the index location for ``30``. >>> s.asof(30) 2.0 Take all columns into consideration >>> df = pd.DataFrame({'a': [10, 20, 30, 40, 50], ... 'b': [None, None, None, None, 500]}, ... index=pd.DatetimeIndex(['2018-02-27 09:01:00', ... '2018-02-27 09:02:00', ... '2018-02-27 09:03:00', ... '2018-02-27 09:04:00', ... '2018-02-27 09:05:00'])) >>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30', ... '2018-02-27 09:04:30'])) a b 2018-02-27 09:03:30 NaN NaN 2018-02-27 09:04:30 NaN NaN Take a single column into consideration >>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30', ... '2018-02-27 09:04:30']), ... subset=['a']) a b 2018-02-27 09:03:30 30.0 NaN 2018-02-27 09:04:30 40.0 NaN \"\"\" if isinstance ( where , str ): where = Timestamp ( where ) if not self . index . is_monotonic : raise ValueError ( \"asof requires a sorted index\" ) is_series = isinstance ( self , ABCSeries ) if is_series : if subset is not None : raise ValueError ( \"subset is not valid for Series\" ) else : if subset is None : subset = self . columns if not is_list_like ( subset ): subset = [ subset ] is_list = is_list_like ( where ) if not is_list : start = self . index [ 0 ] if isinstance ( self . index , PeriodIndex ): where = Period ( where , freq = self . index . freq ) if where < start : if not is_series : return self . _constructor_sliced ( index = self . columns , name = where , dtype = np . float64 ) return np . nan # It's always much faster to use a *while* loop here for # Series than pre-computing all the NAs. However a # *while* loop is extremely expensive for DataFrame # so we later pre-compute all the NAs and use the same # code path whether *where* is a scalar or list. # See PR: https://github.com/pandas-dev/pandas/pull/14476 if is_series : loc = self . index . searchsorted ( where , side = \"right\" ) if loc > 0 : loc -= 1 values = self . _values while loc > 0 and isna ( values [ loc ]): loc -= 1 return values [ loc ] if not isinstance ( where , Index ): where = Index ( where ) if is_list else Index ([ where ]) null s = self . isna () if is_series else self [ subset ]. isna (). any ( 1 ) if null s . all (): if is_series : return self . _constructor ( np . nan , index = where , name = self . name ) elif is_list : return self . _constructor ( np . nan , index = where , columns = self . columns ) else : return self . _constructor_sliced ( np . nan , index = self . columns , name = where [ 0 ] ) locs = self . index . asof_locs ( where , ~ ( null s . _values )) # mask the missing missing = locs == - 1 data = self . take ( locs ) data . index = where data . loc [ missing ] = np . nan return data if is_list else data . iloc [ - 1 ] assign def assign ( self , ** kwargs ) -> 'DataFrame' Assign new columns to a DataFrame. Returns a new object with all original columns in addition to new ones. Existing columns that are re-assigned will be overwritten. Parameters **kwargs : dict of {str: callable or Series} The column names are keywords. If the values are callable, they are computed on the DataFrame and assigned to the new columns. The callable must not change input DataFrame (though pandas doesn't check it). If the values are not callable, (e.g. a Series, scalar, or array), they are simply assigned. Returns DataFrame A new DataFrame with the new columns in addition to all the existing columns. Notes Assigning multiple columns within the same assign is possible. Later items in '**kwargs' may refer to newly created or modified columns in 'df'; items are computed and assigned into 'df' in order. .. versionchanged:: 0.23.0 Keyword argument order is maintained. Examples df = pd.DataFrame({'temp_c': [17.0, 25.0]}, ... index=['Portland', 'Berkeley']) df temp_c Portland 17.0 Berkeley 25.0 Where the value is a callable, evaluated on df : df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32) temp_c temp_f Portland 17.0 62.6 Berkeley 25.0 77.0 Alternatively, the same behavior can be achieved by directly referencing an existing Series or sequence: df.assign(temp_f=df['temp_c'] * 9 / 5 + 32) temp_c temp_f Portland 17.0 62.6 Berkeley 25.0 77.0 You can create multiple columns within the same assign where one of the columns depends on another one defined within the same assign: df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32, ... temp_k=lambda x: (x['temp_f'] + 459.67) * 5 / 9) temp_c temp_f temp_k Portland 17.0 62.6 290.15 Berkeley 25.0 77.0 298.15 View Source def assign ( self , ** kwargs ) -> \"DataFrame\" : r \"\"\" Assign new columns to a DataFrame. Returns a new object with all original columns in addition to new ones. Existing columns that are re-assigned will be overwritten. Parameters ---------- **kwargs : dict of {str: callable or Series} The column names are keywords. If the values are callable, they are computed on the DataFrame and assigned to the new columns. The callable must not change input DataFrame (though pandas doesn't check it). If the values are not callable, (e.g. a Series, scalar, or array), they are simply assigned. Returns ------- DataFrame A new DataFrame with the new columns in addition to all the existing columns. Notes ----- Assigning multiple columns within the same ``assign`` is possible. Later items in '\\*\\*kwargs' may refer to newly created or modified columns in 'df'; items are computed and assigned into 'df' in order. .. versionchanged:: 0.23.0 Keyword argument order is maintained. Examples -------- >>> df = pd.DataFrame({'temp_c': [17.0, 25.0]}, ... index=['Portland', 'Berkeley']) >>> df temp_c Portland 17.0 Berkeley 25.0 Where the value is a callable, evaluated on `df`: >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32) temp_c temp_f Portland 17.0 62.6 Berkeley 25.0 77.0 Alternatively, the same behavior can be achieved by directly referencing an existing Series or sequence: >>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32) temp_c temp_f Portland 17.0 62.6 Berkeley 25.0 77.0 You can create multiple columns within the same assign where one of the columns depends on another one defined within the same assign: >>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32, ... temp_k=lambda x: (x['temp_f'] + 459.67) * 5 / 9) temp_c temp_f temp_k Portland 17.0 62.6 290.15 Berkeley 25.0 77.0 298.15 \"\"\" data = self . copy () for k , v in kwargs . items (): data [ k ] = com . apply_if_callable ( v , data ) return data astype def astype ( self : ~ FrameOrSeries , dtype , copy : bool = True , errors : str = 'raise' ) -> ~ FrameOrSeries Cast a pandas object to a specified dtype dtype . Parameters dtype : data type, or dict of column name -> data type Use a numpy.dtype or Python type to cast entire pandas object to the same type. Alternatively, use {col: dtype, ...}, where col is a column label and dtype is a numpy.dtype or Python type to cast one or more of the DataFrame's columns to column-specific types. copy : bool, default True Return a copy when copy=True (be very careful setting copy=False as changes to values then may propagate to other pandas objects). errors : {'raise', 'ignore'}, default 'raise' Control raising of exceptions on invalid data for provided dtype. - `` raise `` : allow exceptions to be raised - `` ignore `` : suppress exceptions . On error return original object . Returns casted : same type as caller See Also to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type. numpy.ndarray.astype : Cast a numpy array to a specified type. Examples Create a DataFrame: d = {'col1': [1, 2], 'col2': [3, 4]} df = pd.DataFrame(data=d) df.dtypes col1 int64 col2 int64 dtype: object Cast all columns to int32: df.astype('int32').dtypes col1 int32 col2 int32 dtype: object Cast col1 to int32 using a dictionary: df.astype({'col1': 'int32'}).dtypes col1 int32 col2 int64 dtype: object Create a series: ser = pd.Series([1, 2], dtype='int32') ser 0 1 1 2 dtype: int32 ser.astype('int64') 0 1 1 2 dtype: int64 Convert to categorical type: ser.astype('category') 0 1 1 2 dtype: category Categories (2, int64): [1, 2] Convert to ordered categorical type with custom ordering: cat_dtype = pd.api.types.CategoricalDtype( ... categories=[2, 1], ordered=True) ser.astype(cat_dtype) 0 1 1 2 dtype: category Categories (2, int64): [2 < 1] Note that using copy=False and changing data on a new pandas object may propagate changes: s1 = pd.Series([1, 2]) s2 = s1.astype('int64', copy=False) s2[0] = 10 s1 # note that s1[0] has changed too 0 10 1 2 dtype: int64 Create a series of dates: ser_date = pd.Series(pd.date_range('20200101', periods=3)) ser_date 0 2020-01-01 1 2020-01-02 2 2020-01-03 dtype: datetime64[ns] Datetimes are localized to UTC first before converting to the specified timezone: ser_date.astype('datetime64[ns, US/Eastern]') 0 2019-12-31 19:00:00-05:00 1 2020-01-01 19:00:00-05:00 2 2020-01-02 19:00:00-05:00 dtype: datetime64[ns, US/Eastern] View Source def astype ( self : FrameOrSeries , dtype , copy : bool_t = True , errors : str = \"raise\" ) -> FrameOrSeries : \"\"\" Cast a pandas object to a specified dtype ``dtype``. Parameters ---------- dtype : data type, or dict of column name -> data type Use a numpy.dtype or Python type to cast entire pandas object to the same type. Alternatively, use {col: dtype, ...}, where col is a column label and dtype is a numpy.dtype or Python type to cast one or more of the DataFrame's columns to column-specific types. copy : bool, default True Return a copy when ``copy=True`` (be very careful setting ``copy=False`` as changes to values then may propagate to other pandas objects). errors : {'raise', 'ignore'}, default 'raise' Control raising of exceptions on invalid data for provided dtype. - ``raise`` : allow exceptions to be raised - ``ignore`` : suppress exceptions. On error return original object. Returns ------- casted : same type as caller See Also -------- to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type. numpy.ndarray.astype : Cast a numpy array to a specified type. Examples -------- Create a DataFrame: >>> d = {'col1': [1, 2], 'col2': [3, 4]} >>> df = pd.DataFrame(data=d) >>> df.dtypes col1 int64 col2 int64 dtype: object Cast all columns to int32: >>> df.astype('int32').dtypes col1 int32 col2 int32 dtype: object Cast col1 to int32 using a dictionary: >>> df.astype({'col1': 'int32'}).dtypes col1 int32 col2 int64 dtype: object Create a series: >>> ser = pd.Series([1, 2], dtype='int32') >>> ser 0 1 1 2 dtype: int32 >>> ser.astype('int64') 0 1 1 2 dtype: int64 Convert to categorical type: >>> ser.astype('category') 0 1 1 2 dtype: category Categories (2, int64): [1, 2] Convert to ordered categorical type with custom ordering: >>> cat_dtype = pd.api.types.CategoricalDtype( ... categories=[2, 1], ordered=True) >>> ser.astype(cat_dtype) 0 1 1 2 dtype: category Categories (2, int64): [2 < 1] Note that using ``copy=False`` and changing data on a new pandas object may propagate changes: >>> s1 = pd.Series([1, 2]) >>> s2 = s1.astype('int64', copy=False) >>> s2[0] = 10 >>> s1 # note that s1[0] has changed too 0 10 1 2 dtype: int64 Create a series of dates: >>> ser_date = pd.Series(pd.date_range('20200101', periods=3)) >>> ser_date 0 2020-01-01 1 2020-01-02 2 2020-01-03 dtype: datetime64[ns] Datetimes are localized to UTC first before converting to the specified timezone: >>> ser_date.astype('datetime64[ns, US/Eastern]') 0 2019-12-31 19:00:00-05:00 1 2020-01-01 19:00:00-05:00 2 2020-01-02 19:00:00-05:00 dtype: datetime64[ns, US/Eastern] \"\"\" if is_dict_like ( dtype ): if self . ndim == 1 : # i.e. Series if len ( dtype ) > 1 or self . name not in dtype : raise KeyError ( \"Only the Series name can be used for \" \"the key in Series dtype mappings.\" ) new_type = dtype [ self . name ] return self . astype ( new_type , copy , errors ) for col_name in dtype . keys (): if col_name not in self : raise KeyError ( \"Only a column name can be used for the \" \"key in a dtype mappings argument.\" ) results = [] for col_name , col in self . items (): if col_name in dtype : results . append ( col . astype ( dtype = dtype [ col_name ], copy = copy , errors = errors ) ) else : results . append ( col . copy () if copy else col ) elif is_extension_array_dtype ( dtype ) and self . ndim > 1 : # GH 18099/22869: columnwise conversion to extension dtype # GH 24704: use iloc to handle duplicate column names results = [ self . iloc [:, i ]. astype ( dtype , copy = copy ) for i in range ( len ( self . columns )) ] else : # else, only a single dtype is given new_data = self . _mgr . astype ( dtype = dtype , copy = copy , errors = errors ,) return self . _constructor ( new_data ). __finalize__ ( self , method = \"astype\" ) # GH 33113: handle empty frame or series if not results : return self . copy () # GH 19920: retain column metadata after concat result = pd . concat ( results , axis = 1 , copy = False ) result . columns = self . columns return result at_time def at_time ( self : ~ FrameOrSeries , time , asof : bool = False , axis = None ) -> ~ FrameOrSeries Select values at particular time of day (e.g., 9:30AM). Parameters time : datetime.time or str axis : {0 or 'index', 1 or 'columns'}, default 0 .. versionadded :: 0.24.0 Returns Series or DataFrame Raises TypeError If the index is not a :class: DatetimeIndex See Also between_time : Select values between particular times of the day. first : Select initial periods of time series based on a date offset. last : Select final periods of time series based on a date offset. DatetimeIndex.indexer_at_time : Get just the index locations for values at particular time of the day. Examples i = pd.date_range('2018-04-09', periods=4, freq='12H') ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) ts A 2018-04-09 00:00:00 1 2018-04-09 12:00:00 2 2018-04-10 00:00:00 3 2018-04-10 12:00:00 4 ts.at_time('12:00') A 2018-04-09 12:00:00 2 2018-04-10 12:00:00 4 View Source def at_time ( self : FrameOrSeries , time , asof : bool_t = False , axis = None ) -> FrameOrSeries : \"\"\" Select values at particular time of day (e.g., 9:30AM). Parameters ---------- time : datetime.time or str axis : {0 or 'index', 1 or 'columns'}, default 0 .. versionadded:: 0.24.0 Returns ------- Series or DataFrame Raises ------ TypeError If the index is not a :class:`DatetimeIndex` See Also -------- between_time : Select values between particular times of the day. first : Select initial periods of time series based on a date offset. last : Select final periods of time series based on a date offset. DatetimeIndex.indexer_at_time : Get just the index locations for values at particular time of the day. Examples -------- >>> i = pd.date_range('2018-04-09', periods=4, freq='12H') >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) >>> ts A 2018-04-09 00:00:00 1 2018-04-09 12:00:00 2 2018-04-10 00:00:00 3 2018-04-10 12:00:00 4 >>> ts.at_time('12:00') A 2018-04-09 12:00:00 2 2018-04-10 12:00:00 4 \"\"\" if axis is None : axis = self . _stat_axis_number axis = self . _get_axis_number ( axis ) index = self . _get_axis ( axis ) if not isinstance ( index , DatetimeIndex ): raise TypeError ( \"Index must be DatetimeIndex\" ) indexer = index . indexer_at_time ( time , asof = asof ) return self . _take_with_is_copy ( indexer , axis = axis ) backfill def backfill ( self : ~ FrameOrSeries , axis = None , inplace : bool = False , limit = None , downcast = None ) -> Union [ ~ FrameOrSeries , NoneType ] Synonym for :meth: DataFrame.fillna with method='bfill' . Returns {klass} or None Object with missing values filled or None if inplace=True . View Source def bfill ( self : FrameOrSeries , axis = None , inplace : bool_t = False , limit = None , downcast = None , ) -> Optional [ FrameOrSeries ] : \"\"\" Synonym for :meth:`DataFrame.fillna` with ``method='bfill'``. Returns ------- {klass} or None Object with missing values filled or None if ``inplace=True``. \"\"\" return self . fillna ( method = \"bfill\" , axis = axis , inplace = inplace , limit = limit , downcast = downcast ) between_time def between_time ( self : ~ FrameOrSeries , start_time , end_time , include_start : bool = True , include_end : bool = True , axis = None ) -> ~ FrameOrSeries Select values between particular times of the day (e.g., 9:00-9:30 AM). By setting start_time to be later than end_time , you can get the times that are not between the two times. Parameters start_time : datetime.time or str Initial time as a time filter limit. end_time : datetime.time or str End time as a time filter limit. include_start : bool, default True Whether the start time needs to be included in the result. include_end : bool, default True Whether the end time needs to be included in the result. axis : {0 or 'index', 1 or 'columns'}, default 0 Determine range time on index or columns value. .. versionadded :: 0.24.0 Returns Series or DataFrame Data from the original object filtered to the specified dates range. Raises TypeError If the index is not a :class: DatetimeIndex See Also at_time : Select values at a particular time of the day. first : Select initial periods of time series based on a date offset. last : Select final periods of time series based on a date offset. DatetimeIndex.indexer_between_time : Get just the index locations for values between particular times of the day. Examples i = pd.date_range('2018-04-09', periods=4, freq='1D20min') ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) ts A 2018-04-09 00:00:00 1 2018-04-10 00:20:00 2 2018-04-11 00:40:00 3 2018-04-12 01:00:00 4 ts.between_time('0:15', '0:45') A 2018-04-10 00:20:00 2 2018-04-11 00:40:00 3 You get the times that are not between two times by setting start_time later than end_time : ts.between_time('0:45', '0:15') A 2018-04-09 00:00:00 1 2018-04-12 01:00:00 4 View Source def between_time ( self : FrameOrSeries , start_time , end_time , include_start : bool_t = True , include_end : bool_t = True , axis = None , ) -> FrameOrSeries : \"\"\" Select values between particular times of the day (e.g., 9:00-9:30 AM). By setting ``start_time`` to be later than ``end_time``, you can get the times that are *not* between the two times. Parameters ---------- start_time : datetime.time or str Initial time as a time filter limit. end_time : datetime.time or str End time as a time filter limit. include_start : bool, default True Whether the start time needs to be included in the result. include_end : bool, default True Whether the end time needs to be included in the result. axis : {0 or 'index', 1 or 'columns'}, default 0 Determine range time on index or columns value. .. versionadded:: 0.24.0 Returns ------- Series or DataFrame Data from the original object filtered to the specified dates range. Raises ------ TypeError If the index is not a :class:`DatetimeIndex` See Also -------- at_time : Select values at a particular time of the day. first : Select initial periods of time series based on a date offset. last : Select final periods of time series based on a date offset. DatetimeIndex.indexer_between_time : Get just the index locations for values between particular times of the day. Examples -------- >>> i = pd.date_range('2018-04-09', periods=4, freq='1D20min') >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) >>> ts A 2018-04-09 00:00:00 1 2018-04-10 00:20:00 2 2018-04-11 00:40:00 3 2018-04-12 01:00:00 4 >>> ts.between_time('0:15', '0:45') A 2018-04-10 00:20:00 2 2018-04-11 00:40:00 3 You get the times that are *not* between two times by setting ``start_time`` later than ``end_time``: >>> ts.between_time('0:45', '0:15') A 2018-04-09 00:00:00 1 2018-04-12 01:00:00 4 \"\"\" if axis is None : axis = self . _stat_axis_number axis = self . _get_axis_number ( axis ) index = self . _get_axis ( axis ) if not isinstance ( index , DatetimeIndex ): raise TypeError ( \"Index must be DatetimeIndex\" ) indexer = index . indexer_between_time ( start_time , end_time , include_start = include_start , include_end = include_end , ) return self . _take_with_is_copy ( indexer , axis = axis ) bfill def bfill ( self : ~ FrameOrSeries , axis = None , inplace : bool = False , limit = None , downcast = None ) -> Union [ ~ FrameOrSeries , NoneType ] Synonym for :meth: DataFrame.fillna with method='bfill' . Returns {klass} or None Object with missing values filled or None if inplace=True . View Source def bfill ( self : FrameOrSeries , axis = None , inplace : bool_t = False , limit = None , downcast = None , ) -> Optional [ FrameOrSeries ] : \"\"\" Synonym for :meth:`DataFrame.fillna` with ``method='bfill'``. Returns ------- {klass} or None Object with missing values filled or None if ``inplace=True``. \"\"\" return self . fillna ( method = \"bfill\" , axis = axis , inplace = inplace , limit = limit , downcast = downcast ) bool def bool ( self ) Return the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception). Returns bool The value in the Series or DataFrame. See Also Series.astype : Change the data type of a Series, including to boolean. DataFrame.astype : Change the data type of a DataFrame, including to boolean. numpy.bool_ : NumPy boolean data type, used by pandas for boolean values. Examples The method will only work for single element objects with a boolean value: pd.Series([True]).bool() True pd.Series([False]).bool() False pd.DataFrame({'col': [True]}).bool() True pd.DataFrame({'col': [False]}).bool() False View Source def bool ( self ) : \"\"\" Return the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception). Returns ------- bool The value in the Series or DataFrame. See Also -------- Series.astype : Change the data type of a Series, including to boolean. DataFrame.astype : Change the data type of a DataFrame, including to boolean. numpy.bool_ : NumPy boolean data type, used by pandas for boolean values. Examples -------- The method will only work for single element objects with a boolean value: >>> pd.Series([True]).bool() True >>> pd.Series([False]).bool() False >>> pd.DataFrame({'col': [True]}).bool() True >>> pd.DataFrame({'col': [False]}).bool() False \"\"\" v = self . squeeze () if isinstance ( v , ( bool , np . bool_ )) : return bool ( v ) elif is_scalar ( v ) : raise ValueError ( \"bool cannot act on a non-boolean single element \" f \"{type(self).__name__}\" ) self . __nonzero__ () boxplot def boxplot ( self , column = None , by = None , ax = None , fontsize = None , rot = 0 , grid = True , figsize = None , layout = None , return_type = None , backend = None , ** kwargs ) Make a box plot from DataFrame columns. Make a box-and-whisker plot from DataFrame columns, optionally grouped by some other columns. A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. By default, they extend no more than 1.5 * IQR (IQR = Q3 - Q1) from the edges of the box, ending at the farthest data point within that interval. Outliers are plotted as separate dots. For further details see Wikipedia's entry for boxplot <https://en.wikipedia.org/wiki/Box_plot> _. Parameters column : str or list of str, optional Column name or list of names, or vector. Can be any valid input to :meth: pandas.DataFrame.groupby . by : str or array-like, optional Column in the DataFrame to :meth: pandas.DataFrame.groupby . One box-plot will be done per value of columns in by . ax : object of class matplotlib.axes.Axes, optional The matplotlib axes to be used by boxplot. fontsize : float or str Tick label font size in points or as a string (e.g., large ). rot : int or float, default 0 The rotation angle of labels (in degrees) with respect to the screen coordinate system. grid : bool, default True Setting this to True will show the grid. figsize : A tuple (width, height) in inches The size of the figure to create in matplotlib. layout : tuple (rows, columns), optional For example, (3, 5) will display the subplots using 3 columns and 5 rows, starting from the top-left. return_type : {'axes', 'dict', 'both'} or None, default 'axes' The kind of object to return. The default is axes . * 'axes' returns the matplotlib axes the boxplot is drawn on . * 'dict' returns a dictionary whose values are the matplotlib Lines of the boxplot . * 'both' returns a namedtuple with the axes and dict . * when grouping with `` by `` , a Series mapping columns to `` return_type `` is returned . If `` return_type `` is `None` , a NumPy array of axes with the same shape as `` layout `` is returned . backend : str, default None Backend to use instead of the backend specified in the option plotting.backend . For instance, 'matplotlib'. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend . .. versionadded :: 1.0.0 **kwargs All other plotting keyword arguments to be passed to :func: matplotlib.pyplot.boxplot . Returns result See Notes. See Also Series.plot.hist: Make a histogram. matplotlib.pyplot.boxplot : Matplotlib equivalent plot. Notes The return type depends on the return_type parameter: 'axes' : object of class matplotlib.axes.Axes 'dict' : dict of matplotlib.lines.Line2D objects 'both' : a namedtuple with structure (ax, lines) For data grouped with by , return a Series of the above or a numpy array: :class: ~pandas.Series :class: ~numpy.array (for return_type = None ) Use return_type='dict' when you want to tweak the appearance of the lines after plotting. In this case a dict containing the Lines making up the boxes, caps, fliers, medians, and whiskers is returned. Examples Boxplots can be created for every column in the dataframe by df.boxplot() or indicating the columns to be used: .. plot:: :context: close-figs >>> np.random.seed(1234) >>> df = pd.DataFrame(np.random.randn(10, 4), ... columns=['Col1', 'Col2', 'Col3', 'Col4']) >>> boxplot = df.boxplot(column=['Col1', 'Col2', 'Col3']) Boxplots of variables distributions grouped by the values of a third variable can be created using the option by . For instance: .. plot:: :context: close-figs >>> df = pd.DataFrame(np.random.randn(10, 2), ... columns=['Col1', 'Col2']) >>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A', ... 'B', 'B', 'B', 'B', 'B']) >>> boxplot = df.boxplot(by='X') A list of strings (i.e. ['X', 'Y'] ) can be passed to boxplot in order to group the data by combination of the variables in the x-axis: .. plot:: :context: close-figs >>> df = pd.DataFrame(np.random.randn(10, 3), ... columns=['Col1', 'Col2', 'Col3']) >>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A', ... 'B', 'B', 'B', 'B', 'B']) >>> df['Y'] = pd.Series(['A', 'B', 'A', 'B', 'A', ... 'B', 'A', 'B', 'A', 'B']) >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by=['X', 'Y']) The layout of boxplot can be adjusted giving a tuple to layout : .. plot:: :context: close-figs >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X', ... layout=(2, 1)) Additional formatting can be done to the boxplot, like suppressing the grid ( grid=False ), rotating the labels in the x-axis (i.e. rot=45 ) or changing the fontsize (i.e. fontsize=15 ): .. plot:: :context: close-figs >>> boxplot = df.boxplot(grid=False, rot=45, fontsize=15) The parameter return_type can be used to select the type of element returned by boxplot . When return_type='axes' is selected, the matplotlib axes on which the boxplot is drawn are returned: >>> boxplot = df.boxplot(column=['Col1', 'Col2'], return_type='axes') >>> type(boxplot) <class 'matplotlib.axes._subplots.AxesSubplot'> When grouping with by , a Series mapping columns to return_type is returned: >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X', ... return_type='axes') >>> type(boxplot) <class 'pandas.core.series.Series'> If return_type is None , a NumPy array of axes with the same shape as layout is returned: >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X', ... return_type=None) >>> type(boxplot) <class 'numpy.ndarray'> View Source @Substitution ( backend = _backend_doc ) @Appender ( _boxplot_doc ) def boxplot_frame ( self , column = None , by = None , ax = None , fontsize = None , rot = 0 , grid = True , figsize = None , layout = None , return_type = None , backend = None , ** kwargs , ) : plot_backend = _get_plot_backend ( backend ) return plot_backend . boxplot_frame ( self , column = column , by = by , ax = ax , fontsize = fontsize , rot = rot , grid = grid , figsize = figsize , layout = layout , return_type = return_type , ** kwargs , ) cleanfs def cleanfs ( self ) View Source def cleanfs ( self ): try : self . lock . acquire () try : os . unlink ( self . csv ) os . unlink ( self . md5file ) self . drop ( self . index , axis = 1 , inplace = True ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e clip def clip ( self : ~ FrameOrSeries , lower = None , upper = None , axis = None , inplace : bool = False , * args , ** kwargs ) -> ~ FrameOrSeries Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis. Parameters lower : float or array_like, default None Minimum threshold value. All values below this threshold will be set to it. upper : float or array_like, default None Maximum threshold value. All values above this threshold will be set to it. axis : int or str axis name, optional Align object with lower and upper along the given axis. inplace : bool, default False Whether to perform the operation in place on the data. args, *kwargs Additional keywords have no effect but might be accepted for compatibility with numpy. Returns Series or DataFrame Same type as calling object with the values outside the clip boundaries replaced. See Also Series.clip : Trim values at input threshold in series. DataFrame.clip : Trim values at input threshold in dataframe. numpy.clip : Clip (limit) the values in an array. Examples data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]} df = pd.DataFrame(data) df col_0 col_1 0 9 -2 1 -3 -7 2 0 6 3 -1 8 4 5 -5 Clips per column using lower and upper thresholds: df.clip(-4, 6) col_0 col_1 0 6 -2 1 -3 -4 2 0 6 3 -1 6 4 5 -4 Clips using specific lower and upper thresholds per column element: t = pd.Series([2, -4, -1, 6, 3]) t 0 2 1 -4 2 -1 3 6 4 3 dtype: int64 df.clip(t, t + 4, axis=0) col_0 col_1 0 6 2 1 -3 -4 2 0 3 3 6 8 4 5 3 View Source def clip ( self : FrameOrSeries , lower = None , upper = None , axis = None , inplace : bool_t = False , * args , ** kwargs , ) -> FrameOrSeries : \"\"\" Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis. Parameters ---------- lower : float or array_like, default None Minimum threshold value. All values below this threshold will be set to it. upper : float or array_like, default None Maximum threshold value. All values above this threshold will be set to it. axis : int or str axis name, optional Align object with lower and upper along the given axis. inplace : bool, default False Whether to perform the operation in place on the data. *args, **kwargs Additional keywords have no effect but might be accepted for compatibility with numpy. Returns ------- Series or DataFrame Same type as calling object with the values outside the clip boundaries replaced. See Also -------- Series.clip : Trim values at input threshold in series. DataFrame.clip : Trim values at input threshold in dataframe. numpy.clip : Clip (limit) the values in an array. Examples -------- >>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]} >>> df = pd.DataFrame(data) >>> df col_0 col_1 0 9 -2 1 -3 -7 2 0 6 3 -1 8 4 5 -5 Clips per column using lower and upper thresholds: >>> df.clip(-4, 6) col_0 col_1 0 6 -2 1 -3 -4 2 0 6 3 -1 6 4 5 -4 Clips using specific lower and upper thresholds per column element: >>> t = pd.Series([2, -4, -1, 6, 3]) >>> t 0 2 1 -4 2 -1 3 6 4 3 dtype: int64 >>> df.clip(t, t + 4, axis=0) col_0 col_1 0 6 2 1 -3 -4 2 0 3 3 6 8 4 5 3 \"\"\" inplace = validate_bool_kwarg ( inplace , \"inplace\" ) axis = nv . validate_clip_with_axis ( axis , args , kwargs ) if axis is not None : axis = self . _get_axis_number ( axis ) # GH 17276 # numpy doesn 't like NaN as a clip value # so ignore # GH 19992 # numpy doesn' t drop a list - like bound containing NaN if not is_list_like ( lower ) and np . any ( isna ( lower )): lower = None if not is_list_like ( upper ) and np . any ( isna ( upper )): upper = None # GH 2747 ( arguments were reversed ) if lower is not None and upper is not None : if is_scalar ( lower ) and is_scalar ( upper ): lower , upper = min ( lower , upper ), max ( lower , upper ) # fast - path for scalars if ( lower is None or ( is_scalar ( lower ) and is_number ( lower ))) and ( upper is None or ( is_scalar ( upper ) and is_number ( upper )) ): return self . _clip_with_scalar ( lower , upper , inplace = inplace ) result = self if lower is not None : result = result . _clip_with_one_bound ( lower , method = self . ge , axis = axis , inplace = inplace ) if upper is not None : if inplace : result = self result = result . _clip_with_one_bound ( upper , method = self . le , axis = axis , inplace = inplace ) return result combine def combine ( self , other : 'DataFrame' , func , fill_value = None , overwrite = True ) -> 'DataFrame' Perform column-wise combine with another DataFrame. Combines a DataFrame with other DataFrame using func to element-wise combine columns. The row and column indexes of the resulting DataFrame will be the union of the two. Parameters other : DataFrame The DataFrame to merge column-wise. func : function Function that takes two series as inputs and return a Series or a scalar. Used to merge the two dataframes column by columns. fill_value : scalar value, default None The value to fill NaNs with prior to passing any column to the merge func. overwrite : bool, default True If True, columns in self that do not exist in other will be overwritten with NaNs. Returns DataFrame Combination of the provided DataFrames. See Also DataFrame.combine_first : Combine two DataFrame objects and default to non-null values in frame calling the method. Examples Combine using a simple function that chooses the smaller column. df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]}) df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2 df1.combine(df2, take_smaller) A B 0 0 3 1 0 3 Example using a true element-wise combine function. df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]}) df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) df1.combine(df2, np.minimum) A B 0 1 2 1 0 3 Using fill_value fills Nones prior to passing the column to the merge function. df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]}) df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) df1.combine(df2, take_smaller, fill_value=-5) A B 0 0 -5.0 1 0 4.0 However, if the same element in both dataframes is None, that None is preserved df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]}) df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]}) df1.combine(df2, take_smaller, fill_value=-5) A B 0 0 -5.0 1 0 3.0 Example that demonstrates the use of overwrite and behavior when the axis differ between the dataframes. df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]}) df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2]) df1.combine(df2, take_smaller) A B C 0 NaN NaN NaN 1 NaN 3.0 -10.0 2 NaN 3.0 1.0 df1.combine(df2, take_smaller, overwrite=False) A B C 0 0.0 NaN NaN 1 0.0 3.0 -10.0 2 NaN 3.0 1.0 Demonstrating the preference of the passed in dataframe. df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2]) df2.combine(df1, take_smaller) A B C 0 0.0 NaN NaN 1 0.0 3.0 NaN 2 NaN 3.0 NaN df2.combine(df1, take_smaller, overwrite=False) A B C 0 0.0 NaN NaN 1 0.0 3.0 1.0 2 NaN 3.0 1.0 View Source def combine ( self , other : \"DataFrame\" , func , fill_value = None , overwrite = True ) -> \"DataFrame\" : \"\"\" Perform column-wise combine with another DataFrame. Combines a DataFrame with `other` DataFrame using `func` to element-wise combine columns. The row and column indexes of the resulting DataFrame will be the union of the two. Parameters ---------- other : DataFrame The DataFrame to merge column-wise. func : function Function that takes two series as inputs and return a Series or a scalar. Used to merge the two dataframes column by columns. fill_value : scalar value, default None The value to fill NaNs with prior to passing any column to the merge func. overwrite : bool, default True If True, columns in `self` that do not exist in `other` will be overwritten with NaNs. Returns ------- DataFrame Combination of the provided DataFrames. See Also -------- DataFrame.combine_first : Combine two DataFrame objects and default to non-null values in frame calling the method. Examples -------- Combine using a simple function that chooses the smaller column. >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]}) >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) >>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2 >>> df1.combine(df2, take_smaller) A B 0 0 3 1 0 3 Example using a true element-wise combine function. >>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]}) >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) >>> df1.combine(df2, np.minimum) A B 0 1 2 1 0 3 Using `fill_value` fills Nones prior to passing the column to the merge function. >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]}) >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) >>> df1.combine(df2, take_smaller, fill_value=-5) A B 0 0 -5.0 1 0 4.0 However, if the same element in both dataframes is None, that None is preserved >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]}) >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]}) >>> df1.combine(df2, take_smaller, fill_value=-5) A B 0 0 -5.0 1 0 3.0 Example that demonstrates the use of `overwrite` and behavior when the axis differ between the dataframes. >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]}) >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2]) >>> df1.combine(df2, take_smaller) A B C 0 NaN NaN NaN 1 NaN 3.0 -10.0 2 NaN 3.0 1.0 >>> df1.combine(df2, take_smaller, overwrite=False) A B C 0 0.0 NaN NaN 1 0.0 3.0 -10.0 2 NaN 3.0 1.0 Demonstrating the preference of the passed in dataframe. >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2]) >>> df2.combine(df1, take_smaller) A B C 0 0.0 NaN NaN 1 0.0 3.0 NaN 2 NaN 3.0 NaN >>> df2.combine(df1, take_smaller, overwrite=False) A B C 0 0.0 NaN NaN 1 0.0 3.0 1.0 2 NaN 3.0 1.0 \"\"\" other_idxlen = len ( other . index ) # save for compare this , other = self . align ( other , copy = False ) new_index = this . index if other . empty and len ( new_index ) == len ( self . index ) : return self . copy () if self . empty and len ( other ) == other_idxlen : return other . copy () # sorts if possible new_columns = this . columns . union ( other . columns ) do_fill = fill_value is not None result = {} for col in new_columns : series = this [ col ] otherSeries = other [ col ] this_dtype = series . dtype other_dtype = otherSeries . dtype this_mask = isna ( series ) other_mask = isna ( otherSeries ) # don ' t overwrite columns unnecessarily # DO propagate if this column is not in the intersection if not overwrite and other_mask . all () : result [ col ] = this [ col ] . copy () continue if do_fill : series = series . copy () otherSeries = otherSeries . copy () series [ this_mask ] = fill_value otherSeries [ other_mask ] = fill_value if col not in self . columns : # If self DataFrame does not have col in other DataFrame , # try to promote series , which is all NaN , as other_dtype . new_dtype = other_dtype try : series = series . astype ( new_dtype , copy = False ) except ValueError : # e . g . new_dtype is integer types pass else : # if we have different dtypes , possibly promote new_dtype = find_common_type ( [ this_dtype, other_dtype ] ) if not is_dtype_equal ( this_dtype , new_dtype ) : series = series . astype ( new_dtype ) if not is_dtype_equal ( other_dtype , new_dtype ) : otherSeries = otherSeries . astype ( new_dtype ) arr = func ( series , otherSeries ) arr = maybe_downcast_to_dtype ( arr , this_dtype ) result [ col ] = arr # convert_objects just in case return self . _constructor ( result , index = new_index , columns = new_columns ) combine_first def combine_first ( self , other : 'DataFrame' ) -> 'DataFrame' Update null elements with value in the same location in other . Combine two DataFrame objects by filling null values in one DataFrame with non-null values from other DataFrame. The row and column indexes of the resulting DataFrame will be the union of the two. Parameters other : DataFrame Provided DataFrame to use to fill null values. Returns DataFrame See Also DataFrame.combine : Perform series-wise operation on two DataFrames using a given function. Examples df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]}) df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) df1.combine_first(df2) A B 0 1.0 3.0 1 0.0 4.0 Null values still persist if the location of that null value does not exist in other df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]}) df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2]) df1.combine_first(df2) A B C 0 NaN 4.0 NaN 1 0.0 3.0 1.0 2 NaN 3.0 1.0 View Source def combine_first ( self , other : \"DataFrame\" ) -> \"DataFrame\" : \"\"\" Update null elements with value in the same location in `other`. Combine two DataFrame objects by filling null values in one DataFrame with non-null values from other DataFrame. The row and column indexes of the resulting DataFrame will be the union of the two. Parameters ---------- other : DataFrame Provided DataFrame to use to fill null values. Returns ------- DataFrame See Also -------- DataFrame.combine : Perform series-wise operation on two DataFrames using a given function. Examples -------- >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]}) >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) >>> df1.combine_first(df2) A B 0 1.0 3.0 1 0.0 4.0 Null values still persist if the location of that null value does not exist in `other` >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]}) >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2]) >>> df1.combine_first(df2) A B C 0 NaN 4.0 NaN 1 0.0 3.0 1.0 2 NaN 3.0 1.0 \"\"\" import pandas . core . computation . expressions as expressions def extract_values ( arr ): # Does two things: # 1. maybe gets the values from the Series / Index # 2. convert datelike to i8 # TODO: extract_array? if isinstance ( arr , ( Index , Series )): arr = arr . _values if needs_i8_conversion ( arr . dtype ): if is_extension_array_dtype ( arr . dtype ): arr = arr . asi8 else : arr = arr . view ( \"i8\" ) return arr def combiner ( x , y ): mask = isna ( x ) # TODO: extract_array? if isinstance ( mask , ( Index , Series )): mask = mask . _values x_values = extract_values ( x ) y_values = extract_values ( y ) # If the column y in other DataFrame is not in first DataFrame, # just return y_values. if y . name not in self . columns : return y_values return expressions . where ( mask , y_values , x_values ) return self . combine ( other , combiner , overwrite = False ) compare def compare ( self , other : 'DataFrame' , align_axis : Union [ str , int ] = 1 , keep_shape : bool = False , keep_equal : bool = False ) -> 'DataFrame' Compare to another DataFrame and show the differences. .. versionadded:: 1.1.0 Parameters other : DataFrame Object to compare with. align_axis : {0 or 'index', 1 or 'columns'}, default 1 Determine which axis to align the comparison on. * 0, or 'index' : Resulting differences are stacked vertically with rows drawn alternately from self and other. * 1, or 'columns' : Resulting differences are aligned horizontally with columns drawn alternately from self and other. keep_shape : bool, default False If true, all rows and columns are kept. Otherwise, only the ones with different values are kept. keep_equal : bool, default False If true, the result keeps values that are equal. Otherwise, equal values are shown as NaNs. Returns DataFrame DataFrame that shows the differences stacked side by side. The resulting index will be a MultiIndex with 'self' and 'other' stacked alternately at the inner level. See Also Series.compare : Compare with another Series and show differences. Notes Matching NaNs will not appear as a difference. Examples df = pd.DataFrame( ... { ... \"col1\": [\"a\", \"a\", \"b\", \"b\", \"a\"], ... \"col2\": [1.0, 2.0, 3.0, np.nan, 5.0], ... \"col3\": [1.0, 2.0, 3.0, 4.0, 5.0] ... }, ... columns=[\"col1\", \"col2\", \"col3\"], ... ) df col1 col2 col3 0 a 1.0 1.0 1 a 2.0 2.0 2 b 3.0 3.0 3 b NaN 4.0 4 a 5.0 5.0 df2 = df.copy() df2.loc[0, 'col1'] = 'c' df2.loc[2, 'col3'] = 4.0 df2 col1 col2 col3 0 c 1.0 1.0 1 a 2.0 2.0 2 b 3.0 4.0 3 b NaN 4.0 4 a 5.0 5.0 Align the differences on columns df.compare(df2) col1 col3 self other self other 0 a c NaN NaN 2 NaN NaN 3.0 4.0 Stack the differences on rows df.compare(df2, align_axis=0) col1 col3 0 self a NaN other c NaN 2 self NaN 3.0 other NaN 4.0 Keep the equal values df.compare(df2, keep_equal=True) col1 col3 self other self other 0 a c 1.0 1.0 2 b b 3.0 4.0 Keep all original rows and columns df.compare(df2, keep_shape=True) col1 col2 col3 self other self other self other 0 a c NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN NaN 2 NaN NaN NaN NaN 3.0 4.0 3 NaN NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN NaN NaN Keep all original rows and columns and also all original values df.compare(df2, keep_shape=True, keep_equal=True) col1 col2 col3 self other self other self other 0 a c 1.0 1.0 1.0 1.0 1 a a 2.0 2.0 2.0 2.0 2 b b 3.0 3.0 3.0 4.0 3 b b NaN NaN 4.0 4.0 4 a a 5.0 5.0 5.0 5.0 View Source @Appender ( \"\"\" Returns ------- DataFrame DataFrame that shows the differences stacked side by side. The resulting index will be a MultiIndex with 'self' and 'other' stacked alternately at the inner level. See Also -------- Series.compare : Compare with another Series and show differences. Notes ----- Matching NaNs will not appear as a difference. Examples -------- >>> df = pd.DataFrame( ... { ... \" col1 \": [\" a \", \" a \", \" b \", \" b \", \" a \"], ... \" col2 \": [1.0, 2.0, 3.0, np.nan, 5.0], ... \" col3 \": [1.0, 2.0, 3.0, 4.0, 5.0] ... }, ... columns=[\" col1 \", \" col2 \", \" col3 \"], ... ) >>> df col1 col2 col3 0 a 1.0 1.0 1 a 2.0 2.0 2 b 3.0 3.0 3 b NaN 4.0 4 a 5.0 5.0 >>> df2 = df.copy() >>> df2.loc[0, 'col1'] = 'c' >>> df2.loc[2, 'col3'] = 4.0 >>> df2 col1 col2 col3 0 c 1.0 1.0 1 a 2.0 2.0 2 b 3.0 4.0 3 b NaN 4.0 4 a 5.0 5.0 Align the differences on columns >>> df.compare(df2) col1 col3 self other self other 0 a c NaN NaN 2 NaN NaN 3.0 4.0 Stack the differences on rows >>> df.compare(df2, align_axis=0) col1 col3 0 self a NaN other c NaN 2 self NaN 3.0 other NaN 4.0 Keep the equal values >>> df.compare(df2, keep_equal=True) col1 col3 self other self other 0 a c 1.0 1.0 2 b b 3.0 4.0 Keep all original rows and columns >>> df.compare(df2, keep_shape=True) col1 col2 col3 self other self other self other 0 a c NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN NaN 2 NaN NaN NaN NaN 3.0 4.0 3 NaN NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN NaN NaN Keep all original rows and columns and also all original values >>> df.compare(df2, keep_shape=True, keep_equal=True) col1 col2 col3 self other self other self other 0 a c 1.0 1.0 1.0 1.0 1 a a 2.0 2.0 2.0 2.0 2 b b 3.0 3.0 3.0 4.0 3 b b NaN NaN 4.0 4.0 4 a a 5.0 5.0 5.0 5.0 \"\"\" ) @Appender ( _shared_docs [ \"compare\" ] % _shared_doc_kwargs ) def compare ( self , other : \"DataFrame\" , align_axis : Axis = 1 , keep_shape : bool = False , keep_equal : bool = False , ) -> \"DataFrame\" : return super (). compare ( other = other , align_axis = align_axis , keep_shape = keep_shape , keep_equal = keep_equal , ) convert_dtypes def convert_dtypes ( self : ~ FrameOrSeries , infer_objects : bool = True , convert_string : bool = True , convert_integer : bool = True , convert_boolean : bool = True ) -> ~ FrameOrSeries Convert columns to best possible dtypes using dtypes supporting pd.NA . .. versionadded:: 1.0.0 Parameters infer_objects : bool, default True Whether object dtypes should be converted to the best possible types. convert_string : bool, default True Whether object dtypes should be converted to StringDtype() . convert_integer : bool, default True Whether, if possible, conversion can be done to integer extension types. convert_boolean : bool, defaults True Whether object dtypes should be converted to BooleanDtypes() . Returns Series or DataFrame Copy of input object with new dtype. See Also infer_objects : Infer dtypes of objects. to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type. Notes By default, convert_dtypes will attempt to convert a Series (or each Series in a DataFrame) to dtypes that support pd.NA . By using the options convert_string , convert_integer , and convert_boolean , it is possible to turn off individual conversions to StringDtype , the integer extension types or BooleanDtype , respectively. For object-dtyped columns, if infer_objects is True , use the inference rules as during normal Series/DataFrame construction. Then, if possible, convert to StringDtype , BooleanDtype or an appropriate integer extension type, otherwise leave as object . If the dtype is integer, convert to an appropriate integer extension type. If the dtype is numeric, and consists of all integers, convert to an appropriate integer extension type. In the future, as new dtypes are added that support pd.NA , the results of this method will change to support those new dtypes. Examples df = pd.DataFrame( ... { ... \"a\": pd.Series([1, 2, 3], dtype=np.dtype(\"int32\")), ... \"b\": pd.Series([\"x\", \"y\", \"z\"], dtype=np.dtype(\"O\")), ... \"c\": pd.Series([True, False, np.nan], dtype=np.dtype(\"O\")), ... \"d\": pd.Series([\"h\", \"i\", np.nan], dtype=np.dtype(\"O\")), ... \"e\": pd.Series([10, np.nan, 20], dtype=np.dtype(\"float\")), ... \"f\": pd.Series([np.nan, 100.5, 200], dtype=np.dtype(\"float\")), ... } ... ) Start with a DataFrame with default dtypes. df a b c d e f 0 1 x True h 10.0 NaN 1 2 y False i NaN 100.5 2 3 z NaN NaN 20.0 200.0 df.dtypes a int32 b object c object d object e float64 f float64 dtype: object Convert the DataFrame to use best possible dtypes. dfn = df.convert_dtypes() dfn a b c d e f 0 1 x True h 10 NaN 1 2 y False i 100.5 2 3 z 20 200.0 dfn.dtypes a Int32 b string c boolean d string e Int64 f float64 dtype: object Start with a Series of strings and missing data represented by np.nan . s = pd.Series([\"a\", \"b\", np.nan]) s 0 a 1 b 2 NaN dtype: object Obtain a Series with dtype StringDtype . s.convert_dtypes() 0 a 1 b 2 dtype: string View Source def convert_dtypes ( self : FrameOrSeries , infer_objects : bool_t = True , convert_string : bool_t = True , convert_integer : bool_t = True , convert_boolean : bool_t = True , ) -> FrameOrSeries : \"\"\" Convert columns to best possible dtypes using dtypes supporting ``pd.NA``. .. versionadded:: 1.0.0 Parameters ---------- infer_objects : bool, default True Whether object dtypes should be converted to the best possible types. convert_string : bool, default True Whether object dtypes should be converted to ``StringDtype()``. convert_integer : bool, default True Whether, if possible, conversion can be done to integer extension types. convert_boolean : bool, defaults True Whether object dtypes should be converted to ``BooleanDtypes()``. Returns ------- Series or DataFrame Copy of input object with new dtype. See Also -------- infer_objects : Infer dtypes of objects. to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type. Notes ----- By default, ``convert_dtypes`` will attempt to convert a Series (or each Series in a DataFrame) to dtypes that support ``pd.NA``. By using the options ``convert_string``, ``convert_integer``, and ``convert_boolean``, it is possible to turn off individual conversions to ``StringDtype``, the integer extension types or ``BooleanDtype``, respectively. For object-dtyped columns, if ``infer_objects`` is ``True``, use the inference rules as during normal Series/DataFrame construction. Then, if possible, convert to ``StringDtype``, ``BooleanDtype`` or an appropriate integer extension type, otherwise leave as ``object``. If the dtype is integer, convert to an appropriate integer extension type. If the dtype is numeric, and consists of all integers, convert to an appropriate integer extension type. In the future, as new dtypes are added that support ``pd.NA``, the results of this method will change to support those new dtypes. Examples -------- >>> df = pd.DataFrame( ... { ... \" a \": pd.Series([1, 2, 3], dtype=np.dtype(\" int32 \")), ... \" b \": pd.Series([\" x \", \" y \", \" z \"], dtype=np.dtype(\" O \")), ... \" c \": pd.Series([True, False, np.nan], dtype=np.dtype(\" O \")), ... \" d \": pd.Series([\" h \", \" i \", np.nan], dtype=np.dtype(\" O \")), ... \" e \": pd.Series([10, np.nan, 20], dtype=np.dtype(\" float \")), ... \" f \": pd.Series([np.nan, 100.5, 200], dtype=np.dtype(\" float \")), ... } ... ) Start with a DataFrame with default dtypes. >>> df a b c d e f 0 1 x True h 10.0 NaN 1 2 y False i NaN 100.5 2 3 z NaN NaN 20.0 200.0 >>> df.dtypes a int32 b object c object d object e float64 f float64 dtype: object Convert the DataFrame to use best possible dtypes. >>> dfn = df.convert_dtypes() >>> dfn a b c d e f 0 1 x True h 10 NaN 1 2 y False i <NA> 100.5 2 3 z <NA> <NA> 20 200.0 >>> dfn.dtypes a Int32 b string c boolean d string e Int64 f float64 dtype: object Start with a Series of strings and missing data represented by ``np.nan``. >>> s = pd.Series([\" a \", \" b \", np.nan]) >>> s 0 a 1 b 2 NaN dtype: object Obtain a Series with dtype ``StringDtype``. >>> s.convert_dtypes() 0 a 1 b 2 <NA> dtype: string \"\"\" if self . ndim == 1 : return self . _convert_dtypes ( infer_objects , convert_string , convert_integer , convert_boolean ) else : results = [ col . _convert_dtypes ( infer_objects , convert_string , convert_integer , convert_boolean ) for col_name , col in self . items () ] result = pd . concat ( results , axis = 1 , copy = False ) return result copy def copy ( self : ~ FrameOrSeries , deep : bool = True ) -> ~ FrameOrSeries Make a copy of this object's indices and data. When deep=True (default), a new object will be created with a copy of the calling object's data and indices. Modifications to the data or indices of the copy will not be reflected in the original object (see notes below). When deep=False , a new object will be created without copying the calling object's data or index (only references to the data and index are copied). Any changes to the data of the original will be reflected in the shallow copy (and vice versa). Parameters deep : bool, default True Make a deep copy, including a copy of the data and the indices. With deep=False neither the indices nor the data are copied. Returns copy : Series or DataFrame Object type matches caller. Notes When deep=True , data is copied but actual Python objects will not be copied recursively, only the reference to the object. This is in contrast to copy.deepcopy in the Standard Library, which recursively copies object data (see examples below). While Index objects are copied when deep=True , the underlying numpy array is not copied for performance reasons. Since Index is immutable, the underlying data can be safely shared and a copy is not needed. Examples s = pd.Series([1, 2], index=[\"a\", \"b\"]) s a 1 b 2 dtype: int64 s_copy = s.copy() s_copy a 1 b 2 dtype: int64 Shallow copy versus default (deep) copy: s = pd.Series([1, 2], index=[\"a\", \"b\"]) deep = s.copy() shallow = s.copy(deep=False) Shallow copy shares data and index with original. s is shallow False s.values is shallow.values and s.index is shallow.index True Deep copy has own copy of data and index. s is deep False s.values is deep.values or s.index is deep.index False Updates to the data shared by shallow copy and original is reflected in both; deep copy remains unchanged. s[0] = 3 shallow[1] = 4 s a 3 b 4 dtype: int64 shallow a 3 b 4 dtype: int64 deep a 1 b 2 dtype: int64 Note that when copying an object containing Python objects, a deep copy will copy the data, but will not do so recursively. Updating a nested data object will be reflected in the deep copy. s = pd.Series([[1, 2], [3, 4]]) deep = s.copy() s[0][0] = 10 s 0 [10, 2] 1 [3, 4] dtype: object deep 0 [10, 2] 1 [3, 4] dtype: object View Source def copy ( self : FrameOrSeries , deep : bool_t = True ) -> FrameOrSeries : \"\"\" Make a copy of this object's indices and data. When ``deep=True`` (default), a new object will be created with a copy of the calling object's data and indices. Modifications to the data or indices of the copy will not be reflected in the original object (see notes below). When ``deep=False``, a new object will be created without copying the calling object's data or index (only references to the data and index are copied). Any changes to the data of the original will be reflected in the shallow copy (and vice versa). Parameters ---------- deep : bool, default True Make a deep copy, including a copy of the data and the indices. With ``deep=False`` neither the indices nor the data are copied. Returns ------- copy : Series or DataFrame Object type matches caller. Notes ----- When ``deep=True``, data is copied but actual Python objects will not be copied recursively, only the reference to the object. This is in contrast to `copy.deepcopy` in the Standard Library, which recursively copies object data (see examples below). While ``Index`` objects are copied when ``deep=True``, the underlying numpy array is not copied for performance reasons. Since ``Index`` is immutable, the underlying data can be safely shared and a copy is not needed. Examples -------- >>> s = pd.Series([1, 2], index=[\" a \", \" b \"]) >>> s a 1 b 2 dtype: int64 >>> s_copy = s.copy() >>> s_copy a 1 b 2 dtype: int64 **Shallow copy versus default (deep) copy:** >>> s = pd.Series([1, 2], index=[\" a \", \" b \"]) >>> deep = s.copy() >>> shallow = s.copy(deep=False) Shallow copy shares data and index with original. >>> s is shallow False >>> s.values is shallow.values and s.index is shallow.index True Deep copy has own copy of data and index. >>> s is deep False >>> s.values is deep.values or s.index is deep.index False Updates to the data shared by shallow copy and original is reflected in both; deep copy remains unchanged. >>> s[0] = 3 >>> shallow[1] = 4 >>> s a 3 b 4 dtype: int64 >>> shallow a 3 b 4 dtype: int64 >>> deep a 1 b 2 dtype: int64 Note that when copying an object containing Python objects, a deep copy will copy the data, but will not do so recursively. Updating a nested data object will be reflected in the deep copy. >>> s = pd.Series([[1, 2], [3, 4]]) >>> deep = s.copy() >>> s[0][0] = 10 >>> s 0 [10, 2] 1 [3, 4] dtype: object >>> deep 0 [10, 2] 1 [3, 4] dtype: object \"\"\" data = self . _mgr . copy ( deep = deep ) self . _clear_item_cache () return self . _constructor ( data ). __finalize__ ( self , method = \"copy\" ) corr def corr ( self , method = 'pearson' , min_periods = 1 ) -> 'DataFrame' Compute pairwise correlation of columns, excluding NA/null values. Parameters method : {'pearson', 'kendall', 'spearman'} or callable Method of correlation: * pearson : standard correlation coefficient * kendall : Kendall Tau correlation coefficient * spearman : Spearman rank correlation * callable : callable with input two 1 d ndarrays and returning a float . Note that the returned matrix from corr will have 1 along the diagonals and will be symmetric regardless of the callable ' s behavior . .. versionadded :: 0 . 24 . 0 min_periods : int, optional Minimum number of observations required per pair of columns to have a valid result. Currently only available for Pearson and Spearman correlation. Returns DataFrame Correlation matrix. See Also DataFrame.corrwith : Compute pairwise correlation with another DataFrame or Series. Series.corr : Compute the correlation between two Series. Examples def histogram_intersection(a, b): ... v = np.minimum(a, b).sum().round(decimals=1) ... return v df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)], ... columns=['dogs', 'cats']) df.corr(method=histogram_intersection) dogs cats dogs 1.0 0.3 cats 0.3 1.0 View Source def corr ( self , method = \"pearson\" , min_periods = 1 ) -> \"DataFrame\" : \"\"\" Compute pairwise correlation of columns, excluding NA/null values. Parameters ---------- method : {'pearson', 'kendall', 'spearman'} or callable Method of correlation: * pearson : standard correlation coefficient * kendall : Kendall Tau correlation coefficient * spearman : Spearman rank correlation * callable: callable with input two 1d ndarrays and returning a float. Note that the returned matrix from corr will have 1 along the diagonals and will be symmetric regardless of the callable's behavior. .. versionadded:: 0.24.0 min_periods : int, optional Minimum number of observations required per pair of columns to have a valid result. Currently only available for Pearson and Spearman correlation. Returns ------- DataFrame Correlation matrix. See Also -------- DataFrame.corrwith : Compute pairwise correlation with another DataFrame or Series. Series.corr : Compute the correlation between two Series. Examples -------- >>> def histogram_intersection(a, b): ... v = np.minimum(a, b).sum().round(decimals=1) ... return v >>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)], ... columns=['dogs', 'cats']) >>> df.corr(method=histogram_intersection) dogs cats dogs 1.0 0.3 cats 0.3 1.0 \"\"\" numeric_df = self . _get_numeric_data () cols = numeric_df . columns idx = cols . copy () mat = numeric_df . to_numpy ( dtype = float , na_value = np . nan , copy = False ) if method == \"pearson\" : correl = libalgos . nancorr ( mat , minp = min_periods ) elif method == \"spearman\" : correl = libalgos . nancorr_spearman ( mat , minp = min_periods ) elif method == \"kendall\" or callable ( method ) : if min_periods is None : min_periods = 1 mat = mat . T corrf = nanops . get_corr_func ( method ) K = len ( cols ) correl = np . empty (( K , K ), dtype = float ) mask = np . isfinite ( mat ) for i , ac in enumerate ( mat ) : for j , bc in enumerate ( mat ) : if i > j : continue valid = mask [ i ] & mask [ j ] if valid . sum () < min_periods : c = np . nan elif i == j : c = 1.0 elif not valid . all () : c = corrf ( ac [ valid ] , bc [ valid ] ) else : c = corrf ( ac , bc ) correl [ i, j ] = c correl [ j, i ] = c else : raise ValueError ( \"method must be either 'pearson', \" \"'spearman', 'kendall', or a callable, \" f \"'{method}' was supplied\" ) return self . _constructor ( correl , index = idx , columns = cols ) corrwith def corrwith ( self , other , axis = 0 , drop = False , method = 'pearson' ) -> pandas . core . series . Series Compute pairwise correlation. Pairwise correlation is computed between rows or columns of DataFrame with rows or columns of Series or DataFrame. DataFrames are first aligned along both axes before computing the correlations. Parameters other : DataFrame, Series Object with which to compute correlations. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' to compute column-wise, 1 or 'columns' for row-wise. drop : bool, default False Drop missing indices from result. method : {'pearson', 'kendall', 'spearman'} or callable Method of correlation: * pearson : standard correlation coefficient * kendall : Kendall Tau correlation coefficient * spearman : Spearman rank correlation * callable : callable with input two 1 d ndarrays and returning a float . .. versionadded :: 0 . 24 . 0 Returns Series Pairwise correlations. See Also DataFrame.corr : Compute pairwise correlation of columns. View Source def corrwith ( self , other , axis = 0 , drop = False , method = \"pearson\" ) -> Series : \"\"\" Compute pairwise correlation. Pairwise correlation is computed between rows or columns of DataFrame with rows or columns of Series or DataFrame. DataFrames are first aligned along both axes before computing the correlations. Parameters ---------- other : DataFrame, Series Object with which to compute correlations. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' to compute column-wise, 1 or 'columns' for row-wise. drop : bool, default False Drop missing indices from result. method : {'pearson', 'kendall', 'spearman'} or callable Method of correlation: * pearson : standard correlation coefficient * kendall : Kendall Tau correlation coefficient * spearman : Spearman rank correlation * callable: callable with input two 1d ndarrays and returning a float. .. versionadded:: 0.24.0 Returns ------- Series Pairwise correlations. See Also -------- DataFrame.corr : Compute pairwise correlation of columns. \"\"\" axis = self . _get_axis_number ( axis ) this = self . _get_numeric_data () if isinstance ( other , Series ): return this . apply ( lambda x : other . corr ( x , method = method ), axis = axis ) other = other . _get_numeric_data () left , right = this . align ( other , join = \"inner\" , copy = False ) if axis == 1 : left = left . T right = right . T if method == \"pearson\" : # mask missing values left = left + right * 0 right = right + left * 0 # demeaned data ldem = left - left . mean () rdem = right - right . mean () num = ( ldem * rdem ). sum () dom = ( left . count () - 1 ) * left . std () * right . std () correl = num / dom elif method in [ \"kendall\" , \"spearman\" ] or callable ( method ): def c ( x ): return nanops . nancorr ( x [ 0 ], x [ 1 ], method = method ) correl = self . _constructor_sliced ( map ( c , zip ( left . values . T , right . values . T )), index = left . columns ) else : raise ValueError ( f \"Invalid method {method} was passed, \" \"valid methods are: 'pearson', 'kendall', \" \"'spearman', or callable\" ) if not drop : # Find non - matching labels along the given axis # and append missing correlations ( GH 22375 ) raxis = 1 if axis == 0 else 0 result_index = this . _get_axis ( raxis ). union ( other . _get_axis ( raxis )) idx_diff = result_index . difference ( correl . index ) if len ( idx_diff ) > 0 : correl = correl . append ( Series ([ np . nan ] * len ( idx_diff ), index = idx_diff )) return correl count def count ( self , axis = 0 , level = None , numeric_only = False ) Count non-NA cells for each column or row. The values None , NaN , NaT , and optionally numpy.inf (depending on pandas.options.mode.use_inf_as_na ) are considered NA. Parameters axis : {0 or 'index', 1 or 'columns'}, default 0 If 0 or 'index' counts are generated for each column. If 1 or 'columns' counts are generated for each row. level : int or str, optional If the axis is a MultiIndex (hierarchical), count along a particular level , collapsing into a DataFrame . A str specifies the level name. numeric_only : bool, default False Include only float , int or boolean data. Returns Series or DataFrame For each column/row the number of non-NA/null entries. If level is specified returns a DataFrame . See Also Series.count: Number of non-NA elements in a Series. DataFrame.shape: Number of DataFrame rows and columns (including NA elements). DataFrame.isna: Boolean same-sized DataFrame showing places of NA elements. Examples Constructing DataFrame from a dictionary: df = pd.DataFrame({\"Person\": ... [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"], ... \"Age\": [24., np.nan, 21., 33, 26], ... \"Single\": [False, True, True, True, False]}) df Person Age Single 0 John 24.0 False 1 Myla NaN True 2 Lewis 21.0 True 3 John 33.0 True 4 Myla 26.0 False Notice the uncounted NA values: df.count() Person 5 Age 4 Single 5 dtype: int64 Counts for each row : df.count(axis='columns') 0 3 1 2 2 3 3 3 4 3 dtype: int64 Counts for one level of a MultiIndex : df.set_index([\"Person\", \"Single\"]).count(level=\"Person\") Age Person John 2 Lewis 1 Myla 1 View Source def count ( self , axis = 0 , level = None , numeric_only = False ): \"\"\" Count non-NA cells for each column or row. The values `None`, `NaN`, `NaT`, and optionally `numpy.inf` (depending on `pandas.options.mode.use_inf_as_na`) are considered NA. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 If 0 or 'index' counts are generated for each column. If 1 or 'columns' counts are generated for each row. level : int or str, optional If the axis is a `MultiIndex` (hierarchical), count along a particular `level`, collapsing into a `DataFrame`. A `str` specifies the level name. numeric_only : bool, default False Include only `float`, `int` or `boolean` data. Returns ------- Series or DataFrame For each column/row the number of non-NA/null entries. If `level` is specified returns a `DataFrame`. See Also -------- Series.count: Number of non-NA elements in a Series. DataFrame.shape: Number of DataFrame rows and columns (including NA elements). DataFrame.isna: Boolean same-sized DataFrame showing places of NA elements. Examples -------- Constructing DataFrame from a dictionary: >>> df = pd.DataFrame({\" Person \": ... [\" John \", \" Myla \", \" Lewis \", \" John \", \" Myla \"], ... \" Age \": [24., np.nan, 21., 33, 26], ... \" Single \": [False, True, True, True, False]}) >>> df Person Age Single 0 John 24.0 False 1 Myla NaN True 2 Lewis 21.0 True 3 John 33.0 True 4 Myla 26.0 False Notice the uncounted NA values: >>> df.count() Person 5 Age 4 Single 5 dtype: int64 Counts for each **row**: >>> df.count(axis='columns') 0 3 1 2 2 3 3 3 4 3 dtype: int64 Counts for one level of a `MultiIndex`: >>> df.set_index([\" Person \", \" Single \"]).count(level=\" Person \") Age Person John 2 Lewis 1 Myla 1 \"\"\" axis = self . _get_axis_number ( axis ) if level is not None : return self . _count_level ( level , axis = axis , numeric_only = numeric_only ) if numeric_only : frame = self . _get_numeric_data () else : frame = self # GH #423 if len ( frame . _get_axis ( axis )) == 0 : result = self . _constructor_sliced ( 0 , index = frame . _get_agg_axis ( axis )) else : if frame . _is_mixed_type or frame . _mgr . any_extension_types : # the or any_extension_types is really only hit for single- # column frames with an extension array result = notna ( frame ). sum ( axis = axis ) else : # GH13407 series_counts = notna ( frame ). sum ( axis = axis ) counts = series_counts . values result = self . _constructor_sliced ( counts , index = frame . _get_agg_axis ( axis ) ) return result . astype ( \"int64\" ) cov def cov ( self , min_periods : Union [ int , NoneType ] = None , ddof : Union [ int , NoneType ] = 1 ) -> 'DataFrame' Compute pairwise covariance of columns, excluding NA/null values. Compute the pairwise covariance among the series of a DataFrame. The returned data frame is the covariance matrix <https://en.wikipedia.org/wiki/Covariance_matrix> __ of the columns of the DataFrame. Both NA and null values are automatically excluded from the calculation. (See the note below about bias from missing values.) A threshold can be set for the minimum number of observations for each value created. Comparisons with observations below this threshold will be returned as NaN . This method is generally used for the analysis of time series data to understand the relationship between different measures across time. Parameters min_periods : int, optional Minimum number of observations required per pair of columns to have a valid result. ddof : int, default 1 Delta degrees of freedom. The divisor used in calculations is N - ddof , where N represents the number of elements. .. versionadded :: 1.1.0 Returns DataFrame The covariance matrix of the series of the DataFrame. See Also Series.cov : Compute covariance with another Series. core.window.ExponentialMovingWindow.cov: Exponential weighted sample covariance. core.window.Expanding.cov : Expanding sample covariance. core.window.Rolling.cov : Rolling sample covariance. Notes Returns the covariance matrix of the DataFrame's time series. The covariance is normalized by N-ddof. For DataFrames that have Series that are missing data (assuming that data is missing at random <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random> __) the returned covariance matrix will be an unbiased estimate of the variance and covariance between the member Series. However, for many applications this estimate may not be acceptable because the estimate covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimate correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See Estimation of covariance matrices <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_ matrices> __ for more details. Examples df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)], ... columns=['dogs', 'cats']) df.cov() dogs cats dogs 0.666667 -1.000000 cats -1.000000 1.666667 np.random.seed(42) df = pd.DataFrame(np.random.randn(1000, 5), ... columns=['a', 'b', 'c', 'd', 'e']) df.cov() a b c d e a 0.998438 -0.020161 0.059277 -0.008943 0.014144 b -0.020161 1.059352 -0.008543 -0.024738 0.009826 c 0.059277 -0.008543 1.010670 -0.001486 -0.000271 d -0.008943 -0.024738 -0.001486 0.921297 -0.013692 e 0.014144 0.009826 -0.000271 -0.013692 0.977795 Minimum number of periods This method also supports an optional min_periods keyword that specifies the required minimum number of non-NA observations for each column pair in order to have a valid result: np.random.seed(42) df = pd.DataFrame(np.random.randn(20, 3), ... columns=['a', 'b', 'c']) df.loc[df.index[:5], 'a'] = np.nan df.loc[df.index[5:10], 'b'] = np.nan df.cov(min_periods=12) a b c a 0.316741 NaN -0.150812 b NaN 1.248003 0.191417 c -0.150812 0.191417 0.895202 View Source def cov ( self , min_periods : Optional [ int ] = None , ddof : Optional [ int ] = 1 ) -> \"DataFrame\" : \"\"\" Compute pairwise covariance of columns, excluding NA/null values. Compute the pairwise covariance among the series of a DataFrame. The returned data frame is the `covariance matrix <https://en.wikipedia.org/wiki/Covariance_matrix>`__ of the columns of the DataFrame. Both NA and null values are automatically excluded from the calculation. (See the note below about bias from missing values.) A threshold can be set for the minimum number of observations for each value created. Comparisons with observations below this threshold will be returned as ``NaN``. This method is generally used for the analysis of time series data to understand the relationship between different measures across time. Parameters ---------- min_periods : int, optional Minimum number of observations required per pair of columns to have a valid result. ddof : int, default 1 Delta degrees of freedom. The divisor used in calculations is ``N - ddof``, where ``N`` represents the number of elements. .. versionadded:: 1.1.0 Returns ------- DataFrame The covariance matrix of the series of the DataFrame. See Also -------- Series.cov : Compute covariance with another Series. core.window.ExponentialMovingWindow.cov: Exponential weighted sample covariance. core.window.Expanding.cov : Expanding sample covariance. core.window.Rolling.cov : Rolling sample covariance. Notes ----- Returns the covariance matrix of the DataFrame's time series. The covariance is normalized by N-ddof. For DataFrames that have Series that are missing data (assuming that data is `missing at random <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random>`__) the returned covariance matrix will be an unbiased estimate of the variance and covariance between the member Series. However, for many applications this estimate may not be acceptable because the estimate covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimate correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See `Estimation of covariance matrices <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_ matrices>`__ for more details. Examples -------- >>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)], ... columns=['dogs', 'cats']) >>> df.cov() dogs cats dogs 0.666667 -1.000000 cats -1.000000 1.666667 >>> np.random.seed(42) >>> df = pd.DataFrame(np.random.randn(1000, 5), ... columns=['a', 'b', 'c', 'd', 'e']) >>> df.cov() a b c d e a 0.998438 -0.020161 0.059277 -0.008943 0.014144 b -0.020161 1.059352 -0.008543 -0.024738 0.009826 c 0.059277 -0.008543 1.010670 -0.001486 -0.000271 d -0.008943 -0.024738 -0.001486 0.921297 -0.013692 e 0.014144 0.009826 -0.000271 -0.013692 0.977795 **Minimum number of periods** This method also supports an optional ``min_periods`` keyword that specifies the required minimum number of non-NA observations for each column pair in order to have a valid result: >>> np.random.seed(42) >>> df = pd.DataFrame(np.random.randn(20, 3), ... columns=['a', 'b', 'c']) >>> df.loc[df.index[:5], 'a'] = np.nan >>> df.loc[df.index[5:10], 'b'] = np.nan >>> df.cov(min_periods=12) a b c a 0.316741 NaN -0.150812 b NaN 1.248003 0.191417 c -0.150812 0.191417 0.895202 \"\"\" numeric_df = self . _get_numeric_data () cols = numeric_df . columns idx = cols . copy () mat = numeric_df . to_numpy ( dtype = float , na_value = np . nan , copy = False ) if notna ( mat ). all (): if min_periods is not None and min_periods > len ( mat ): base_cov = np . empty (( mat . shape [ 1 ], mat . shape [ 1 ])) base_cov . fill ( np . nan ) else : base_cov = np . cov ( mat . T , ddof = ddof ) base_cov = base_cov . reshape (( len ( cols ), len ( cols ))) else : base_cov = libalgos . nancorr ( mat , cov = True , minp = min_periods ) return self . _constructor ( base_cov , index = idx , columns = cols ) cummax def cummax ( self , axis = None , skipna = True , * args , ** kwargs ) Return cumulative maximum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative maximum. Parameters axis : {0 or 'index', 1 or 'columns'}, default 0 The index or the name of the axis. 0 is equivalent to None or 'index'. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. args, *kwargs Additional keywords have no effect but might be accepted for compatibility with NumPy. Returns Series or DataFrame Return cumulative maximum of Series or DataFrame. See Also core.window.Expanding.max : Similar functionality but ignores NaN values. DataFrame.max : Return the maximum over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis. Examples Series s = pd.Series([2, np.nan, 5, -1, 0]) s 0 2.0 1 NaN 2 5.0 3 -1.0 4 0.0 dtype: float64 By default, NA values are ignored. s.cummax() 0 2.0 1 NaN 2 5.0 3 5.0 4 5.0 dtype: float64 To include NA values in the operation, use skipna=False s.cummax(skipna=False) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 DataFrame df = pd.DataFrame([[2.0, 1.0], ... [3.0, np.nan], ... [1.0, 0.0]], ... columns=list('AB')) df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the maximum in each column. This is equivalent to axis=None or axis='index' . df.cummax() A B 0 2.0 1.0 1 3.0 NaN 2 3.0 1.0 To iterate over columns and find the maximum in each row, use axis=1 df.cummax(axis=1) A B 0 2.0 2.0 1 3.0 NaN 2 1.0 1.0 View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , accum_func_name = accum_func_name , examples = examples , ) @Appender ( _cnum_doc ) def cum_func ( self , axis = None , skipna = True , * args , ** kwargs ) : skipna = nv . validate_cum_func_with_skipna ( skipna , args , kwargs , name ) if axis is None : axis = self . _stat_axis_number else : axis = self . _get_axis_number ( axis ) if axis == 1 : return cum_func ( self . T , axis = 0 , skipna = skipna , * args , ** kwargs ). T def block_accum_func ( blk_values ) : values = blk_values . T if hasattr ( blk_values , \"T\" ) else blk_values result = nanops . na_accum_func ( values , accum_func , skipna = skipna ) result = result . T if hasattr ( result , \"T\" ) else result return result result = self . _mgr . apply ( block_accum_func ) return self . _constructor ( result ). __finalize__ ( self , method = name ) cummin def cummin ( self , axis = None , skipna = True , * args , ** kwargs ) Return cumulative minimum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative minimum. Parameters axis : {0 or 'index', 1 or 'columns'}, default 0 The index or the name of the axis. 0 is equivalent to None or 'index'. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. args, *kwargs Additional keywords have no effect but might be accepted for compatibility with NumPy. Returns Series or DataFrame Return cumulative minimum of Series or DataFrame. See Also core.window.Expanding.min : Similar functionality but ignores NaN values. DataFrame.min : Return the minimum over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis. Examples Series s = pd.Series([2, np.nan, 5, -1, 0]) s 0 2.0 1 NaN 2 5.0 3 -1.0 4 0.0 dtype: float64 By default, NA values are ignored. s.cummin() 0 2.0 1 NaN 2 2.0 3 -1.0 4 -1.0 dtype: float64 To include NA values in the operation, use skipna=False s.cummin(skipna=False) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 DataFrame df = pd.DataFrame([[2.0, 1.0], ... [3.0, np.nan], ... [1.0, 0.0]], ... columns=list('AB')) df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the minimum in each column. This is equivalent to axis=None or axis='index' . df.cummin() A B 0 2.0 1.0 1 2.0 NaN 2 1.0 0.0 To iterate over columns and find the minimum in each row, use axis=1 df.cummin(axis=1) A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , accum_func_name = accum_func_name , examples = examples , ) @Appender ( _cnum_doc ) def cum_func ( self , axis = None , skipna = True , * args , ** kwargs ) : skipna = nv . validate_cum_func_with_skipna ( skipna , args , kwargs , name ) if axis is None : axis = self . _stat_axis_number else : axis = self . _get_axis_number ( axis ) if axis == 1 : return cum_func ( self . T , axis = 0 , skipna = skipna , * args , ** kwargs ). T def block_accum_func ( blk_values ) : values = blk_values . T if hasattr ( blk_values , \"T\" ) else blk_values result = nanops . na_accum_func ( values , accum_func , skipna = skipna ) result = result . T if hasattr ( result , \"T\" ) else result return result result = self . _mgr . apply ( block_accum_func ) return self . _constructor ( result ). __finalize__ ( self , method = name ) cumprod def cumprod ( self , axis = None , skipna = True , * args , ** kwargs ) Return cumulative product over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative product. Parameters axis : {0 or 'index', 1 or 'columns'}, default 0 The index or the name of the axis. 0 is equivalent to None or 'index'. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. args, *kwargs Additional keywords have no effect but might be accepted for compatibility with NumPy. Returns Series or DataFrame Return cumulative product of Series or DataFrame. See Also core.window.Expanding.prod : Similar functionality but ignores NaN values. DataFrame.prod : Return the product over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis. Examples Series s = pd.Series([2, np.nan, 5, -1, 0]) s 0 2.0 1 NaN 2 5.0 3 -1.0 4 0.0 dtype: float64 By default, NA values are ignored. s.cumprod() 0 2.0 1 NaN 2 10.0 3 -10.0 4 -0.0 dtype: float64 To include NA values in the operation, use skipna=False s.cumprod(skipna=False) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 DataFrame df = pd.DataFrame([[2.0, 1.0], ... [3.0, np.nan], ... [1.0, 0.0]], ... columns=list('AB')) df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the product in each column. This is equivalent to axis=None or axis='index' . df.cumprod() A B 0 2.0 1.0 1 6.0 NaN 2 6.0 0.0 To iterate over columns and find the product in each row, use axis=1 df.cumprod(axis=1) A B 0 2.0 2.0 1 3.0 NaN 2 1.0 0.0 View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , accum_func_name = accum_func_name , examples = examples , ) @Appender ( _cnum_doc ) def cum_func ( self , axis = None , skipna = True , * args , ** kwargs ) : skipna = nv . validate_cum_func_with_skipna ( skipna , args , kwargs , name ) if axis is None : axis = self . _stat_axis_number else : axis = self . _get_axis_number ( axis ) if axis == 1 : return cum_func ( self . T , axis = 0 , skipna = skipna , * args , ** kwargs ). T def block_accum_func ( blk_values ) : values = blk_values . T if hasattr ( blk_values , \"T\" ) else blk_values result = nanops . na_accum_func ( values , accum_func , skipna = skipna ) result = result . T if hasattr ( result , \"T\" ) else result return result result = self . _mgr . apply ( block_accum_func ) return self . _constructor ( result ). __finalize__ ( self , method = name ) cumsum def cumsum ( self , axis = None , skipna = True , * args , ** kwargs ) Return cumulative sum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative sum. Parameters axis : {0 or 'index', 1 or 'columns'}, default 0 The index or the name of the axis. 0 is equivalent to None or 'index'. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. args, *kwargs Additional keywords have no effect but might be accepted for compatibility with NumPy. Returns Series or DataFrame Return cumulative sum of Series or DataFrame. See Also core.window.Expanding.sum : Similar functionality but ignores NaN values. DataFrame.sum : Return the sum over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis. Examples Series s = pd.Series([2, np.nan, 5, -1, 0]) s 0 2.0 1 NaN 2 5.0 3 -1.0 4 0.0 dtype: float64 By default, NA values are ignored. s.cumsum() 0 2.0 1 NaN 2 7.0 3 6.0 4 6.0 dtype: float64 To include NA values in the operation, use skipna=False s.cumsum(skipna=False) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 DataFrame df = pd.DataFrame([[2.0, 1.0], ... [3.0, np.nan], ... [1.0, 0.0]], ... columns=list('AB')) df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the sum in each column. This is equivalent to axis=None or axis='index' . df.cumsum() A B 0 2.0 1.0 1 5.0 NaN 2 6.0 1.0 To iterate over columns and find the sum in each row, use axis=1 df.cumsum(axis=1) A B 0 2.0 3.0 1 3.0 NaN 2 1.0 1.0 View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , accum_func_name = accum_func_name , examples = examples , ) @Appender ( _cnum_doc ) def cum_func ( self , axis = None , skipna = True , * args , ** kwargs ) : skipna = nv . validate_cum_func_with_skipna ( skipna , args , kwargs , name ) if axis is None : axis = self . _stat_axis_number else : axis = self . _get_axis_number ( axis ) if axis == 1 : return cum_func ( self . T , axis = 0 , skipna = skipna , * args , ** kwargs ). T def block_accum_func ( blk_values ) : values = blk_values . T if hasattr ( blk_values , \"T\" ) else blk_values result = nanops . na_accum_func ( values , accum_func , skipna = skipna ) result = result . T if hasattr ( result , \"T\" ) else result return result result = self . _mgr . apply ( block_accum_func ) return self . _constructor ( result ). __finalize__ ( self , method = name ) describe def describe ( self : ~ FrameOrSeries , percentiles = None , include = None , exclude = None , datetime_is_numeric = False ) -> ~ FrameOrSeries Generate descriptive statistics. Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding NaN values. Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail. Parameters percentiles : list-like of numbers, optional The percentiles to include in the output. All should fall between 0 and 1. The default is [.25, .5, .75] , which returns the 25th, 50th, and 75th percentiles. include : 'all', list-like of dtypes or None (default), optional A white list of data types to include in the result. Ignored for Series . Here are the options: - 'all' : All columns of the input will be included in the output . - A list - like of dtypes : Limits the results to the provided data types . To limit the result to numeric types submit `` numpy . number `` . To limit it instead to object columns submit the `` numpy . object `` data type . Strings can also be used in the style of `` select_dtypes `` ( e . g . `` df . describe ( include = [ 'O' ]) `` ). To select pandas categorical columns , use `` 'category' `` - None ( default ) : The result will include all numeric columns . exclude : list-like of dtypes or None (default), optional, A black list of data types to omit from the result. Ignored for Series . Here are the options: - A list - like of dtypes : Excludes the provided data types from the result . To exclude numeric types submit `` numpy . number `` . To exclude object columns submit the data type `` numpy . object `` . Strings can also be used in the style of `` select_dtypes `` ( e . g . `` df . describe ( include = [ 'O' ]) `` ). To exclude pandas categorical columns , use `` 'category' `` - None ( default ) : The result will exclude nothing . datetime_is_numeric : bool, default False Whether to treat datetime dtypes as numeric. This affects statistics calculated for the column. For DataFrame input, this also controls whether datetime columns are included by default. .. versionadded :: 1.1.0 Returns Series or DataFrame Summary statistics of the Series or Dataframe provided. See Also DataFrame.count: Count number of non-NA/null observations. DataFrame.max: Maximum of the values in the object. DataFrame.min: Minimum of the values in the object. DataFrame.mean: Mean of the values. DataFrame.std: Standard deviation of the observations. DataFrame.select_dtypes: Subset of a DataFrame including/excluding columns based on their dtype. Notes For numeric data, the result's index will include count , mean , std , min , max as well as lower, 50 and upper percentiles. By default the lower percentile is 25 and the upper percentile is 75 . The 50 percentile is the same as the median. For object data (e.g. strings or timestamps), the result's index will include count , unique , top , and freq . The top is the most common value. The freq is the most common value's frequency. Timestamps also include the first and last items. If multiple object values have the highest count, then the count and top results will be arbitrarily chosen from among those with the highest count. For mixed data types provided via a DataFrame , the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type. The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series . Examples Describing a numeric Series . s = pd.Series([1, 2, 3]) s.describe() count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 dtype: float64 Describing a categorical Series . s = pd.Series(['a', 'a', 'b', 'c']) s.describe() count 4 unique 3 top a freq 2 dtype: object Describing a timestamp Series . s = pd.Series([ ... np.datetime64(\"2000-01-01\"), ... np.datetime64(\"2010-01-01\"), ... np.datetime64(\"2010-01-01\") ... ]) s.describe(datetime_is_numeric=True) count 3 mean 2006-09-01 08:00:00 min 2000-01-01 00:00:00 25% 2004-12-31 12:00:00 50% 2010-01-01 00:00:00 75% 2010-01-01 00:00:00 max 2010-01-01 00:00:00 dtype: object Describing a DataFrame . By default only numeric fields are returned. df = pd.DataFrame({'categorical': pd.Categorical(['d','e','f']), ... 'numeric': [1, 2, 3], ... 'object': ['a', 'b', 'c'] ... }) df.describe() numeric count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 Describing all columns of a DataFrame regardless of data type. df.describe(include='all') # doctest: +SKIP categorical numeric object count 3 3.0 3 unique 3 NaN 3 top f NaN a freq 1 NaN 1 mean NaN 2.0 NaN std NaN 1.0 NaN min NaN 1.0 NaN 25% NaN 1.5 NaN 50% NaN 2.0 NaN 75% NaN 2.5 NaN max NaN 3.0 NaN Describing a column from a DataFrame by accessing it as an attribute. df.numeric.describe() count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 Name: numeric, dtype: float64 Including only numeric columns in a DataFrame description. df.describe(include=[np.number]) numeric count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 Including only string columns in a DataFrame description. df.describe(include=[object]) # doctest: +SKIP object count 3 unique 3 top a freq 1 Including only categorical columns from a DataFrame description. df.describe(include=['category']) categorical count 3 unique 3 top f freq 1 Excluding numeric columns from a DataFrame description. df.describe(exclude=[np.number]) # doctest: +SKIP categorical object count 3 3 unique 3 3 top f a freq 1 1 Excluding object columns from a DataFrame description. df.describe(exclude=[object]) # doctest: +SKIP categorical numeric count 3 3.0 unique 3 NaN top f NaN freq 1 NaN mean NaN 2.0 std NaN 1.0 min NaN 1.0 25% NaN 1.5 50% NaN 2.0 75% NaN 2.5 max NaN 3.0 View Source def describe ( self : FrameOrSeries , percentiles = None , include = None , exclude = None , datetime_is_numeric = False , ) -> FrameOrSeries : \"\"\" Generate descriptive statistics. Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding ``NaN`` values. Analyzes both numeric and object series, as well as ``DataFrame`` column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail. Parameters ---------- percentiles : list-like of numbers, optional The percentiles to include in the output. All should fall between 0 and 1. The default is ``[.25, .5, .75]``, which returns the 25th, 50th, and 75th percentiles. include : 'all', list-like of dtypes or None (default), optional A white list of data types to include in the result. Ignored for ``Series``. Here are the options: - 'all' : All columns of the input will be included in the output. - A list-like of dtypes : Limits the results to the provided data types. To limit the result to numeric types submit ``numpy.number``. To limit it instead to object columns submit the ``numpy.object`` data type. Strings can also be used in the style of ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To select pandas categorical columns, use ``'category'`` - None (default) : The result will include all numeric columns. exclude : list-like of dtypes or None (default), optional, A black list of data types to omit from the result. Ignored for ``Series``. Here are the options: - A list-like of dtypes : Excludes the provided data types from the result. To exclude numeric types submit ``numpy.number``. To exclude object columns submit the data type ``numpy.object``. Strings can also be used in the style of ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To exclude pandas categorical columns, use ``'category'`` - None (default) : The result will exclude nothing. datetime_is_numeric : bool, default False Whether to treat datetime dtypes as numeric. This affects statistics calculated for the column. For DataFrame input, this also controls whether datetime columns are included by default. .. versionadded:: 1.1.0 Returns ------- Series or DataFrame Summary statistics of the Series or Dataframe provided. See Also -------- DataFrame.count: Count number of non-NA/null observations. DataFrame.max: Maximum of the values in the object. DataFrame.min: Minimum of the values in the object. DataFrame.mean: Mean of the values. DataFrame.std: Standard deviation of the observations. DataFrame.select_dtypes: Subset of a DataFrame including/excluding columns based on their dtype. Notes ----- For numeric data, the result's index will include ``count``, ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and upper percentiles. By default the lower percentile is ``25`` and the upper percentile is ``75``. The ``50`` percentile is the same as the median. For object data (e.g. strings or timestamps), the result's index will include ``count``, ``unique``, ``top``, and ``freq``. The ``top`` is the most common value. The ``freq`` is the most common value's frequency. Timestamps also include the ``first`` and ``last`` items. If multiple object values have the highest count, then the ``count`` and ``top`` results will be arbitrarily chosen from among those with the highest count. For mixed data types provided via a ``DataFrame``, the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If ``include='all'`` is provided as an option, the result will include a union of attributes of each type. The `include` and `exclude` parameters can be used to limit which columns in a ``DataFrame`` are analyzed for the output. The parameters are ignored when analyzing a ``Series``. Examples -------- Describing a numeric ``Series``. >>> s = pd.Series([1, 2, 3]) >>> s.describe() count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 dtype: float64 Describing a categorical ``Series``. >>> s = pd.Series(['a', 'a', 'b', 'c']) >>> s.describe() count 4 unique 3 top a freq 2 dtype: object Describing a timestamp ``Series``. >>> s = pd.Series([ ... np.datetime64(\" 2000 - 01 - 01 \"), ... np.datetime64(\" 2010 - 01 - 01 \"), ... np.datetime64(\" 2010 - 01 - 01 \") ... ]) >>> s.describe(datetime_is_numeric=True) count 3 mean 2006-09-01 08:00:00 min 2000-01-01 00:00:00 25% 2004-12-31 12:00:00 50% 2010-01-01 00:00:00 75% 2010-01-01 00:00:00 max 2010-01-01 00:00:00 dtype: object Describing a ``DataFrame``. By default only numeric fields are returned. >>> df = pd.DataFrame({'categorical': pd.Categorical(['d','e','f']), ... 'numeric': [1, 2, 3], ... 'object': ['a', 'b', 'c'] ... }) >>> df.describe() numeric count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 Describing all columns of a ``DataFrame`` regardless of data type. >>> df.describe(include='all') # doctest: +SKIP categorical numeric object count 3 3.0 3 unique 3 NaN 3 top f NaN a freq 1 NaN 1 mean NaN 2.0 NaN std NaN 1.0 NaN min NaN 1.0 NaN 25% NaN 1.5 NaN 50% NaN 2.0 NaN 75% NaN 2.5 NaN max NaN 3.0 NaN Describing a column from a ``DataFrame`` by accessing it as an attribute. >>> df.numeric.describe() count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 Name: numeric, dtype: float64 Including only numeric columns in a ``DataFrame`` description. >>> df.describe(include=[np.number]) numeric count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 Including only string columns in a ``DataFrame`` description. >>> df.describe(include=[object]) # doctest: +SKIP object count 3 unique 3 top a freq 1 Including only categorical columns from a ``DataFrame`` description. >>> df.describe(include=['category']) categorical count 3 unique 3 top f freq 1 Excluding numeric columns from a ``DataFrame`` description. >>> df.describe(exclude=[np.number]) # doctest: +SKIP categorical object count 3 3 unique 3 3 top f a freq 1 1 Excluding object columns from a ``DataFrame`` description. >>> df.describe(exclude=[object]) # doctest: +SKIP categorical numeric count 3 3.0 unique 3 NaN top f NaN freq 1 NaN mean NaN 2.0 std NaN 1.0 min NaN 1.0 25% NaN 1.5 50% NaN 2.0 75% NaN 2.5 max NaN 3.0 \"\"\" if self . ndim == 2 and self . columns . size == 0 : raise ValueError ( \"Cannot describe a DataFrame without columns\" ) if percentiles is not None : # explicit conversion of `percentiles` to list percentiles = list ( percentiles ) # get them all to be in [0, 1] validate_percentile ( percentiles ) # median should always be included if 0 . 5 not in percentiles : percentiles . append ( 0 . 5 ) percentiles = np . asarray ( percentiles ) else : percentiles = np . array ([ 0 . 25 , 0 . 5 , 0 . 75 ]) # sort and check for duplicates unique_pcts = np . unique ( percentiles ) if len ( unique_pcts ) < len ( percentiles ): raise ValueError ( \"percentiles cannot contain duplicates\" ) percentiles = unique_pcts formatted_percentiles = format_percentiles ( percentiles ) def describe_numeric_1d ( series ): stat_index = ( [ \"count\" , \"mean\" , \"std\" , \"min\" ] + formatted_percentiles + [ \"max\" ] ) d = ( [ series . count (), series . mean (), series . std (), series . min ()] + series . quantile ( percentiles ). tolist () + [ series . max ()] ) return pd . Series ( d , index = stat_index , name = series . name ) def describe_categorical_1d ( data ): names = [ \"count\" , \"unique\" ] objcounts = data . value_counts () count_unique = len ( objcounts [ objcounts != 0 ]) result = [ data . count (), count_unique ] dtype = None if result [ 1 ] > 0 : top , freq = objcounts . index [ 0 ], objcounts . iloc [ 0 ] if is_datetime64_any_dtype ( data . dtype ): if self . ndim == 1 : stacklevel = 4 else : stacklevel = 5 warnings . warn ( \"Treating datetime data as categorical rather than numeric in \" \"`.describe` is deprecated and will be removed in a future \" \"version of pandas. Specify `datetime_is_numeric=True` to \" \"silence this warning and adopt the future behavior now.\" , FutureWarning , stacklevel = stacklevel , ) tz = data . dt . tz asint = data . dropna (). values . view ( \"i8\" ) top = Timestamp ( top ) if top . tzinfo is not None and tz is not None : # Don't tz_localize(None) if key is already tz-aware top = top . tz_convert ( tz ) else : top = top . tz_localize ( tz ) names += [ \"top\" , \"freq\" , \"first\" , \"last\" ] result += [ top , freq , Timestamp ( asint . min (), tz = tz ), Timestamp ( asint . max (), tz = tz ), ] else : names += [ \"top\" , \"freq\" ] result += [ top , freq ] # If the DataFrame is empty, set 'top' and 'freq' to None # to maintain output shape consistency else : names += [ \"top\" , \"freq\" ] result += [ np . nan , np . nan ] dtype = \"object\" return pd . Series ( result , index = names , name = data . name , dtype = dtype ) def describe_timestamp_1d ( data ): # GH-30164 stat_index = [ \"count\" , \"mean\" , \"min\" ] + formatted_percentiles + [ \"max\" ] d = ( [ data . count (), data . mean (), data . min ()] + data . quantile ( percentiles ). tolist () + [ data . max ()] ) return pd . Series ( d , index = stat_index , name = data . name ) def describe_1d ( data ): if is_bool_dtype ( data . dtype ): return describe_categorical_1d ( data ) elif is_numeric_dtype ( data ): return describe_numeric_1d ( data ) elif is_datetime64_any_dtype ( data . dtype ) and datetime_is_numeric : return describe_timestamp_1d ( data ) elif is_timedelta64_dtype ( data . dtype ): return describe_numeric_1d ( data ) else : return describe_categorical_1d ( data ) if self . ndim == 1 : return describe_1d ( self ) elif ( include is None ) and ( exclude is None ): # when some numerics are found, keep only numerics default_include = [ np . number ] if datetime_is_numeric : default_include . append ( \"datetime\" ) data = self . select_dtypes ( include = default_include ) if len ( data . columns ) == 0 : data = self elif include == \"all\" : if exclude is not None : msg = \"exclude must be None when include is 'all'\" raise ValueError ( msg ) data = self else : data = self . select_dtypes ( include = include , exclude = exclude ) ldesc = [ describe_1d ( s ) for _ , s in data . items ()] # set a convenient order for rows names : List [ Label ] = [] ldesc_indexes = sorted (( x . index for x in ldesc ), key = len ) for idxnames in ldesc_indexes : for name in idxnames : if name not in names : names . append ( name ) d = pd . concat ([ x . reindex ( names , copy = False ) for x in ldesc ], axis = 1 , sort = False ) d . columns = data . columns . copy () return d diff def diff ( self , periods : int = 1 , axis : Union [ str , int ] = 0 ) -> 'DataFrame' First discrete difference of element. Calculates the difference of a Dataframe element compared with another element in the Dataframe (default is element in previous row). Parameters periods : int, default 1 Periods to shift for calculating difference, accepts negative values. axis : {0 or 'index', 1 or 'columns'}, default 0 Take difference over rows (0) or columns (1). Returns Dataframe First differences of the Series. See Also Dataframe.pct_change: Percent change over given number of periods. Dataframe.shift: Shift index by desired number of periods with an optional time freq. Series.diff: First discrete difference of object. Notes For boolean dtypes, this uses :meth: operator.xor rather than :meth: operator.sub . The result is calculated according to current dtype in Dataframe, however dtype of the result is always float64. Examples Difference with previous row df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6], ... 'b': [1, 1, 2, 3, 5, 8], ... 'c': [1, 4, 9, 16, 25, 36]}) df a b c 0 1 1 1 1 2 1 4 2 3 2 9 3 4 3 16 4 5 5 25 5 6 8 36 df.diff() a b c 0 NaN NaN NaN 1 1.0 0.0 3.0 2 1.0 1.0 5.0 3 1.0 1.0 7.0 4 1.0 2.0 9.0 5 1.0 3.0 11.0 Difference with previous column df.diff(axis=1) a b c 0 NaN 0.0 0.0 1 NaN -1.0 3.0 2 NaN -1.0 7.0 3 NaN -1.0 13.0 4 NaN 0.0 20.0 5 NaN 2.0 28.0 Difference with 3rd previous row df.diff(periods=3) a b c 0 NaN NaN NaN 1 NaN NaN NaN 2 NaN NaN NaN 3 3.0 2.0 15.0 4 3.0 4.0 21.0 5 3.0 6.0 27.0 Difference with following row df.diff(periods=-1) a b c 0 -1.0 0.0 -3.0 1 -1.0 -1.0 -5.0 2 -1.0 -1.0 -7.0 3 -1.0 -2.0 -9.0 4 -1.0 -3.0 -11.0 5 NaN NaN NaN Overflow in input dtype df = pd.DataFrame({'a': [1, 0]}, dtype=np.uint8) df.diff() a 0 NaN 1 255.0 View Source @doc ( Series . diff , klass = \"Dataframe\" , extra_params = \"axis : {0 or 'index', 1 or 'columns'}, default 0\\n \" \"Take difference over rows (0) or columns (1).\\n\" , other_klass = \"Series\" , examples = dedent ( \"\"\" Difference with previous row >>> df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6], ... 'b': [1, 1, 2, 3, 5, 8], ... 'c': [1, 4, 9, 16, 25, 36]}) >>> df a b c 0 1 1 1 1 2 1 4 2 3 2 9 3 4 3 16 4 5 5 25 5 6 8 36 >>> df.diff() a b c 0 NaN NaN NaN 1 1.0 0.0 3.0 2 1.0 1.0 5.0 3 1.0 1.0 7.0 4 1.0 2.0 9.0 5 1.0 3.0 11.0 Difference with previous column >>> df.diff(axis=1) a b c 0 NaN 0.0 0.0 1 NaN -1.0 3.0 2 NaN -1.0 7.0 3 NaN -1.0 13.0 4 NaN 0.0 20.0 5 NaN 2.0 28.0 Difference with 3rd previous row >>> df.diff(periods=3) a b c 0 NaN NaN NaN 1 NaN NaN NaN 2 NaN NaN NaN 3 3.0 2.0 15.0 4 3.0 4.0 21.0 5 3.0 6.0 27.0 Difference with following row >>> df.diff(periods=-1) a b c 0 -1.0 0.0 -3.0 1 -1.0 -1.0 -5.0 2 -1.0 -1.0 -7.0 3 -1.0 -2.0 -9.0 4 -1.0 -3.0 -11.0 5 NaN NaN NaN Overflow in input dtype >>> df = pd.DataFrame({'a': [1, 0]}, dtype=np.uint8) >>> df.diff() a 0 NaN 1 255.0\"\"\" ), ) def diff ( self , periods : int = 1 , axis : Axis = 0 ) -> \"DataFrame\" : bm_axis = self . _get_block_manager_axis ( axis ) self . _consolidate_inplace () if bm_axis == 0 and periods != 0 : return self . T . diff ( periods , axis = 0 ). T new_data = self . _mgr . diff ( n = periods , axis = bm_axis ) return self . _constructor ( new_data ) div def div ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Floating division of dataframe and other, element-wise (binary operator truediv ). Equivalent to dataframe / other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) divide def divide ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Floating division of dataframe and other, element-wise (binary operator truediv ). Equivalent to dataframe / other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) dot def dot ( self , other ) Compute the matrix multiplication between the DataFrame and other. This method computes the matrix product between the DataFrame and the values of an other Series, DataFrame or a numpy array. It can also be called using self @ other in Python >= 3.5. Parameters other : Series, DataFrame or array-like The other object to compute the matrix product with. Returns Series or DataFrame If other is a Series, return the matrix product between self and other as a Series. If other is a DataFrame or a numpy.array, return the matrix product of self and other in a DataFrame of a np.array. See Also Series.dot: Similar method for Series. Notes The dimensions of DataFrame and other must be compatible in order to compute the matrix multiplication. In addition, the column names of DataFrame and the index of other must contain the same values, as they will be aligned prior to the multiplication. The dot method for Series computes the inner product, instead of the matrix product here. Examples Here we multiply a DataFrame with a Series. df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]]) s = pd.Series([1, 1, 2, 1]) df.dot(s) 0 -4 1 5 dtype: int64 Here we multiply a DataFrame with another DataFrame. other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]]) df.dot(other) 0 1 0 1 4 1 2 2 Note that the dot method give the same result as @ df @ other 0 1 0 1 4 1 2 2 The dot method works also if other is an np.array. arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]]) df.dot(arr) 0 1 0 1 4 1 2 2 Note how shuffling of the objects does not change the result. s2 = s.reindex([1, 0, 2, 3]) df.dot(s2) 0 -4 1 5 dtype: int64 View Source def dot ( self , other ): \"\"\" Compute the matrix multiplication between the DataFrame and other. This method computes the matrix product between the DataFrame and the values of an other Series, DataFrame or a numpy array. It can also be called using ``self @ other`` in Python >= 3.5. Parameters ---------- other : Series, DataFrame or array-like The other object to compute the matrix product with. Returns ------- Series or DataFrame If other is a Series, return the matrix product between self and other as a Series. If other is a DataFrame or a numpy.array, return the matrix product of self and other in a DataFrame of a np.array. See Also -------- Series.dot: Similar method for Series. Notes ----- The dimensions of DataFrame and other must be compatible in order to compute the matrix multiplication. In addition, the column names of DataFrame and the index of other must contain the same values, as they will be aligned prior to the multiplication. The dot method for Series computes the inner product, instead of the matrix product here. Examples -------- Here we multiply a DataFrame with a Series. >>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]]) >>> s = pd.Series([1, 1, 2, 1]) >>> df.dot(s) 0 -4 1 5 dtype: int64 Here we multiply a DataFrame with another DataFrame. >>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]]) >>> df.dot(other) 0 1 0 1 4 1 2 2 Note that the dot method give the same result as @ >>> df @ other 0 1 0 1 4 1 2 2 The dot method works also if other is an np.array. >>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]]) >>> df.dot(arr) 0 1 0 1 4 1 2 2 Note how shuffling of the objects does not change the result. >>> s2 = s.reindex([1, 0, 2, 3]) >>> df.dot(s2) 0 -4 1 5 dtype: int64 \"\"\" if isinstance ( other , ( Series , DataFrame )): common = self . columns . union ( other . index ) if len ( common ) > len ( self . columns ) or len ( common ) > len ( other . index ): raise ValueError ( \"matrices are not aligned\" ) left = self . reindex ( columns = common , copy = False ) right = other . reindex ( index = common , copy = False ) lvals = left . values rvals = right . _values else : left = self lvals = self . values rvals = np . asarray ( other ) if lvals . shape [ 1 ] != rvals . shape [ 0 ]: raise ValueError ( f \"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\" ) if isinstance ( other , DataFrame ): return self . _constructor ( np . dot ( lvals , rvals ), index = left . index , columns = other . columns ) elif isinstance ( other , Series ): return self . _constructor_sliced ( np . dot ( lvals , rvals ), index = left . index ) elif isinstance ( rvals , ( np . ndarray , Index )): result = np . dot ( lvals , rvals ) if result . ndim == 2 : return self . _constructor ( result , index = left . index ) else : return self . _constructor_sliced ( result , index = left . index ) else : # pragma : no cover raise TypeError ( f \"unsupported type: {type(other)}\" ) drop def drop ( self , labels = None , axis = 0 , index = None , columns = None , level = None , inplace = False , errors = 'raise' ) Drop specified labels from rows or columns. Remove rows or columns by specifying label names and corresponding axis, or by specifying directly index or column names. When using a multi-index, labels on different levels can be removed by specifying the level. Parameters labels : single label or list-like Index or column labels to drop. axis : {0 or 'index', 1 or 'columns'}, default 0 Whether to drop labels from the index (0 or 'index') or columns (1 or 'columns'). index : single label or list-like Alternative to specifying axis ( labels, axis=0 is equivalent to index=labels ). columns : single label or list-like Alternative to specifying axis ( labels, axis=1 is equivalent to columns=labels ). level : int or level name, optional For MultiIndex, level from which the labels will be removed. inplace : bool, default False If False, return a copy. Otherwise, do operation inplace and return None. errors : {'ignore', 'raise'}, default 'raise' If 'ignore', suppress error and only existing labels are dropped. Returns DataFrame DataFrame without the removed index or column labels. Raises KeyError If any of the labels is not found in the selected axis. See Also DataFrame.loc : Label-location based indexer for selection by label. DataFrame.dropna : Return DataFrame with labels on given axis omitted where (all or any) data are missing. DataFrame.drop_duplicates : Return DataFrame with duplicate rows removed, optionally only considering certain columns. Series.drop : Return Series with specified index labels removed. Examples df = pd.DataFrame(np.arange(12).reshape(3, 4), ... columns=['A', 'B', 'C', 'D']) df A B C D 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 Drop columns df.drop(['B', 'C'], axis=1) A D 0 0 3 1 4 7 2 8 11 df.drop(columns=['B', 'C']) A D 0 0 3 1 4 7 2 8 11 Drop a row by index df.drop([0, 1]) A B C D 2 8 9 10 11 Drop columns and/or rows of MultiIndex DataFrame midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'], ... ['speed', 'weight', 'length']], ... codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2], ... [0, 1, 2, 0, 1, 2, 0, 1, 2]]) df = pd.DataFrame(index=midx, columns=['big', 'small'], ... data=[[45, 30], [200, 100], [1.5, 1], [30, 20], ... [250, 150], [1.5, 0.8], [320, 250], ... [1, 0.8], [0.3, 0.2]]) df big small lama speed 45.0 30.0 weight 200.0 100.0 length 1.5 1.0 cow speed 30.0 20.0 weight 250.0 150.0 length 1.5 0.8 falcon speed 320.0 250.0 weight 1.0 0.8 length 0.3 0.2 df.drop(index='cow', columns='small') big lama speed 45.0 weight 200.0 length 1.5 falcon speed 320.0 weight 1.0 length 0.3 df.drop(index='length', level=1) big small lama speed 45.0 30.0 weight 200.0 100.0 cow speed 30.0 20.0 weight 250.0 150.0 falcon speed 320.0 250.0 weight 1.0 0.8 View Source def drop ( self , labels = None , axis = 0 , index = None , columns = None , level = None , inplace = False , errors = \"raise\" , ): \"\"\" Drop specified labels from rows or columns. Remove rows or columns by specifying label names and corresponding axis, or by specifying directly index or column names. When using a multi-index, labels on different levels can be removed by specifying the level. Parameters ---------- labels : single label or list-like Index or column labels to drop. axis : {0 or 'index', 1 or 'columns'}, default 0 Whether to drop labels from the index (0 or 'index') or columns (1 or 'columns'). index : single label or list-like Alternative to specifying axis (``labels, axis=0`` is equivalent to ``index=labels``). columns : single label or list-like Alternative to specifying axis (``labels, axis=1`` is equivalent to ``columns=labels``). level : int or level name, optional For MultiIndex, level from which the labels will be removed. inplace : bool, default False If False, return a copy. Otherwise, do operation inplace and return None. errors : {'ignore', 'raise'}, default 'raise' If 'ignore', suppress error and only existing labels are dropped. Returns ------- DataFrame DataFrame without the removed index or column labels. Raises ------ KeyError If any of the labels is not found in the selected axis. See Also -------- DataFrame.loc : Label-location based indexer for selection by label. DataFrame.dropna : Return DataFrame with labels on given axis omitted where (all or any) data are missing. DataFrame.drop_duplicates : Return DataFrame with duplicate rows removed, optionally only considering certain columns. Series.drop : Return Series with specified index labels removed. Examples -------- >>> df = pd.DataFrame(np.arange(12).reshape(3, 4), ... columns=['A', 'B', 'C', 'D']) >>> df A B C D 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 Drop columns >>> df.drop(['B', 'C'], axis=1) A D 0 0 3 1 4 7 2 8 11 >>> df.drop(columns=['B', 'C']) A D 0 0 3 1 4 7 2 8 11 Drop a row by index >>> df.drop([0, 1]) A B C D 2 8 9 10 11 Drop columns and/or rows of MultiIndex DataFrame >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'], ... ['speed', 'weight', 'length']], ... codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2], ... [0, 1, 2, 0, 1, 2, 0, 1, 2]]) >>> df = pd.DataFrame(index=midx, columns=['big', 'small'], ... data=[[45, 30], [200, 100], [1.5, 1], [30, 20], ... [250, 150], [1.5, 0.8], [320, 250], ... [1, 0.8], [0.3, 0.2]]) >>> df big small lama speed 45.0 30.0 weight 200.0 100.0 length 1.5 1.0 cow speed 30.0 20.0 weight 250.0 150.0 length 1.5 0.8 falcon speed 320.0 250.0 weight 1.0 0.8 length 0.3 0.2 >>> df.drop(index='cow', columns='small') big lama speed 45.0 weight 200.0 length 1.5 falcon speed 320.0 weight 1.0 length 0.3 >>> df.drop(index='length', level=1) big small lama speed 45.0 30.0 weight 200.0 100.0 cow speed 30.0 20.0 weight 250.0 150.0 falcon speed 320.0 250.0 weight 1.0 0.8 \"\"\" return super (). drop ( labels = labels , axis = axis , index = index , columns = columns , level = level , inplace = inplace , errors = errors , ) drop_duplicates def drop_duplicates ( self , subset : Union [ Hashable , Sequence [ Hashable ], NoneType ] = None , keep : Union [ str , bool ] = 'first' , inplace : bool = False , ignore_index : bool = False ) -> Union [ ForwardRef ( 'DataFrame' ), NoneType ] Return DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored. Parameters subset : column label or sequence of labels, optional Only consider certain columns for identifying duplicates, by default use all of the columns. keep : {'first', 'last', False}, default 'first' Determines which duplicates (if any) to keep. - first : Drop duplicates except for the first occurrence. - last : Drop duplicates except for the last occurrence. - False : Drop all duplicates. inplace : bool, default False Whether to drop duplicates in place or to return a copy. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. .. versionadded :: 1.0.0 Returns DataFrame DataFrame with duplicates removed or None if inplace=True . See Also DataFrame.value_counts: Count unique combinations of columns. Examples Consider dataset containing ramen rating. df = pd.DataFrame({ ... 'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'], ... 'style': ['cup', 'cup', 'cup', 'pack', 'pack'], ... 'rating': [4, 4, 3.5, 15, 5] ... }) df brand style rating 0 Yum Yum cup 4.0 1 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 By default, it removes duplicate rows based on all columns. df.drop_duplicates() brand style rating 0 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 To remove duplicates on specific column(s), use subset . df.drop_duplicates(subset=['brand']) brand style rating 0 Yum Yum cup 4.0 2 Indomie cup 3.5 To remove duplicates and keep last occurences, use keep . df.drop_duplicates(subset=['brand', 'style'], keep='last') brand style rating 1 Yum Yum cup 4.0 2 Indomie cup 3.5 4 Indomie pack 5.0 View Source def drop_duplicates ( self , subset : Optional [ Union [ Hashable , Sequence [ Hashable ]]] = None , keep : Union [ str , bool ] = \"first\" , inplace : bool = False , ignore_index : bool = False , ) -> Optional [ \"DataFrame\" ]: \"\"\" Return DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored. Parameters ---------- subset : column label or sequence of labels, optional Only consider certain columns for identifying duplicates, by default use all of the columns. keep : {'first', 'last', False}, default 'first' Determines which duplicates (if any) to keep. - ``first`` : Drop duplicates except for the first occurrence. - ``last`` : Drop duplicates except for the last occurrence. - False : Drop all duplicates. inplace : bool, default False Whether to drop duplicates in place or to return a copy. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. .. versionadded:: 1.0.0 Returns ------- DataFrame DataFrame with duplicates removed or None if ``inplace=True``. See Also -------- DataFrame.value_counts: Count unique combinations of columns. Examples -------- Consider dataset containing ramen rating. >>> df = pd.DataFrame({ ... 'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'], ... 'style': ['cup', 'cup', 'cup', 'pack', 'pack'], ... 'rating': [4, 4, 3.5, 15, 5] ... }) >>> df brand style rating 0 Yum Yum cup 4.0 1 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 By default, it removes duplicate rows based on all columns. >>> df.drop_duplicates() brand style rating 0 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 To remove duplicates on specific column(s), use ``subset``. >>> df.drop_duplicates(subset=['brand']) brand style rating 0 Yum Yum cup 4.0 2 Indomie cup 3.5 To remove duplicates and keep last occurences, use ``keep``. >>> df.drop_duplicates(subset=['brand', 'style'], keep='last') brand style rating 1 Yum Yum cup 4.0 2 Indomie cup 3.5 4 Indomie pack 5.0 \"\"\" if self . empty : return self . copy () inplace = validate_bool_kwarg ( inplace , \"inplace\" ) duplicated = self . duplicated ( subset , keep = keep ) result = self [ - duplicated ] if ignore_index : result . index = ibase . default_index ( len ( result )) if inplace : self . _update_inplace ( result ) return None else : return result droplevel def droplevel ( self : ~ FrameOrSeries , level , axis = 0 ) -> ~ FrameOrSeries Return DataFrame with requested index / column level(s) removed. .. versionadded:: 0.24.0 Parameters level : int, str, or list-like If a string is given, must be the name of a level If list-like, elements must be names or positional indexes of levels. axis : {0 or 'index', 1 or 'columns'}, default 0 Axis along which the level(s) is removed: * 0 or 'index': remove level(s) in column. * 1 or 'columns': remove level(s) in row. Returns DataFrame DataFrame with requested index / column level(s) removed. Examples df = pd.DataFrame([ ... [1, 2, 3, 4], ... [5, 6, 7, 8], ... [9, 10, 11, 12] ... ]).set_index([0, 1]).rename_axis(['a', 'b']) df.columns = pd.MultiIndex.from_tuples([ ... ('c', 'e'), ('d', 'f') ... ], names=['level_1', 'level_2']) df level_1 c d level_2 e f a b 1 2 3 4 5 6 7 8 9 10 11 12 df.droplevel('a') level_1 c d level_2 e f b 2 3 4 6 7 8 10 11 12 df.droplevel('level_2', axis=1) level_1 c d a b 1 2 3 4 5 6 7 8 9 10 11 12 View Source def droplevel ( self : FrameOrSeries , level , axis = 0 ) -> FrameOrSeries : \"\"\" Return DataFrame with requested index / column level(s) removed. .. versionadded:: 0.24.0 Parameters ---------- level : int, str, or list-like If a string is given, must be the name of a level If list-like, elements must be names or positional indexes of levels. axis : {0 or 'index', 1 or 'columns'}, default 0 Axis along which the level(s) is removed: * 0 or 'index': remove level(s) in column. * 1 or 'columns': remove level(s) in row. Returns ------- DataFrame DataFrame with requested index / column level(s) removed. Examples -------- >>> df = pd.DataFrame([ ... [1, 2, 3, 4], ... [5, 6, 7, 8], ... [9, 10, 11, 12] ... ]).set_index([0, 1]).rename_axis(['a', 'b']) >>> df.columns = pd.MultiIndex.from_tuples([ ... ('c', 'e'), ('d', 'f') ... ], names=['level_1', 'level_2']) >>> df level_1 c d level_2 e f a b 1 2 3 4 5 6 7 8 9 10 11 12 >>> df.droplevel('a') level_1 c d level_2 e f b 2 3 4 6 7 8 10 11 12 >>> df.droplevel('level_2', axis=1) level_1 c d a b 1 2 3 4 5 6 7 8 9 10 11 12 \"\"\" labels = self . _get_axis ( axis ) new_labels = labels . droplevel ( level ) result = self . set_axis ( new_labels , axis = axis , inplace = False ) return result dropna def dropna ( self , axis = 0 , how = 'any' , thresh = None , subset = None , inplace = False ) Remove missing values. See the :ref: User Guide <missing_data> for more on which values are considered missing, and how to work with missing data. Parameters axis : {0 or 'index', 1 or 'columns'}, default 0 Determine if rows or columns which contain missing values are removed. * 0 , or 'index' : Drop rows which contain missing values . * 1 , or 'columns' : Drop columns which contain missing value . .. versionchanged :: 1 . 0 . 0 Pass tuple or list to drop on multiple axes . Only a single axis is allowed . how : {'any', 'all'}, default 'any' Determine if row or column is removed from DataFrame, when we have at least one NA or all NA. * 'any' : If any NA values are present, drop that row or column. * 'all' : If all values are NA, drop that row or column. thresh : int, optional Require that many non-NA values. subset : array-like, optional Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include. inplace : bool, default False If True, do operation inplace and return None. Returns DataFrame DataFrame with NA entries dropped from it. See Also DataFrame.isna: Indicate missing values. DataFrame.notna : Indicate existing (non-missing) values. DataFrame.fillna : Replace missing values. Series.dropna : Drop missing values. Index.dropna : Drop missing indices. Examples df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'], ... \"toy\": [np.nan, 'Batmobile', 'Bullwhip'], ... \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"), ... pd.NaT]}) df name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Drop the rows where at least one element is missing. df.dropna() name toy born 1 Batman Batmobile 1940-04-25 Drop the columns where at least one element is missing. df.dropna(axis='columns') name 0 Alfred 1 Batman 2 Catwoman Drop the rows where all elements are missing. df.dropna(how='all') name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Keep only the rows with at least 2 non-NA values. df.dropna(thresh=2) name toy born 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Define in which columns to look for missing values. df.dropna(subset=['name', 'born']) name toy born 1 Batman Batmobile 1940-04-25 Keep the DataFrame with valid entries in the same variable. df.dropna(inplace=True) df name toy born 1 Batman Batmobile 1940-04-25 View Source def dropna ( self , axis = 0 , how = \"any\" , thresh = None , subset = None , inplace = False ) : \"\"\" Remove missing values. See the :ref:`User Guide <missing_data>` for more on which values are considered missing, and how to work with missing data. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 Determine if rows or columns which contain missing values are removed. * 0, or 'index' : Drop rows which contain missing values. * 1, or 'columns' : Drop columns which contain missing value. .. versionchanged:: 1.0.0 Pass tuple or list to drop on multiple axes. Only a single axis is allowed. how : {'any', 'all'}, default 'any' Determine if row or column is removed from DataFrame, when we have at least one NA or all NA. * 'any' : If any NA values are present, drop that row or column. * 'all' : If all values are NA, drop that row or column. thresh : int, optional Require that many non-NA values. subset : array-like, optional Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include. inplace : bool, default False If True, do operation inplace and return None. Returns ------- DataFrame DataFrame with NA entries dropped from it. See Also -------- DataFrame.isna: Indicate missing values. DataFrame.notna : Indicate existing (non-missing) values. DataFrame.fillna : Replace missing values. Series.dropna : Drop missing values. Index.dropna : Drop missing indices. Examples -------- >>> df = pd.DataFrame({\" name \": ['Alfred', 'Batman', 'Catwoman'], ... \" toy \": [np.nan, 'Batmobile', 'Bullwhip'], ... \" born \": [pd.NaT, pd.Timestamp(\" 1940 - 04 - 25 \"), ... pd.NaT]}) >>> df name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Drop the rows where at least one element is missing. >>> df.dropna() name toy born 1 Batman Batmobile 1940-04-25 Drop the columns where at least one element is missing. >>> df.dropna(axis='columns') name 0 Alfred 1 Batman 2 Catwoman Drop the rows where all elements are missing. >>> df.dropna(how='all') name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Keep only the rows with at least 2 non-NA values. >>> df.dropna(thresh=2) name toy born 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Define in which columns to look for missing values. >>> df.dropna(subset=['name', 'born']) name toy born 1 Batman Batmobile 1940-04-25 Keep the DataFrame with valid entries in the same variable. >>> df.dropna(inplace=True) >>> df name toy born 1 Batman Batmobile 1940-04-25 \"\"\" inplace = validate_bool_kwarg ( inplace , \"inplace\" ) if isinstance ( axis , ( tuple , list )) : # GH20987 raise TypeError ( \"supplying multiple axes to axis is no longer supported.\" ) axis = self . _get_axis_number ( axis ) agg_axis = 1 - axis agg_obj = self if subset is not None : ax = self . _get_axis ( agg_axis ) indices = ax . get_indexer_for ( subset ) check = indices == - 1 if check . any () : raise KeyError ( list ( np . compress ( check , subset ))) agg_obj = self . take ( indices , axis = agg_axis ) count = agg_obj . count ( axis = agg_axis ) if thresh is not None : mask = count >= thresh elif how == \"any\" : mask = count == len ( agg_obj . _get_axis ( agg_axis )) elif how == \"all\" : mask = count > 0 else : if how is not None : raise ValueError ( f \"invalid how option: {how}\" ) else : raise TypeError ( \"must specify how or thresh\" ) result = self . loc ( axis = axis ) [ mask ] if inplace : self . _update_inplace ( result ) else : return result duplicated def duplicated ( self , subset : Union [ Hashable , Sequence [ Hashable ], NoneType ] = None , keep : Union [ str , bool ] = 'first' ) -> 'Series' Return boolean Series denoting duplicate rows. Considering certain columns is optional. Parameters subset : column label or sequence of labels, optional Only consider certain columns for identifying duplicates, by default use all of the columns. keep : {'first', 'last', False}, default 'first' Determines which duplicates (if any) to mark. - `` first `` : Mark duplicates as `` True `` except for the first occurrence . - `` last `` : Mark duplicates as `` True `` except for the last occurrence . - False : Mark all duplicates as `` True `` . Returns Series Boolean series for each duplicated rows. See Also Index.duplicated : Equivalent method on index. Series.duplicated : Equivalent method on Series. Series.drop_duplicates : Remove duplicate values from Series. DataFrame.drop_duplicates : Remove duplicate values from DataFrame. Examples Consider dataset containing ramen rating. df = pd.DataFrame({ ... 'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'], ... 'style': ['cup', 'cup', 'cup', 'pack', 'pack'], ... 'rating': [4, 4, 3.5, 15, 5] ... }) df brand style rating 0 Yum Yum cup 4.0 1 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 By default, for each set of duplicated values, the first occurrence is set on False and all others on True. df.duplicated() 0 False 1 True 2 False 3 False 4 False dtype: bool By using 'last', the last occurrence of each set of duplicated values is set on False and all others on True. df.duplicated(keep='last') 0 True 1 False 2 False 3 False 4 False dtype: bool By setting keep on False, all duplicates are True. df.duplicated(keep=False) 0 True 1 True 2 False 3 False 4 False dtype: bool To find duplicates on specific column(s), use subset . df.duplicated(subset=['brand']) 0 False 1 True 2 False 3 True 4 True dtype: bool View Source def duplicated ( self , subset : Optional [ Union [ Hashable , Sequence [ Hashable ]]] = None , keep : Union [ str , bool ] = \"first\" , ) -> \"Series\" : \"\"\" Return boolean Series denoting duplicate rows. Considering certain columns is optional. Parameters ---------- subset : column label or sequence of labels, optional Only consider certain columns for identifying duplicates, by default use all of the columns. keep : {'first', 'last', False}, default 'first' Determines which duplicates (if any) to mark. - ``first`` : Mark duplicates as ``True`` except for the first occurrence. - ``last`` : Mark duplicates as ``True`` except for the last occurrence. - False : Mark all duplicates as ``True``. Returns ------- Series Boolean series for each duplicated rows. See Also -------- Index.duplicated : Equivalent method on index. Series.duplicated : Equivalent method on Series. Series.drop_duplicates : Remove duplicate values from Series. DataFrame.drop_duplicates : Remove duplicate values from DataFrame. Examples -------- Consider dataset containing ramen rating. >>> df = pd.DataFrame({ ... 'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'], ... 'style': ['cup', 'cup', 'cup', 'pack', 'pack'], ... 'rating': [4, 4, 3.5, 15, 5] ... }) >>> df brand style rating 0 Yum Yum cup 4.0 1 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 By default, for each set of duplicated values, the first occurrence is set on False and all others on True. >>> df.duplicated() 0 False 1 True 2 False 3 False 4 False dtype: bool By using 'last', the last occurrence of each set of duplicated values is set on False and all others on True. >>> df.duplicated(keep='last') 0 True 1 False 2 False 3 False 4 False dtype: bool By setting ``keep`` on False, all duplicates are True. >>> df.duplicated(keep=False) 0 True 1 True 2 False 3 False 4 False dtype: bool To find duplicates on specific column(s), use ``subset``. >>> df.duplicated(subset=['brand']) 0 False 1 True 2 False 3 True 4 True dtype: bool \"\"\" from pandas . _libs . hashtable import _SIZE_HINT_LIMIT , duplicated_int64 from pandas . core . sorting import get_group_index if self . empty : return self . _constructor_sliced ( dtype = bool ) def f ( vals ): labels , shape = algorithms . factorize ( vals , size_hint = min ( len ( self ), _SIZE_HINT_LIMIT ) ) return labels . astype ( \"i8\" , copy = False ), len ( shape ) if subset is None : subset = self . columns elif ( not np . iterable ( subset ) or isinstance ( subset , str ) or isinstance ( subset , tuple ) and subset in self . columns ): subset = ( subset ,) # needed for mypy since can't narrow types using np.iterable subset = cast ( Iterable , subset ) # Verify all columns in subset exist in the queried dataframe # Otherwise, raise a KeyError, same as if you try to __getitem__ with a # key that doesn't exist. diff = Index ( subset ). difference ( self . columns ) if not diff . empty : raise KeyError ( diff ) vals = ( col . values for name , col in self . items () if name in subset ) labels , shape = map ( list , zip ( * map ( f , vals ))) ids = get_group_index ( labels , shape , sort = False , xnull = False ) return self . _constructor_sliced ( duplicated_int64 ( ids , keep ), index = self . index ) eq def eq ( self , other , axis = 'columns' , level = None ) Get Equal to of dataframe and other, element-wise (binary operator eq ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , =! , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'}, default 'columns' Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. Returns DataFrame of bool Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise. DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples df = pd.DataFrame({'cost': [250, 150, 100], ... 'revenue': [100, 250, 300]}, ... index=['A', 'B', 'C']) df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: df == 100 cost revenue A False True B False False C True False df.eq(100) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: df != pd.Series([100, 250], index=[\"cost\", \"revenue\"]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index') cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : df == [250, 100] cost revenue A True True B False False C False False Use the method to control the axis: df.eq([250, 250, 100], axis='index') cost revenue A True False B False True C True False Compare to a DataFrame of different shape. other = pd.DataFrame({'revenue': [300, 250, 100, 150]}, ... index=['A', 'B', 'C', 'D']) other revenue A 300 B 250 C 100 D 150 df.gt(other) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220], ... 'revenue': [100, 250, 300, 200, 175, 225]}, ... index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'], ... ['A', 'B', 'C', 'A', 'B', 'C']]) df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 df.le(df_multindex, level=1) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None ) : axis = self . _get_axis_number ( axis ) if axis is not None else 1 self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) new_data = dispatch_to_series ( self , other , op , axis = axis ) return self . _construct_result ( new_data ) equals def equals ( self , other ) Test whether two objects contain the same elements. This function allows two Series or DataFrames to be compared against each other to see if they have the same shape and elements. NaNs in the same location are considered equal. The column headers do not need to have the same type, but the elements within the columns must be the same dtype. Parameters other : Series or DataFrame The other Series or DataFrame to be compared with the first. Returns bool True if all elements are the same in both objects, False otherwise. See Also Series.eq : Compare two Series objects of the same length and return a Series where each element is True if the element in each Series is equal, False otherwise. DataFrame.eq : Compare two DataFrame objects of the same shape and return a DataFrame where each element is True if the respective element in each DataFrame is equal, False otherwise. testing.assert_series_equal : Raises an AssertionError if left and right are not equal. Provides an easy interface to ignore inequality in dtypes, indexes and precision among others. testing.assert_frame_equal : Like assert_series_equal, but targets DataFrames. numpy.array_equal : Return True if two arrays have the same shape and elements, False otherwise. Notes This function requires that the elements have the same dtype as their respective elements in the other Series or DataFrame. However, the column labels do not need to have the same type, as long as they are still considered equal. Examples df = pd.DataFrame({1: [10], 2: [20]}) df 1 2 0 10 20 DataFrames df and exactly_equal have the same types and values for their elements and column labels, which will return True. exactly_equal = pd.DataFrame({1: [10], 2: [20]}) exactly_equal 1 2 0 10 20 df.equals(exactly_equal) True DataFrames df and different_column_type have the same element types and values, but have different types for the column labels, which will still return True. different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]}) different_column_type 1.0 2.0 0 10 20 df.equals(different_column_type) True DataFrames df and different_data_type have different types for the same values for their elements, and will return False even though their column labels are the same values and types. different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]}) different_data_type 1 2 0 10.0 20.0 df.equals(different_data_type) False View Source def equals ( self , other ): \"\"\" Test whether two objects contain the same elements. This function allows two Series or DataFrames to be compared against each other to see if they have the same shape and elements. NaNs in the same location are considered equal. The column headers do not need to have the same type, but the elements within the columns must be the same dtype. Parameters ---------- other : Series or DataFrame The other Series or DataFrame to be compared with the first. Returns ------- bool True if all elements are the same in both objects, False otherwise. See Also -------- Series.eq : Compare two Series objects of the same length and return a Series where each element is True if the element in each Series is equal, False otherwise. DataFrame.eq : Compare two DataFrame objects of the same shape and return a DataFrame where each element is True if the respective element in each DataFrame is equal, False otherwise. testing.assert_series_equal : Raises an AssertionError if left and right are not equal. Provides an easy interface to ignore inequality in dtypes, indexes and precision among others. testing.assert_frame_equal : Like assert_series_equal, but targets DataFrames. numpy.array_equal : Return True if two arrays have the same shape and elements, False otherwise. Notes ----- This function requires that the elements have the same dtype as their respective elements in the other Series or DataFrame. However, the column labels do not need to have the same type, as long as they are still considered equal. Examples -------- >>> df = pd.DataFrame({1: [10], 2: [20]}) >>> df 1 2 0 10 20 DataFrames df and exactly_equal have the same types and values for their elements and column labels, which will return True. >>> exactly_equal = pd.DataFrame({1: [10], 2: [20]}) >>> exactly_equal 1 2 0 10 20 >>> df.equals(exactly_equal) True DataFrames df and different_column_type have the same element types and values, but have different types for the column labels, which will still return True. >>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]}) >>> different_column_type 1.0 2.0 0 10 20 >>> df.equals(different_column_type) True DataFrames df and different_data_type have different types for the same values for their elements, and will return False even though their column labels are the same values and types. >>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]}) >>> different_data_type 1 2 0 10.0 20.0 >>> df.equals(different_data_type) False \"\"\" if not ( isinstance ( other , type ( self )) or isinstance ( self , type ( other ))): return False return self . _mgr . equals ( other . _mgr ) eval def eval ( self , expr , inplace = False , ** kwargs ) Evaluate a string describing operations on DataFrame columns. Operates on columns only, not specific rows or elements. This allows eval to run arbitrary code, which can make you vulnerable to code injection if you pass user input to this function. Parameters expr : str The expression string to evaluate. inplace : bool, default False If the expression contains an assignment, whether to perform the operation inplace and mutate the existing DataFrame. Otherwise, a new DataFrame is returned. **kwargs See the documentation for :func: eval for complete details on the keyword arguments accepted by :meth: ~pandas.DataFrame.query . Returns ndarray, scalar, or pandas object The result of the evaluation. See Also DataFrame.query : Evaluates a boolean expression to query the columns of a frame. DataFrame.assign : Can evaluate an expression or function to create new values for a column. eval : Evaluate a Python expression as a string using various backends. Notes For more details see the API documentation for :func: ~eval . For detailed examples see :ref: enhancing performance with eval <enhancingperf.eval> . Examples df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2)}) df A B 0 1 10 1 2 8 2 3 6 3 4 4 4 5 2 df.eval('A + B') 0 11 1 10 2 9 3 8 4 7 dtype: int64 Assignment is allowed though by default the original DataFrame is not modified. df.eval('C = A + B') A B C 0 1 10 11 1 2 8 10 2 3 6 9 3 4 4 8 4 5 2 7 df A B 0 1 10 1 2 8 2 3 6 3 4 4 4 5 2 Use inplace=True to modify the original DataFrame. df.eval('C = A + B', inplace=True) df A B C 0 1 10 11 1 2 8 10 2 3 6 9 3 4 4 8 4 5 2 7 Multiple columns can be assigned to using multi-line expressions: df.eval( ... ''' ... C = A + B ... D = A - B ... ''' ... ) A B C D 0 1 10 11 -9 1 2 8 10 -6 2 3 6 9 -3 3 4 4 8 0 4 5 2 7 3 View Source def eval ( self , expr , inplace = False , ** kwargs ): \"\"\" Evaluate a string describing operations on DataFrame columns. Operates on columns only, not specific rows or elements. This allows `eval` to run arbitrary code, which can make you vulnerable to code injection if you pass user input to this function. Parameters ---------- expr : str The expression string to evaluate. inplace : bool, default False If the expression contains an assignment, whether to perform the operation inplace and mutate the existing DataFrame. Otherwise, a new DataFrame is returned. **kwargs See the documentation for :func:`eval` for complete details on the keyword arguments accepted by :meth:`~pandas.DataFrame.query`. Returns ------- ndarray, scalar, or pandas object The result of the evaluation. See Also -------- DataFrame.query : Evaluates a boolean expression to query the columns of a frame. DataFrame.assign : Can evaluate an expression or function to create new values for a column. eval : Evaluate a Python expression as a string using various backends. Notes ----- For more details see the API documentation for :func:`~eval`. For detailed examples see :ref:`enhancing performance with eval <enhancingperf.eval>`. Examples -------- >>> df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2)}) >>> df A B 0 1 10 1 2 8 2 3 6 3 4 4 4 5 2 >>> df.eval('A + B') 0 11 1 10 2 9 3 8 4 7 dtype: int64 Assignment is allowed though by default the original DataFrame is not modified. >>> df.eval('C = A + B') A B C 0 1 10 11 1 2 8 10 2 3 6 9 3 4 4 8 4 5 2 7 >>> df A B 0 1 10 1 2 8 2 3 6 3 4 4 4 5 2 Use ``inplace=True`` to modify the original DataFrame. >>> df.eval('C = A + B', inplace=True) >>> df A B C 0 1 10 11 1 2 8 10 2 3 6 9 3 4 4 8 4 5 2 7 Multiple columns can be assigned to using multi-line expressions: >>> df.eval( ... ''' ... C = A + B ... D = A - B ... ''' ... ) A B C D 0 1 10 11 -9 1 2 8 10 -6 2 3 6 9 -3 3 4 4 8 0 4 5 2 7 3 \"\"\" from pandas . core . computation . eval import eval as _eval inplace = validate_bool_kwarg ( inplace , \"inplace\" ) resolvers = kwargs . pop ( \"resolvers\" , None ) kwargs [ \"level\" ] = kwargs . pop ( \"level\" , 0 ) + 1 if resolvers is None : index_resolvers = self . _get_index_resolvers () column_resolvers = self . _get_cleaned_column_resolvers () resolvers = column_resolvers , index_resolvers if \"target\" not in kwargs : kwargs [ \"target\" ] = self kwargs [ \"resolvers\" ] = kwargs . get ( \"resolvers\" , ()) + tuple ( resolvers ) return _eval ( expr , inplace = inplace , ** kwargs ) ewm def ewm ( self , com = None , span = None , halflife = None , alpha = None , min_periods = 0 , adjust = True , ignore_na = False , axis = 0 , times = None ) Provide exponential weighted (EW) functions. Available EW functions: mean() , var() , std() , corr() , cov() . Exactly one parameter: com , span , halflife , or alpha must be provided. Parameters com : float, optional Specify decay in terms of center of mass, :math: \\alpha = 1 / (1 + com) , for :math: com \\geq 0 . span : float, optional Specify decay in terms of span, :math: \\alpha = 2 / (span + 1) , for :math: span \\geq 1 . halflife : float, str, timedelta, optional Specify decay in terms of half-life, :math: \\alpha = 1 - \\exp\\left(-\\ln(2) / halflife\\right) , for :math: halflife > 0 . If `` times `` is specified , the time unit ( str or timedelta ) over which an observation decays to half its value . Only applicable to `` mean () `` and halflife value will not apply to the other functions . .. versionadded :: 1 . 1 . 0 alpha : float, optional Specify smoothing factor :math: \\alpha directly, :math: 0 < \\alpha \\leq 1 . min_periods : int, default 0 Minimum number of observations in window required to have a value (otherwise result is NA). adjust : bool, default True Divide by decaying adjustment factor in beginning periods to account for imbalance in relative weightings (viewing EWMA as a moving average). - When `` adjust = True `` ( default ), the EW function is calculated using weights : math : ` w_i = ( 1 - \\ alpha ) ^ i ` . For example , the EW moving average of the series [: math : ` x_0 , x_1 , ..., x_t ` ] would be : .. math :: y_t = \\ frac { x_t + ( 1 - \\ alpha ) x_ { t - 1 } + ( 1 - \\ alpha ) ^ 2 x_ { t - 2 } + ... + ( 1 - \\ alpha ) ^ t x_0 }{ 1 + ( 1 - \\ alpha ) + ( 1 - \\ alpha ) ^ 2 + ... + ( 1 - \\ alpha ) ^ t } - When `` adjust = False `` , the exponentially weighted function is calculated recursively : .. math :: \\ begin { split } y_0 &= x_0 \\\\ y_t &= ( 1 - \\ alpha ) y_ { t - 1 } + \\ alpha x_t , \\ end { split } ignore_na : bool, default False Ignore missing values when calculating weights; specify True to reproduce pre-0.15.0 behavior. - When `` ignore_na = False `` ( default ), weights are based on absolute positions . For example , the weights of : math : `x_0` and : math : `x_2` used in calculating the final weighted average of [: math : `x_0` , None , : math : `x_2` ] are : math : `(1-\\alpha)^2` and : math : `1` if `` adjust = True `` , and : math : `(1-\\alpha)^2` and : math : `\\alpha` if `` adjust = False `` . - When `` ignore_na = True `` ( reproducing pre - 0 . 15 . 0 behavior ), weights are based on relative positions . For example , the weights of : math : `x_0` and : math : `x_2` used in calculating the final weighted average of [: math : `x_0` , None , : math : `x_2` ] are : math : `1-\\alpha` and : math : `1` if `` adjust = True `` , and : math : `1-\\alpha` and : math : `\\alpha` if `` adjust = False `` . axis : {0, 1}, default 0 The axis to use. The value 0 identifies the rows, and 1 identifies the columns. times : str, np.ndarray, Series, default None .. versionadded : : 1.1.0 Times corresponding to the observations . Must be monotonically increasing and `` datetime64 [ ns ] `` dtype . If str , the name of the column in the DataFrame representing the times . If 1 - D array like , a sequence with the same shape as the observations . Only applicable to `` mean () `` . Returns DataFrame A Window sub-classed for the particular operation. See Also rolling : Provides rolling window calculations. expanding : Provides expanding transformations. Notes More details can be found at: :ref: Exponentially weighted windows <stats.moments.exponentially_weighted> . Examples df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]}) df B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 df.ewm(com=0.5).mean() B 0 0.000000 1 0.750000 2 1.615385 3 1.615385 4 3.670213 Specifying times with a timedelta halflife when computing mean. times = ['2020-01-01', '2020-01-03', '2020-01-10', '2020-01-15', '2020-01-17'] df.ewm(halflife='4 days', times=pd.DatetimeIndex(times)).mean() B 0 0.000000 1 0.585786 2 1.523889 3 1.523889 4 3.233686 View Source @doc ( ExponentialMovingWindow ) def ewm ( self , com = None , span = None , halflife = None , alpha = None , min_periods = 0 , adjust = True , ignore_na = False , axis = 0 , times = None , ) : axis = self . _get_axis_number ( axis ) return ExponentialMovingWindow ( self , com = com , span = span , halflife = halflife , alpha = alpha , min_periods = min_periods , adjust = adjust , ignore_na = ignore_na , axis = axis , times = times , ) expanding def expanding ( self , min_periods = 1 , center = None , axis = 0 ) Provide expanding transformations. Parameters min_periods : int, default 1 Minimum number of observations in window required to have a value (otherwise result is NA). center : bool, default False Set the labels at the center of the window. axis : int or str, default 0 Returns a Window sub-classed for the particular operation See Also rolling : Provides rolling window calculations. ewm : Provides exponential weighted functions. Notes By default, the result is set to the right edge of the window. This can be changed to the center of the window by setting center=True . Examples df = pd.DataFrame({\"B\": [0, 1, 2, np.nan, 4]}) df B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 df.expanding(2).sum() B 0 NaN 1 1.0 2 3.0 3 3.0 4 7.0 View Source @ doc ( Expanding ) def expanding ( self , min_periods = 1 , center = None , axis = 0 ): axis = self . _get_axis_number ( axis ) if center is not None : warnings . warn ( \"The `center` argument on `expanding` \" \"will be removed in the future\" , FutureWarning , stacklevel = 2 , ) else : center = False return Expanding ( self , min_periods = min_periods , center = center , axis = axis ) explode def explode ( self , column : Union [ str , Tuple ], ignore_index : bool = False ) -> 'DataFrame' Transform each element of a list-like to a row, replicating index values. .. versionadded:: 0.25.0 Parameters column : str or tuple Column to explode. ignore_index : bool, default False If True, the resulting index will be labeled 0, 1, \u2026, n - 1. .. versionadded :: 1.1.0 Returns DataFrame Exploded lists to rows of the subset columns; index will be duplicated for these rows. Raises ValueError : if columns of the frame are not unique. See Also DataFrame.unstack : Pivot a level of the (necessarily hierarchical) index labels. DataFrame.melt : Unpivot a DataFrame from wide format to long format. Series.explode : Explode a DataFrame from list-like columns to long format. Notes This routine will explode list-likes including lists, tuples, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged. Empty list-likes will result in a np.nan for that row. Examples df = pd.DataFrame({'A': [[1, 2, 3], 'foo', [], [3, 4]], 'B': 1}) df A B 0 [1, 2, 3] 1 1 foo 1 2 [] 1 3 [3, 4] 1 df.explode('A') A B 0 1 1 0 2 1 0 3 1 1 foo 1 2 NaN 1 3 3 1 3 4 1 View Source def explode ( self , column : Union [ str, Tuple ] , ignore_index : bool = False ) -> \"DataFrame\" : \"\"\" Transform each element of a list-like to a row, replicating index values. .. versionadded:: 0.25.0 Parameters ---------- column : str or tuple Column to explode. ignore_index : bool, default False If True, the resulting index will be labeled 0, 1, \u2026, n - 1. .. versionadded:: 1.1.0 Returns ------- DataFrame Exploded lists to rows of the subset columns; index will be duplicated for these rows. Raises ------ ValueError : if columns of the frame are not unique. See Also -------- DataFrame.unstack : Pivot a level of the (necessarily hierarchical) index labels. DataFrame.melt : Unpivot a DataFrame from wide format to long format. Series.explode : Explode a DataFrame from list-like columns to long format. Notes ----- This routine will explode list-likes including lists, tuples, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged. Empty list-likes will result in a np.nan for that row. Examples -------- >>> df = pd.DataFrame({'A': [[1, 2, 3], 'foo', [], [3, 4]], 'B': 1}) >>> df A B 0 [1, 2, 3] 1 1 foo 1 2 [] 1 3 [3, 4] 1 >>> df.explode('A') A B 0 1 1 0 2 1 0 3 1 1 foo 1 2 NaN 1 3 3 1 3 4 1 \"\"\" if not ( is_scalar ( column ) or isinstance ( column , tuple )) : raise ValueError ( \"column must be a scalar\" ) if not self . columns . is_unique : raise ValueError ( \"columns must be unique\" ) df = self . reset_index ( drop = True ) # TODO : use overload to refine return type of reset_index assert df is not None # needed for mypy result = df [ column ] . explode () result = df . drop ( [ column ] , axis = 1 ). join ( result ) if ignore_index : result . index = ibase . default_index ( len ( result )) else : result . index = self . index . take ( result . index ) result = result . reindex ( columns = self . columns , copy = False ) return result ffill def ffill ( self : ~ FrameOrSeries , axis = None , inplace : bool = False , limit = None , downcast = None ) -> Union [ ~ FrameOrSeries , NoneType ] Synonym for :meth: DataFrame.fillna with method='ffill' . Returns {klass} or None Object with missing values filled or None if inplace=True . View Source def ffill ( self : FrameOrSeries , axis = None , inplace : bool_t = False , limit = None , downcast = None , ) -> Optional [ FrameOrSeries ] : \"\"\" Synonym for :meth:`DataFrame.fillna` with ``method='ffill'``. Returns ------- {klass} or None Object with missing values filled or None if ``inplace=True``. \"\"\" return self . fillna ( method = \"ffill\" , axis = axis , inplace = inplace , limit = limit , downcast = downcast ) fillna def fillna ( self , value = None , method = None , axis = None , inplace = False , limit = None , downcast = None ) -> Union [ ForwardRef ( 'DataFrame' ), NoneType ] Fill NA/NaN values using the specified method. Parameters value : scalar, dict, Series, or DataFrame Value to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame). Values not in the dict/Series/DataFrame will not be filled. This value cannot be a list. method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None Method to use for filling holes in reindexed Series pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use next valid observation to fill gap. axis : {0 or 'index', 1 or 'columns'} Axis along which to fill missing values. inplace : bool, default False If True, fill in-place. Note: this will modify any other views on this object (e.g., a no-copy slice for a column in a DataFrame). limit : int, default None If method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. downcast : dict, default is None A dict of item->dtype of what to downcast if possible, or the string 'infer' which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible). Returns DataFrame or None Object with missing values filled or None if inplace=True . See Also interpolate : Fill NaN values using interpolation. reindex : Conform object to new index. asfreq : Convert TimeSeries to specified frequency. Examples df = pd.DataFrame([[np.nan, 2, np.nan, 0], ... [3, 4, np.nan, 1], ... [np.nan, np.nan, np.nan, 5], ... [np.nan, 3, np.nan, 4]], ... columns=list('ABCD')) df A B C D 0 NaN 2.0 NaN 0 1 3.0 4.0 NaN 1 2 NaN NaN NaN 5 3 NaN 3.0 NaN 4 Replace all NaN elements with 0s. df.fillna(0) A B C D 0 0.0 2.0 0.0 0 1 3.0 4.0 0.0 1 2 0.0 0.0 0.0 5 3 0.0 3.0 0.0 4 We can also propagate non-null values forward or backward. df.fillna(method='ffill') A B C D 0 NaN 2.0 NaN 0 1 3.0 4.0 NaN 1 2 3.0 4.0 NaN 5 3 3.0 3.0 NaN 4 Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1, 2, and 3 respectively. values = {'A': 0, 'B': 1, 'C': 2, 'D': 3} df.fillna(value=values) A B C D 0 0.0 2.0 2.0 0 1 3.0 4.0 2.0 1 2 0.0 1.0 2.0 5 3 0.0 3.0 2.0 4 Only replace the first NaN element. df.fillna(value=values, limit=1) A B C D 0 0.0 2.0 2.0 0 1 3.0 4.0 NaN 1 2 NaN 1.0 NaN 5 3 NaN 3.0 NaN 4 View Source @doc ( NDFrame . fillna , ** _shared_doc_kwargs ) def fillna ( self , value = None , method = None , axis = None , inplace = False , limit = None , downcast = None , ) -> Optional [ \"DataFrame\" ] : return super (). fillna ( value = value , method = method , axis = axis , inplace = inplace , limit = limit , downcast = downcast , ) filter def filter ( self : ~ FrameOrSeries , items = None , like : Union [ str , NoneType ] = None , regex : Union [ str , NoneType ] = None , axis = None ) -> ~ FrameOrSeries Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index. Parameters items : list-like Keep labels from axis which are in items. like : str Keep labels from axis for which \"like in label == True\". regex : str (regular expression) Keep labels from axis for which re.search(regex, label) == True. axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None The axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, 'index' for Series, 'columns' for DataFrame. Returns same type as input object See Also DataFrame.loc : Access a group of rows and columns by label(s) or a boolean array. Notes The items , like , and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with [] . Examples df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])), ... index=['mouse', 'rabbit'], ... columns=['one', 'two', 'three']) df one two three mouse 1 2 3 rabbit 4 5 6 select columns by name df.filter(items=['one', 'three']) one three mouse 1 3 rabbit 4 6 select columns by regular expression df.filter(regex='e$', axis=1) one three mouse 1 3 rabbit 4 6 select rows containing 'bbi' df.filter(like='bbi', axis=0) one two three rabbit 4 5 6 View Source def filter ( self : FrameOrSeries , items = None , like : Optional [ str ] = None , regex : Optional [ str ] = None , axis = None , ) -> FrameOrSeries : \"\"\" Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index. Parameters ---------- items : list-like Keep labels from axis which are in items. like : str Keep labels from axis for which \" like in label == True \". regex : str (regular expression) Keep labels from axis for which re.search(regex, label) == True. axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None The axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, 'index' for Series, 'columns' for DataFrame. Returns ------- same type as input object See Also -------- DataFrame.loc : Access a group of rows and columns by label(s) or a boolean array. Notes ----- The ``items``, ``like``, and ``regex`` parameters are enforced to be mutually exclusive. ``axis`` defaults to the info axis that is used when indexing with ``[]``. Examples -------- >>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])), ... index=['mouse', 'rabbit'], ... columns=['one', 'two', 'three']) >>> df one two three mouse 1 2 3 rabbit 4 5 6 >>> # select columns by name >>> df.filter(items=['one', 'three']) one three mouse 1 3 rabbit 4 6 >>> # select columns by regular expression >>> df.filter(regex='e$', axis=1) one three mouse 1 3 rabbit 4 6 >>> # select rows containing 'bbi' >>> df.filter(like='bbi', axis=0) one two three rabbit 4 5 6 \"\"\" nkw = com . count_not_none ( items , like , regex ) if nkw > 1 : raise TypeError ( \"Keyword arguments `items`, `like`, or `regex` \" \"are mutually exclusive\" ) if axis is None : axis = self . _info_axis_name labels = self . _get_axis ( axis ) if items is not None : name = self . _get_axis_name ( axis ) return self . reindex ( ** { name : [ r for r in items if r in labels ] } ) elif like : def f ( x ): return like in ensure_str ( x ) values = labels . map ( f ) return self . loc ( axis = axis )[ values ] elif regex : def f ( x ): return matcher . search ( ensure_str ( x )) is not None matcher = re . compile ( regex ) values = labels . map ( f ) return self . loc ( axis = axis )[ values ] else : raise TypeError ( \"Must pass either `items`, `like`, or `regex`\" ) first def first ( self : ~ FrameOrSeries , offset ) -> ~ FrameOrSeries Select initial periods of time series data based on a date offset. When having a DataFrame with dates as index, this function can select the first few rows based on a date offset. Parameters offset : str, DateOffset or dateutil.relativedelta The offset length of the data that will be selected. For instance, '1M' will display all the rows having their index within the first month. Returns Series or DataFrame A subset of the caller. Raises TypeError If the index is not a :class: DatetimeIndex See Also last : Select final periods of time series based on a date offset. at_time : Select values at a particular time of the day. between_time : Select values between particular times of the day. Examples i = pd.date_range('2018-04-09', periods=4, freq='2D') ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) ts A 2018-04-09 1 2018-04-11 2 2018-04-13 3 2018-04-15 4 Get the rows for the first 3 days: ts.first('3D') A 2018-04-09 1 2018-04-11 2 Notice the data for 3 first calendar days were returned, not the first 3 days observed in the dataset, and therefore data for 2018-04-13 was not returned. View Source def first ( self : FrameOrSeries , offset ) -> FrameOrSeries : \"\"\" Select initial periods of time series data based on a date offset. When having a DataFrame with dates as index, this function can select the first few rows based on a date offset. Parameters ---------- offset : str, DateOffset or dateutil.relativedelta The offset length of the data that will be selected. For instance, '1M' will display all the rows having their index within the first month. Returns ------- Series or DataFrame A subset of the caller. Raises ------ TypeError If the index is not a :class:`DatetimeIndex` See Also -------- last : Select final periods of time series based on a date offset. at_time : Select values at a particular time of the day. between_time : Select values between particular times of the day. Examples -------- >>> i = pd.date_range('2018-04-09', periods=4, freq='2D') >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) >>> ts A 2018-04-09 1 2018-04-11 2 2018-04-13 3 2018-04-15 4 Get the rows for the first 3 days: >>> ts.first('3D') A 2018-04-09 1 2018-04-11 2 Notice the data for 3 first calendar days were returned, not the first 3 days observed in the dataset, and therefore data for 2018-04-13 was not returned. \"\"\" if not isinstance ( self . index , DatetimeIndex ): raise TypeError ( \"'first' only supports a DatetimeIndex index\" ) if len ( self . index ) == 0 : return self offset = to_offset ( offset ) end_date = end = self . index [ 0 ] + offset # Tick-like, e.g. 3 weeks if isinstance ( offset , Tick ): if end_date in self . index : end = self . index . searchsorted ( end_date , side = \"left\" ) return self . iloc [: end ] return self . loc [: end ] first_valid_index def first_valid_index ( self ) Return index for first non-NA/null value. Returns scalar : type of index Notes If all elements are non-NA/null, returns None. Also returns None for empty Series/DataFrame. View Source @doc ( position = \"first\" , klass = _shared_doc_kwargs [ \"klass\" ] ) def first_valid_index ( self ) : \"\"\" Return index for {position} non-NA/null value. Returns ------- scalar : type of index Notes ----- If all elements are non-NA/null, returns None. Also returns None for empty {klass}. \"\"\" return self . _find_valid_index ( \"first\" ) floordiv def floordiv ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Integer division of dataframe and other, element-wise (binary operator floordiv ). Equivalent to dataframe // other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rfloordiv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) ge def ge ( self , other , axis = 'columns' , level = None ) Get Greater than or equal to of dataframe and other, element-wise (binary operator ge ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , =! , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'}, default 'columns' Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. Returns DataFrame of bool Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise. DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples df = pd.DataFrame({'cost': [250, 150, 100], ... 'revenue': [100, 250, 300]}, ... index=['A', 'B', 'C']) df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: df == 100 cost revenue A False True B False False C True False df.eq(100) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: df != pd.Series([100, 250], index=[\"cost\", \"revenue\"]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index') cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : df == [250, 100] cost revenue A True True B False False C False False Use the method to control the axis: df.eq([250, 250, 100], axis='index') cost revenue A True False B False True C True False Compare to a DataFrame of different shape. other = pd.DataFrame({'revenue': [300, 250, 100, 150]}, ... index=['A', 'B', 'C', 'D']) other revenue A 300 B 250 C 100 D 150 df.gt(other) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220], ... 'revenue': [100, 250, 300, 200, 175, 225]}, ... index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'], ... ['A', 'B', 'C', 'A', 'B', 'C']]) df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 df.le(df_multindex, level=1) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None ) : axis = self . _get_axis_number ( axis ) if axis is not None else 1 self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) new_data = dispatch_to_series ( self , other , op , axis = axis ) return self . _construct_result ( new_data ) get def get ( self , force_reload = False ) Get item from object for given key (ex: DataFrame column). Returns default value if not found. Parameters key : object Returns value : same type as items contained in object View Source def get ( self , force_reload = False ): self . __chk_and_reload_cache ( force = force_reload ) return self . db groupby def groupby ( self , by = None , axis = 0 , level = None , as_index : bool = True , sort : bool = True , group_keys : bool = True , squeeze : bool = < object object at 0x7fb7e907ade0 > , observed : bool = False , dropna : bool = True ) -> 'DataFrameGroupBy' Group DataFrame using a mapper or by a Series of columns. A groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups. Parameters by : mapping, function, label, or list of labels Used to determine the groups for the groupby. If by is a function, it's called on each value of the object's index. If a dict or Series is passed, the Series or dict VALUES will be used to determine the groups (the Series' values are first aligned; see .align() method). If an ndarray is passed, the values are used as-is determine the groups. A label or list of labels may be passed to group by the columns in self . Notice that a tuple is interpreted as a (single) key. axis : {0 or 'index', 1 or 'columns'}, default 0 Split along rows (0) or columns (1). level : int, level name, or sequence of such, default None If the axis is a MultiIndex (hierarchical), group by a particular level or levels. as_index : bool, default True For aggregated output, return object with group labels as the index. Only relevant for DataFrame input. as_index=False is effectively \"SQL-style\" grouped output. sort : bool, default True Sort group keys. Get better performance by turning this off. Note this does not influence the order of observations within each group. Groupby preserves the order of rows within each group. group_keys : bool, default True When calling apply, add group keys to index to identify pieces. squeeze : bool, default False Reduce the dimensionality of the return type if possible, otherwise return a consistent type. .. deprecated :: 1.1.0 observed : bool, default False This only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. .. versionadded :: 0.23.0 dropna : bool, default True If True, and if group keys contain NA values, NA values together with row/column will be dropped. If False, NA values will also be treated as the key in groups .. versionadded :: 1.1.0 Returns DataFrameGroupBy Returns a groupby object that contains information about the groups. See Also resample : Convenience method for frequency conversion and resampling of time series. Notes See the user guide <https://pandas.pydata.org/pandas-docs/stable/groupby.html> _ for more. Examples df = pd.DataFrame({'Animal': ['Falcon', 'Falcon', ... 'Parrot', 'Parrot'], ... 'Max Speed': [380., 370., 24., 26.]}) df Animal Max Speed 0 Falcon 380.0 1 Falcon 370.0 2 Parrot 24.0 3 Parrot 26.0 df.groupby(['Animal']).mean() Max Speed Animal Falcon 375.0 Parrot 25.0 Hierarchical Indexes We can groupby different levels of a hierarchical index using the level parameter: arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'], ... ['Captive', 'Wild', 'Captive', 'Wild']] index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type')) df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]}, ... index=index) df Max Speed Animal Type Falcon Captive 390.0 Wild 350.0 Parrot Captive 30.0 Wild 20.0 df.groupby(level=0).mean() Max Speed Animal Falcon 370.0 Parrot 25.0 df.groupby(level=\"Type\").mean() Max Speed Type Captive 210.0 Wild 185.0 We can also choose to include NA in group keys or not by setting dropna parameter, the default setting is True : l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]] df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"]) df.groupby(by=[\"b\"]).sum() a c b 1.0 2 3 2.0 2 5 df.groupby(by=[\"b\"], dropna=False).sum() a c b 1.0 2 3 2.0 2 5 NaN 1 4 l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]] df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"]) df.groupby(by=\"a\").sum() b c a a 13.0 13.0 b 12.3 123.0 df.groupby(by=\"a\", dropna=False).sum() b c a a 13.0 13.0 b 12.3 123.0 NaN 12.3 33.0 View Source @ Appender ( \"\"\" Examples -------- >>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon', ... 'Parrot', 'Parrot'], ... 'Max Speed': [380., 370., 24., 26.]}) >>> df Animal Max Speed 0 Falcon 380.0 1 Falcon 370.0 2 Parrot 24.0 3 Parrot 26.0 >>> df.groupby(['Animal']).mean() Max Speed Animal Falcon 375.0 Parrot 25.0 **Hierarchical Indexes** We can groupby different levels of a hierarchical index using the `level` parameter: >>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'], ... ['Captive', 'Wild', 'Captive', 'Wild']] >>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type')) >>> df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]}, ... index=index) >>> df Max Speed Animal Type Falcon Captive 390.0 Wild 350.0 Parrot Captive 30.0 Wild 20.0 >>> df.groupby(level=0).mean() Max Speed Animal Falcon 370.0 Parrot 25.0 >>> df.groupby(level=\" Type \").mean() Max Speed Type Captive 210.0 Wild 185.0 We can also choose to include NA in group keys or not by setting `dropna` parameter, the default setting is `True`: >>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]] >>> df = pd.DataFrame(l, columns=[\" a \", \" b \", \" c \"]) >>> df.groupby(by=[\" b \"]).sum() a c b 1.0 2 3 2.0 2 5 >>> df.groupby(by=[\" b \"], dropna=False).sum() a c b 1.0 2 3 2.0 2 5 NaN 1 4 >>> l = [[\" a \", 12, 12], [None, 12.3, 33.], [\" b \", 12.3, 123], [\" a \", 1, 1]] >>> df = pd.DataFrame(l, columns=[\" a \", \" b \", \" c \"]) >>> df.groupby(by=\" a \").sum() b c a a 13.0 13.0 b 12.3 123.0 >>> df.groupby(by=\" a \", dropna=False).sum() b c a a 13.0 13.0 b 12.3 123.0 NaN 12.3 33.0 \"\"\" ) @ Appender ( _shared_docs [ \"groupby\" ] % _shared_doc_kwargs ) def groupby ( self , by = None , axis = 0 , level = None , as_index : bool = True , sort : bool = True , group_keys : bool = True , squeeze : bool = no_default , observed : bool = False , dropna : bool = True , ) -> \"DataFrameGroupBy\" : from pandas . core . groupby . generic import DataFrameGroupBy if squeeze is not no_default : warnings . warn ( ( \"The `squeeze` parameter is deprecated and \" \"will be removed in a future version.\" ), FutureWarning , stacklevel = 2 , ) else : squeeze = False if level is None and by is None : raise TypeError ( \"You have to supply one of 'by' and 'level'\" ) axis = self . _get_axis_number ( axis ) return DataFrameGroupBy ( obj = self , keys = by , axis = axis , level = level , as_index = as_index , sort = sort , group_keys = group_keys , squeeze = squeeze , observed = observed , dropna = dropna , ) gt def gt ( self , other , axis = 'columns' , level = None ) Get Greater than of dataframe and other, element-wise (binary operator gt ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , =! , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'}, default 'columns' Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. Returns DataFrame of bool Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise. DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples df = pd.DataFrame({'cost': [250, 150, 100], ... 'revenue': [100, 250, 300]}, ... index=['A', 'B', 'C']) df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: df == 100 cost revenue A False True B False False C True False df.eq(100) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: df != pd.Series([100, 250], index=[\"cost\", \"revenue\"]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index') cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : df == [250, 100] cost revenue A True True B False False C False False Use the method to control the axis: df.eq([250, 250, 100], axis='index') cost revenue A True False B False True C True False Compare to a DataFrame of different shape. other = pd.DataFrame({'revenue': [300, 250, 100, 150]}, ... index=['A', 'B', 'C', 'D']) other revenue A 300 B 250 C 100 D 150 df.gt(other) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220], ... 'revenue': [100, 250, 300, 200, 175, 225]}, ... index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'], ... ['A', 'B', 'C', 'A', 'B', 'C']]) df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 df.le(df_multindex, level=1) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None ) : axis = self . _get_axis_number ( axis ) if axis is not None else 1 self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) new_data = dispatch_to_series ( self , other , op , axis = axis ) return self . _construct_result ( new_data ) head def head ( self : ~ FrameOrSeries , n : int = 5 ) -> ~ FrameOrSeries Return the first n rows. This function returns the first n rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it. For negative values of n , this function returns all rows except the last n rows, equivalent to df[:-n] . Parameters n : int, default 5 Number of rows to select. Returns same type as caller The first n rows of the caller object. See Also DataFrame.tail: Returns the last n rows. Examples df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion', ... 'monkey', 'parrot', 'shark', 'whale', 'zebra']}) df animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the first 5 lines df.head() animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey Viewing the first n lines (three in this case) df.head(3) animal 0 alligator 1 bee 2 falcon For negative values of n df.head(-3) animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot View Source def head ( self : FrameOrSeries , n : int = 5 ) -> FrameOrSeries : \"\"\" Return the first `n` rows. This function returns the first `n` rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it. For negative values of `n`, this function returns all rows except the last `n` rows, equivalent to ``df[:-n]``. Parameters ---------- n : int, default 5 Number of rows to select. Returns ------- same type as caller The first `n` rows of the caller object. See Also -------- DataFrame.tail: Returns the last `n` rows. Examples -------- >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion', ... 'monkey', 'parrot', 'shark', 'whale', 'zebra']}) >>> df animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the first 5 lines >>> df.head() animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey Viewing the first `n` lines (three in this case) >>> df.head(3) animal 0 alligator 1 bee 2 falcon For negative values of `n` >>> df.head(-3) animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot \"\"\" return self . iloc [ :n ] hist def hist ( data : 'DataFrame' , column : Union [ Hashable , NoneType , Sequence [ Union [ Hashable , NoneType ]]] = None , by = None , grid : bool = True , xlabelsize : Union [ int , NoneType ] = None , xrot : Union [ float , NoneType ] = None , ylabelsize : Union [ int , NoneType ] = None , yrot : Union [ float , NoneType ] = None , ax = None , sharex : bool = False , sharey : bool = False , figsize : Union [ Tuple [ int , int ], NoneType ] = None , layout : Union [ Tuple [ int , int ], NoneType ] = None , bins : Union [ int , Sequence [ int ]] = 10 , backend : Union [ str , NoneType ] = None , legend : bool = False , ** kwargs ) Make a histogram of the DataFrame's. A histogram _ is a representation of the distribution of data. This function calls :meth: matplotlib.pyplot.hist , on each series in the DataFrame, resulting in one histogram per column. .. _histogram: https://en.wikipedia.org/wiki/Histogram Parameters data : DataFrame The pandas object holding the data. column : str or sequence If passed, will be used to limit data to a subset of columns. by : object, optional If passed, then used to form histograms for separate groups. grid : bool, default True Whether to show axis grid lines. xlabelsize : int, default None If specified changes the x-axis label size. xrot : float, default None Rotation of x axis labels. For example, a value of 90 displays the x labels rotated 90 degrees clockwise. ylabelsize : int, default None If specified changes the y-axis label size. yrot : float, default None Rotation of y axis labels. For example, a value of 90 displays the y labels rotated 90 degrees clockwise. ax : Matplotlib axes object, default None The axes to plot the histogram on. sharex : bool, default True if ax is None else False In case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in. Note that passing in both an ax and sharex=True will alter all x axis labels for all subplots in a figure. sharey : bool, default False In case subplots=True, share y axis and set some y axis labels to invisible. figsize : tuple The size in inches of the figure to create. Uses the value in matplotlib.rcParams by default. layout : tuple, optional Tuple of (rows, columns) for the layout of the histograms. bins : int or sequence, default 10 Number of histogram bins to be used. If an integer is given, bins + 1 bin edges are calculated and returned. If bins is a sequence, gives bin edges, including left edge of first bin and right edge of last bin. In this case, bins is returned unmodified. backend : str, default None Backend to use instead of the backend specified in the option plotting.backend . For instance, 'matplotlib'. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend . .. versionadded :: 1.0.0 legend : bool, default False Whether to show the legend. .. versionadded :: 1.1.0 **kwargs All other plotting keyword arguments to be passed to :meth: matplotlib.pyplot.hist . Returns matplotlib.AxesSubplot or numpy.ndarray of them See Also matplotlib.pyplot.hist : Plot a histogram using matplotlib. Examples This example draws a histogram based on the length and width of some animals, displayed in three bins .. plot:: :context: close-figs >>> df = pd.DataFrame({ ... 'length': [1.5, 0.5, 1.2, 0.9, 3], ... 'width': [0.7, 0.2, 0.15, 0.2, 1.1] ... }, index=['pig', 'rabbit', 'duck', 'chicken', 'horse']) >>> hist = df.hist(bins=3) View Source def hist_frame ( data : \"DataFrame\" , column : Union [ Label, Sequence[Label ] ] = None , by = None , grid : bool = True , xlabelsize : Optional [ int ] = None , xrot : Optional [ float ] = None , ylabelsize : Optional [ int ] = None , yrot : Optional [ float ] = None , ax = None , sharex : bool = False , sharey : bool = False , figsize : Optional [ Tuple[int, int ] ] = None , layout : Optional [ Tuple[int, int ] ] = None , bins : Union [ int, Sequence[int ] ] = 10 , backend : Optional [ str ] = None , legend : bool = False , ** kwargs , ) : \"\"\" Make a histogram of the DataFrame's. A `histogram`_ is a representation of the distribution of data. This function calls :meth:`matplotlib.pyplot.hist`, on each series in the DataFrame, resulting in one histogram per column. .. _histogram: https://en.wikipedia.org/wiki/Histogram Parameters ---------- data : DataFrame The pandas object holding the data. column : str or sequence If passed, will be used to limit data to a subset of columns. by : object, optional If passed, then used to form histograms for separate groups. grid : bool, default True Whether to show axis grid lines. xlabelsize : int, default None If specified changes the x-axis label size. xrot : float, default None Rotation of x axis labels. For example, a value of 90 displays the x labels rotated 90 degrees clockwise. ylabelsize : int, default None If specified changes the y-axis label size. yrot : float, default None Rotation of y axis labels. For example, a value of 90 displays the y labels rotated 90 degrees clockwise. ax : Matplotlib axes object, default None The axes to plot the histogram on. sharex : bool, default True if ax is None else False In case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in. Note that passing in both an ax and sharex=True will alter all x axis labels for all subplots in a figure. sharey : bool, default False In case subplots=True, share y axis and set some y axis labels to invisible. figsize : tuple The size in inches of the figure to create. Uses the value in `matplotlib.rcParams` by default. layout : tuple, optional Tuple of (rows, columns) for the layout of the histograms. bins : int or sequence, default 10 Number of histogram bins to be used. If an integer is given, bins + 1 bin edges are calculated and returned. If bins is a sequence, gives bin edges, including left edge of first bin and right edge of last bin. In this case, bins is returned unmodified. backend : str, default None Backend to use instead of the backend specified in the option ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to specify the ``plotting.backend`` for the whole session, set ``pd.options.plotting.backend``. .. versionadded:: 1.0.0 legend : bool, default False Whether to show the legend. .. versionadded:: 1.1.0 **kwargs All other plotting keyword arguments to be passed to :meth:`matplotlib.pyplot.hist`. Returns ------- matplotlib.AxesSubplot or numpy.ndarray of them See Also -------- matplotlib.pyplot.hist : Plot a histogram using matplotlib. Examples -------- This example draws a histogram based on the length and width of some animals, displayed in three bins .. plot:: :context: close-figs >>> df = pd.DataFrame({ ... 'length': [1.5, 0.5, 1.2, 0.9, 3], ... 'width': [0.7, 0.2, 0.15, 0.2, 1.1] ... }, index=['pig', 'rabbit', 'duck', 'chicken', 'horse']) >>> hist = df.hist(bins=3) \"\"\" plot_backend = _get_plot_backend ( backend ) return plot_backend . hist_frame ( data , column = column , by = by , grid = grid , xlabelsize = xlabelsize , xrot = xrot , ylabelsize = ylabelsize , yrot = yrot , ax = ax , sharex = sharex , sharey = sharey , figsize = figsize , layout = layout , legend = legend , bins = bins , ** kwargs , ) idxmax def idxmax ( self , axis = 0 , skipna = True ) -> pandas . core . series . Series Return index of first occurrence of maximum over requested axis. NA/null values are excluded. Parameters axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. Returns Series Indexes of maxima along the specified axis. Raises ValueError * If the row/column is empty See Also Series.idxmax : Return index of the maximum element. Notes This method is the DataFrame version of ndarray.argmax . Examples Consider a dataset containing food consumption in Argentina. df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48], ... 'co2_emissions': [37.2, 19.66, 1712]}, ... index=['Pork', 'Wheat Products', 'Beef']) df consumption co2_emissions Pork 10.51 37.20 Wheat Products 103.11 19.66 Beef 55.48 1712.00 By default, it returns the index for the maximum value in each column. df.idxmax() consumption Wheat Products co2_emissions Beef dtype: object To return the index for the maximum value in each row, use axis=\"columns\" . df.idxmax(axis=\"columns\") Pork co2_emissions Wheat Products consumption Beef co2_emissions dtype: object View Source def idxmax ( self , axis = 0 , skipna = True ) -> Series : \"\"\" Return index of first occurrence of maximum over requested axis. NA/null values are excluded. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. Returns ------- Series Indexes of maxima along the specified axis. Raises ------ ValueError * If the row/column is empty See Also -------- Series.idxmax : Return index of the maximum element. Notes ----- This method is the DataFrame version of ``ndarray.argmax``. Examples -------- Consider a dataset containing food consumption in Argentina. >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48], ... 'co2_emissions': [37.2, 19.66, 1712]}, ... index=['Pork', 'Wheat Products', 'Beef']) >>> df consumption co2_emissions Pork 10.51 37.20 Wheat Products 103.11 19.66 Beef 55.48 1712.00 By default, it returns the index for the maximum value in each column. >>> df.idxmax() consumption Wheat Products co2_emissions Beef dtype: object To return the index for the maximum value in each row, use ``axis=\" columns \"``. >>> df.idxmax(axis=\" columns \") Pork co2_emissions Wheat Products consumption Beef co2_emissions dtype: object \"\"\" axis = self . _get_axis_number ( axis ) indices = nanops . nanargmax ( self . values , axis = axis , skipna = skipna ) # indices will always be np . ndarray since axis is not None and # values is a 2 d array for DataFrame # error : Item \"int\" of \"Union[int, Any]\" has no attribute \"__iter__\" assert isinstance ( indices , np . ndarray ) # for mypy index = self . _get_axis ( axis ) result = [ index[i ] if i >= 0 else np . nan for i in indices ] return self . _constructor_sliced ( result , index = self . _get_agg_axis ( axis )) idxmin def idxmin ( self , axis = 0 , skipna = True ) -> pandas . core . series . Series Return index of first occurrence of minimum over requested axis. NA/null values are excluded. Parameters axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. Returns Series Indexes of minima along the specified axis. Raises ValueError * If the row/column is empty See Also Series.idxmin : Return index of the minimum element. Notes This method is the DataFrame version of ndarray.argmin . Examples Consider a dataset containing food consumption in Argentina. df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48], ... 'co2_emissions': [37.2, 19.66, 1712]}, ... index=['Pork', 'Wheat Products', 'Beef']) df consumption co2_emissions Pork 10.51 37.20 Wheat Products 103.11 19.66 Beef 55.48 1712.00 By default, it returns the index for the minimum value in each column. df.idxmin() consumption Pork co2_emissions Wheat Products dtype: object To return the index for the minimum value in each row, use axis=\"columns\" . df.idxmin(axis=\"columns\") Pork consumption Wheat Products co2_emissions Beef consumption dtype: object View Source def idxmin ( self , axis = 0 , skipna = True ) -> Series : \"\"\" Return index of first occurrence of minimum over requested axis. NA/null values are excluded. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. Returns ------- Series Indexes of minima along the specified axis. Raises ------ ValueError * If the row/column is empty See Also -------- Series.idxmin : Return index of the minimum element. Notes ----- This method is the DataFrame version of ``ndarray.argmin``. Examples -------- Consider a dataset containing food consumption in Argentina. >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48], ... 'co2_emissions': [37.2, 19.66, 1712]}, ... index=['Pork', 'Wheat Products', 'Beef']) >>> df consumption co2_emissions Pork 10.51 37.20 Wheat Products 103.11 19.66 Beef 55.48 1712.00 By default, it returns the index for the minimum value in each column. >>> df.idxmin() consumption Pork co2_emissions Wheat Products dtype: object To return the index for the minimum value in each row, use ``axis=\" columns \"``. >>> df.idxmin(axis=\" columns \") Pork consumption Wheat Products co2_emissions Beef consumption dtype: object \"\"\" axis = self . _get_axis_number ( axis ) indices = nanops . nanargmin ( self . values , axis = axis , skipna = skipna ) # indices will always be np . ndarray since axis is not None and # values is a 2 d array for DataFrame # error : Item \"int\" of \"Union[int, Any]\" has no attribute \"__iter__\" assert isinstance ( indices , np . ndarray ) # for mypy index = self . _get_axis ( axis ) result = [ index[i ] if i >= 0 else np . nan for i in indices ] return self . _constructor_sliced ( result , index = self . _get_agg_axis ( axis )) infer_objects def infer_objects ( self : ~ FrameOrSeries ) -> ~ FrameOrSeries Attempt to infer better dtypes for object columns. Attempts soft conversion of object-dtyped columns, leaving non-object and unconvertible columns unchanged. The inference rules are the same as during normal Series/DataFrame construction. Returns converted : same type as input object See Also to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to numeric type. convert_dtypes : Convert argument to best possible dtype. Examples df = pd.DataFrame({\"A\": [\"a\", 1, 2, 3]}) df = df.iloc[1:] df A 1 1 2 2 3 3 df.dtypes A object dtype: object df.infer_objects().dtypes A int64 dtype: object View Source def infer_objects ( self : FrameOrSeries ) -> FrameOrSeries : \"\"\" Attempt to infer better dtypes for object columns. Attempts soft conversion of object-dtyped columns, leaving non-object and unconvertible columns unchanged. The inference rules are the same as during normal Series/DataFrame construction. Returns ------- converted : same type as input object See Also -------- to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to numeric type. convert_dtypes : Convert argument to best possible dtype. Examples -------- >>> df = pd.DataFrame({\" A \": [\" a \", 1, 2, 3]}) >>> df = df.iloc[1:] >>> df A 1 1 2 2 3 3 >>> df.dtypes A object dtype: object >>> df.infer_objects().dtypes A int64 dtype: object \"\"\" # numeric = False necessary to only soft convert ; # python objects will still be converted to # native numpy numeric types return self . _constructor ( self . _mgr . convert ( datetime = True , numeric = False , timedelta = True , coerce = False , copy = True ) ). __finalize__ ( self , method = \"infer_objects\" ) info def info ( self , verbose : Union [ bool , NoneType ] = None , buf : Union [ IO [ str ], NoneType ] = None , max_cols : Union [ int , NoneType ] = None , memory_usage : Union [ bool , str , NoneType ] = None , null_counts : Union [ bool , NoneType ] = None ) -> None Print a concise summary of a DataFrame. This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage. Parameters data : DataFrame DataFrame to print information about. verbose : bool, optional Whether to print the full summary. By default, the setting in pandas.options.display.max_info_columns is followed. buf : writable buffer, defaults to sys.stdout Where to send the output. By default, the output is printed to sys.stdout. Pass a writable buffer if you need to further process the output. max_cols : int, optional When to switch from the verbose to the truncated output. If the DataFrame has more than max_cols columns, the truncated output is used. By default, the setting in pandas.options.display.max_info_columns is used. memory_usage : bool, str, optional Specifies whether total memory usage of the DataFrame elements (including the index) should be displayed. By default, this follows the pandas.options.display.memory_usage setting. True always show memory usage. False never shows memory usage. A value of 'deep' is equivalent to \"True with deep introspection\". Memory usage is shown in human-readable units (base-2 representation). Without deep introspection a memory estimation is made based in column dtype and number of rows assuming values consume the same memory amount for corresponding dtypes. With deep memory introspection, a real memory usage calculation is performed at the cost of computational resources. null_counts : bool, optional Whether to show the non-null counts. By default, this is shown only if the DataFrame is smaller than pandas.options.display.max_info_rows and pandas.options.display.max_info_columns . A value of True always shows the counts, and False never shows the counts. Returns None This method prints a summary of a DataFrame and returns None. See Also DataFrame.describe: Generate descriptive statistics of DataFrame columns. DataFrame.memory_usage: Memory usage of DataFrame columns. Examples >>> int_values = [ 1 , 2 , 3 , 4 , 5 ] >>> text_values = [ 'alpha' , 'beta' , 'gamma' , 'delta' , 'epsilon' ] >>> float_values = [ 0 . 0 , 0 . 25 , 0 . 5 , 0 . 75 , 1 . 0 ] >>> df = pd . DataFrame ( { \"int_col\" : int_values , \"text_col\" : text_values , ... \"float_col\" : float_values } ) >>> df int_col text_col float_col 0 1 alpha 0 . 00 1 2 beta 0 . 25 2 3 gamma 0 . 50 3 4 delta 0 . 75 4 5 epsilon 1 . 00 Prints information of all columns : >>> df . info ( verbose = True ) < class 'pandas.core.frame.DataFrame' > RangeIndex : 5 entries , 0 to 4 Data columns ( total 3 columns ): # Column Non-Null Count Dtype - -- ------ -------------- ----- 0 int_col 5 non - null int64 1 text_col 5 non - null object 2 float_col 5 non - null float64 dtypes : float64 ( 1 ), int64 ( 1 ), object ( 1 ) memory usage : 248 . 0 + bytes Prints a summary of columns count and its dtypes but not per column information : >>> df . info ( verbose = False ) < class 'pandas.core.frame.DataFrame' > RangeIndex : 5 entries , 0 to 4 Columns : 3 entries , int_col to float_col dtypes : float64 ( 1 ), int64 ( 1 ), object ( 1 ) memory usage : 248 . 0 + bytes Pipe output of DataFrame . info to buffer instead of sys . stdout , get buffer content and writes to a text file : >>> import io >>> buffer = io . StringIO () >>> df . info ( buf = buffer ) >>> s = buffer . getvalue () >>> with open ( \"df_info.txt\" , \"w\" , ... encoding = \"utf-8\" ) as f : # doctest: +SKIP ... f . write ( s ) 260 The `memory_usage` parameter allows deep introspection mode , specially useful for big DataFrames and fine - tune memory optimization : >>> random_strings_array = np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ) >>> df = pd . DataFrame ( { ... 'column_1' : np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ), ... 'column_2' : np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ), ... 'column_3' : np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ) ... } ) >>> df . info () < class 'pandas.core.frame.DataFrame' > RangeIndex : 1000000 entries , 0 to 999999 Data columns ( total 3 columns ): # Column Non-Null Count Dtype - -- ------ -------------- ----- 0 column_1 1000000 non - null object 1 column_2 1000000 non - null object 2 column_3 1000000 non - null object dtypes : object ( 3 ) memory usage : 22 . 9 + MB >>> df . info ( memory_usage = 'deep' ) < class 'pandas.core.frame.DataFrame' > RangeIndex : 1000000 entries , 0 to 999999 Data columns ( total 3 columns ): # Column Non-Null Count Dtype - -- ------ -------------- ----- 0 column_1 1000000 non - null object 1 column_2 1000000 non - null object 2 column_3 1000000 non - null object dtypes : object ( 3 ) memory usage : 188 . 8 MB View Source @Substitution ( klass = \"DataFrame\" , type_sub = \" and columns\" , max_cols_sub = ( \"\"\"max_cols : int, optional When to switch from the verbose to the truncated output. If the DataFrame has more than `max_cols` columns, the truncated output is used. By default, the setting in ``pandas.options.display.max_info_columns`` is used. \"\"\" ), examples_sub = ( \"\"\" >>> int_values = [1, 2, 3, 4, 5] >>> text_values = ['alpha', 'beta', 'gamma', 'delta', 'epsilon'] >>> float_values = [0.0, 0.25, 0.5, 0.75, 1.0] >>> df = pd.DataFrame({\" int_col \": int_values, \" text_col \": text_values, ... \" float_col \": float_values}) >>> df int_col text_col float_col 0 1 alpha 0.00 1 2 beta 0.25 2 3 gamma 0.50 3 4 delta 0.75 4 5 epsilon 1.00 Prints information of all columns: >>> df.info(verbose=True) <class 'pandas.core.frame.DataFrame'> RangeIndex: 5 entries, 0 to 4 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 int_col 5 non-null int64 1 text_col 5 non-null object 2 float_col 5 non-null float64 dtypes: float64(1), int64(1), object(1) memory usage: 248.0+ bytes Prints a summary of columns count and its dtypes but not per column information: >>> df.info(verbose=False) <class 'pandas.core.frame.DataFrame'> RangeIndex: 5 entries, 0 to 4 Columns: 3 entries, int_col to float_col dtypes: float64(1), int64(1), object(1) memory usage: 248.0+ bytes Pipe output of DataFrame.info to buffer instead of sys.stdout, get buffer content and writes to a text file: >>> import io >>> buffer = io.StringIO() >>> df.info(buf=buffer) >>> s = buffer.getvalue() >>> with open(\" df_info . txt \", \" w \", ... encoding=\" utf - 8 \") as f: # doctest: +SKIP ... f.write(s) 260 The `memory_usage` parameter allows deep introspection mode, specially useful for big DataFrames and fine-tune memory optimization: >>> random_strings_array = np.random.choice(['a', 'b', 'c'], 10 ** 6) >>> df = pd.DataFrame({ ... 'column_1': np.random.choice(['a', 'b', 'c'], 10 ** 6), ... 'column_2': np.random.choice(['a', 'b', 'c'], 10 ** 6), ... 'column_3': np.random.choice(['a', 'b', 'c'], 10 ** 6) ... }) >>> df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 1000000 entries, 0 to 999999 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 column_1 1000000 non-null object 1 column_2 1000000 non-null object 2 column_3 1000000 non-null object dtypes: object(3) memory usage: 22.9+ MB >>> df.info(memory_usage='deep') <class 'pandas.core.frame.DataFrame'> RangeIndex: 1000000 entries, 0 to 999999 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 column_1 1000000 non-null object 1 column_2 1000000 non-null object 2 column_3 1000000 non-null object dtypes: object(3) memory usage: 188.8 MB\"\"\" ), see_also_sub = ( \"\"\" DataFrame.describe: Generate descriptive statistics of DataFrame columns. DataFrame.memory_usage: Memory usage of DataFrame columns.\"\"\" ), ) @doc ( DataFrameInfo . info ) def info ( self , verbose : Optional [ bool ] = None , buf : Optional [ IO[str ] ] = None , max_cols : Optional [ int ] = None , memory_usage : Optional [ Union[bool, str ] ] = None , null_counts : Optional [ bool ] = None , ) -> None : return DataFrameInfo ( self , verbose , buf , max_cols , memory_usage , null_counts ). info () insert def insert ( self , loc , column , value , allow_duplicates = False ) -> None Insert column into DataFrame at specified location. Raises a ValueError if column is already contained in the DataFrame, unless allow_duplicates is set to True. Parameters loc : int Insertion index. Must verify 0 <= loc <= len(columns). column : str, number, or hashable object Label of the inserted column. value : int, Series, or array-like allow_duplicates : bool, optional View Source def insert ( self , loc , column , value , allow_duplicates = False ) -> None : \"\"\" Insert column into DataFrame at specified location. Raises a ValueError if `column` is already contained in the DataFrame, unless `allow_duplicates` is set to True. Parameters ---------- loc : int Insertion index. Must verify 0 <= loc <= len(columns). column : str, number, or hashable object Label of the inserted column. value : int, Series, or array-like allow_duplicates : bool, optional \"\"\" self . _ensure_valid_index ( value ) value = self . _sanitize_column ( column , value , broadcast = False ) self . _mgr . insert ( loc , column , value , allow_duplicates = allow_duplicates ) interpolate def interpolate ( self : ~ FrameOrSeries , method : str = 'linear' , axis : Union [ str , int ] = 0 , limit : Union [ int , NoneType ] = None , inplace : bool = False , limit_direction : Union [ str , NoneType ] = None , limit_area : Union [ str , NoneType ] = None , downcast : Union [ str , NoneType ] = None , ** kwargs ) -> Union [ ~ FrameOrSeries , NoneType ] Please note that only method='linear' is supported for DataFrame/Series with a MultiIndex. Parameters method : str, default 'linear' Interpolation technique to use. One of: * 'linear' : Ignore the index and treat the values as equally spaced . This is the only method supported on MultiIndexes . * 'time' : Works on daily and higher resolution data to interpolate given length of interval . * 'index' , 'values' : use the actual numerical values of the index . * 'pad' : Fill in NaNs using existing values . * 'nearest' , 'zero' , 'slinear' , 'quadratic' , 'cubic' , 'spline' , 'barycentric' , 'polynomial' : Passed to `scipy.interpolate.interp1d` . These methods use the numerical values of the index . Both 'polynomial' and 'spline' require that you also specify an `order` ( int ), e . g . `` df . interpolate ( method = 'polynomial' , order = 5 ) `` . * 'krogh' , 'piecewise_polynomial' , 'spline' , 'pchip' , 'akima' , 'cubicspline' : Wrappers around the SciPy interpolation methods of similar names . See `Notes` . * 'from_derivatives' : Refers to `scipy.interpolate.BPoly.from_derivatives` which replaces 'piecewise_polynomial' interpolation method in scipy 0 . 18 . axis : {{0 or 'index', 1 or 'columns', None}}, default None Axis to interpolate along. limit : int, optional Maximum number of consecutive NaNs to fill. Must be greater than 0. inplace : bool, default False Update the data in place if possible. limit_direction : {{'forward', 'backward', 'both'}}, Optional Consecutive NaNs will be filled in this direction. If limit is specified : * If 'method' is 'pad' or 'ffill' , 'limit_direction' must be 'forward' . * If 'method' is 'backfill' or 'bfill' , 'limit_direction' must be 'backwards' . If 'limit' is not specified : * If 'method' is 'backfill' or 'bfill' , the default is 'backward' * else the default is 'forward' .. versionchanged :: 1 . 1 . 0 raises ValueError if `limit_direction` is 'forward' or 'both' and method is 'backfill' or 'bfill' . raises ValueError if `limit_direction` is 'backward' or 'both' and method is 'pad' or 'ffill' . limit_area : {{ None , 'inside', 'outside'}}, default None If limit is specified, consecutive NaNs will be filled with this restriction. * `` None `` : No fill restriction . * 'inside' : Only fill NaNs surrounded by valid values ( interpolate ). * 'outside' : Only fill NaNs outside valid values ( extrapolate ). .. versionadded :: 0 . 23 . 0 downcast : optional, 'infer' or None, defaults to None Downcast dtypes if possible. **kwargs Keyword arguments to pass on to the interpolating function. Returns Series or DataFrame Returns the same object type as the caller, interpolated at some or all NaN values. See Also fillna : Fill missing values using different methods. scipy.interpolate.Akima1DInterpolator : Piecewise cubic polynomials (Akima interpolator). scipy.interpolate.BPoly.from_derivatives : Piecewise polynomial in the Bernstein basis. scipy.interpolate.interp1d : Interpolate a 1-D function. scipy.interpolate.KroghInterpolator : Interpolate polynomial (Krogh interpolator). scipy.interpolate.PchipInterpolator : PCHIP 1-d monotonic cubic interpolation. scipy.interpolate.CubicSpline : Cubic spline data interpolator. Notes The 'krogh', 'piecewise_polynomial', 'spline', 'pchip' and 'akima' methods are wrappers around the respective SciPy implementations of similar names. These use the actual numerical values of the index. For more information on their behavior, see the SciPy documentation <https://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation> and SciPy tutorial <https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html> . Examples Filling in NaN in a :class: ~pandas.Series via linear interpolation. s = pd.Series([0, 1, np.nan, 3]) s 0 0.0 1 1.0 2 NaN 3 3.0 dtype: float64 s.interpolate() 0 0.0 1 1.0 2 2.0 3 3.0 dtype: float64 Filling in NaN in a Series by padding, but filling at most two consecutive NaN at a time. s = pd.Series([np.nan, \"single_one\", np.nan, ... \"fill_two_more\", np.nan, np.nan, np.nan, ... 4.71, np.nan]) s 0 NaN 1 single_one 2 NaN 3 fill_two_more 4 NaN 5 NaN 6 NaN 7 4.71 8 NaN dtype: object s.interpolate(method='pad', limit=2) 0 NaN 1 single_one 2 single_one 3 fill_two_more 4 fill_two_more 5 fill_two_more 6 NaN 7 4.71 8 4.71 dtype: object Filling in NaN in a Series via polynomial interpolation or splines: Both 'polynomial' and 'spline' methods require that you also specify an order (int). s = pd.Series([0, 2, np.nan, 8]) s.interpolate(method='polynomial', order=2) 0 0.000000 1 2.000000 2 4.666667 3 8.000000 dtype: float64 Fill the DataFrame forward (that is, going down) along each column using linear interpolation. Note how the last entry in column 'a' is interpolated differently, because there is no entry after it to use for interpolation. Note how the first entry in column 'b' remains NaN , because there is no entry before it to use for interpolation. df = pd.DataFrame([(0.0, np.nan, -1.0, 1.0), ... (np.nan, 2.0, np.nan, np.nan), ... (2.0, 3.0, np.nan, 9.0), ... (np.nan, 4.0, -4.0, 16.0)], ... columns=list('abcd')) df a b c d 0 0.0 NaN -1.0 1.0 1 NaN 2.0 NaN NaN 2 2.0 3.0 NaN 9.0 3 NaN 4.0 -4.0 16.0 df.interpolate(method='linear', limit_direction='forward', axis=0) a b c d 0 0.0 NaN -1.0 1.0 1 1.0 2.0 -2.0 5.0 2 2.0 3.0 -3.0 9.0 3 2.0 4.0 -4.0 16.0 Using polynomial interpolation. df['d'].interpolate(method='polynomial', order=2) 0 1.0 1 4.0 2 9.0 3 16.0 Name: d, dtype: float64 View Source def interpolate ( self : FrameOrSeries , method : str = \"linear\" , axis : Axis = 0 , limit : Optional [ int ] = None , inplace : bool_t = False , limit_direction : Optional [ str ] = None , limit_area : Optional [ str ] = None , downcast : Optional [ str ] = None , ** kwargs , ) -> Optional [ FrameOrSeries ]: \"\"\" Please note that only ``method='linear'`` is supported for DataFrame/Series with a MultiIndex. Parameters ---------- method : str, default 'linear' Interpolation technique to use. One of: * 'linear': Ignore the index and treat the values as equally spaced. This is the only method supported on MultiIndexes. * 'time': Works on daily and higher resolution data to interpolate given length of interval. * 'index', 'values': use the actual numerical values of the index. * 'pad': Fill in NaNs using existing values. * 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'spline', 'barycentric', 'polynomial': Passed to `scipy.interpolate.interp1d`. These methods use the numerical values of the index. Both 'polynomial' and 'spline' require that you also specify an `order` (int), e.g. ``df.interpolate(method='polynomial', order=5)``. * 'krogh', 'piecewise_polynomial', 'spline', 'pchip', 'akima', 'cubicspline': Wrappers around the SciPy interpolation methods of similar names. See `Notes`. * 'from_derivatives': Refers to `scipy.interpolate.BPoly.from_derivatives` which replaces 'piecewise_polynomial' interpolation method in scipy 0.18. axis : {{0 or 'index', 1 or 'columns', None}}, default None Axis to interpolate along. limit : int, optional Maximum number of consecutive NaNs to fill. Must be greater than 0. inplace : bool, default False Update the data in place if possible. limit_direction : {{'forward', 'backward', 'both'}}, Optional Consecutive NaNs will be filled in this direction. If limit is specified: * If 'method' is 'pad' or 'ffill', 'limit_direction' must be 'forward'. * If 'method' is 'backfill' or 'bfill', 'limit_direction' must be 'backwards'. If 'limit' is not specified: * If 'method' is 'backfill' or 'bfill', the default is 'backward' * else the default is 'forward' .. versionchanged:: 1.1.0 raises ValueError if `limit_direction` is 'forward' or 'both' and method is 'backfill' or 'bfill'. raises ValueError if `limit_direction` is 'backward' or 'both' and method is 'pad' or 'ffill'. limit_area : {{`None`, 'inside', 'outside'}}, default None If limit is specified, consecutive NaNs will be filled with this restriction. * ``None``: No fill restriction. * 'inside': Only fill NaNs surrounded by valid values (interpolate). * 'outside': Only fill NaNs outside valid values (extrapolate). .. versionadded:: 0.23.0 downcast : optional, 'infer' or None, defaults to None Downcast dtypes if possible. **kwargs Keyword arguments to pass on to the interpolating function. Returns ------- Series or DataFrame Returns the same object type as the caller, interpolated at some or all ``NaN`` values. See Also -------- fillna : Fill missing values using different methods. scipy.interpolate.Akima1DInterpolator : Piecewise cubic polynomials (Akima interpolator). scipy.interpolate.BPoly.from_derivatives : Piecewise polynomial in the Bernstein basis. scipy.interpolate.interp1d : Interpolate a 1-D function. scipy.interpolate.KroghInterpolator : Interpolate polynomial (Krogh interpolator). scipy.interpolate.PchipInterpolator : PCHIP 1-d monotonic cubic interpolation. scipy.interpolate.CubicSpline : Cubic spline data interpolator. Notes ----- The 'krogh', 'piecewise_polynomial', 'spline', 'pchip' and 'akima' methods are wrappers around the respective SciPy implementations of similar names. These use the actual numerical values of the index. For more information on their behavior, see the `SciPy documentation <https://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation>`__ and `SciPy tutorial <https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html>`__. Examples -------- Filling in ``NaN`` in a :class:`~pandas.Series` via linear interpolation. >>> s = pd.Series([0, 1, np.nan, 3]) >>> s 0 0.0 1 1.0 2 NaN 3 3.0 dtype: float64 >>> s.interpolate() 0 0.0 1 1.0 2 2.0 3 3.0 dtype: float64 Filling in ``NaN`` in a Series by padding, but filling at most two consecutive ``NaN`` at a time. >>> s = pd.Series([np.nan, \" single_one \", np.nan, ... \" fill_two_more \", np.nan, np.nan, np.nan, ... 4.71, np.nan]) >>> s 0 NaN 1 single_one 2 NaN 3 fill_two_more 4 NaN 5 NaN 6 NaN 7 4.71 8 NaN dtype: object >>> s.interpolate(method='pad', limit=2) 0 NaN 1 single_one 2 single_one 3 fill_two_more 4 fill_two_more 5 fill_two_more 6 NaN 7 4.71 8 4.71 dtype: object Filling in ``NaN`` in a Series via polynomial interpolation or splines: Both 'polynomial' and 'spline' methods require that you also specify an ``order`` (int). >>> s = pd.Series([0, 2, np.nan, 8]) >>> s.interpolate(method='polynomial', order=2) 0 0.000000 1 2.000000 2 4.666667 3 8.000000 dtype: float64 Fill the DataFrame forward (that is, going down) along each column using linear interpolation. Note how the last entry in column 'a' is interpolated differently, because there is no entry after it to use for interpolation. Note how the first entry in column 'b' remains ``NaN``, because there is no entry before it to use for interpolation. >>> df = pd.DataFrame([(0.0, np.nan, -1.0, 1.0), ... (np.nan, 2.0, np.nan, np.nan), ... (2.0, 3.0, np.nan, 9.0), ... (np.nan, 4.0, -4.0, 16.0)], ... columns=list('abcd')) >>> df a b c d 0 0.0 NaN -1.0 1.0 1 NaN 2.0 NaN NaN 2 2.0 3.0 NaN 9.0 3 NaN 4.0 -4.0 16.0 >>> df.interpolate(method='linear', limit_direction='forward', axis=0) a b c d 0 0.0 NaN -1.0 1.0 1 1.0 2.0 -2.0 5.0 2 2.0 3.0 -3.0 9.0 3 2.0 4.0 -4.0 16.0 Using polynomial interpolation. >>> df['d'].interpolate(method='polynomial', order=2) 0 1.0 1 4.0 2 9.0 3 16.0 Name: d, dtype: float64 \"\"\" inplace = validate_bool_kwarg ( inplace , \"inplace\" ) axis = self . _get_axis_number ( axis ) fillna_methods = [ \"ffill\" , \"bfill\" , \"pad\" , \"backfill\" ] should_transpose = axis == 1 and method not in fillna_methods obj = self . T if should_transpose else self if obj . empty : return self . copy () if method not in fillna_methods : axis = self . _info_axis_number if isinstance ( obj . index , MultiIndex ) and method != \"linear\" : raise ValueError ( \"Only `method=linear` interpolation is supported on MultiIndexes.\" ) # Set `limit_direction` depending on `method` if limit_direction is None : limit_direction = ( \"backward\" if method in ( \"backfill\" , \"bfill\" ) else \"forward\" ) else : if method in ( \"pad\" , \"ffill\" ) and limit_direction != \"forward\" : raise ValueError ( f \"`limit_direction` must be 'forward' for method `{method}`\" ) if method in ( \"backfill\" , \"bfill\" ) and limit_direction != \"backward\" : raise ValueError ( f \"`limit_direction` must be 'backward' for method `{method}`\" ) if obj . ndim == 2 and np . all ( obj . dtypes == np . dtype ( object )): raise TypeError ( \"Cannot interpolate with all object-dtype columns \" \"in the DataFrame. Try setting at least one \" \"column to a numeric dtype.\" ) # create/use the index if method == \"linear\" : # prior default index = np . arange ( len ( obj . index )) else : index = obj . index methods = { \"index\" , \"values\" , \"nearest\" , \"time\" } is_numeric_or_datetime = ( is_numeric_dtype ( index . dtype ) or is_datetime64_any_dtype ( index . dtype ) or is_timedelta64_dtype ( index . dtype ) ) if method not in methods and not is_numeric_or_datetime : raise ValueError ( \"Index column must be numeric or datetime type when \" f \"using {method} method other than linear. \" \"Try setting a numeric or datetime index column before \" \"interpolating.\" ) if isna ( index ). any (): raise NotImplementedError ( \"Interpolation with NaNs in the index \" \"has not been implemented. Try filling \" \"those NaNs before interpolating.\" ) new_data = obj . _mgr . interpolate ( method = method , axis = axis , index = index , limit = limit , limit_direction = limit_direction , limit_area = limit_area , inplace = inplace , downcast = downcast , ** kwargs , ) result = self . _constructor ( new_data ) if should_transpose : result = result . T if inplace : return self . _update_inplace ( result ) else : return result . __finalize__ ( self , method = \"interpolate\" ) isin def isin ( self , values ) -> 'DataFrame' Whether each element in the DataFrame is contained in values. Parameters values : iterable, Series, DataFrame or dict The result will only be true at a location if all the labels match. If values is a Series, that's the index. If values is a dict, the keys must be the column names, which must match. If values is a DataFrame, then both the index and column labels must match. Returns DataFrame DataFrame of booleans showing whether each element in the DataFrame is contained in values. See Also DataFrame.eq: Equality test for DataFrame. Series.isin: Equivalent method on Series. Series.str.contains: Test if pattern or regex is contained within a string of a Series or Index. Examples df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]}, ... index=['falcon', 'dog']) df num_legs num_wings falcon 2 2 dog 4 0 When values is a list check whether every value in the DataFrame is present in the list (which animals have 0 or 2 legs or wings) df.isin([0, 2]) num_legs num_wings falcon True True dog False True When values is a dict, we can pass values to check for each column separately: df.isin({'num_wings': [0, 3]}) num_legs num_wings falcon False False dog False True When values is a Series or DataFrame the index and column must match. Note that 'falcon' does not match based on the number of legs in df2. other = pd.DataFrame({'num_legs': [8, 2], 'num_wings': [0, 2]}, ... index=['spider', 'falcon']) df.isin(other) num_legs num_wings falcon True True dog False False View Source def isin ( self , values ) -> \"DataFrame\" : \"\"\" Whether each element in the DataFrame is contained in values. Parameters ---------- values : iterable, Series, DataFrame or dict The result will only be true at a location if all the labels match. If `values` is a Series, that's the index. If `values` is a dict, the keys must be the column names, which must match. If `values` is a DataFrame, then both the index and column labels must match. Returns ------- DataFrame DataFrame of booleans showing whether each element in the DataFrame is contained in values. See Also -------- DataFrame.eq: Equality test for DataFrame. Series.isin: Equivalent method on Series. Series.str.contains: Test if pattern or regex is contained within a string of a Series or Index. Examples -------- >>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]}, ... index=['falcon', 'dog']) >>> df num_legs num_wings falcon 2 2 dog 4 0 When ``values`` is a list check whether every value in the DataFrame is present in the list (which animals have 0 or 2 legs or wings) >>> df.isin([0, 2]) num_legs num_wings falcon True True dog False True When ``values`` is a dict, we can pass values to check for each column separately: >>> df.isin({'num_wings': [0, 3]}) num_legs num_wings falcon False False dog False True When ``values`` is a Series or DataFrame the index and column must match. Note that 'falcon' does not match based on the number of legs in df2. >>> other = pd.DataFrame({'num_legs': [8, 2], 'num_wings': [0, 2]}, ... index=['spider', 'falcon']) >>> df.isin(other) num_legs num_wings falcon True True dog False False \"\"\" if isinstance ( values , dict ): from pandas . core . reshape . concat import concat values = collections . defaultdict ( list , values ) return concat ( ( self . iloc [:, [ i ]]. isin ( values [ col ]) for i , col in enumerate ( self . columns ) ), axis = 1 , ) elif isinstance ( values , Series ): if not values . index . is_unique : raise ValueError ( \"cannot compute isin with a duplicate axis.\" ) return self . eq ( values . reindex_like ( self ), axis = \"index\" ) elif isinstance ( values , DataFrame ): if not ( values . columns . is_unique and values . index . is_unique ): raise ValueError ( \"cannot compute isin with a duplicate axis.\" ) return self . eq ( values . reindex_like ( self )) else : if not is_list_like ( values ): raise TypeError ( \"only list-like or dict-like objects are allowed \" \"to be passed to DataFrame.isin(), \" f \"you passed a '{type(values).__name__}'\" ) return self . _constructor ( algorithms . isin ( self . values . ravel (), values ). reshape ( self . shape ), self . index , self . columns , ) isna def isna ( self ) -> 'DataFrame' Detect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None or :attr: numpy.NaN , gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). Returns DataFrame Mask of bool values for each element in DataFrame that indicates whether an element is not an NA value. See Also DataFrame.isnull : Alias of isna. DataFrame.notna : Boolean inverse of isna. DataFrame.dropna : Omit axes labels with missing values. isna : Top-level isna. Examples Show which entries in a DataFrame are NA. df = pd.DataFrame({'age': [5, 6, np.NaN], ... 'born': [pd.NaT, pd.Timestamp('1939-05-27'), ... pd.Timestamp('1940-04-25')], ... 'name': ['Alfred', 'Batman', ''], ... 'toy': [None, 'Batmobile', 'Joker']}) df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939-05-27 Batman Batmobile 2 NaN 1940-04-25 Joker df.isna() age born name toy 0 False True False True 1 False False False False 2 True False False False Show which entries in a Series are NA. ser = pd.Series([5, 6, np.NaN]) ser 0 5.0 1 6.0 2 NaN dtype: float64 ser.isna() 0 False 1 False 2 True dtype: bool View Source @doc ( NDFrame . isna , klass = _shared_doc_kwargs [ \"klass\" ] ) def isna ( self ) -> \"DataFrame\" : result = self . _constructor ( self . _data . isna ( func = isna )) return result . __finalize__ ( self , method = \"isna\" ) isnull def isnull ( self ) -> 'DataFrame' Detect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None or :attr: numpy.NaN , gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). Returns DataFrame Mask of bool values for each element in DataFrame that indicates whether an element is not an NA value. See Also DataFrame.isnull : Alias of isna. DataFrame.notna : Boolean inverse of isna. DataFrame.dropna : Omit axes labels with missing values. isna : Top-level isna. Examples Show which entries in a DataFrame are NA. df = pd.DataFrame({'age': [5, 6, np.NaN], ... 'born': [pd.NaT, pd.Timestamp('1939-05-27'), ... pd.Timestamp('1940-04-25')], ... 'name': ['Alfred', 'Batman', ''], ... 'toy': [None, 'Batmobile', 'Joker']}) df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939-05-27 Batman Batmobile 2 NaN 1940-04-25 Joker df.isna() age born name toy 0 False True False True 1 False False False False 2 True False False False Show which entries in a Series are NA. ser = pd.Series([5, 6, np.NaN]) ser 0 5.0 1 6.0 2 NaN dtype: float64 ser.isna() 0 False 1 False 2 True dtype: bool View Source @doc ( NDFrame . isna , klass = _shared_doc_kwargs [ \"klass\" ] ) def isnull ( self ) -> \"DataFrame\" : return self . isna () items def items ( self ) -> Iterable [ Tuple [ Union [ Hashable , NoneType ], pandas . core . series . Series ]] Iterate over (column name, Series) pairs. Iterates over the DataFrame columns, returning a tuple with the column name and the content as a Series. Yields label : object The column names for the DataFrame being iterated over. content : Series The column entries belonging to each label, as a Series. See Also DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs. DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values. Examples df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'], ... 'population': [1864, 22000, 80000]}, ... index=['panda', 'polar', 'koala']) df species population panda bear 1864 polar bear 22000 koala marsupial 80000 for label, content in df.items(): ... print(f'label: {label}') ... print(f'content: {content}', sep='\\n') ... label: species content: panda bear polar bear koala marsupial Name: species, dtype: object label: population content: panda 1864 polar 22000 koala 80000 Name: population, dtype: int64 View Source @Appender ( _shared_docs [ \"items\" ] ) def items ( self ) -> Iterable [ Tuple[Label, Series ] ]: if self . columns . is_unique and hasattr ( self , \"_item_cache\" ) : for k in self . columns : yield k , self . _get_item_cache ( k ) else : for i , k in enumerate ( self . columns ) : yield k , self . _ixs ( i , axis = 1 ) iteritems def iteritems ( self ) -> Iterable [ Tuple [ Union [ Hashable , NoneType ], pandas . core . series . Series ]] Iterate over (column name, Series) pairs. Iterates over the DataFrame columns, returning a tuple with the column name and the content as a Series. Yields label : object The column names for the DataFrame being iterated over. content : Series The column entries belonging to each label, as a Series. See Also DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs. DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values. Examples df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'], ... 'population': [1864, 22000, 80000]}, ... index=['panda', 'polar', 'koala']) df species population panda bear 1864 polar bear 22000 koala marsupial 80000 for label, content in df.items(): ... print(f'label: {label}') ... print(f'content: {content}', sep='\\n') ... label: species content: panda bear polar bear koala marsupial Name: species, dtype: object label: population content: panda 1864 polar 22000 koala 80000 Name: population, dtype: int64 View Source @Appender ( _shared_docs [ \"items\" ] ) def iteritems ( self ) -> Iterable [ Tuple[Label, Series ] ]: yield from self . items () iterrows def iterrows ( self ) -> Iterable [ Tuple [ Union [ Hashable , NoneType ], pandas . core . series . Series ]] Iterate over DataFrame rows as (index, Series) pairs. Yields index : label or tuple of label The index of the row. A tuple for a MultiIndex . data : Series The data of the row as a Series. it : generator A generator that iterates over the rows of the frame. See Also DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values. DataFrame.items : Iterate over (column name, Series) pairs. Notes Because iterrows returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). For example, df = pd.DataFrame([[1, 1.5]], columns=['int', 'float']) row = next(df.iterrows())[1] row int 1.0 float 1.5 Name: 0, dtype: float64 print(row['int'].dtype) float64 print(df['int'].dtype) int64 To preserve dtypes while iterating over the rows, it is better to use :meth: itertuples which returns namedtuples of the values and which is generally faster than iterrows . You should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect. View Source def iterrows ( self ) -> Iterable [ Tuple [ Label , Series ]]: \"\"\" Iterate over DataFrame rows as (index, Series) pairs. Yields ------ index : label or tuple of label The index of the row. A tuple for a `MultiIndex`. data : Series The data of the row as a Series. it : generator A generator that iterates over the rows of the frame. See Also -------- DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values. DataFrame.items : Iterate over (column name, Series) pairs. Notes ----- 1. Because ``iterrows`` returns a Series for each row, it does **not** preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). For example, >>> df = pd.DataFrame([[1, 1.5]], columns=['int', 'float']) >>> row = next(df.iterrows())[1] >>> row int 1.0 float 1.5 Name: 0, dtype: float64 >>> print(row['int'].dtype) float64 >>> print(df['int'].dtype) int64 To preserve dtypes while iterating over the rows, it is better to use :meth:`itertuples` which returns namedtuples of the values and which is generally faster than ``iterrows``. 2. You should **never modify** something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect. \"\"\" columns = self . columns klass = self . _constructor_sliced for k , v in zip ( self . index , self . values ): s = klass ( v , index = columns , name = k ) yield k , s itertuples def itertuples ( self , index = True , name = 'Pandas' ) Iterate over DataFrame rows as namedtuples. Parameters index : bool, default True If True, return the index as the first element of the tuple. name : str or None, default \"Pandas\" The name of the returned namedtuples or None to return regular tuples. Returns iterator An object to iterate over namedtuples for each row in the DataFrame with the first field possibly being the index and following fields being the column values. See Also DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs. DataFrame.items : Iterate over (column name, Series) pairs. Notes The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. On python versions < 3.7 regular tuples are returned for DataFrames with a large number of columns (>254). Examples df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]}, ... index=['dog', 'hawk']) df num_legs num_wings dog 4 0 hawk 2 2 for row in df.itertuples(): ... print(row) ... Pandas(Index='dog', num_legs=4, num_wings=0) Pandas(Index='hawk', num_legs=2, num_wings=2) By setting the index parameter to False we can remove the index as the first element of the tuple: for row in df.itertuples(index=False): ... print(row) ... Pandas(num_legs=4, num_wings=0) Pandas(num_legs=2, num_wings=2) With the name parameter set we set a custom name for the yielded namedtuples: for row in df.itertuples(name='Animal'): ... print(row) ... Animal(Index='dog', num_legs=4, num_wings=0) Animal(Index='hawk', num_legs=2, num_wings=2) View Source def itertuples ( self , index = True , name = \"Pandas\" ): \"\"\" Iterate over DataFrame rows as namedtuples. Parameters ---------- index : bool, default True If True, return the index as the first element of the tuple. name : str or None, default \" Pandas \" The name of the returned namedtuples or None to return regular tuples. Returns ------- iterator An object to iterate over namedtuples for each row in the DataFrame with the first field possibly being the index and following fields being the column values. See Also -------- DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs. DataFrame.items : Iterate over (column name, Series) pairs. Notes ----- The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. On python versions < 3.7 regular tuples are returned for DataFrames with a large number of columns (>254). Examples -------- >>> df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]}, ... index=['dog', 'hawk']) >>> df num_legs num_wings dog 4 0 hawk 2 2 >>> for row in df.itertuples(): ... print(row) ... Pandas(Index='dog', num_legs=4, num_wings=0) Pandas(Index='hawk', num_legs=2, num_wings=2) By setting the `index` parameter to False we can remove the index as the first element of the tuple: >>> for row in df.itertuples(index=False): ... print(row) ... Pandas(num_legs=4, num_wings=0) Pandas(num_legs=2, num_wings=2) With the `name` parameter set we set a custom name for the yielded namedtuples: >>> for row in df.itertuples(name='Animal'): ... print(row) ... Animal(Index='dog', num_legs=4, num_wings=0) Animal(Index='hawk', num_legs=2, num_wings=2) \"\"\" arrays = [] fields = list ( self . columns ) if index : arrays . append ( self . index ) fields . insert ( 0 , \"Index\" ) # use integer indexing because of possible duplicate column names arrays . extend ( self . iloc [:, k ] for k in range ( len ( self . columns ))) # Python versions before 3.7 support at most 255 arguments to constructors can_return_named_tuples = PY37 or len ( self . columns ) + index < 255 if name is not None and can_return_named_tuples : itertuple = collections . namedtuple ( name , fields , rename = True ) return map ( itertuple . _make , zip ( * arrays )) # fallback to regular tuples return zip ( * arrays ) join def join ( self , other , on = None , how = 'left' , lsuffix = '' , rsuffix = '' , sort = False ) -> 'DataFrame' Join columns of another DataFrame. Join columns with other DataFrame either on index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list. Parameters other : DataFrame, Series, or list of DataFrame Index should be similar to one of the columns in this one. If a Series is passed, its name attribute must be set, and that will be used as the column name in the resulting joined DataFrame. on : str, list of str, or array-like, optional Column or index level name(s) in the caller to join on the index in other , otherwise joins index-on-index. If multiple values given, the other DataFrame must have a MultiIndex. Can pass an array as the join key if it is not already contained in the calling DataFrame. Like an Excel VLOOKUP operation. how : {'left', 'right', 'outer', 'inner'}, default 'left' How to handle the operation of the two objects. * left : use calling frame 's index (or column if on is specified) * right: use `other`' s index . * outer : form union of calling frame 's index (or column if on is specified) with `other`' s index , and sort it . lexicographically . * inner : form intersection of calling frame 's index (or column if on is specified) with `other`' s index , preserving the order of the calling ' s one . lsuffix : str, default '' Suffix to use from left frame's overlapping columns. rsuffix : str, default '' Suffix to use from right frame's overlapping columns. sort : bool, default False Order result DataFrame lexicographically by the join key. If False, the order of the join key depends on the join type (how keyword). Returns DataFrame A dataframe containing columns from both the caller and other . See Also DataFrame.merge : For column(s)-on-columns(s) operations. Notes Parameters on , lsuffix , and rsuffix are not supported when passing a list of DataFrame objects. Support for specifying index levels as the on parameter was added in version 0.23.0. Examples df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'], ... 'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']}) df key A 0 K0 A0 1 K1 A1 2 K2 A2 3 K3 A3 4 K4 A4 5 K5 A5 other = pd.DataFrame({'key': ['K0', 'K1', 'K2'], ... 'B': ['B0', 'B1', 'B2']}) other key B 0 K0 B0 1 K1 B1 2 K2 B2 Join DataFrames using their indexes. df.join(other, lsuffix='_caller', rsuffix='_other') key_caller A key_other B 0 K0 A0 K0 B0 1 K1 A1 K1 B1 2 K2 A2 K2 B2 3 K3 A3 NaN NaN 4 K4 A4 NaN NaN 5 K5 A5 NaN NaN If we want to join using the key columns, we need to set key to be the index in both df and other . The joined DataFrame will have key as its index. df.set_index('key').join(other.set_index('key')) A B key K0 A0 B0 K1 A1 B1 K2 A2 B2 K3 A3 NaN K4 A4 NaN K5 A5 NaN Another option to join using the key columns is to use the on parameter. DataFrame.join always uses other 's index but we can use any column in df . This method preserves the original DataFrame's index in the result. df.join(other.set_index('key'), on='key') key A B 0 K0 A0 B0 1 K1 A1 B1 2 K2 A2 B2 3 K3 A3 NaN 4 K4 A4 NaN 5 K5 A5 NaN View Source def join ( self , other , on = None , how = \"left\" , lsuffix = \"\" , rsuffix = \"\" , sort = False ) -> \"DataFrame\" : \"\"\" Join columns of another DataFrame. Join columns with `other` DataFrame either on index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list. Parameters ---------- other : DataFrame, Series, or list of DataFrame Index should be similar to one of the columns in this one. If a Series is passed, its name attribute must be set, and that will be used as the column name in the resulting joined DataFrame. on : str, list of str, or array-like, optional Column or index level name(s) in the caller to join on the index in `other`, otherwise joins index-on-index. If multiple values given, the `other` DataFrame must have a MultiIndex. Can pass an array as the join key if it is not already contained in the calling DataFrame. Like an Excel VLOOKUP operation. how : {'left', 'right', 'outer', 'inner'}, default 'left' How to handle the operation of the two objects. * left: use calling frame's index (or column if on is specified) * right: use `other`'s index. * outer: form union of calling frame's index (or column if on is specified) with `other`'s index, and sort it. lexicographically. * inner: form intersection of calling frame's index (or column if on is specified) with `other`'s index, preserving the order of the calling's one. lsuffix : str, default '' Suffix to use from left frame's overlapping columns. rsuffix : str, default '' Suffix to use from right frame's overlapping columns. sort : bool, default False Order result DataFrame lexicographically by the join key. If False, the order of the join key depends on the join type (how keyword). Returns ------- DataFrame A dataframe containing columns from both the caller and `other`. See Also -------- DataFrame.merge : For column(s)-on-columns(s) operations. Notes ----- Parameters `on`, `lsuffix`, and `rsuffix` are not supported when passing a list of `DataFrame` objects. Support for specifying index levels as the `on` parameter was added in version 0.23.0. Examples -------- >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'], ... 'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']}) >>> df key A 0 K0 A0 1 K1 A1 2 K2 A2 3 K3 A3 4 K4 A4 5 K5 A5 >>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'], ... 'B': ['B0', 'B1', 'B2']}) >>> other key B 0 K0 B0 1 K1 B1 2 K2 B2 Join DataFrames using their indexes. >>> df.join(other, lsuffix='_caller', rsuffix='_other') key_caller A key_other B 0 K0 A0 K0 B0 1 K1 A1 K1 B1 2 K2 A2 K2 B2 3 K3 A3 NaN NaN 4 K4 A4 NaN NaN 5 K5 A5 NaN NaN If we want to join using the key columns, we need to set key to be the index in both `df` and `other`. The joined DataFrame will have key as its index. >>> df.set_index('key').join(other.set_index('key')) A B key K0 A0 B0 K1 A1 B1 K2 A2 B2 K3 A3 NaN K4 A4 NaN K5 A5 NaN Another option to join using the key columns is to use the `on` parameter. DataFrame.join always uses `other`'s index but we can use any column in `df`. This method preserves the original DataFrame's index in the result. >>> df.join(other.set_index('key'), on='key') key A B 0 K0 A0 B0 1 K1 A1 B1 2 K2 A2 B2 3 K3 A3 NaN 4 K4 A4 NaN 5 K5 A5 NaN \"\"\" return self . _join_compat ( other , on = on , how = how , lsuffix = lsuffix , rsuffix = rsuffix , sort = sort ) keys def keys ( self ) Get the 'info axis' (see Indexing for more). This is index for Series, columns for DataFrame. Returns Index Info axis. View Source def keys ( self ): \"\"\" Get the 'info axis' (see Indexing for more). This is index for Series, columns for DataFrame. Returns ------- Index Info axis. \"\"\" return self . _info_axis kurt def kurt ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return unbiased kurtosis over requested axis. Kurtosis obtained using Fisher's definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1. Parameters axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function. Returns Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only ) kurtosis def kurtosis ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return unbiased kurtosis over requested axis. Kurtosis obtained using Fisher's definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1. Parameters axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function. Returns Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only ) last def last ( self : ~ FrameOrSeries , offset ) -> ~ FrameOrSeries Select final periods of time series data based on a date offset. When having a DataFrame with dates as index, this function can select the last few rows based on a date offset. Parameters offset : str, DateOffset, dateutil.relativedelta The offset length of the data that will be selected. For instance, '3D' will display all the rows having their index within the last 3 days. Returns Series or DataFrame A subset of the caller. Raises TypeError If the index is not a :class: DatetimeIndex See Also first : Select initial periods of time series based on a date offset. at_time : Select values at a particular time of the day. between_time : Select values between particular times of the day. Examples i = pd.date_range('2018-04-09', periods=4, freq='2D') ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) ts A 2018-04-09 1 2018-04-11 2 2018-04-13 3 2018-04-15 4 Get the rows for the last 3 days: ts.last('3D') A 2018-04-13 3 2018-04-15 4 Notice the data for 3 last calendar days were returned, not the last 3 observed days in the dataset, and therefore data for 2018-04-11 was not returned. View Source def last ( self : FrameOrSeries , offset ) -> FrameOrSeries : \"\"\" Select final periods of time series data based on a date offset. When having a DataFrame with dates as index, this function can select the last few rows based on a date offset. Parameters ---------- offset : str, DateOffset, dateutil.relativedelta The offset length of the data that will be selected. For instance, '3D' will display all the rows having their index within the last 3 days. Returns ------- Series or DataFrame A subset of the caller. Raises ------ TypeError If the index is not a :class:`DatetimeIndex` See Also -------- first : Select initial periods of time series based on a date offset. at_time : Select values at a particular time of the day. between_time : Select values between particular times of the day. Examples -------- >>> i = pd.date_range('2018-04-09', periods=4, freq='2D') >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) >>> ts A 2018-04-09 1 2018-04-11 2 2018-04-13 3 2018-04-15 4 Get the rows for the last 3 days: >>> ts.last('3D') A 2018-04-13 3 2018-04-15 4 Notice the data for 3 last calendar days were returned, not the last 3 observed days in the dataset, and therefore data for 2018-04-11 was not returned. \"\"\" if not isinstance ( self . index , DatetimeIndex ): raise TypeError ( \"'last' only supports a DatetimeIndex index\" ) if len ( self . index ) == 0 : return self offset = to_offset ( offset ) start_date = self . index [ - 1 ] - offset start = self . index . searchsorted ( start_date , side = \"right\" ) return self . iloc [ start :] last_valid_index def last_valid_index ( self ) Return index for last non-NA/null value. Returns scalar : type of index Notes If all elements are non-NA/null, returns None. Also returns None for empty Series/DataFrame. View Source @doc ( first_valid_index , position = \"last\" , klass = _shared_doc_kwargs [ \"klass\" ] ) def last_valid_index ( self ) : return self . _find_valid_index ( \"last\" ) le def le ( self , other , axis = 'columns' , level = None ) Get Less than or equal to of dataframe and other, element-wise (binary operator le ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , =! , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'}, default 'columns' Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. Returns DataFrame of bool Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise. DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples df = pd.DataFrame({'cost': [250, 150, 100], ... 'revenue': [100, 250, 300]}, ... index=['A', 'B', 'C']) df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: df == 100 cost revenue A False True B False False C True False df.eq(100) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: df != pd.Series([100, 250], index=[\"cost\", \"revenue\"]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index') cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : df == [250, 100] cost revenue A True True B False False C False False Use the method to control the axis: df.eq([250, 250, 100], axis='index') cost revenue A True False B False True C True False Compare to a DataFrame of different shape. other = pd.DataFrame({'revenue': [300, 250, 100, 150]}, ... index=['A', 'B', 'C', 'D']) other revenue A 300 B 250 C 100 D 150 df.gt(other) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220], ... 'revenue': [100, 250, 300, 200, 175, 225]}, ... index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'], ... ['A', 'B', 'C', 'A', 'B', 'C']]) df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 df.le(df_multindex, level=1) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None ) : axis = self . _get_axis_number ( axis ) if axis is not None else 1 self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) new_data = dispatch_to_series ( self , other , op , axis = axis ) return self . _construct_result ( new_data ) lookup def lookup ( self , row_labels , col_labels ) -> numpy . ndarray Label-based \"fancy indexing\" function for DataFrame. Given equal-length arrays of row and column labels, return an array of the values corresponding to each (row, col) pair. Parameters row_labels : sequence The row labels to use for lookup. col_labels : sequence The column labels to use for lookup. Returns numpy.ndarray The found values. View Source def lookup ( self , row_labels , col_labels ) -> np . ndarray : \"\"\" Label-based \" fancy indexing \" function for DataFrame. Given equal-length arrays of row and column labels, return an array of the values corresponding to each (row, col) pair. Parameters ---------- row_labels : sequence The row labels to use for lookup. col_labels : sequence The column labels to use for lookup. Returns ------- numpy.ndarray The found values. \"\"\" n = len ( row_labels ) if n != len ( col_labels ) : raise ValueError ( \"Row labels must have same size as column labels\" ) if not ( self . index . is_unique and self . columns . is_unique ) : # GH#33041 raise ValueError ( \"DataFrame.lookup requires unique index and columns\" ) thresh = 1000 if not self . _is_mixed_type or n > thresh : values = self . values ridx = self . index . get_indexer ( row_labels ) cidx = self . columns . get_indexer ( col_labels ) if ( ridx == - 1 ). any () : raise KeyError ( \"One or more row labels was not found\" ) if ( cidx == - 1 ). any () : raise KeyError ( \"One or more column labels was not found\" ) flat_index = ridx * len ( self . columns ) + cidx result = values . flat [ flat_index ] else : result = np . empty ( n , dtype = \"O\" ) for i , ( r , c ) in enumerate ( zip ( row_labels , col_labels )) : result [ i ] = self . _get_value ( r , c ) if is_object_dtype ( result ) : result = lib . maybe_convert_objects ( result ) return result lt def lt ( self , other , axis = 'columns' , level = None ) Get Less than of dataframe and other, element-wise (binary operator lt ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , =! , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'}, default 'columns' Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. Returns DataFrame of bool Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise. DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples df = pd.DataFrame({'cost': [250, 150, 100], ... 'revenue': [100, 250, 300]}, ... index=['A', 'B', 'C']) df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: df == 100 cost revenue A False True B False False C True False df.eq(100) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: df != pd.Series([100, 250], index=[\"cost\", \"revenue\"]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index') cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : df == [250, 100] cost revenue A True True B False False C False False Use the method to control the axis: df.eq([250, 250, 100], axis='index') cost revenue A True False B False True C True False Compare to a DataFrame of different shape. other = pd.DataFrame({'revenue': [300, 250, 100, 150]}, ... index=['A', 'B', 'C', 'D']) other revenue A 300 B 250 C 100 D 150 df.gt(other) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220], ... 'revenue': [100, 250, 300, 200, 175, 225]}, ... index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'], ... ['A', 'B', 'C', 'A', 'B', 'C']]) df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 df.le(df_multindex, level=1) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None ) : axis = self . _get_axis_number ( axis ) if axis is not None else 1 self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) new_data = dispatch_to_series ( self , other , op , axis = axis ) return self . _construct_result ( new_data ) mad def mad ( self , axis = None , skipna = None , level = None ) Return the mean absolute deviation of the values for the requested axis. Parameters axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default None Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. Returns Series or DataFrame (if level specified) View Source @doc ( desc = \"Return the mean absolute deviation of the values \" \"for the requested axis.\" , name1 = name1 , name2 = name2 , axis_descr = axis_descr , see_also = \"\" , examples = \"\" , ) def mad ( self , axis = None , skipna = None , level = None ) : \"\"\" {desc} Parameters ---------- axis : {axis_descr} Axis for the function to be applied on. skipna : bool, default None Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a {name1}. Returns ------- {name1} or {name2} (if level specified)\\ {see_also}\\ {examples} \"\"\" if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( \"mad\" , axis = axis , level = level , skipna = skipna ) data = self . _get_numeric_data () if axis == 0 : demeaned = data - data . mean ( axis = 0 ) else : demeaned = data . sub ( data . mean ( axis = 1 ), axis = 0 ) return np . abs ( demeaned ). mean ( axis = axis , skipna = skipna ) mask def mask ( self , cond , other = nan , inplace = False , axis = None , level = None , errors = 'raise' , try_cast = False ) Replace values where the condition is True. Parameters cond : bool Series/DataFrame, array-like, or callable Where cond is False, keep the original value. Where True, replace with corresponding value from other . If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn't check it). other : scalar, Series/DataFrame, or callable Entries where cond is True are replaced with corresponding value from other . If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn't check it). inplace : bool, default False Whether to perform the operation in place on the data. axis : int, default None Alignment axis if needed. level : int, default None Alignment level if needed. errors : str, {'raise', 'ignore'}, default 'raise' Note that currently this parameter won't affect the results and will always coerce to a suitable dtype. - 'raise' : allow exceptions to be raised. - 'ignore' : suppress exceptions. On error return original object. try_cast : bool, default False Try to cast the result back to the input type (if possible). Returns Same type as caller See Also :func: DataFrame.where : Return an object of same shape as self. Notes The mask method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is False the element is used; otherwise the corresponding element from the DataFrame other is used. The signature for :func: DataFrame.where differs from :func: numpy.where . Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2) . For further details and examples see the mask documentation in :ref: indexing <indexing.where_mask> . Examples s = pd.Series(range(5)) s.where(s > 0) 0 NaN 1 1.0 2 2.0 3 3.0 4 4.0 dtype: float64 s.mask(s > 0) 0 0.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 s.where(s > 1, 10) 0 10 1 10 2 2 3 3 4 4 dtype: int64 df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B']) df A B 0 0 1 1 2 3 2 4 5 3 6 7 4 8 9 m = df % 3 == 0 df.where(m, -df) A B 0 0 -1 1 -2 3 2 -4 -5 3 6 -7 4 -8 9 df.where(m, -df) == np.where(m, df, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True df.where(m, -df) == df.mask(~m, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True View Source @doc ( where , klass = _shared_doc_kwargs [ \"klass\" ] , cond = \"False\" , cond_rev = \"True\" , name = \"mask\" , name_other = \"where\" , ) def mask ( self , cond , other = np . nan , inplace = False , axis = None , level = None , errors = \"raise\" , try_cast = False , ) : inplace = validate_bool_kwarg ( inplace , \"inplace\" ) cond = com . apply_if_callable ( cond , self ) # see gh - 21891 if not hasattr ( cond , \"__invert__\" ) : cond = np . array ( cond ) return self . where ( ~ cond , other = other , inplace = inplace , axis = axis , level = level , try_cast = try_cast , errors = errors , ) max def max ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return the maximum of the values for the requested axis. If you want the index of the maximum, use idxmax . This isthe equivalent of the numpy.ndarray method argmax . Parameters axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function. Returns Series or DataFrame (if level specified) See Also Series.sum : Return the sum. Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis. Examples idx = pd.MultiIndex.from_arrays([ ... ['warm', 'warm', 'cold', 'cold'], ... ['dog', 'falcon', 'fish', 'spider']], ... names=['blooded', 'animal']) s = pd.Series([4, 2, 0, 8], name='legs', index=idx) s blooded animal warm dog 4 falcon 2 cold fish 0 spider 8 Name: legs, dtype: int64 s.max() 8 Max using level names, as well as indices. s.max(level='blooded') blooded warm 4 cold 8 Name: legs, dtype: int64 s.max(level=0) blooded warm 4 cold 8 Name: legs, dtype: int64 View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only ) mean def mean ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return the mean of the values for the requested axis. Parameters axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function. Returns Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only ) median def median ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return the median of the values for the requested axis. Parameters axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function. Returns Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only ) melt def melt ( self , id_vars = None , value_vars = None , var_name = None , value_name = 'value' , col_level = None , ignore_index = True ) -> 'DataFrame' Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables ( id_vars ), while all other columns, considered measured variables ( value_vars ), are \"unpivoted\" to the row axis, leaving just two non-identifier columns, 'variable' and 'value'. .. versionadded:: 0.20.0 Parameters id_vars : tuple, list, or ndarray, optional Column(s) to use as identifier variables. value_vars : tuple, list, or ndarray, optional Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars . var_name : scalar Name to use for the 'variable' column. If None it uses frame.columns.name or 'variable'. value_name : scalar, default 'value' Name to use for the 'value' column. col_level : int or str, optional If columns are a MultiIndex then use this level to melt. ignore_index : bool, default True If True, original index is ignored. If False, the original index is retained. Index labels will be repeated as necessary. .. versionadded :: 1.1.0 Returns DataFrame Unpivoted DataFrame. See Also melt : Identical method. pivot_table : Create a spreadsheet-style pivot table as a DataFrame. DataFrame.pivot : Return reshaped DataFrame organized by given index / column values. DataFrame.explode : Explode a DataFrame from list-like columns to long format. Examples df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'}, ... 'B': {0: 1, 1: 3, 2: 5}, ... 'C': {0: 2, 1: 4, 2: 6}}) df A B C 0 a 1 2 1 b 3 4 2 c 5 6 df.melt(id_vars=['A'], value_vars=['B']) A variable value 0 a B 1 1 b B 3 2 c B 5 df.melt(id_vars=['A'], value_vars=['B', 'C']) A variable value 0 a B 1 1 b B 3 2 c B 5 3 a C 2 4 b C 4 5 c C 6 The names of 'variable' and 'value' columns can be customized: df.melt(id_vars=['A'], value_vars=['B'], ... var_name='myVarname', value_name='myValname') A myVarname myValname 0 a B 1 1 b B 3 2 c B 5 Original index values can be kept around: df.melt(id_vars=['A'], value_vars=['B', 'C'], ignore_index=False) A variable value 0 a B 1 1 b B 3 2 c B 5 0 a C 2 1 b C 4 2 c C 6 If you have multi-index columns: df.columns = [list('ABC'), list('DEF')] df A B C D E F 0 a 1 2 1 b 3 4 2 c 5 6 df.melt(col_level=0, id_vars=['A'], value_vars=['B']) A variable value 0 a B 1 1 b B 3 2 c B 5 df.melt(id_vars=[('A', 'D')], value_vars=[('B', 'E')]) (A, D) variable_0 variable_1 value 0 a B E 1 1 b B E 3 2 c B E 5 View Source @ Appender ( _shared_docs [ \"melt\" ] % dict( caller = \"df.melt(\" , versionadded = \"\\n .. versionadded:: 0.20.0\\n\" , other = \"melt\" , ) ) def melt( self , id_vars = None , value_vars = None , var_name = None , value_name = \"value\" , col_level = None , ignore_index = True , ) -> \"DataFrame\" : return melt( self , id_vars = id_vars , value_vars = value_vars , var_name = var_name , value_name = value_name , col_level = col_level , ignore_index = ignore_index , ) memory_usage def memory_usage ( self , index = True , deep = False ) -> pandas . core . series . Series Return the memory usage of each column in bytes. The memory usage can optionally include the contribution of the index and elements of object dtype. This value is displayed in DataFrame.info by default. This can be suppressed by setting pandas.options.display.memory_usage to False. Parameters index : bool, default True Specifies whether to include the memory usage of the DataFrame's index in returned Series. If index=True , the memory usage of the index is the first item in the output. deep : bool, default False If True, introspect the data deeply by interrogating object dtypes for system-level memory consumption, and include it in the returned values. Returns Series A Series whose index is the original column names and whose values is the memory usage of each column in bytes. See Also numpy.ndarray.nbytes : Total bytes consumed by the elements of an ndarray. Series.memory_usage : Bytes consumed by a Series. Categorical : Memory-efficient array for string values with many repeated values. DataFrame.info : Concise summary of a DataFrame. Examples dtypes = ['int64', 'float64', 'complex128', 'object', 'bool'] data = dict([(t, np.ones(shape=5000).astype(t)) ... for t in dtypes]) df = pd.DataFrame(data) df.head() int64 float64 complex128 object bool 0 1 1.0 1.000000+0.000000j 1 True 1 1 1.0 1.000000+0.000000j 1 True 2 1 1.0 1.000000+0.000000j 1 True 3 1 1.0 1.000000+0.000000j 1 True 4 1 1.0 1.000000+0.000000j 1 True df.memory_usage() Index 128 int64 40000 float64 40000 complex128 80000 object 40000 bool 5000 dtype: int64 df.memory_usage(index=False) int64 40000 float64 40000 complex128 80000 object 40000 bool 5000 dtype: int64 The memory footprint of object dtype columns is ignored by default: df.memory_usage(deep=True) Index 128 int64 40000 float64 40000 complex128 80000 object 160000 bool 5000 dtype: int64 Use a Categorical for efficient storage of an object-dtype column with many repeated values. df['object'].astype('category').memory_usage(deep=True) 5216 View Source def memory_usage ( self , index = True , deep = False ) -> Series : \"\"\" Return the memory usage of each column in bytes. The memory usage can optionally include the contribution of the index and elements of `object` dtype. This value is displayed in `DataFrame.info` by default. This can be suppressed by setting ``pandas.options.display.memory_usage`` to False. Parameters ---------- index : bool, default True Specifies whether to include the memory usage of the DataFrame's index in returned Series. If ``index=True``, the memory usage of the index is the first item in the output. deep : bool, default False If True, introspect the data deeply by interrogating `object` dtypes for system-level memory consumption, and include it in the returned values. Returns ------- Series A Series whose index is the original column names and whose values is the memory usage of each column in bytes. See Also -------- numpy.ndarray.nbytes : Total bytes consumed by the elements of an ndarray. Series.memory_usage : Bytes consumed by a Series. Categorical : Memory-efficient array for string values with many repeated values. DataFrame.info : Concise summary of a DataFrame. Examples -------- >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool'] >>> data = dict([(t, np.ones(shape=5000).astype(t)) ... for t in dtypes]) >>> df = pd.DataFrame(data) >>> df.head() int64 float64 complex128 object bool 0 1 1.0 1.000000+0.000000j 1 True 1 1 1.0 1.000000+0.000000j 1 True 2 1 1.0 1.000000+0.000000j 1 True 3 1 1.0 1.000000+0.000000j 1 True 4 1 1.0 1.000000+0.000000j 1 True >>> df.memory_usage() Index 128 int64 40000 float64 40000 complex128 80000 object 40000 bool 5000 dtype: int64 >>> df.memory_usage(index=False) int64 40000 float64 40000 complex128 80000 object 40000 bool 5000 dtype: int64 The memory footprint of `object` dtype columns is ignored by default: >>> df.memory_usage(deep=True) Index 128 int64 40000 float64 40000 complex128 80000 object 160000 bool 5000 dtype: int64 Use a Categorical for efficient storage of an object-dtype column with many repeated values. >>> df['object'].astype('category').memory_usage(deep=True) 5216 \"\"\" result = self . _constructor_sliced ( [ c . memory_usage ( index = False , deep = deep ) for col , c in self . items ()], index = self . columns , ) if index : result = self . _constructor_sliced ( self . index . memory_usage ( deep = deep ), index = [ \"Index\" ] ). append ( result ) return result merge def merge ( self , right , how = 'inner' , on = None , left_on = None , right_on = None , left_index = False , right_index = False , sort = False , suffixes = ( '_x' , '_y' ), copy = True , indicator = False , validate = None ) -> 'DataFrame' Merge DataFrame or named Series objects with a database-style join. The join is done on columns or indexes. If joining columns on columns, the DataFrame indexes will be ignored . Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on. Parameters right : DataFrame or named Series Object to merge with. how : {'left', 'right', 'outer', 'inner'}, default 'inner' Type of merge to be performed. * left: use only keys from left frame, similar to a SQL left outer join; preserve key order. * right: use only keys from right frame, similar to a SQL right outer join; preserve key order. * outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically. * inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys. on : label or list Column or index level names to join on. These must be found in both DataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames. left_on : label or list, or array-like Column or index level names to join on in the left DataFrame. Can also be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns. right_on : label or list, or array-like Column or index level names to join on in the right DataFrame. Can also be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns. left_index : bool, default False Use the index from the left DataFrame as the join key(s). If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels. right_index : bool, default False Use the index from the right DataFrame as the join key. Same caveats as left_index. sort : bool, default False Sort the join keys lexicographically in the result DataFrame. If False, the order of the join keys depends on the join type (how keyword). suffixes : list-like, default is (\"_x\", \"_y\") A length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None. copy : bool, default True If False, avoid copy if possible. indicator : bool or str, default False If True, adds a column to the output DataFrame called \"_merge\" with information on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of \"left_only\" for observations whose merge key only appears in the left DataFrame, \"right_only\" for observations whose merge key only appears in the right DataFrame, and \"both\" if the observation's merge key is found in both DataFrames. validate : str, optional If specified, checks if merge is of specified type. * \"one_to_one\" or \"1:1\": check if merge keys are unique in both left and right datasets. * \"one_to_many\" or \"1:m\": check if merge keys are unique in left dataset. * \"many_to_one\" or \"m:1\": check if merge keys are unique in right dataset. * \"many_to_many\" or \"m:m\": allowed, but does not result in checks. Returns DataFrame A DataFrame of the two merged objects. See Also merge_ordered : Merge with optional filling/interpolation. merge_asof : Merge on nearest keys. DataFrame.join : Similar method using indices. Notes Support for specifying index levels as the on , left_on , and right_on parameters was added in version 0.23.0 Support for merging named Series objects was added in version 0.24.0 Examples df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'], ... 'value': [1, 2, 3, 5]}) df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'], ... 'value': [5, 6, 7, 8]}) df1 lkey value 0 foo 1 1 bar 2 2 baz 3 3 foo 5 df2 rkey value 0 foo 5 1 bar 6 2 baz 7 3 foo 8 Merge df1 and df2 on the lkey and rkey columns. The value columns have the default suffixes, _x and _y, appended. df1.merge(df2, left_on='lkey', right_on='rkey') lkey value_x rkey value_y 0 foo 1 foo 5 1 foo 1 foo 8 2 foo 5 foo 5 3 foo 5 foo 8 4 bar 2 bar 6 5 baz 3 baz 7 Merge DataFrames df1 and df2 with specified left and right suffixes appended to any overlapping columns. df1.merge(df2, left_on='lkey', right_on='rkey', ... suffixes=('_left', '_right')) lkey value_left rkey value_right 0 foo 1 foo 5 1 foo 1 foo 8 2 foo 5 foo 5 3 foo 5 foo 8 4 bar 2 bar 6 5 baz 3 baz 7 Merge DataFrames df1 and df2, but raise an exception if the DataFrames have any overlapping columns. df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False)) Traceback (most recent call last): ... ValueError: columns overlap but no suffix specified: Index(['value'], dtype='object') View Source @Substitution ( \"\" ) @Appender ( _merge_doc , indents = 2 ) def merge ( self , right , how = \"inner\" , on = None , left_on = None , right_on = None , left_index = False , right_index = False , sort = False , suffixes = ( \"_x\" , \"_y\" ), copy = True , indicator = False , validate = None , ) -> \"DataFrame\" : from pandas.core.reshape.merge import merge return merge ( self , right , how = how , on = on , left_on = left_on , right_on = right_on , left_index = left_index , right_index = right_index , sort = sort , suffixes = suffixes , copy = copy , indicator = indicator , validate = validate , ) min def min ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return the minimum of the values for the requested axis. If you want the index of the minimum, use idxmin . This isthe equivalent of the numpy.ndarray method argmin . Parameters axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function. Returns Series or DataFrame (if level specified) See Also Series.sum : Return the sum. Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis. Examples idx = pd.MultiIndex.from_arrays([ ... ['warm', 'warm', 'cold', 'cold'], ... ['dog', 'falcon', 'fish', 'spider']], ... names=['blooded', 'animal']) s = pd.Series([4, 2, 0, 8], name='legs', index=idx) s blooded animal warm dog 4 falcon 2 cold fish 0 spider 8 Name: legs, dtype: int64 s.min() 0 Min using level names, as well as indices. s.min(level='blooded') blooded warm 2 cold 0 Name: legs, dtype: int64 s.min(level=0) blooded warm 2 cold 0 Name: legs, dtype: int64 View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only ) mod def mod ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Modulo of dataframe and other, element-wise (binary operator mod ). Equivalent to dataframe % other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmod . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) mode def mode ( self , axis = 0 , numeric_only = False , dropna = True ) -> 'DataFrame' Get the mode(s) of each element along the selected axis. The mode of a set of values is the value that appears most often. It can be multiple values. Parameters axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to iterate over while searching for the mode: * 0 or 'index' : get mode of each column * 1 or 'columns' : get mode of each row. numeric_only : bool, default False If True, only apply to numeric columns. dropna : bool, default True Don't consider counts of NaN/NaT. .. versionadded :: 0.24.0 Returns DataFrame The modes of each column or row. See Also Series.mode : Return the highest frequency value in a Series. Series.value_counts : Return the counts of values in a Series. Examples df = pd.DataFrame([('bird', 2, 2), ... ('mammal', 4, np.nan), ... ('arthropod', 8, 0), ... ('bird', 2, np.nan)], ... index=('falcon', 'horse', 'spider', 'ostrich'), ... columns=('species', 'legs', 'wings')) df species legs wings falcon bird 2 2.0 horse mammal 4 NaN spider arthropod 8 0.0 ostrich bird 2 NaN By default, missing values are not considered, and the mode of wings are both 0 and 2. The second row of species and legs contains NaN , because they have only one mode, but the DataFrame has two rows. df.mode() species legs wings 0 bird 2.0 0.0 1 NaN NaN 2.0 Setting dropna=False NaN values are considered and they can be the mode (like for wings). df.mode(dropna=False) species legs wings 0 bird 2 NaN Setting numeric_only=True , only the mode of numeric columns is computed, and columns of other types are ignored. df.mode(numeric_only=True) legs wings 0 2.0 0.0 1 NaN 2.0 To compute the mode over columns and not rows, use the axis parameter: df.mode(axis='columns', numeric_only=True) 0 1 falcon 2.0 NaN horse 4.0 NaN spider 0.0 8.0 ostrich 2.0 NaN View Source def mode ( self , axis = 0 , numeric_only = False , dropna = True ) -> \"DataFrame\" : \"\"\" Get the mode(s) of each element along the selected axis. The mode of a set of values is the value that appears most often. It can be multiple values. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to iterate over while searching for the mode: * 0 or 'index' : get mode of each column * 1 or 'columns' : get mode of each row. numeric_only : bool, default False If True, only apply to numeric columns. dropna : bool, default True Don't consider counts of NaN/NaT. .. versionadded:: 0.24.0 Returns ------- DataFrame The modes of each column or row. See Also -------- Series.mode : Return the highest frequency value in a Series. Series.value_counts : Return the counts of values in a Series. Examples -------- >>> df = pd.DataFrame([('bird', 2, 2), ... ('mammal', 4, np.nan), ... ('arthropod', 8, 0), ... ('bird', 2, np.nan)], ... index=('falcon', 'horse', 'spider', 'ostrich'), ... columns=('species', 'legs', 'wings')) >>> df species legs wings falcon bird 2 2.0 horse mammal 4 NaN spider arthropod 8 0.0 ostrich bird 2 NaN By default, missing values are not considered, and the mode of wings are both 0 and 2. The second row of species and legs contains ``NaN``, because they have only one mode, but the DataFrame has two rows. >>> df.mode() species legs wings 0 bird 2.0 0.0 1 NaN NaN 2.0 Setting ``dropna=False`` ``NaN`` values are considered and they can be the mode (like for wings). >>> df.mode(dropna=False) species legs wings 0 bird 2 NaN Setting ``numeric_only=True``, only the mode of numeric columns is computed, and columns of other types are ignored. >>> df.mode(numeric_only=True) legs wings 0 2.0 0.0 1 NaN 2.0 To compute the mode over columns and not rows, use the axis parameter: >>> df.mode(axis='columns', numeric_only=True) 0 1 falcon 2.0 NaN horse 4.0 NaN spider 0.0 8.0 ostrich 2.0 NaN \"\"\" data = self if not numeric_only else self . _get_numeric_data () def f ( s ): return s . mode ( dropna = dropna ) return data . apply ( f , axis = axis ) mul def mul ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Multiplication of dataframe and other, element-wise (binary operator mul ). Equivalent to dataframe * other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmul . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) multiply def multiply ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Multiplication of dataframe and other, element-wise (binary operator mul ). Equivalent to dataframe * other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmul . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) ne def ne ( self , other , axis = 'columns' , level = None ) Get Not equal to of dataframe and other, element-wise (binary operator ne ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , =! , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison. Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'}, default 'columns' Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. Returns DataFrame of bool Result of the comparison. See Also DataFrame.eq : Compare DataFrames for equality elementwise. DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise. Notes Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ). Examples df = pd.DataFrame({'cost': [250, 150, 100], ... 'revenue': [100, 250, 300]}, ... index=['A', 'B', 'C']) df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: df == 100 cost revenue A False True B False False C True False df.eq(100) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: df != pd.Series([100, 250], index=[\"cost\", \"revenue\"]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index') cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : df == [250, 100] cost revenue A True True B False False C False False Use the method to control the axis: df.eq([250, 250, 100], axis='index') cost revenue A True False B False True C True False Compare to a DataFrame of different shape. other = pd.DataFrame({'revenue': [300, 250, 100, 150]}, ... index=['A', 'B', 'C', 'D']) other revenue A 300 B 250 C 100 D 150 df.gt(other) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220], ... 'revenue': [100, 250, 300, 200, 175, 225]}, ... index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'], ... ['A', 'B', 'C', 'A', 'B', 'C']]) df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 df.le(df_multindex, level=1) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None ) : axis = self . _get_axis_number ( axis ) if axis is not None else 1 self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) new_data = dispatch_to_series ( self , other , op , axis = axis ) return self . _construct_result ( new_data ) nlargest def nlargest ( self , n , columns , keep = 'first' ) -> 'DataFrame' Return the first n rows ordered by columns in descending order. Return the first n rows with the largest values in columns , in descending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to df.sort_values(columns, ascending=False).head(n) , but more performant. Parameters n : int Number of rows to return. columns : label or list of labels Column label(s) to order by. keep : {'first', 'last', 'all'}, default 'first' Where there are duplicate values: - `first` : prioritize the first occurrence ( s ) - `last` : prioritize the last occurrence ( s ) - `` all `` : do not drop any duplicates , even it means selecting more than `n` items . .. versionadded :: 0 . 24 . 0 Returns DataFrame The first n rows ordered by the given columns in descending order. See Also DataFrame.nsmallest : Return the first n rows ordered by columns in ascending order. DataFrame.sort_values : Sort DataFrame by the values. DataFrame.head : Return the first n rows without re-ordering. Notes This function cannot be used with all column types. For example, when specifying columns with object or category dtypes, TypeError is raised. Examples df = pd.DataFrame({'population': [59000000, 65000000, 434000, ... 434000, 434000, 337000, 11300, ... 11300, 11300], ... 'GDP': [1937894, 2583560 , 12011, 4520, 12128, ... 17036, 182, 38, 311], ... 'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\", ... \"IS\", \"NR\", \"TV\", \"AI\"]}, ... index=[\"Italy\", \"France\", \"Malta\", ... \"Maldives\", \"Brunei\", \"Iceland\", ... \"Nauru\", \"Tuvalu\", \"Anguilla\"]) df population GDP alpha-2 Italy 59000000 1937894 IT France 65000000 2583560 FR Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN Iceland 337000 17036 IS Nauru 11300 182 NR Tuvalu 11300 38 TV Anguilla 11300 311 AI In the following example, we will use nlargest to select the three rows having the largest values in column \"population\". df.nlargest(3, 'population') population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT When using keep='last' , ties are resolved in reverse order: df.nlargest(3, 'population', keep='last') population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Brunei 434000 12128 BN When using keep='all' , all duplicate items are maintained: df.nlargest(3, 'population', keep='all') population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN To order by the largest values in column \"population\" and then \"GDP\", we can specify multiple columns like in the next example. df.nlargest(3, ['population', 'GDP']) population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Brunei 434000 12128 BN View Source def nlargest ( self , n , columns , keep = \"first\" ) -> \"DataFrame\" : \"\"\" Return the first `n` rows ordered by `columns` in descending order. Return the first `n` rows with the largest values in `columns`, in descending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to ``df.sort_values(columns, ascending=False).head(n)``, but more performant. Parameters ---------- n : int Number of rows to return. columns : label or list of labels Column label(s) to order by. keep : {'first', 'last', 'all'}, default 'first' Where there are duplicate values: - `first` : prioritize the first occurrence(s) - `last` : prioritize the last occurrence(s) - ``all`` : do not drop any duplicates, even it means selecting more than `n` items. .. versionadded:: 0.24.0 Returns ------- DataFrame The first `n` rows ordered by the given columns in descending order. See Also -------- DataFrame.nsmallest : Return the first `n` rows ordered by `columns` in ascending order. DataFrame.sort_values : Sort DataFrame by the values. DataFrame.head : Return the first `n` rows without re-ordering. Notes ----- This function cannot be used with all column types. For example, when specifying columns with `object` or `category` dtypes, ``TypeError`` is raised. Examples -------- >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000, ... 434000, 434000, 337000, 11300, ... 11300, 11300], ... 'GDP': [1937894, 2583560 , 12011, 4520, 12128, ... 17036, 182, 38, 311], ... 'alpha-2': [\" IT \", \" FR \", \" MT \", \" MV \", \" BN \", ... \" IS \", \" NR \", \" TV \", \" AI \"]}, ... index=[\" Italy \", \" France \", \" Malta \", ... \" Maldives \", \" Brunei \", \" Iceland \", ... \" Nauru \", \" Tuvalu \", \" Anguilla \"]) >>> df population GDP alpha-2 Italy 59000000 1937894 IT France 65000000 2583560 FR Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN Iceland 337000 17036 IS Nauru 11300 182 NR Tuvalu 11300 38 TV Anguilla 11300 311 AI In the following example, we will use ``nlargest`` to select the three rows having the largest values in column \" population \". >>> df.nlargest(3, 'population') population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT When using ``keep='last'``, ties are resolved in reverse order: >>> df.nlargest(3, 'population', keep='last') population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Brunei 434000 12128 BN When using ``keep='all'``, all duplicate items are maintained: >>> df.nlargest(3, 'population', keep='all') population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN To order by the largest values in column \" population \" and then \" GDP \", we can specify multiple columns like in the next example. >>> df.nlargest(3, ['population', 'GDP']) population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Brunei 434000 12128 BN \"\"\" return algorithms . SelectNFrame ( self , n = n , keep = keep , columns = columns ). nlargest () notna def notna ( self ) -> 'DataFrame' Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). NA values, such as None or :attr: numpy.NaN , get mapped to False values. Returns DataFrame Mask of bool values for each element in DataFrame that indicates whether an element is not an NA value. See Also DataFrame.notnull : Alias of notna. DataFrame.isna : Boolean inverse of notna. DataFrame.dropna : Omit axes labels with missing values. notna : Top-level notna. Examples Show which entries in a DataFrame are not NA. df = pd.DataFrame({'age': [5, 6, np.NaN], ... 'born': [pd.NaT, pd.Timestamp('1939-05-27'), ... pd.Timestamp('1940-04-25')], ... 'name': ['Alfred', 'Batman', ''], ... 'toy': [None, 'Batmobile', 'Joker']}) df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939-05-27 Batman Batmobile 2 NaN 1940-04-25 Joker df.notna() age born name toy 0 True False True False 1 True True True True 2 False True True True Show which entries in a Series are not NA. ser = pd.Series([5, 6, np.NaN]) ser 0 5.0 1 6.0 2 NaN dtype: float64 ser.notna() 0 True 1 True 2 False dtype: bool View Source @doc ( NDFrame . notna , klass = _shared_doc_kwargs [ \"klass\" ] ) def notna ( self ) -> \"DataFrame\" : return ~ self . isna () notnull def notnull ( self ) -> 'DataFrame' Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). NA values, such as None or :attr: numpy.NaN , get mapped to False values. Returns DataFrame Mask of bool values for each element in DataFrame that indicates whether an element is not an NA value. See Also DataFrame.notnull : Alias of notna. DataFrame.isna : Boolean inverse of notna. DataFrame.dropna : Omit axes labels with missing values. notna : Top-level notna. Examples Show which entries in a DataFrame are not NA. df = pd.DataFrame({'age': [5, 6, np.NaN], ... 'born': [pd.NaT, pd.Timestamp('1939-05-27'), ... pd.Timestamp('1940-04-25')], ... 'name': ['Alfred', 'Batman', ''], ... 'toy': [None, 'Batmobile', 'Joker']}) df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939-05-27 Batman Batmobile 2 NaN 1940-04-25 Joker df.notna() age born name toy 0 True False True False 1 True True True True 2 False True True True Show which entries in a Series are not NA. ser = pd.Series([5, 6, np.NaN]) ser 0 5.0 1 6.0 2 NaN dtype: float64 ser.notna() 0 True 1 True 2 False dtype: bool View Source @doc ( NDFrame . notna , klass = _shared_doc_kwargs [ \"klass\" ] ) def notnull ( self ) -> \"DataFrame\" : return ~ self . isna () nsmallest def nsmallest ( self , n , columns , keep = 'first' ) -> 'DataFrame' Return the first n rows ordered by columns in ascending order. Return the first n rows with the smallest values in columns , in ascending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to df.sort_values(columns, ascending=True).head(n) , but more performant. Parameters n : int Number of items to retrieve. columns : list or str Column name or names to order by. keep : {'first', 'last', 'all'}, default 'first' Where there are duplicate values: - `` first `` : take the first occurrence . - `` last `` : take the last occurrence . - `` all `` : do not drop any duplicates , even it means selecting more than `n` items . .. versionadded :: 0 . 24 . 0 Returns DataFrame See Also DataFrame.nlargest : Return the first n rows ordered by columns in descending order. DataFrame.sort_values : Sort DataFrame by the values. DataFrame.head : Return the first n rows without re-ordering. Examples df = pd.DataFrame({'population': [59000000, 65000000, 434000, ... 434000, 434000, 337000, 337000, ... 11300, 11300], ... 'GDP': [1937894, 2583560 , 12011, 4520, 12128, ... 17036, 182, 38, 311], ... 'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\", ... \"IS\", \"NR\", \"TV\", \"AI\"]}, ... index=[\"Italy\", \"France\", \"Malta\", ... \"Maldives\", \"Brunei\", \"Iceland\", ... \"Nauru\", \"Tuvalu\", \"Anguilla\"]) df population GDP alpha-2 Italy 59000000 1937894 IT France 65000000 2583560 FR Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN Iceland 337000 17036 IS Nauru 337000 182 NR Tuvalu 11300 38 TV Anguilla 11300 311 AI In the following example, we will use nsmallest to select the three rows having the smallest values in column \"population\". df.nsmallest(3, 'population') population GDP alpha-2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS When using keep='last' , ties are resolved in reverse order: df.nsmallest(3, 'population', keep='last') population GDP alpha-2 Anguilla 11300 311 AI Tuvalu 11300 38 TV Nauru 337000 182 NR When using keep='all' , all duplicate items are maintained: df.nsmallest(3, 'population', keep='all') population GDP alpha-2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS Nauru 337000 182 NR To order by the smallest values in column \"population\" and then \"GDP\", we can specify multiple columns like in the next example. df.nsmallest(3, ['population', 'GDP']) population GDP alpha-2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Nauru 337000 182 NR View Source def nsmallest ( self , n , columns , keep = \"first\" ) -> \"DataFrame\" : \"\"\" Return the first `n` rows ordered by `columns` in ascending order. Return the first `n` rows with the smallest values in `columns`, in ascending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to ``df.sort_values(columns, ascending=True).head(n)``, but more performant. Parameters ---------- n : int Number of items to retrieve. columns : list or str Column name or names to order by. keep : {'first', 'last', 'all'}, default 'first' Where there are duplicate values: - ``first`` : take the first occurrence. - ``last`` : take the last occurrence. - ``all`` : do not drop any duplicates, even it means selecting more than `n` items. .. versionadded:: 0.24.0 Returns ------- DataFrame See Also -------- DataFrame.nlargest : Return the first `n` rows ordered by `columns` in descending order. DataFrame.sort_values : Sort DataFrame by the values. DataFrame.head : Return the first `n` rows without re-ordering. Examples -------- >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000, ... 434000, 434000, 337000, 337000, ... 11300, 11300], ... 'GDP': [1937894, 2583560 , 12011, 4520, 12128, ... 17036, 182, 38, 311], ... 'alpha-2': [\" IT \", \" FR \", \" MT \", \" MV \", \" BN \", ... \" IS \", \" NR \", \" TV \", \" AI \"]}, ... index=[\" Italy \", \" France \", \" Malta \", ... \" Maldives \", \" Brunei \", \" Iceland \", ... \" Nauru \", \" Tuvalu \", \" Anguilla \"]) >>> df population GDP alpha-2 Italy 59000000 1937894 IT France 65000000 2583560 FR Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN Iceland 337000 17036 IS Nauru 337000 182 NR Tuvalu 11300 38 TV Anguilla 11300 311 AI In the following example, we will use ``nsmallest`` to select the three rows having the smallest values in column \" population \". >>> df.nsmallest(3, 'population') population GDP alpha-2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS When using ``keep='last'``, ties are resolved in reverse order: >>> df.nsmallest(3, 'population', keep='last') population GDP alpha-2 Anguilla 11300 311 AI Tuvalu 11300 38 TV Nauru 337000 182 NR When using ``keep='all'``, all duplicate items are maintained: >>> df.nsmallest(3, 'population', keep='all') population GDP alpha-2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS Nauru 337000 182 NR To order by the smallest values in column \" population \" and then \" GDP \", we can specify multiple columns like in the next example. >>> df.nsmallest(3, ['population', 'GDP']) population GDP alpha-2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Nauru 337000 182 NR \"\"\" return algorithms . SelectNFrame ( self , n = n , keep = keep , columns = columns ). nsmallest () nunique def nunique ( self , axis = 0 , dropna = True ) -> pandas . core . series . Series Count distinct observations over requested axis. Return Series with number of distinct observations. Can ignore NaN values. Parameters axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. dropna : bool, default True Don't include NaN in the counts. Returns Series See Also Series.nunique: Method nunique for Series. DataFrame.count: Count non-NA cells for each column or row. Examples df = pd.DataFrame({'A': [1, 2, 3], 'B': [1, 1, 1]}) df.nunique() A 3 B 1 dtype: int64 df.nunique(axis=1) 0 1 1 2 2 2 dtype: int64 View Source def nunique ( self , axis = 0 , dropna = True ) -> Series : \"\"\" Count distinct observations over requested axis. Return Series with number of distinct observations. Can ignore NaN values. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. dropna : bool, default True Don't include NaN in the counts. Returns ------- Series See Also -------- Series.nunique: Method nunique for Series. DataFrame.count: Count non-NA cells for each column or row. Examples -------- >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [1, 1, 1]}) >>> df.nunique() A 3 B 1 dtype: int64 >>> df.nunique(axis=1) 0 1 1 2 2 2 dtype: int64 \"\"\" return self . apply ( Series . nunique , axis = axis , dropna = dropna ) pad def pad ( self : ~ FrameOrSeries , axis = None , inplace : bool = False , limit = None , downcast = None ) -> Union [ ~ FrameOrSeries , NoneType ] Synonym for :meth: DataFrame.fillna with method='ffill' . Returns {klass} or None Object with missing values filled or None if inplace=True . View Source def ffill ( self : FrameOrSeries , axis = None , inplace : bool_t = False , limit = None , downcast = None , ) -> Optional [ FrameOrSeries ] : \"\"\" Synonym for :meth:`DataFrame.fillna` with ``method='ffill'``. Returns ------- {klass} or None Object with missing values filled or None if ``inplace=True``. \"\"\" return self . fillna ( method = \"ffill\" , axis = axis , inplace = inplace , limit = limit , downcast = downcast ) pct_change def pct_change ( self : ~ FrameOrSeries , periods = 1 , fill_method = 'pad' , limit = None , freq = None , ** kwargs ) -> ~ FrameOrSeries Percentage change between the current and a prior element. Computes the percentage change from the immediately previous row by default. This is useful in comparing the percentage of change in a time series of elements. Parameters periods : int, default 1 Periods to shift for forming percent change. fill_method : str, default 'pad' How to handle NAs before computing percent changes. limit : int, default None The number of consecutive NAs to fill before stopping. freq : DateOffset, timedelta, or str, optional Increment to use from time series API (e.g. 'M' or BDay()). **kwargs Additional keyword arguments are passed into DataFrame.shift or Series.shift . Returns chg : Series or DataFrame The same type as the calling object. See Also Series.diff : Compute the difference of two elements in a Series. DataFrame.diff : Compute the difference of two elements in a DataFrame. Series.shift : Shift the index by some number of periods. DataFrame.shift : Shift the index by some number of periods. Examples Series s = pd.Series([90, 91, 85]) s 0 90 1 91 2 85 dtype: int64 s.pct_change() 0 NaN 1 0.011111 2 -0.065934 dtype: float64 s.pct_change(periods=2) 0 NaN 1 NaN 2 -0.055556 dtype: float64 See the percentage change in a Series where filling NAs with last valid observation forward to next valid. s = pd.Series([90, 91, None, 85]) s 0 90.0 1 91.0 2 NaN 3 85.0 dtype: float64 s.pct_change(fill_method='ffill') 0 NaN 1 0.011111 2 0.000000 3 -0.065934 dtype: float64 DataFrame Percentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01. df = pd.DataFrame({ ... 'FR': [4.0405, 4.0963, 4.3149], ... 'GR': [1.7246, 1.7482, 1.8519], ... 'IT': [804.74, 810.01, 860.13]}, ... index=['1980-01-01', '1980-02-01', '1980-03-01']) df FR GR IT 1980-01-01 4.0405 1.7246 804.74 1980-02-01 4.0963 1.7482 810.01 1980-03-01 4.3149 1.8519 860.13 df.pct_change() FR GR IT 1980-01-01 NaN NaN NaN 1980-02-01 0.013810 0.013684 0.006549 1980-03-01 0.053365 0.059318 0.061876 Percentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns. df = pd.DataFrame({ ... '2016': [1769950, 30586265], ... '2015': [1500923, 40912316], ... '2014': [1371819, 41403351]}, ... index=['GOOG', 'APPL']) df 2016 2015 2014 GOOG 1769950 1500923 1371819 APPL 30586265 40912316 41403351 df.pct_change(axis='columns') 2016 2015 2014 GOOG NaN -0.151997 -0.086016 APPL NaN 0.337604 0.012002 View Source def pct_change ( self : FrameOrSeries , periods = 1 , fill_method = \"pad\" , limit = None , freq = None , ** kwargs , ) -> FrameOrSeries : \"\"\" Percentage change between the current and a prior element. Computes the percentage change from the immediately previous row by default. This is useful in comparing the percentage of change in a time series of elements. Parameters ---------- periods : int, default 1 Periods to shift for forming percent change. fill_method : str, default 'pad' How to handle NAs before computing percent changes. limit : int, default None The number of consecutive NAs to fill before stopping. freq : DateOffset, timedelta, or str, optional Increment to use from time series API (e.g. 'M' or BDay()). **kwargs Additional keyword arguments are passed into `DataFrame.shift` or `Series.shift`. Returns ------- chg : Series or DataFrame The same type as the calling object. See Also -------- Series.diff : Compute the difference of two elements in a Series. DataFrame.diff : Compute the difference of two elements in a DataFrame. Series.shift : Shift the index by some number of periods. DataFrame.shift : Shift the index by some number of periods. Examples -------- **Series** >>> s = pd.Series([90, 91, 85]) >>> s 0 90 1 91 2 85 dtype: int64 >>> s.pct_change() 0 NaN 1 0.011111 2 -0.065934 dtype: float64 >>> s.pct_change(periods=2) 0 NaN 1 NaN 2 -0.055556 dtype: float64 See the percentage change in a Series where filling NAs with last valid observation forward to next valid. >>> s = pd.Series([90, 91, None, 85]) >>> s 0 90.0 1 91.0 2 NaN 3 85.0 dtype: float64 >>> s.pct_change(fill_method='ffill') 0 NaN 1 0.011111 2 0.000000 3 -0.065934 dtype: float64 **DataFrame** Percentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01. >>> df = pd.DataFrame({ ... 'FR': [4.0405, 4.0963, 4.3149], ... 'GR': [1.7246, 1.7482, 1.8519], ... 'IT': [804.74, 810.01, 860.13]}, ... index=['1980-01-01', '1980-02-01', '1980-03-01']) >>> df FR GR IT 1980-01-01 4.0405 1.7246 804.74 1980-02-01 4.0963 1.7482 810.01 1980-03-01 4.3149 1.8519 860.13 >>> df.pct_change() FR GR IT 1980-01-01 NaN NaN NaN 1980-02-01 0.013810 0.013684 0.006549 1980-03-01 0.053365 0.059318 0.061876 Percentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns. >>> df = pd.DataFrame({ ... '2016': [1769950, 30586265], ... '2015': [1500923, 40912316], ... '2014': [1371819, 41403351]}, ... index=['GOOG', 'APPL']) >>> df 2016 2015 2014 GOOG 1769950 1500923 1371819 APPL 30586265 40912316 41403351 >>> df.pct_change(axis='columns') 2016 2015 2014 GOOG NaN -0.151997 -0.086016 APPL NaN 0.337604 0.012002 \"\"\" axis = self . _get_axis_number ( kwargs . pop ( \"axis\" , self . _stat_axis_name )) if fill_method is None : data = self else : _data = self . fillna ( method = fill_method , axis = axis , limit = limit ) assert _data is not None # needed for mypy data = _data rs = data . div ( data . shift ( periods = periods , freq = freq , axis = axis , ** kwargs )) - 1 if freq is not None : # Shift method is implemented differently when freq is not None # We want to restore the original index rs = rs . loc [ ~ rs . index . duplicated ()] rs = rs . reindex_like ( data ) return rs pipe def pipe ( self , func , * args , ** kwargs ) Apply func(self, *args, **kwargs). Parameters func : function Function to apply to the Series/DataFrame. args , and kwargs are passed into func . Alternatively a (callable, data_keyword) tuple where data_keyword is a string indicating the keyword of callable that expects the Series/DataFrame. args : iterable, optional Positional arguments passed into func . kwargs : mapping, optional A dictionary of keyword arguments passed into func . Returns object : the return type of func . See Also DataFrame.apply : Apply a function along input axis of DataFrame. DataFrame.applymap : Apply a function elementwise on a whole DataFrame. Series.map : Apply a mapping correspondence on a :class: ~pandas.Series . Notes Use .pipe when chaining together functions that expect Series, DataFrames or GroupBy objects. Instead of writing func(g(h(df), arg1=a), arg2=b, arg3=c) # doctest: +SKIP You can write (df.pipe(h) ... .pipe(g, arg1=a) ... .pipe(func, arg2=b, arg3=c) ... ) # doctest: +SKIP If you have a function that takes the data as (say) the second argument, pass a tuple indicating which keyword expects the data. For example, suppose f takes its data as arg2 : (df.pipe(h) ... .pipe(g, arg1=a) ... .pipe((func, 'arg2'), arg1=a, arg3=c) ... ) # doctest: +SKIP View Source @ doc ( klass = _shared_doc_kwargs [ \"klass\" ]) def pipe ( self , func , * args , ** kwargs ): r \"\"\" Apply func(self, \\*args, \\*\\*kwargs). Parameters ---------- func : function Function to apply to the {klass}. ``args``, and ``kwargs`` are passed into ``func``. Alternatively a ``(callable, data_keyword)`` tuple where ``data_keyword`` is a string indicating the keyword of ``callable`` that expects the {klass}. args : iterable, optional Positional arguments passed into ``func``. kwargs : mapping, optional A dictionary of keyword arguments passed into ``func``. Returns ------- object : the return type of ``func``. See Also -------- DataFrame.apply : Apply a function along input axis of DataFrame. DataFrame.applymap : Apply a function elementwise on a whole DataFrame. Series.map : Apply a mapping correspondence on a :class:`~pandas.Series`. Notes ----- Use ``.pipe`` when chaining together functions that expect Series, DataFrames or GroupBy objects. Instead of writing >>> func(g(h(df), arg1=a), arg2=b, arg3=c) # doctest: +SKIP You can write >>> (df.pipe(h) ... .pipe(g, arg1=a) ... .pipe(func, arg2=b, arg3=c) ... ) # doctest: +SKIP If you have a function that takes the data as (say) the second argument, pass a tuple indicating which keyword expects the data. For example, suppose ``f`` takes its data as ``arg2``: >>> (df.pipe(h) ... .pipe(g, arg1=a) ... .pipe((func, 'arg2'), arg1=a, arg3=c) ... ) # doctest: +SKIP \"\"\" return com . pipe ( self , func , * args , ** kwargs ) pivot def pivot ( self , index = None , columns = None , values = None ) -> 'DataFrame' Return reshaped DataFrame organized by given index / column values. Reshape data (produce a \"pivot\" table) based on column values. Uses unique values from specified index / columns to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the :ref: User Guide <reshaping> for more on reshaping. Parameters index : str or object or a list of str, optional Column to use to make new frame's index. If None, uses existing index. .. versionchanged :: 1.1.0 Also accept list of index names. columns : str or object or a list of str Column to use to make new frame's columns. .. versionchanged :: 1.1.0 Also accept list of columns names. values : str, object or a list of the previous, optional Column(s) to use for populating new frame's values. If not specified, all remaining columns will be used and the result will have hierarchically indexed columns. .. versionchanged :: 0.23.0 Also accept list of column names. Returns DataFrame Returns reshaped DataFrame. Raises ValueError: When there are any index , columns combinations with multiple values. DataFrame.pivot_table when you need to aggregate. See Also DataFrame.pivot_table : Generalization of pivot that can handle duplicate values for one index/column pair. DataFrame.unstack : Pivot based on the index values instead of a column. Notes For finer-tuned control, see hierarchical indexing documentation along with the related stack/unstack methods. Examples df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two', ... 'two'], ... 'bar': ['A', 'B', 'C', 'A', 'B', 'C'], ... 'baz': [1, 2, 3, 4, 5, 6], ... 'zoo': ['x', 'y', 'z', 'q', 'w', 't']}) df foo bar baz zoo 0 one A 1 x 1 one B 2 y 2 one C 3 z 3 two A 4 q 4 two B 5 w 5 two C 6 t df.pivot(index='foo', columns='bar', values='baz') bar A B C foo one 1 2 3 two 4 5 6 df.pivot(index='foo', columns='bar')['baz'] bar A B C foo one 1 2 3 two 4 5 6 df.pivot(index='foo', columns='bar', values=['baz', 'zoo']) baz zoo bar A B C A B C foo one 1 2 3 x y z two 4 5 6 q w t You could also assign a list of column names or a list of index names. df = pd.DataFrame({ ... \"lev1\": [1, 1, 1, 2, 2, 2], ... \"lev2\": [1, 1, 2, 1, 1, 2], ... \"lev3\": [1, 2, 1, 2, 1, 2], ... \"lev4\": [1, 2, 3, 4, 5, 6], ... \"values\": [0, 1, 2, 3, 4, 5]}) df lev1 lev2 lev3 lev4 values 0 1 1 1 1 0 1 1 1 2 2 1 2 1 2 1 3 2 3 2 1 2 4 3 4 2 1 1 5 4 5 2 2 2 6 5 df.pivot(index=\"lev1\", columns=[\"lev2\", \"lev3\"],values=\"values\") lev2 1 2 lev3 1 2 1 2 lev1 1 0.0 1.0 2.0 NaN 2 4.0 3.0 NaN 5.0 df.pivot(index=[\"lev1\", \"lev2\"], columns=[\"lev3\"],values=\"values\") lev3 1 2 lev1 lev2 1 1 0.0 1.0 2 2.0 NaN 2 1 4.0 3.0 2 NaN 5.0 A ValueError is raised if there are any duplicates. df = pd.DataFrame({\"foo\": ['one', 'one', 'two', 'two'], ... \"bar\": ['A', 'A', 'B', 'C'], ... \"baz\": [1, 2, 3, 4]}) df foo bar baz 0 one A 1 1 one A 2 2 two B 3 3 two C 4 Notice that the first two rows are the same for our index and columns arguments. df.pivot(index='foo', columns='bar', values='baz') Traceback (most recent call last): ... ValueError: Index contains duplicate entries, cannot reshape View Source @Substitution ( \"\" ) @Appender ( _shared_docs [ \"pivot\" ]) def pivot ( self , index = None , columns = None , values = None ) -> \"DataFrame\" : from pandas.core.reshape.pivot import pivot return pivot ( self , index = index , columns = columns , values = values ) pivot_table def pivot_table ( self , values = None , index = None , columns = None , aggfunc = 'mean' , fill_value = None , margins = False , dropna = True , margins_name = 'All' , observed = False ) -> 'DataFrame' Create a spreadsheet-style pivot table as a DataFrame. The levels in the pivot table will be stored in MultiIndex objects (hierarchical indexes) on the index and columns of the result DataFrame. Parameters values : column to aggregate, optional index : column, Grouper, array, or list of the previous If an array is passed, it must be the same length as the data. The list can contain any of the other types (except list). Keys to group by on the pivot table index. If an array is passed, it is being used as the same manner as column values. columns : column, Grouper, array, or list of the previous If an array is passed, it must be the same length as the data. The list can contain any of the other types (except list). Keys to group by on the pivot table column. If an array is passed, it is being used as the same manner as column values. aggfunc : function, list of functions, dict, default numpy.mean If list of functions passed, the resulting pivot table will have hierarchical columns whose top level are the function names (inferred from the function objects themselves) If dict is passed, the key is column to aggregate and value is function or list of functions. fill_value : scalar, default None Value to replace missing values with (in the resulting pivot table, after aggregation). margins : bool, default False Add all row / columns (e.g. for subtotal / grand totals). dropna : bool, default True Do not include columns whose entries are all NaN. margins_name : str, default 'All' Name of the row / column that will contain the totals when margins is True. observed : bool, default False This only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. .. versionchanged :: 0.25.0 Returns DataFrame An Excel style pivot table. See Also DataFrame.pivot : Pivot without aggregation that can handle non-numeric data. Examples df = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\", ... \"bar\", \"bar\", \"bar\", \"bar\"], ... \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\", ... \"one\", \"one\", \"two\", \"two\"], ... \"C\": [\"small\", \"large\", \"large\", \"small\", ... \"small\", \"large\", \"small\", \"small\", ... \"large\"], ... \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7], ... \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]}) df A B C D E 0 foo one small 1 2 1 foo one large 2 4 2 foo one large 2 5 3 foo two small 3 5 4 foo two small 3 6 5 bar one large 4 6 6 bar one small 5 8 7 bar two small 6 9 8 bar two large 7 9 This first example aggregates values by taking the sum. table = pd.pivot_table(df, values='D', index=['A', 'B'], ... columns=['C'], aggfunc=np.sum) table C large small A B bar one 4.0 5.0 two 7.0 6.0 foo one 4.0 1.0 two NaN 6.0 We can also fill missing values using the fill_value parameter. table = pd.pivot_table(df, values='D', index=['A', 'B'], ... columns=['C'], aggfunc=np.sum, fill_value=0) table C large small A B bar one 4 5 two 7 6 foo one 4 1 two 0 6 The next example aggregates by taking the mean across multiple columns. table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'], ... aggfunc={'D': np.mean, ... 'E': np.mean}) table D E A C bar large 5.500000 7.500000 small 5.500000 8.500000 foo large 2.000000 4.500000 small 2.333333 4.333333 We can also calculate multiple types of aggregations for any given value column. table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'], ... aggfunc={'D': np.mean, ... 'E': [min, max, np.mean]}) table D E mean max mean min A C bar large 5.500000 9.0 7.500000 6.0 small 5.500000 9.0 8.500000 8.0 foo large 2.000000 5.0 4.500000 4.0 small 2.333333 6.0 4.333333 2.0 View Source @Substitution ( \"\" ) @Appender ( _shared_docs [ \"pivot_table\" ]) def pivot_table ( self , values = None , index = None , columns = None , aggfunc = \"mean\" , fill_value = None , margins = False , dropna = True , margins_name = \"All\" , observed = False , ) -> \"DataFrame\" : from pandas.core.reshape.pivot import pivot_table return pivot_table ( self , values = values , index = index , columns = columns , aggfunc = aggfunc , fill_value = fill_value , margins = margins , dropna = dropna , margins_name = margins_name , observed = observed , ) pop def pop ( self , item : Union [ Hashable , NoneType ] ) -> pandas . core . series . Series Return item and drop from frame. Raise KeyError if not found. Parameters item : label Label of column to be popped. Returns Series Examples df = pd.DataFrame([('falcon', 'bird', 389.0), ... ('parrot', 'bird', 24.0), ... ('lion', 'mammal', 80.5), ... ('monkey', 'mammal', np.nan)], ... columns=('name', 'class', 'max_speed')) df name class max_speed 0 falcon bird 389.0 1 parrot bird 24.0 2 lion mammal 80.5 3 monkey mammal NaN df.pop('class') 0 bird 1 bird 2 mammal 3 mammal Name: class, dtype: object df name max_speed 0 falcon 389.0 1 parrot 24.0 2 lion 80.5 3 monkey NaN View Source def pop ( self , item : Label ) -> Series : \"\"\" Return item and drop from frame. Raise KeyError if not found. Parameters ---------- item : label Label of column to be popped. Returns ------- Series Examples -------- >>> df = pd.DataFrame([('falcon', 'bird', 389.0), ... ('parrot', 'bird', 24.0), ... ('lion', 'mammal', 80.5), ... ('monkey', 'mammal', np.nan)], ... columns=('name', 'class', 'max_speed')) >>> df name class max_speed 0 falcon bird 389.0 1 parrot bird 24.0 2 lion mammal 80.5 3 monkey mammal NaN >>> df.pop('class') 0 bird 1 bird 2 mammal 3 mammal Name: class, dtype: object >>> df name max_speed 0 falcon 389.0 1 parrot 24.0 2 lion 80.5 3 monkey NaN \"\"\" return super (). pop ( item = item ) pow def pow ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Exponential power of dataframe and other, element-wise (binary operator pow ). Equivalent to dataframe ** other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rpow . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) prod def prod ( self , axis = None , skipna = None , level = None , numeric_only = None , min_count = 0 , ** kwargs ) Return the product of the values for the requested axis. Parameters axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. min_count : int, default 0 The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. .. versionadded :: 0.22.0 Added with the default being 0. This means the sum of an all-NA or empty Series is 0, and the product of an all-NA or empty Series is 1. **kwargs Additional keyword arguments to be passed to the function. Returns Series or DataFrame (if level specified) Examples By default, the product of an empty or all-NA Series is 1 pd.Series([]).prod() 1.0 This can be controlled with the min_count parameter pd.Series([]).prod(min_count=1) nan Thanks to the skipna parameter, min_count handles all-NA and empty series identically. pd.Series([np.nan]).prod() 1.0 pd.Series([np.nan]).prod(min_count=1) nan View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = _min_count_stub , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , min_count = 0 , ** kwargs , ) : if name == \"sum\" : nv . validate_sum ( tuple (), kwargs ) elif name == \"prod\" : nv . validate_prod ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna , min_count = min_count ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only , min_count = min_count , ) product def product ( self , axis = None , skipna = None , level = None , numeric_only = None , min_count = 0 , ** kwargs ) Return the product of the values for the requested axis. Parameters axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. min_count : int, default 0 The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. .. versionadded :: 0.22.0 Added with the default being 0. This means the sum of an all-NA or empty Series is 0, and the product of an all-NA or empty Series is 1. **kwargs Additional keyword arguments to be passed to the function. Returns Series or DataFrame (if level specified) Examples By default, the product of an empty or all-NA Series is 1 pd.Series([]).prod() 1.0 This can be controlled with the min_count parameter pd.Series([]).prod(min_count=1) nan Thanks to the skipna parameter, min_count handles all-NA and empty series identically. pd.Series([np.nan]).prod() 1.0 pd.Series([np.nan]).prod(min_count=1) nan View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = _min_count_stub , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , min_count = 0 , ** kwargs , ) : if name == \"sum\" : nv . validate_sum ( tuple (), kwargs ) elif name == \"prod\" : nv . validate_prod ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna , min_count = min_count ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only , min_count = min_count , ) quantile def quantile ( self , q = 0.5 , axis = 0 , numeric_only = True , interpolation = 'linear' ) Return values at the given quantile over requested axis. Parameters q : float or array-like, default 0.5 (50% quantile) Value between 0 <= q <= 1, the quantile(s) to compute. axis : {0, 1, 'index', 'columns'}, default 0 Equals 0 or 'index' for row-wise, 1 or 'columns' for column-wise. numeric_only : bool, default True If False, the quantile of datetime and timedelta data will be computed as well. interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'} This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points i and j : * linear : `i + (j - i) * fraction` , where `fraction` is the fractional part of the index surrounded by `i` and `j` . * lower : `i` . * higher : `j` . * nearest : `i` or `j` whichever is nearest . * midpoint : ( `i` + `j` ) / 2 . Returns Series or DataFrame If `` q `` is an array , a DataFrame will be returned where the index is `` q `` , the columns are the columns of self , and the values are the quantiles . If `` q `` is a float , a Series will be returned where the index is the columns of self and the values are the quantiles . See Also core.window.Rolling.quantile: Rolling quantile. numpy.percentile: Numpy function to compute the percentile. Examples df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]), ... columns=['a', 'b']) df.quantile(.1) a 1.3 b 3.7 Name: 0.1, dtype: float64 df.quantile([.1, .5]) a b 0.1 1.3 3.7 0.5 2.5 55.0 Specifying numeric_only=False will also compute the quantile of datetime and timedelta data. df = pd.DataFrame({'A': [1, 2], ... 'B': [pd.Timestamp('2010'), ... pd.Timestamp('2011')], ... 'C': [pd.Timedelta('1 days'), ... pd.Timedelta('2 days')]}) df.quantile(0.5, numeric_only=False) A 1.5 B 2010-07-02 12:00:00 C 1 days 12:00:00 Name: 0.5, dtype: object View Source def quantile ( self , q = 0 . 5 , axis = 0 , numeric_only = True , interpolation = \"linear\" ): \"\"\" Return values at the given quantile over requested axis. Parameters ---------- q : float or array-like, default 0.5 (50% quantile) Value between 0 <= q <= 1, the quantile(s) to compute. axis : {0, 1, 'index', 'columns'}, default 0 Equals 0 or 'index' for row-wise, 1 or 'columns' for column-wise. numeric_only : bool, default True If False, the quantile of datetime and timedelta data will be computed as well. interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'} This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points `i` and `j`: * linear: `i + (j - i) * fraction`, where `fraction` is the fractional part of the index surrounded by `i` and `j`. * lower: `i`. * higher: `j`. * nearest: `i` or `j` whichever is nearest. * midpoint: (`i` + `j`) / 2. Returns ------- Series or DataFrame If ``q`` is an array, a DataFrame will be returned where the index is ``q``, the columns are the columns of self, and the values are the quantiles. If ``q`` is a float, a Series will be returned where the index is the columns of self and the values are the quantiles. See Also -------- core.window.Rolling.quantile: Rolling quantile. numpy.percentile: Numpy function to compute the percentile. Examples -------- >>> df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]), ... columns=['a', 'b']) >>> df.quantile(.1) a 1.3 b 3.7 Name: 0.1, dtype: float64 >>> df.quantile([.1, .5]) a b 0.1 1.3 3.7 0.5 2.5 55.0 Specifying `numeric_only=False` will also compute the quantile of datetime and timedelta data. >>> df = pd.DataFrame({'A': [1, 2], ... 'B': [pd.Timestamp('2010'), ... pd.Timestamp('2011')], ... 'C': [pd.Timedelta('1 days'), ... pd.Timedelta('2 days')]}) >>> df.quantile(0.5, numeric_only=False) A 1.5 B 2010-07-02 12:00:00 C 1 days 12:00:00 Name: 0.5, dtype: object \"\"\" validate_percentile ( q ) data = self . _get_numeric_data () if numeric_only else self axis = self . _get_axis_number ( axis ) is_transposed = axis == 1 if is_transposed : data = data . T if len ( data . columns ) == 0 : # GH#23925 _get_numeric_data may have dropped all columns cols = Index ([], name = self . columns . name ) if is_list_like ( q ): return self . _constructor ([], index = q , columns = cols ) return self . _constructor_sliced ([], index = cols , name = q , dtype = np . float64 ) result = data . _mgr . quantile ( qs = q , axis = 1 , interpolation = interpolation , transposed = is_transposed ) if result . ndim == 2 : result = self . _constructor ( result ) else : result = self . _constructor_sliced ( result , name = q ) if is_transposed : result = result . T return result query def query ( self , expr , inplace = False , ** kwargs ) Query the columns of a DataFrame with a boolean expression. Parameters expr : str The query string to evaluate. You can refer to variables in the environment by prefixing them with an '@' character like `` @ a + b `` . You can refer to column names that contain spaces or operators by surrounding them in backticks . This way you can also escape names that start with a digit , or those that are a Python keyword . Basically when it is not valid Python identifier . See notes down for more details . For example , if one of your columns is called `` a a `` and you want to sum it with `` b `` , your query should be ```a a` + b `` . .. versionadded :: 0 . 25 . 0 Backtick quoting introduced . .. versionadded :: 1 . 0 . 0 Expanding functionality of backtick quoting for more than only spaces . inplace : bool Whether the query should modify the data in place or return a modified copy. **kwargs See the documentation for :func: eval for complete details on the keyword arguments accepted by :meth: DataFrame.query . Returns DataFrame DataFrame resulting from the provided query expression. See Also eval : Evaluate a string describing operations on DataFrame columns. DataFrame.eval : Evaluate a string describing operations on DataFrame columns. Notes The result of the evaluation of this expression is first passed to :attr: DataFrame.loc and if that fails because of a multidimensional key (e.g., a DataFrame) then the result will be passed to :meth: DataFrame.__getitem__ . This method uses the top-level :func: eval function to evaluate the passed query. The :meth: ~pandas.DataFrame.query method uses a slightly modified Python syntax by default. For example, the & and | (bitwise) operators have the precedence of their boolean cousins, :keyword: and and :keyword: or . This is syntactically valid Python, however the semantics are different. You can change the semantics of the expression by passing the keyword argument parser='python' . This enforces the same semantics as evaluation in Python space. Likewise, you can pass engine='python' to evaluate an expression using Python itself as a backend. This is not recommended as it is inefficient compared to using numexpr as the engine. The :attr: DataFrame.index and :attr: DataFrame.columns attributes of the :class: ~pandas.DataFrame instance are placed in the query namespace by default, which allows you to treat both the index and columns of the frame as a column in the frame. The identifier index is used for the frame index; you can also use the name of the index to identify it in a query. Please note that Python keywords may not be used as identifiers. For further details and examples see the query documentation in :ref: indexing <indexing.query> . Backtick quoted variables Backtick quoted variables are parsed as literal Python code and are converted internally to a Python valid identifier. This can lead to the following problems. During parsing a number of disallowed characters inside the backtick quoted string are replaced by strings that are allowed as a Python identifier. These characters include all operators in Python, the space character, the question mark, the exclamation mark, the dollar sign, and the euro sign. For other characters that fall outside the ASCII range (U+0001..U+007F) and those that are not further specified in PEP 3131, the query parser will raise an error. This excludes whitespace different than the space character, but also the hashtag (as it is used for comments) and the backtick itself (backtick can also not be escaped). In a special case, quotes that make a pair around a backtick can confuse the parser. For example, it's` > `that's will raise an error, as it forms a quoted string ( 's > `that' ) with a backtick inside. See also the Python documentation about lexical analysis (https://docs.python.org/3/reference/lexical_analysis.html) in combination with the source code in :mod: pandas.core.computation.parsing . Examples df = pd.DataFrame({'A': range(1, 6), ... 'B': range(10, 0, -2), ... 'C C': range(10, 5, -1)}) df A B C C 0 1 10 10 1 2 8 9 2 3 6 8 3 4 4 7 4 5 2 6 df.query('A > B') A B C C 4 5 2 6 The previous expression is equivalent to df[df.A > df.B] A B C C 4 5 2 6 For columns with spaces in their name, you can use backtick quoting. df.query('B == C C ') A B C C 0 1 10 10 The previous expression is equivalent to df[df.B == df['C C']] A B C C 0 1 10 10 View Source def query ( self , expr , inplace = False , ** kwargs ): \"\"\" Query the columns of a DataFrame with a boolean expression. Parameters ---------- expr : str The query string to evaluate. You can refer to variables in the environment by prefixing them with an '@' character like ``@a + b``. You can refer to column names that contain spaces or operators by surrounding them in backticks. This way you can also escape names that start with a digit, or those that are a Python keyword. Basically when it is not valid Python identifier. See notes down for more details. For example, if one of your columns is called ``a a`` and you want to sum it with ``b``, your query should be ```a a` + b``. .. versionadded:: 0.25.0 Backtick quoting introduced. .. versionadded:: 1.0.0 Expanding functionality of backtick quoting for more than only spaces. inplace : bool Whether the query should modify the data in place or return a modified copy. **kwargs See the documentation for :func:`eval` for complete details on the keyword arguments accepted by :meth:`DataFrame.query`. Returns ------- DataFrame DataFrame resulting from the provided query expression. See Also -------- eval : Evaluate a string describing operations on DataFrame columns. DataFrame.eval : Evaluate a string describing operations on DataFrame columns. Notes ----- The result of the evaluation of this expression is first passed to :attr:`DataFrame.loc` and if that fails because of a multidimensional key (e.g., a DataFrame) then the result will be passed to :meth:`DataFrame.__getitem__`. This method uses the top-level :func:`eval` function to evaluate the passed query. The :meth:`~pandas.DataFrame.query` method uses a slightly modified Python syntax by default. For example, the ``&`` and ``|`` (bitwise) operators have the precedence of their boolean cousins, :keyword:`and` and :keyword:`or`. This *is* syntactically valid Python, however the semantics are different. You can change the semantics of the expression by passing the keyword argument ``parser='python'``. This enforces the same semantics as evaluation in Python space. Likewise, you can pass ``engine='python'`` to evaluate an expression using Python itself as a backend. This is not recommended as it is inefficient compared to using ``numexpr`` as the engine. The :attr:`DataFrame.index` and :attr:`DataFrame.columns` attributes of the :class:`~pandas.DataFrame` instance are placed in the query namespace by default, which allows you to treat both the index and columns of the frame as a column in the frame. The identifier ``index`` is used for the frame index; you can also use the name of the index to identify it in a query. Please note that Python keywords may not be used as identifiers. For further details and examples see the ``query`` documentation in :ref:`indexing <indexing.query>`. *Backtick quoted variables* Backtick quoted variables are parsed as literal Python code and are converted internally to a Python valid identifier. This can lead to the following problems. During parsing a number of disallowed characters inside the backtick quoted string are replaced by strings that are allowed as a Python identifier. These characters include all operators in Python, the space character, the question mark, the exclamation mark, the dollar sign, and the euro sign. For other characters that fall outside the ASCII range (U+0001..U+007F) and those that are not further specified in PEP 3131, the query parser will raise an error. This excludes whitespace different than the space character, but also the hashtag (as it is used for comments) and the backtick itself (backtick can also not be escaped). In a special case, quotes that make a pair around a backtick can confuse the parser. For example, ```it's` > `that's``` will raise an error, as it forms a quoted string (``'s > `that'``) with a backtick inside. See also the Python documentation about lexical analysis (https://docs.python.org/3/reference/lexical_analysis.html) in combination with the source code in :mod:`pandas.core.computation.parsing`. Examples -------- >>> df = pd.DataFrame({'A': range(1, 6), ... 'B': range(10, 0, -2), ... 'C C': range(10, 5, -1)}) >>> df A B C C 0 1 10 10 1 2 8 9 2 3 6 8 3 4 4 7 4 5 2 6 >>> df.query('A > B') A B C C 4 5 2 6 The previous expression is equivalent to >>> df[df.A > df.B] A B C C 4 5 2 6 For columns with spaces in their name, you can use backtick quoting. >>> df.query('B == `C C`') A B C C 0 1 10 10 The previous expression is equivalent to >>> df[df.B == df['C C']] A B C C 0 1 10 10 \"\"\" inplace = validate_bool_kwarg ( inplace , \"inplace\" ) if not isinstance ( expr , str ): msg = f \"expr must be a string to be evaluated, {type(expr)} given\" raise ValueError ( msg ) kwargs [ \"level\" ] = kwargs . pop ( \"level\" , 0 ) + 1 kwargs [ \"target\" ] = None res = self . eval ( expr , ** kwargs ) try : result = self . loc [ res ] except ValueError : # when res is multi-dimensional loc raises, but this is sometimes a # valid query result = self [ res ] if inplace : self . _update_inplace ( result ) else : return result radd def radd ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Addition of dataframe and other, element-wise (binary operator radd ). Equivalent to other + dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, add . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) rank def rank ( self : ~ FrameOrSeries , axis = 0 , method : str = 'average' , numeric_only : Union [ bool , NoneType ] = None , na_option : str = 'keep' , ascending : bool = True , pct : bool = False ) -> ~ FrameOrSeries Compute numerical data ranks (1 through n) along axis. By default, equal values are assigned a rank that is the average of the ranks of those values. Parameters axis : {0 or 'index', 1 or 'columns'}, default 0 Index to direct ranking. method : {'average', 'min', 'max', 'first', 'dense'}, default 'average' How to rank the group of records that have the same value (i.e. ties): * average: average rank of the group * min: lowest rank in the group * max: highest rank in the group * first: ranks assigned in order they appear in the array * dense: like 'min', but rank always increases by 1 between groups. numeric_only : bool, optional For DataFrame objects, rank only numeric columns if set to True. na_option : {'keep', 'top', 'bottom'}, default 'keep' How to rank NaN values: * keep: assign NaN rank to NaN values * top: assign smallest rank to NaN values if ascending * bottom: assign highest rank to NaN values if ascending. ascending : bool, default True Whether or not the elements should be ranked in ascending order. pct : bool, default False Whether or not to display the returned rankings in percentile form. Returns same type as caller Return a Series or DataFrame with data ranks as values. See Also core.groupby.GroupBy.rank : Rank of values within each group. Examples df = pd.DataFrame(data={'Animal': ['cat', 'penguin', 'dog', ... 'spider', 'snake'], ... 'Number_legs': [4, 2, 4, 8, np.nan]}) df Animal Number_legs 0 cat 4.0 1 penguin 2.0 2 dog 4.0 3 spider 8.0 4 snake NaN The following example shows how the method behaves with the above parameters: default_rank: this is the default behaviour obtained without using any parameter. max_rank: setting method = 'max' the records that have the same values are ranked using the highest rank (e.g.: since 'cat' and 'dog' are both in the 2nd and 3rd position, rank 3 is assigned.) NA_bottom: choosing na_option = 'bottom' , if there are records with NaN values they are placed at the bottom of the ranking. pct_rank: when setting pct = True , the ranking is expressed as percentile rank. df['default_rank'] = df['Number_legs'].rank() df['max_rank'] = df['Number_legs'].rank(method='max') df['NA_bottom'] = df['Number_legs'].rank(na_option='bottom') df['pct_rank'] = df['Number_legs'].rank(pct=True) df Animal Number_legs default_rank max_rank NA_bottom pct_rank 0 cat 4.0 2.5 3.0 2.5 0.625 1 penguin 2.0 1.0 1.0 1.0 0.250 2 dog 4.0 2.5 3.0 2.5 0.625 3 spider 8.0 4.0 4.0 4.0 1.000 4 snake NaN NaN NaN 5.0 NaN View Source def rank ( self : FrameOrSeries , axis = 0 , method : str = \"average\" , numeric_only : Optional [ bool_t ] = None , na_option : str = \"keep\" , ascending : bool_t = True , pct : bool_t = False , ) -> FrameOrSeries : \"\"\" Compute numerical data ranks (1 through n) along axis. By default, equal values are assigned a rank that is the average of the ranks of those values. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 Index to direct ranking. method : {'average', 'min', 'max', 'first', 'dense'}, default 'average' How to rank the group of records that have the same value (i.e. ties): * average: average rank of the group * min: lowest rank in the group * max: highest rank in the group * first: ranks assigned in order they appear in the array * dense: like 'min', but rank always increases by 1 between groups. numeric_only : bool, optional For DataFrame objects, rank only numeric columns if set to True. na_option : {'keep', 'top', 'bottom'}, default 'keep' How to rank NaN values: * keep: assign NaN rank to NaN values * top: assign smallest rank to NaN values if ascending * bottom: assign highest rank to NaN values if ascending. ascending : bool, default True Whether or not the elements should be ranked in ascending order. pct : bool, default False Whether or not to display the returned rankings in percentile form. Returns ------- same type as caller Return a Series or DataFrame with data ranks as values. See Also -------- core.groupby.GroupBy.rank : Rank of values within each group. Examples -------- >>> df = pd.DataFrame(data={'Animal': ['cat', 'penguin', 'dog', ... 'spider', 'snake'], ... 'Number_legs': [4, 2, 4, 8, np.nan]}) >>> df Animal Number_legs 0 cat 4.0 1 penguin 2.0 2 dog 4.0 3 spider 8.0 4 snake NaN The following example shows how the method behaves with the above parameters: * default_rank: this is the default behaviour obtained without using any parameter. * max_rank: setting ``method = 'max'`` the records that have the same values are ranked using the highest rank (e.g.: since 'cat' and 'dog' are both in the 2nd and 3rd position, rank 3 is assigned.) * NA_bottom: choosing ``na_option = 'bottom'``, if there are records with NaN values they are placed at the bottom of the ranking. * pct_rank: when setting ``pct = True``, the ranking is expressed as percentile rank. >>> df['default_rank'] = df['Number_legs'].rank() >>> df['max_rank'] = df['Number_legs'].rank(method='max') >>> df['NA_bottom'] = df['Number_legs'].rank(na_option='bottom') >>> df['pct_rank'] = df['Number_legs'].rank(pct=True) >>> df Animal Number_legs default_rank max_rank NA_bottom pct_rank 0 cat 4.0 2.5 3.0 2.5 0.625 1 penguin 2.0 1.0 1.0 1.0 0.250 2 dog 4.0 2.5 3.0 2.5 0.625 3 spider 8.0 4.0 4.0 4.0 1.000 4 snake NaN NaN NaN 5.0 NaN \"\"\" axis = self . _get_axis_number ( axis ) if na_option not in { \"keep\" , \"top\" , \"bottom\" }: msg = \"na_option must be one of 'keep', 'top', or 'bottom'\" raise ValueError ( msg ) def ranker ( data ) : ranks = algos . rank ( data . values , axis = axis , method = method , ascending = ascending , na_option = na_option , pct = pct , ) ranks = self . _constructor ( ranks , ** data . _construct_axes_dict ()) return ranks . __finalize__ ( self , method = \"rank\" ) # if numeric_only is None , and we can ' t get anything , we try with # numeric_only = True if numeric_only is None : try : return ranker ( self ) except TypeError : numeric_only = True if numeric_only : data = self . _get_numeric_data () else : data = self return ranker ( data ) rdiv def rdiv ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Floating division of dataframe and other, element-wise (binary operator rtruediv ). Equivalent to other / dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, truediv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) reindex def reindex ( self , labels = None , index = None , columns = None , axis = None , method = None , copy = True , level = None , fill_value = nan , limit = None , tolerance = None ) Conform Series/DataFrame to new index with optional filling logic. Places NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False . Parameters keywords for axes : array-like, optional New labels / index to conform to, should be specified using keywords. Preferably an Index object to avoid duplicating data. method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'} Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index. * None (default): don't fill gaps * pad / ffill: Propagate last valid observation forward to next valid. * backfill / bfill: Use next valid observation to fill gap. * nearest: Use nearest valid observations to fill gap. copy : bool, default True Return a new object, even if the passed indexes are the same. level : int or name Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : scalar, default np.NaN Value to use for missing values. Defaults to NaN, but can be any \"compatible\" value. limit : int, default None Maximum number of consecutive elements to forward or backward fill. tolerance : optional Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation abs(index[indexer] - target) <= tolerance . Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index's type. Returns Series/DataFrame with changed index. See Also DataFrame.set_index : Set row labels. DataFrame.reset_index : Remove row labels or move them to new columns. DataFrame.reindex_like : Change to same indices as other DataFrame. Examples DataFrame.reindex supports two calling conventions (index=index_labels, columns=column_labels, ...) (labels, axis={'index', 'columns'}, ...) We highly recommend using keyword arguments to clarify your intent. Create a dataframe with some fictional data. index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror'] df = pd.DataFrame({'http_status': [200, 200, 404, 404, 301], ... 'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]}, ... index=index) df http_status response_time Firefox 200 0.04 Chrome 200 0.02 Safari 404 0.07 IE10 404 0.08 Konqueror 301 1.00 Create a new index and reindex the dataframe. By default values in the new index that do not have corresponding records in the dataframe are assigned NaN . new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10', ... 'Chrome'] df.reindex(new_index) http_status response_time Safari 404.0 0.07 Iceweasel NaN NaN Comodo Dragon NaN NaN IE10 404.0 0.08 Chrome 200.0 0.02 We can fill in the missing values by passing a value to the keyword fill_value . Because the index is not monotonically increasing or decreasing, we cannot use arguments to the keyword method to fill the NaN values. df.reindex(new_index, fill_value=0) http_status response_time Safari 404 0.07 Iceweasel 0 0.00 Comodo Dragon 0 0.00 IE10 404 0.08 Chrome 200 0.02 df.reindex(new_index, fill_value='missing') http_status response_time Safari 404 0.07 Iceweasel missing missing Comodo Dragon missing missing IE10 404 0.08 Chrome 200 0.02 We can also reindex the columns. df.reindex(columns=['http_status', 'user_agent']) http_status user_agent Firefox 200 NaN Chrome 200 NaN Safari 404 NaN IE10 404 NaN Konqueror 301 NaN Or we can use \"axis-style\" keyword arguments df.reindex(['http_status', 'user_agent'], axis=\"columns\") http_status user_agent Firefox 200 NaN Chrome 200 NaN Safari 404 NaN IE10 404 NaN Konqueror 301 NaN To further illustrate the filling functionality in reindex , we will create a dataframe with a monotonically increasing index (for example, a sequence of dates). date_index = pd.date_range('1/1/2010', periods=6, freq='D') df2 = pd.DataFrame({\"prices\": [100, 101, np.nan, 100, 89, 88]}, ... index=date_index) df2 prices 2010-01-01 100.0 2010-01-02 101.0 2010-01-03 NaN 2010-01-04 100.0 2010-01-05 89.0 2010-01-06 88.0 Suppose we decide to expand the dataframe to cover a wider date range. date_index2 = pd.date_range('12/29/2009', periods=10, freq='D') df2.reindex(date_index2) prices 2009-12-29 NaN 2009-12-30 NaN 2009-12-31 NaN 2010-01-01 100.0 2010-01-02 101.0 2010-01-03 NaN 2010-01-04 100.0 2010-01-05 89.0 2010-01-06 88.0 2010-01-07 NaN The index entries that did not have a value in the original data frame (for example, '2009-12-29') are by default filled with NaN . If desired, we can fill in the missing values using one of several options. For example, to back-propagate the last valid value to fill the NaN values, pass bfill as an argument to the method keyword. df2.reindex(date_index2, method='bfill') prices 2009-12-29 100.0 2009-12-30 100.0 2009-12-31 100.0 2010-01-01 100.0 2010-01-02 101.0 2010-01-03 NaN 2010-01-04 100.0 2010-01-05 89.0 2010-01-06 88.0 2010-01-07 NaN Please note that the NaN value present in the original dataframe (at index value 2010-01-03) will not be filled by any of the value propagation schemes. This is because filling while reindexing does not look at dataframe values, but only compares the original and desired indexes. If you do want to fill in the NaN values present in the original dataframe, use the fillna() method. See the :ref: user guide <basics.reindexing> for more. View Source @ Substitution ( ** _shared_doc_kwargs ) @ Appender ( NDFrame . reindex . __doc__ ) @ rewrite_axis_style_signature ( \"labels\" , [ ( \"method\" , None ), ( \"copy\" , True ), ( \"level\" , None ), ( \"fill_value\" , np . nan ), ( \"limit\" , None ), ( \"tolerance\" , None ), ], ) def reindex ( self , * args , ** kwargs ) -> \"DataFrame\" : axes = validate_axis_style_args ( self , args , kwargs , \"labels\" , \"reindex\" ) kwargs . update ( axes ) # Pop these, since the values are in `kwargs` under different names kwargs . pop ( \"axis\" , None ) kwargs . pop ( \"labels\" , None ) return super (). reindex ( ** kwargs ) reindex_like def reindex_like ( self : ~ FrameOrSeries , other , method : Union [ str , NoneType ] = None , copy : bool = True , limit = None , tolerance = None ) -> ~ FrameOrSeries Return an object with matching indices as other object. Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False. Parameters other : Object of the same data type Its row and column indices are used to define the new indices of this object. method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'} Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index. * None (default): don't fill gaps * pad / ffill: propagate last valid observation forward to next valid * backfill / bfill: use next valid observation to fill gap * nearest: use nearest valid observations to fill gap. copy : bool, default True Return a new object, even if the passed indexes are the same. limit : int, default None Maximum number of consecutive labels to fill for inexact matches. tolerance : optional Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation abs(index[indexer] - target) <= tolerance . Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index's type. Returns Series or DataFrame Same type as caller, but with changed indices on each axis. See Also DataFrame.set_index : Set row labels. DataFrame.reset_index : Remove row labels or move them to new columns. DataFrame.reindex : Change to new indices or expand indices. Notes Same as calling .reindex(index=other.index, columns=other.columns,...) . Examples df1 = pd.DataFrame([[24.3, 75.7, 'high'], ... [31, 87.8, 'high'], ... [22, 71.6, 'medium'], ... [35, 95, 'medium']], ... columns=['temp_celsius', 'temp_fahrenheit', ... 'windspeed'], ... index=pd.date_range(start='2014-02-12', ... end='2014-02-15', freq='D')) df1 temp_celsius temp_fahrenheit windspeed 2014-02-12 24.3 75.7 high 2014-02-13 31.0 87.8 high 2014-02-14 22.0 71.6 medium 2014-02-15 35.0 95.0 medium df2 = pd.DataFrame([[28, 'low'], ... [30, 'low'], ... [35.1, 'medium']], ... columns=['temp_celsius', 'windspeed'], ... index=pd.DatetimeIndex(['2014-02-12', '2014-02-13', ... '2014-02-15'])) df2 temp_celsius windspeed 2014-02-12 28.0 low 2014-02-13 30.0 low 2014-02-15 35.1 medium df2.reindex_like(df1) temp_celsius temp_fahrenheit windspeed 2014-02-12 28.0 NaN low 2014-02-13 30.0 NaN low 2014-02-14 NaN NaN NaN 2014-02-15 35.1 NaN medium View Source def reindex_like ( self : FrameOrSeries , other , method : Optional [ str ] = None , copy : bool_t = True , limit = None , tolerance = None , ) -> FrameOrSeries : \"\"\" Return an object with matching indices as other object. Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False. Parameters ---------- other : Object of the same data type Its row and column indices are used to define the new indices of this object. method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'} Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index. * None (default): don't fill gaps * pad / ffill: propagate last valid observation forward to next valid * backfill / bfill: use next valid observation to fill gap * nearest: use nearest valid observations to fill gap. copy : bool, default True Return a new object, even if the passed indexes are the same. limit : int, default None Maximum number of consecutive labels to fill for inexact matches. tolerance : optional Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation ``abs(index[indexer] - target) <= tolerance``. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index's type. Returns ------- Series or DataFrame Same type as caller, but with changed indices on each axis. See Also -------- DataFrame.set_index : Set row labels. DataFrame.reset_index : Remove row labels or move them to new columns. DataFrame.reindex : Change to new indices or expand indices. Notes ----- Same as calling ``.reindex(index=other.index, columns=other.columns,...)``. Examples -------- >>> df1 = pd.DataFrame([[24.3, 75.7, 'high'], ... [31, 87.8, 'high'], ... [22, 71.6, 'medium'], ... [35, 95, 'medium']], ... columns=['temp_celsius', 'temp_fahrenheit', ... 'windspeed'], ... index=pd.date_range(start='2014-02-12', ... end='2014-02-15', freq='D')) >>> df1 temp_celsius temp_fahrenheit windspeed 2014-02-12 24.3 75.7 high 2014-02-13 31.0 87.8 high 2014-02-14 22.0 71.6 medium 2014-02-15 35.0 95.0 medium >>> df2 = pd.DataFrame([[28, 'low'], ... [30, 'low'], ... [35.1, 'medium']], ... columns=['temp_celsius', 'windspeed'], ... index=pd.DatetimeIndex(['2014-02-12', '2014-02-13', ... '2014-02-15'])) >>> df2 temp_celsius windspeed 2014-02-12 28.0 low 2014-02-13 30.0 low 2014-02-15 35.1 medium >>> df2.reindex_like(df1) temp_celsius temp_fahrenheit windspeed 2014-02-12 28.0 NaN low 2014-02-13 30.0 NaN low 2014-02-14 NaN NaN NaN 2014-02-15 35.1 NaN medium \"\"\" d = other . _construct_axes_dict ( axes = self . _AXIS_ORDERS , method = method , copy = copy , limit = limit , tolerance = tolerance , ) return self . reindex ( ** d ) rename def rename ( self , mapper = None , index = None , columns = None , axis = None , copy = True , inplace = False , level = None , errors = 'ignore' ) Alter axes labels. Function / dict values must be unique (1-to-1). Labels not contained in a dict / Series will be left as-is. Extra labels listed don't throw an error. See the :ref: user guide <basics.rename> for more. Parameters mapper : dict-like or function Dict-like or functions transformations to apply to that axis' values. Use either mapper and axis to specify the axis to target with mapper , or index and columns . index : dict-like or function Alternative to specifying axis ( mapper, axis=0 is equivalent to index=mapper ). columns : dict-like or function Alternative to specifying axis ( mapper, axis=1 is equivalent to columns=mapper ). axis : {0 or 'index', 1 or 'columns'}, default 0 Axis to target with mapper . Can be either the axis name ('index', 'columns') or number (0, 1). The default is 'index'. copy : bool, default True Also copy underlying data. inplace : bool, default False Whether to return a new DataFrame. If True then value of copy is ignored. level : int or level name, default None In case of a MultiIndex, only rename labels in the specified level. errors : {'ignore', 'raise'}, default 'ignore' If 'raise', raise a KeyError when a dict-like mapper , index , or columns contains labels that are not present in the Index being transformed. If 'ignore', existing keys will be renamed and extra keys will be ignored. Returns DataFrame DataFrame with the renamed axis labels. Raises KeyError If any of the labels is not found in the selected axis and \"errors='raise'\". See Also DataFrame.rename_axis : Set the name of the axis. Examples DataFrame.rename supports two calling conventions (index=index_mapper, columns=columns_mapper, ...) (mapper, axis={'index', 'columns'}, ...) We highly recommend using keyword arguments to clarify your intent. Rename columns using a mapping: df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]}) df.rename(columns={\"A\": \"a\", \"B\": \"c\"}) a c 0 1 4 1 2 5 2 3 6 Rename index using a mapping: df.rename(index={0: \"x\", 1: \"y\", 2: \"z\"}) A B x 1 4 y 2 5 z 3 6 Cast index labels to a different type: df.index RangeIndex(start=0, stop=3, step=1) df.rename(index=str).index Index(['0', '1', '2'], dtype='object') df.rename(columns={\"A\": \"a\", \"B\": \"b\", \"C\": \"c\"}, errors=\"raise\") Traceback (most recent call last): KeyError: ['C'] not found in axis Using axis-style parameters df.rename(str.lower, axis='columns') a b 0 1 4 1 2 5 2 3 6 df.rename({1: 2, 2: 4}, axis='index') A B 0 1 4 2 2 5 4 3 6 View Source @ rewrite_axis_style_signature ( \"mapper\" , [( \"copy\" , True ), ( \"inplace\" , False ), ( \"level\" , None ), ( \"errors\" , \"ignore\" )], ) def rename ( self , mapper : Optional [ Renamer ] = None , * , index : Optional [ Renamer ] = None , columns : Optional [ Renamer ] = None , axis : Optional [ Axis ] = None , copy : bool = True , inplace : bool = False , level : Optional [ Level ] = None , errors : str = \"ignore\" , ) -> Optional [ \"DataFrame\" ]: \"\"\" Alter axes labels. Function / dict values must be unique (1-to-1). Labels not contained in a dict / Series will be left as-is. Extra labels listed don't throw an error. See the :ref:`user guide <basics.rename>` for more. Parameters ---------- mapper : dict-like or function Dict-like or functions transformations to apply to that axis' values. Use either ``mapper`` and ``axis`` to specify the axis to target with ``mapper``, or ``index`` and ``columns``. index : dict-like or function Alternative to specifying axis (``mapper, axis=0`` is equivalent to ``index=mapper``). columns : dict-like or function Alternative to specifying axis (``mapper, axis=1`` is equivalent to ``columns=mapper``). axis : {0 or 'index', 1 or 'columns'}, default 0 Axis to target with ``mapper``. Can be either the axis name ('index', 'columns') or number (0, 1). The default is 'index'. copy : bool, default True Also copy underlying data. inplace : bool, default False Whether to return a new DataFrame. If True then value of copy is ignored. level : int or level name, default None In case of a MultiIndex, only rename labels in the specified level. errors : {'ignore', 'raise'}, default 'ignore' If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`, or `columns` contains labels that are not present in the Index being transformed. If 'ignore', existing keys will be renamed and extra keys will be ignored. Returns ------- DataFrame DataFrame with the renamed axis labels. Raises ------ KeyError If any of the labels is not found in the selected axis and \" errors = 'raise' \". See Also -------- DataFrame.rename_axis : Set the name of the axis. Examples -------- ``DataFrame.rename`` supports two calling conventions * ``(index=index_mapper, columns=columns_mapper, ...)`` * ``(mapper, axis={'index', 'columns'}, ...)`` We *highly* recommend using keyword arguments to clarify your intent. Rename columns using a mapping: >>> df = pd.DataFrame({\" A \": [1, 2, 3], \" B \": [4, 5, 6]}) >>> df.rename(columns={\" A \": \" a \", \" B \": \" c \"}) a c 0 1 4 1 2 5 2 3 6 Rename index using a mapping: >>> df.rename(index={0: \" x \", 1: \" y \", 2: \" z \"}) A B x 1 4 y 2 5 z 3 6 Cast index labels to a different type: >>> df.index RangeIndex(start=0, stop=3, step=1) >>> df.rename(index=str).index Index(['0', '1', '2'], dtype='object') >>> df.rename(columns={\" A \": \" a \", \" B \": \" b \", \" C \": \" c \"}, errors=\" raise \") Traceback (most recent call last): KeyError: ['C'] not found in axis Using axis-style parameters >>> df.rename(str.lower, axis='columns') a b 0 1 4 1 2 5 2 3 6 >>> df.rename({1: 2, 2: 4}, axis='index') A B 0 1 4 2 2 5 4 3 6 \"\"\" return super (). rename ( mapper = mapper , index = index , columns = columns , axis = axis , copy = copy , inplace = inplace , level = level , errors = errors , ) rename_axis def rename_axis ( self , mapper = None , index = None , columns = None , axis = None , copy = True , inplace = False ) Set the name of the axis for the index or columns. Parameters mapper : scalar, list-like, optional Value to set the axis name attribute. index, columns : scalar, list-like, dict-like or function, optional A scalar, list-like, dict-like or functions transformations to apply to that axis' values. Note that the columns parameter is not allowed if the object is a Series. This parameter only apply for DataFrame type objects. Use either `` mapper `` and `` axis `` to specify the axis to target with `` mapper `` , or `` index `` and / or `` columns `` . .. versionchanged :: 0 . 24 . 0 axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to rename. copy : bool, default True Also copy underlying data. inplace : bool, default False Modifies the object directly, instead of creating a new Series or DataFrame. Returns Series, DataFrame, or None The same type as the caller or None if inplace is True. See Also Series.rename : Alter Series index labels or name. DataFrame.rename : Alter DataFrame index labels or name. Index.rename : Set new names on index. Notes DataFrame.rename_axis supports two calling conventions (index=index_mapper, columns=columns_mapper, ...) (mapper, axis={'index', 'columns'}, ...) The first calling convention will only modify the names of the index and/or the names of the Index object that is the columns. In this case, the parameter copy is ignored. The second calling convention will modify the names of the the corresponding index if mapper is a list or a scalar. However, if mapper is dict-like or a function, it will use the deprecated behavior of modifying the axis labels . We highly recommend using keyword arguments to clarify your intent. Examples Series s = pd.Series([\"dog\", \"cat\", \"monkey\"]) s 0 dog 1 cat 2 monkey dtype: object s.rename_axis(\"animal\") animal 0 dog 1 cat 2 monkey dtype: object DataFrame df = pd.DataFrame({\"num_legs\": [4, 4, 2], ... \"num_arms\": [0, 0, 2]}, ... [\"dog\", \"cat\", \"monkey\"]) df num_legs num_arms dog 4 0 cat 4 0 monkey 2 2 df = df.rename_axis(\"animal\") df num_legs num_arms animal dog 4 0 cat 4 0 monkey 2 2 df = df.rename_axis(\"limbs\", axis=\"columns\") df limbs num_legs num_arms animal dog 4 0 cat 4 0 monkey 2 2 MultiIndex df.index = pd.MultiIndex.from_product([['mammal'], ... ['dog', 'cat', 'monkey']], ... names=['type', 'name']) df limbs num_legs num_arms type name mammal dog 4 0 cat 4 0 monkey 2 2 df.rename_axis(index={'type': 'class'}) limbs num_legs num_arms class name mammal dog 4 0 cat 4 0 monkey 2 2 df.rename_axis(columns=str.upper) LIMBS num_legs num_arms type name mammal dog 4 0 cat 4 0 monkey 2 2 View Source @ rewrite_axis_style_signature ( \"mapper\" , [( \"copy\" , True ), ( \"inplace\" , False )]) def rename_axis ( self , mapper = lib . no_default , ** kwargs ): \"\"\" Set the name of the axis for the index or columns. Parameters ---------- mapper : scalar, list-like, optional Value to set the axis name attribute. index, columns : scalar, list-like, dict-like or function, optional A scalar, list-like, dict-like or functions transformations to apply to that axis' values. Note that the ``columns`` parameter is not allowed if the object is a Series. This parameter only apply for DataFrame type objects. Use either ``mapper`` and ``axis`` to specify the axis to target with ``mapper``, or ``index`` and/or ``columns``. .. versionchanged:: 0.24.0 axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to rename. copy : bool, default True Also copy underlying data. inplace : bool, default False Modifies the object directly, instead of creating a new Series or DataFrame. Returns ------- Series, DataFrame, or None The same type as the caller or None if `inplace` is True. See Also -------- Series.rename : Alter Series index labels or name. DataFrame.rename : Alter DataFrame index labels or name. Index.rename : Set new names on index. Notes ----- ``DataFrame.rename_axis`` supports two calling conventions * ``(index=index_mapper, columns=columns_mapper, ...)`` * ``(mapper, axis={'index', 'columns'}, ...)`` The first calling convention will only modify the names of the index and/or the names of the Index object that is the columns. In this case, the parameter ``copy`` is ignored. The second calling convention will modify the names of the the corresponding index if mapper is a list or a scalar. However, if mapper is dict-like or a function, it will use the deprecated behavior of modifying the axis *labels*. We *highly* recommend using keyword arguments to clarify your intent. Examples -------- **Series** >>> s = pd.Series([\" dog \", \" cat \", \" monkey \"]) >>> s 0 dog 1 cat 2 monkey dtype: object >>> s.rename_axis(\" animal \") animal 0 dog 1 cat 2 monkey dtype: object **DataFrame** >>> df = pd.DataFrame({\" num_legs \": [4, 4, 2], ... \" num_arms \": [0, 0, 2]}, ... [\" dog \", \" cat \", \" monkey \"]) >>> df num_legs num_arms dog 4 0 cat 4 0 monkey 2 2 >>> df = df.rename_axis(\" animal \") >>> df num_legs num_arms animal dog 4 0 cat 4 0 monkey 2 2 >>> df = df.rename_axis(\" limbs \", axis=\" columns \") >>> df limbs num_legs num_arms animal dog 4 0 cat 4 0 monkey 2 2 **MultiIndex** >>> df.index = pd.MultiIndex.from_product([['mammal'], ... ['dog', 'cat', 'monkey']], ... names=['type', 'name']) >>> df limbs num_legs num_arms type name mammal dog 4 0 cat 4 0 monkey 2 2 >>> df.rename_axis(index={'type': 'class'}) limbs num_legs num_arms class name mammal dog 4 0 cat 4 0 monkey 2 2 >>> df.rename_axis(columns=str.upper) LIMBS num_legs num_arms type name mammal dog 4 0 cat 4 0 monkey 2 2 \"\"\" axes , kwargs = self . _construct_axes_from_arguments ( (), kwargs , sentinel = lib . no_default ) copy = kwargs . pop ( \"copy\" , True ) inplace = kwargs . pop ( \"inplace\" , False ) axis = kwargs . pop ( \"axis\" , 0 ) if axis is not None : axis = self . _get_axis_number ( axis ) if kwargs : raise TypeError ( \"rename_axis() got an unexpected keyword \" f 'argument \"{list(kwargs.keys())[0]}\"' ) inplace = validate_bool_kwarg ( inplace , \"inplace\" ) if mapper is not lib . no_default : # Use v0.23 behavior if a scalar or list non_mapper = is_scalar ( mapper ) or ( is_list_like ( mapper ) and not is_dict_like ( mapper ) ) if non_mapper : return self . _set_axis_name ( mapper , axis = axis , inplace = inplace ) else : raise ValueError ( \"Use `.rename` to alter labels with a mapper.\" ) else : # Use new behavior. Means that index and/or columns # is specified result = self if inplace else self . copy ( deep = copy ) for axis in range ( self . _AXIS_LEN ): v = axes . get ( self . _get_axis_name ( axis )) if v is lib . no_default : continue non_mapper = is_scalar ( v ) or ( is_list_like ( v ) and not is_dict_like ( v )) if non_mapper : newnames = v else : f = com . get_rename_function ( v ) curnames = self . _get_axis ( axis ). names newnames = [ f ( name ) for name in curnames ] result . _set_axis_name ( newnames , axis = axis , inplace = True ) if not inplace : return result reorder_levels def reorder_levels ( self , order , axis = 0 ) -> 'DataFrame' Rearrange index levels using input order. May not drop or duplicate levels. Parameters order : list of int or list of str List representing new level order. Reference level by number (position) or by key (label). axis : {0 or 'index', 1 or 'columns'}, default 0 Where to reorder levels. Returns DataFrame View Source def reorder_levels ( self , order , axis = 0 ) -> \"DataFrame\" : \"\"\" Rearrange index levels using input order. May not drop or duplicate levels. Parameters ---------- order : list of int or list of str List representing new level order. Reference level by number (position) or by key (label). axis : {0 or 'index', 1 or 'columns'}, default 0 Where to reorder levels. Returns ------- DataFrame \"\"\" axis = self . _get_axis_number ( axis ) if not isinstance ( self . _get_axis ( axis ), MultiIndex ): # pragma : no cover raise TypeError ( \"Can only reorder levels on a hierarchical axis.\" ) result = self . copy () if axis == 0 : assert isinstance ( result . index , MultiIndex ) result . index = result . index . reorder_levels ( order ) else : assert isinstance ( result . columns , MultiIndex ) result . columns = result . columns . reorder_levels ( order ) return result replace def replace ( self , to_replace = None , value = None , inplace = False , limit = None , regex = False , method = 'pad' ) Replace values given in to_replace with value . Values of the DataFrame are replaced with other values dynamically. This differs from updating with .loc or .iloc , which require you to specify a location to update with some value. Parameters to_replace : str, regex, list, dict, Series, int, float, or None How to find the values that will be replaced. * numeric , str or regex : - numeric : numeric values equal to `to_replace` will be replaced with `value` - str : string exactly matching `to_replace` will be replaced with `value` - regex : regexs matching `to_replace` will be replaced with `value` * list of str , regex , or numeric : - First , if `to_replace` and `value` are both lists , they ** must ** be the same length . - Second , if `` regex = True `` then all of the strings in ** both ** lists will be interpreted as regexs otherwise they will match directly . This doesn 't matter much for `value` since there are only a few possible substitution regexes you can use. - str, regex and numeric rules apply as above. * dict: - Dicts can be used to specify different replacement values for different existing values. For example, ``{' a ': ' b ', ' y ': ' z '}`` replaces the value ' a ' with ' b ' and ' y ' with ' z '. To use a dict in this way the `value` parameter should be `None`. - For a DataFrame a dict can specify that different values should be replaced in different columns. For example, ``{' a ': 1, ' b ': ' z '}`` looks for the value 1 in column ' a ' and the value ' z ' in column ' b ' and replaces these values with whatever is specified in `value`. The `value` parameter should not be ``None`` in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. - For a DataFrame nested dictionaries, e.g., ``{' a ': {' b ': np.nan}}``, are read as follows: look in column ' a ' for the value ' b ' and replace it with NaN . The `value` parameter should be `` None `` to use a nested dict in this way . You can nest regular expressions as well . Note that column names ( the top - level dictionary keys in a nested dictionary ) ** cannot ** be regular expressions . * None : - This means that the `regex` argument must be a string , compiled regular expression , or list , dict , ndarray or Series of such elements . If `value` is also `` None `` then this ** must ** be a nested dictionary or Series . See the examples section for examples of each of these . value : scalar, dict, list, str, regex, default None Value to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed. inplace : bool, default False If True, in place. Note: this will modify any other views on this object (e.g. a column from a DataFrame). Returns the caller if this is True. limit : int, default None Maximum size gap to forward or backward fill. regex : bool or same types as to_replace , default False Whether to interpret to_replace and/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None . method : {'pad', 'ffill', 'bfill', None } The method to use when for replacement, when to_replace is a scalar, list or tuple and value is None . .. versionchanged :: 0.23.0 Added to DataFrame. Returns DataFrame Object after replacement. Raises AssertionError * If regex is not a bool and to_replace is not None . TypeError * If to_replace is not a scalar, array-like, dict , or None * If to_replace is a dict and value is not a list , dict , ndarray , or Series * If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. * When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced ValueError * If a list or an ndarray is passed to to_replace and value but they are not the same length. See Also DataFrame.fillna : Fill NA values. DataFrame.where : Replace values based on boolean condition. Series.str.replace : Simple string replacement. Notes Regex substitution is performed under the hood with re.sub . The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter. Examples Scalar to_replace and value s = pd.Series([0, 1, 2, 3, 4]) s.replace(0, 5) 0 5 1 1 2 2 3 3 4 4 dtype: int64 df = pd.DataFrame({'A': [0, 1, 2, 3, 4], ... 'B': [5, 6, 7, 8, 9], ... 'C': ['a', 'b', 'c', 'd', 'e']}) df.replace(0, 5) A B C 0 5 5 a 1 1 6 b 2 2 7 c 3 3 8 d 4 4 9 e List-like to_replace df.replace([0, 1, 2, 3], 4) A B C 0 4 5 a 1 4 6 b 2 4 7 c 3 4 8 d 4 4 9 e df.replace([0, 1, 2, 3], [4, 3, 2, 1]) A B C 0 4 5 a 1 3 6 b 2 2 7 c 3 1 8 d 4 4 9 e s.replace([1, 2], method='bfill') 0 0 1 3 2 3 3 3 4 4 dtype: int64 dict-like to_replace df.replace({0: 10, 1: 100}) A B C 0 10 5 a 1 100 6 b 2 2 7 c 3 3 8 d 4 4 9 e df.replace({'A': 0, 'B': 5}, 100) A B C 0 100 100 a 1 1 6 b 2 2 7 c 3 3 8 d 4 4 9 e df.replace({'A': {0: 100, 4: 400}}) A B C 0 100 5 a 1 1 6 b 2 2 7 c 3 3 8 d 4 400 9 e Regular expression to_replace df = pd.DataFrame({'A': ['bat', 'foo', 'bait'], ... 'B': ['abc', 'bar', 'xyz']}) df.replace(to_replace=r'^ba.$', value='new', regex=True) A B 0 new abc 1 foo new 2 bait xyz df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True) A B 0 new abc 1 foo bar 2 bait xyz df.replace(regex=r'^ba.$', value='new') A B 0 new abc 1 foo new 2 bait xyz df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'}) A B 0 new abc 1 xyz new 2 bait xyz df.replace(regex=[r'^ba.$', 'foo'], value='new') A B 0 new abc 1 new new 2 bait xyz Note that when replacing multiple bool or datetime64 objects, the data types in the to_replace parameter must match the data type of the value being replaced: df = pd.DataFrame({'A': [True, False, True], ... 'B': [False, True, False]}) df.replace({'a string': 'new value', True: False}) # raises Traceback (most recent call last): ... TypeError: Cannot compare types 'ndarray(dtype=bool)' and 'str' This raises a TypeError because one of the dict keys is not of the correct type for replacement. Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter: s = pd.Series([10, 'a', 'a', 'b', 'a']) When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None) : s.replace({'a': None}) 0 10 1 None 2 None 3 b 4 None dtype: object When value=None and to_replace is a scalar, list or tuple, replace uses the method parameter (default 'pad') to do the replacement. So this is why the 'a' values are being replaced by 10 in rows 1 and 2 and 'b' in row 4 in this case. The command s.replace('a', None) is actually equivalent to s.replace(to_replace='a', value=None, method='pad') : s.replace('a', None) 0 10 1 10 2 10 3 b 4 b dtype: object View Source @doc ( NDFrame . replace , ** _shared_doc_kwargs ) def replace ( self , to_replace = None , value = None , inplace = False , limit = None , regex = False , method = \"pad\" , ) : return super (). replace ( to_replace = to_replace , value = value , inplace = inplace , limit = limit , regex = regex , method = method , ) resample def resample ( self , rule , axis = 0 , closed : Union [ str , NoneType ] = None , label : Union [ str , NoneType ] = None , convention : str = 'start' , kind : Union [ str , NoneType ] = None , loffset = None , base : Union [ int , NoneType ] = None , on = None , level = None , origin : Union [ str , ForwardRef ( 'Timestamp' ), datetime . datetime , numpy . datetime64 , int , numpy . int64 , float ] = 'start_day' , offset : Union [ ForwardRef ( 'Timedelta' ), datetime . timedelta , numpy . timedelta64 , int , numpy . int64 , float , str , NoneType ] = None ) -> 'Resampler' Resample time-series data. Convenience method for frequency conversion and resampling of time series. Object must have a datetime-like index ( DatetimeIndex , PeriodIndex , or TimedeltaIndex ), or pass datetime-like values to the on or level keyword. Parameters rule : DateOffset, Timedelta or str The offset string or object representing target conversion. axis : {0 or 'index', 1 or 'columns'}, default 0 Which axis to use for up- or down-sampling. For Series this will default to 0, i.e. along the rows. Must be DatetimeIndex , TimedeltaIndex or PeriodIndex . closed : {'right', 'left'}, default None Which side of bin interval is closed. The default is 'left' for all frequency offsets except for 'M', 'A', 'Q', 'BM', 'BA', 'BQ', and 'W' which all have a default of 'right'. label : {'right', 'left'}, default None Which bin edge label to label bucket with. The default is 'left' for all frequency offsets except for 'M', 'A', 'Q', 'BM', 'BA', 'BQ', and 'W' which all have a default of 'right'. convention : {'start', 'end', 's', 'e'}, default 'start' For PeriodIndex only, controls whether to use the start or end of rule . kind : {'timestamp', 'period'}, optional, default None Pass 'timestamp' to convert the resulting index to a DateTimeIndex or 'period' to convert it to a PeriodIndex . By default the input representation is retained. loffset : timedelta, default None Adjust the resampled time labels. .. deprecated :: 1.1.0 You should add the loffset to the `df.index` after the resample. See below. base : int, default 0 For frequencies that evenly subdivide 1 day, the \"origin\" of the aggregated intervals. For example, for '5min' frequency, base could range from 0 through 4. Defaults to 0. .. deprecated :: 1.1.0 The new arguments that you should use are 'offset' or 'origin'. on : str, optional For a DataFrame, column to use instead of index for resampling. Column must be datetime-like. level : str or int, optional For a MultiIndex, level (name or number) to use for resampling. level must be datetime-like. origin : {'epoch', 'start', 'start_day'}, Timestamp or str, default 'start_day' The timestamp on which to adjust the grouping. The timezone of origin must match the timezone of the index. If a timestamp is not used, these values are also supported: - 'epoch' : `origin` is 1970 - 01 - 01 - 'start' : `origin` is the first value of the timeseries - 'start_day' : `origin` is the first day at midnight of the timeseries .. versionadded :: 1 . 1 . 0 offset : Timedelta or str, default is None An offset timedelta added to the origin. .. versionadded :: 1.1.0 Returns Resampler object See Also groupby : Group by mapping, function, label, or list of labels. Series.resample : Resample a Series. DataFrame.resample: Resample a DataFrame. Notes See the user guide <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling> _ for more. To learn more about the offset strings, please see this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects> __. Examples Start by creating a series with 9 one minute timestamps. index = pd.date_range('1/1/2000', periods=9, freq='T') series = pd.Series(range(9), index=index) series 2000-01-01 00:00:00 0 2000-01-01 00:01:00 1 2000-01-01 00:02:00 2 2000-01-01 00:03:00 3 2000-01-01 00:04:00 4 2000-01-01 00:05:00 5 2000-01-01 00:06:00 6 2000-01-01 00:07:00 7 2000-01-01 00:08:00 8 Freq: T, dtype: int64 Downsample the series into 3 minute bins and sum the values of the timestamps falling into a bin. series.resample('3T').sum() 2000-01-01 00:00:00 3 2000-01-01 00:03:00 12 2000-01-01 00:06:00 21 Freq: 3T, dtype: int64 Downsample the series into 3 minute bins as above, but label each bin using the right edge instead of the left. Please note that the value in the bucket used as the label is not included in the bucket, which it labels. For example, in the original series the bucket 2000-01-01 00:03:00 contains the value 3, but the summed value in the resampled bucket with the label 2000-01-01 00:03:00 does not include 3 (if it did, the summed value would be 6, not 3). To include this value close the right side of the bin interval as illustrated in the example below this one. series.resample('3T', label='right').sum() 2000-01-01 00:03:00 3 2000-01-01 00:06:00 12 2000-01-01 00:09:00 21 Freq: 3T, dtype: int64 Downsample the series into 3 minute bins as above, but close the right side of the bin interval. series.resample('3T', label='right', closed='right').sum() 2000-01-01 00:00:00 0 2000-01-01 00:03:00 6 2000-01-01 00:06:00 15 2000-01-01 00:09:00 15 Freq: 3T, dtype: int64 Upsample the series into 30 second bins. series.resample('30S').asfreq()[0:5] # Select first 5 rows 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 NaN 2000-01-01 00:01:00 1.0 2000-01-01 00:01:30 NaN 2000-01-01 00:02:00 2.0 Freq: 30S, dtype: float64 Upsample the series into 30 second bins and fill the NaN values using the pad method. series.resample('30S').pad()[0:5] 2000-01-01 00:00:00 0 2000-01-01 00:00:30 0 2000-01-01 00:01:00 1 2000-01-01 00:01:30 1 2000-01-01 00:02:00 2 Freq: 30S, dtype: int64 Upsample the series into 30 second bins and fill the NaN values using the bfill method. series.resample('30S').bfill()[0:5] 2000-01-01 00:00:00 0 2000-01-01 00:00:30 1 2000-01-01 00:01:00 1 2000-01-01 00:01:30 2 2000-01-01 00:02:00 2 Freq: 30S, dtype: int64 Pass a custom function via apply def custom_resampler(array_like): ... return np.sum(array_like) + 5 ... series.resample('3T').apply(custom_resampler) 2000-01-01 00:00:00 8 2000-01-01 00:03:00 17 2000-01-01 00:06:00 26 Freq: 3T, dtype: int64 For a Series with a PeriodIndex, the keyword convention can be used to control whether to use the start or end of rule . Resample a year by quarter using 'start' convention . Values are assigned to the first quarter of the period. s = pd.Series([1, 2], index=pd.period_range('2012-01-01', ... freq='A', ... periods=2)) s 2012 1 2013 2 Freq: A-DEC, dtype: int64 s.resample('Q', convention='start').asfreq() 2012Q1 1.0 2012Q2 NaN 2012Q3 NaN 2012Q4 NaN 2013Q1 2.0 2013Q2 NaN 2013Q3 NaN 2013Q4 NaN Freq: Q-DEC, dtype: float64 Resample quarters by month using 'end' convention . Values are assigned to the last month of the period. q = pd.Series([1, 2, 3, 4], index=pd.period_range('2018-01-01', ... freq='Q', ... periods=4)) q 2018Q1 1 2018Q2 2 2018Q3 3 2018Q4 4 Freq: Q-DEC, dtype: int64 q.resample('M', convention='end').asfreq() 2018-03 1.0 2018-04 NaN 2018-05 NaN 2018-06 2.0 2018-07 NaN 2018-08 NaN 2018-09 3.0 2018-10 NaN 2018-11 NaN 2018-12 4.0 Freq: M, dtype: float64 For DataFrame objects, the keyword on can be used to specify the column instead of the index for resampling. d = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19], ... 'volume': [50, 60, 40, 100, 50, 100, 40, 50]}) df = pd.DataFrame(d) df['week_starting'] = pd.date_range('01/01/2018', ... periods=8, ... freq='W') df price volume week_starting 0 10 50 2018-01-07 1 11 60 2018-01-14 2 9 40 2018-01-21 3 13 100 2018-01-28 4 14 50 2018-02-04 5 18 100 2018-02-11 6 17 40 2018-02-18 7 19 50 2018-02-25 df.resample('M', on='week_starting').mean() price volume week_starting 2018-01-31 10.75 62.5 2018-02-28 17.00 60.0 For a DataFrame with MultiIndex, the keyword level can be used to specify on which level the resampling needs to take place. days = pd.date_range('1/1/2000', periods=4, freq='D') d2 = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19], ... 'volume': [50, 60, 40, 100, 50, 100, 40, 50]}) df2 = pd.DataFrame(d2, ... index=pd.MultiIndex.from_product([days, ... ['morning', ... 'afternoon']] ... )) df2 price volume 2000-01-01 morning 10 50 afternoon 11 60 2000-01-02 morning 9 40 afternoon 13 100 2000-01-03 morning 14 50 afternoon 18 100 2000-01-04 morning 17 40 afternoon 19 50 df2.resample('D', level=0).sum() price volume 2000-01-01 21 110 2000-01-02 22 140 2000-01-03 32 150 2000-01-04 36 90 If you want to adjust the start of the bins based on a fixed timestamp: start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00' rng = pd.date_range(start, end, freq='7min') ts = pd.Series(np.arange(len(rng)) * 3, index=rng) ts 2000-10-01 23:30:00 0 2000-10-01 23:37:00 3 2000-10-01 23:44:00 6 2000-10-01 23:51:00 9 2000-10-01 23:58:00 12 2000-10-02 00:05:00 15 2000-10-02 00:12:00 18 2000-10-02 00:19:00 21 2000-10-02 00:26:00 24 Freq: 7T, dtype: int64 ts.resample('17min').sum() 2000-10-01 23:14:00 0 2000-10-01 23:31:00 9 2000-10-01 23:48:00 21 2000-10-02 00:05:00 54 2000-10-02 00:22:00 24 Freq: 17T, dtype: int64 ts.resample('17min', origin='epoch').sum() 2000-10-01 23:18:00 0 2000-10-01 23:35:00 18 2000-10-01 23:52:00 27 2000-10-02 00:09:00 39 2000-10-02 00:26:00 24 Freq: 17T, dtype: int64 ts.resample('17min', origin='2000-01-01').sum() 2000-10-01 23:24:00 3 2000-10-01 23:41:00 15 2000-10-01 23:58:00 45 2000-10-02 00:15:00 45 Freq: 17T, dtype: int64 If you want to adjust the start of the bins with an offset Timedelta, the two following lines are equivalent: ts.resample('17min', origin='start').sum() 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17T, dtype: int64 ts.resample('17min', offset='23h30min').sum() 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17T, dtype: int64 To replace the use of the deprecated base argument, you can now use offset , in this example it is equivalent to have base=2 : ts.resample('17min', offset='2min').sum() 2000-10-01 23:16:00 0 2000-10-01 23:33:00 9 2000-10-01 23:50:00 36 2000-10-02 00:07:00 39 2000-10-02 00:24:00 24 Freq: 17T, dtype: int64 To replace the use of the deprecated loffset argument: from pandas.tseries.frequencies import to_offset loffset = '19min' ts_out = ts.resample('17min').sum() ts_out.index = ts_out.index + to_offset(loffset) ts_out 2000-10-01 23:33:00 0 2000-10-01 23:50:00 9 2000-10-02 00:07:00 21 2000-10-02 00:24:00 54 2000-10-02 00:41:00 24 Freq: 17T, dtype: int64 View Source def resample ( self , rule , axis = 0 , closed : Optional [ str ] = None , label : Optional [ str ] = None , convention : str = \"start\" , kind : Optional [ str ] = None , loffset = None , base : Optional [ int ] = None , on = None , level = None , origin : Union [ str , TimestampConvertibleTypes ] = \"start_day\" , offset : Optional [ TimedeltaConvertibleTypes ] = None , ) -> \"Resampler\" : \"\"\" Resample time-series data. Convenience method for frequency conversion and resampling of time series. Object must have a datetime-like index (`DatetimeIndex`, `PeriodIndex`, or `TimedeltaIndex`), or pass datetime-like values to the `on` or `level` keyword. Parameters ---------- rule : DateOffset, Timedelta or str The offset string or object representing target conversion. axis : {0 or 'index', 1 or 'columns'}, default 0 Which axis to use for up- or down-sampling. For `Series` this will default to 0, i.e. along the rows. Must be `DatetimeIndex`, `TimedeltaIndex` or `PeriodIndex`. closed : {'right', 'left'}, default None Which side of bin interval is closed. The default is 'left' for all frequency offsets except for 'M', 'A', 'Q', 'BM', 'BA', 'BQ', and 'W' which all have a default of 'right'. label : {'right', 'left'}, default None Which bin edge label to label bucket with. The default is 'left' for all frequency offsets except for 'M', 'A', 'Q', 'BM', 'BA', 'BQ', and 'W' which all have a default of 'right'. convention : {'start', 'end', 's', 'e'}, default 'start' For `PeriodIndex` only, controls whether to use the start or end of `rule`. kind : {'timestamp', 'period'}, optional, default None Pass 'timestamp' to convert the resulting index to a `DateTimeIndex` or 'period' to convert it to a `PeriodIndex`. By default the input representation is retained. loffset : timedelta, default None Adjust the resampled time labels. .. deprecated:: 1.1.0 You should add the loffset to the `df.index` after the resample. See below. base : int, default 0 For frequencies that evenly subdivide 1 day, the \" origin \" of the aggregated intervals. For example, for '5min' frequency, base could range from 0 through 4. Defaults to 0. .. deprecated:: 1.1.0 The new arguments that you should use are 'offset' or 'origin'. on : str, optional For a DataFrame, column to use instead of index for resampling. Column must be datetime-like. level : str or int, optional For a MultiIndex, level (name or number) to use for resampling. `level` must be datetime-like. origin : {'epoch', 'start', 'start_day'}, Timestamp or str, default 'start_day' The timestamp on which to adjust the grouping. The timezone of origin must match the timezone of the index. If a timestamp is not used, these values are also supported: - 'epoch': `origin` is 1970-01-01 - 'start': `origin` is the first value of the timeseries - 'start_day': `origin` is the first day at midnight of the timeseries .. versionadded:: 1.1.0 offset : Timedelta or str, default is None An offset timedelta added to the origin. .. versionadded:: 1.1.0 Returns ------- Resampler object See Also -------- groupby : Group by mapping, function, label, or list of labels. Series.resample : Resample a Series. DataFrame.resample: Resample a DataFrame. Notes ----- See the `user guide <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling>`_ for more. To learn more about the offset strings, please see `this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects>`__. Examples -------- Start by creating a series with 9 one minute timestamps. >>> index = pd.date_range('1/1/2000', periods=9, freq='T') >>> series = pd.Series(range(9), index=index) >>> series 2000-01-01 00:00:00 0 2000-01-01 00:01:00 1 2000-01-01 00:02:00 2 2000-01-01 00:03:00 3 2000-01-01 00:04:00 4 2000-01-01 00:05:00 5 2000-01-01 00:06:00 6 2000-01-01 00:07:00 7 2000-01-01 00:08:00 8 Freq: T, dtype: int64 Downsample the series into 3 minute bins and sum the values of the timestamps falling into a bin. >>> series.resample('3T').sum() 2000-01-01 00:00:00 3 2000-01-01 00:03:00 12 2000-01-01 00:06:00 21 Freq: 3T, dtype: int64 Downsample the series into 3 minute bins as above, but label each bin using the right edge instead of the left. Please note that the value in the bucket used as the label is not included in the bucket, which it labels. For example, in the original series the bucket ``2000-01-01 00:03:00`` contains the value 3, but the summed value in the resampled bucket with the label ``2000-01-01 00:03:00`` does not include 3 (if it did, the summed value would be 6, not 3). To include this value close the right side of the bin interval as illustrated in the example below this one. >>> series.resample('3T', label='right').sum() 2000-01-01 00:03:00 3 2000-01-01 00:06:00 12 2000-01-01 00:09:00 21 Freq: 3T, dtype: int64 Downsample the series into 3 minute bins as above, but close the right side of the bin interval. >>> series.resample('3T', label='right', closed='right').sum() 2000-01-01 00:00:00 0 2000-01-01 00:03:00 6 2000-01-01 00:06:00 15 2000-01-01 00:09:00 15 Freq: 3T, dtype: int64 Upsample the series into 30 second bins. >>> series.resample('30S').asfreq()[0:5] # Select first 5 rows 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 NaN 2000-01-01 00:01:00 1.0 2000-01-01 00:01:30 NaN 2000-01-01 00:02:00 2.0 Freq: 30S, dtype: float64 Upsample the series into 30 second bins and fill the ``NaN`` values using the ``pad`` method. >>> series.resample('30S').pad()[0:5] 2000-01-01 00:00:00 0 2000-01-01 00:00:30 0 2000-01-01 00:01:00 1 2000-01-01 00:01:30 1 2000-01-01 00:02:00 2 Freq: 30S, dtype: int64 Upsample the series into 30 second bins and fill the ``NaN`` values using the ``bfill`` method. >>> series.resample('30S').bfill()[0:5] 2000-01-01 00:00:00 0 2000-01-01 00:00:30 1 2000-01-01 00:01:00 1 2000-01-01 00:01:30 2 2000-01-01 00:02:00 2 Freq: 30S, dtype: int64 Pass a custom function via ``apply`` >>> def custom_resampler(array_like): ... return np.sum(array_like) + 5 ... >>> series.resample('3T').apply(custom_resampler) 2000-01-01 00:00:00 8 2000-01-01 00:03:00 17 2000-01-01 00:06:00 26 Freq: 3T, dtype: int64 For a Series with a PeriodIndex, the keyword `convention` can be used to control whether to use the start or end of `rule`. Resample a year by quarter using 'start' `convention`. Values are assigned to the first quarter of the period. >>> s = pd.Series([1, 2], index=pd.period_range('2012-01-01', ... freq='A', ... periods=2)) >>> s 2012 1 2013 2 Freq: A-DEC, dtype: int64 >>> s.resample('Q', convention='start').asfreq() 2012Q1 1.0 2012Q2 NaN 2012Q3 NaN 2012Q4 NaN 2013Q1 2.0 2013Q2 NaN 2013Q3 NaN 2013Q4 NaN Freq: Q-DEC, dtype: float64 Resample quarters by month using 'end' `convention`. Values are assigned to the last month of the period. >>> q = pd.Series([1, 2, 3, 4], index=pd.period_range('2018-01-01', ... freq='Q', ... periods=4)) >>> q 2018Q1 1 2018Q2 2 2018Q3 3 2018Q4 4 Freq: Q-DEC, dtype: int64 >>> q.resample('M', convention='end').asfreq() 2018-03 1.0 2018-04 NaN 2018-05 NaN 2018-06 2.0 2018-07 NaN 2018-08 NaN 2018-09 3.0 2018-10 NaN 2018-11 NaN 2018-12 4.0 Freq: M, dtype: float64 For DataFrame objects, the keyword `on` can be used to specify the column instead of the index for resampling. >>> d = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19], ... 'volume': [50, 60, 40, 100, 50, 100, 40, 50]}) >>> df = pd.DataFrame(d) >>> df['week_starting'] = pd.date_range('01/01/2018', ... periods=8, ... freq='W') >>> df price volume week_starting 0 10 50 2018-01-07 1 11 60 2018-01-14 2 9 40 2018-01-21 3 13 100 2018-01-28 4 14 50 2018-02-04 5 18 100 2018-02-11 6 17 40 2018-02-18 7 19 50 2018-02-25 >>> df.resample('M', on='week_starting').mean() price volume week_starting 2018-01-31 10.75 62.5 2018-02-28 17.00 60.0 For a DataFrame with MultiIndex, the keyword `level` can be used to specify on which level the resampling needs to take place. >>> days = pd.date_range('1/1/2000', periods=4, freq='D') >>> d2 = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19], ... 'volume': [50, 60, 40, 100, 50, 100, 40, 50]}) >>> df2 = pd.DataFrame(d2, ... index=pd.MultiIndex.from_product([days, ... ['morning', ... 'afternoon']] ... )) >>> df2 price volume 2000-01-01 morning 10 50 afternoon 11 60 2000-01-02 morning 9 40 afternoon 13 100 2000-01-03 morning 14 50 afternoon 18 100 2000-01-04 morning 17 40 afternoon 19 50 >>> df2.resample('D', level=0).sum() price volume 2000-01-01 21 110 2000-01-02 22 140 2000-01-03 32 150 2000-01-04 36 90 If you want to adjust the start of the bins based on a fixed timestamp: >>> start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00' >>> rng = pd.date_range(start, end, freq='7min') >>> ts = pd.Series(np.arange(len(rng)) * 3, index=rng) >>> ts 2000-10-01 23:30:00 0 2000-10-01 23:37:00 3 2000-10-01 23:44:00 6 2000-10-01 23:51:00 9 2000-10-01 23:58:00 12 2000-10-02 00:05:00 15 2000-10-02 00:12:00 18 2000-10-02 00:19:00 21 2000-10-02 00:26:00 24 Freq: 7T, dtype: int64 >>> ts.resample('17min').sum() 2000-10-01 23:14:00 0 2000-10-01 23:31:00 9 2000-10-01 23:48:00 21 2000-10-02 00:05:00 54 2000-10-02 00:22:00 24 Freq: 17T, dtype: int64 >>> ts.resample('17min', origin='epoch').sum() 2000-10-01 23:18:00 0 2000-10-01 23:35:00 18 2000-10-01 23:52:00 27 2000-10-02 00:09:00 39 2000-10-02 00:26:00 24 Freq: 17T, dtype: int64 >>> ts.resample('17min', origin='2000-01-01').sum() 2000-10-01 23:24:00 3 2000-10-01 23:41:00 15 2000-10-01 23:58:00 45 2000-10-02 00:15:00 45 Freq: 17T, dtype: int64 If you want to adjust the start of the bins with an `offset` Timedelta, the two following lines are equivalent: >>> ts.resample('17min', origin='start').sum() 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17T, dtype: int64 >>> ts.resample('17min', offset='23h30min').sum() 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17T, dtype: int64 To replace the use of the deprecated `base` argument, you can now use `offset`, in this example it is equivalent to have `base=2`: >>> ts.resample('17min', offset='2min').sum() 2000-10-01 23:16:00 0 2000-10-01 23:33:00 9 2000-10-01 23:50:00 36 2000-10-02 00:07:00 39 2000-10-02 00:24:00 24 Freq: 17T, dtype: int64 To replace the use of the deprecated `loffset` argument: >>> from pandas.tseries.frequencies import to_offset >>> loffset = '19min' >>> ts_out = ts.resample('17min').sum() >>> ts_out.index = ts_out.index + to_offset(loffset) >>> ts_out 2000-10-01 23:33:00 0 2000-10-01 23:50:00 9 2000-10-02 00:07:00 21 2000-10-02 00:24:00 54 2000-10-02 00:41:00 24 Freq: 17T, dtype: int64 \"\"\" from pandas . core . resample import get_resampler axis = self . _get_axis_number ( axis ) return get_resampler ( self , freq = rule , label = label , closed = closed , axis = axis , kind = kind , loffset = loffset , convention = convention , base = base , key = on , level = level , origin = origin , offset = offset , ) reset_index def reset_index ( self , level : Union [ Hashable , Sequence [ Hashable ], NoneType ] = None , drop : bool = False , inplace : bool = False , col_level : Hashable = 0 , col_fill : Union [ Hashable , NoneType ] = '' ) -> Union [ ForwardRef ( 'DataFrame' ), NoneType ] Reset the index, or a level of it. Reset the index of the DataFrame, and use the default one instead. If the DataFrame has a MultiIndex, this method can remove one or more levels. Parameters level : int, str, tuple, or list, default None Only remove the given levels from the index. Removes all levels by default. drop : bool, default False Do not try to insert index into dataframe columns. This resets the index to the default integer index. inplace : bool, default False Modify the DataFrame in place (do not create a new object). col_level : int or str, default 0 If the columns have multiple levels, determines which level the labels are inserted into. By default it is inserted into the first level. col_fill : object, default '' If the columns have multiple levels, determines how the other levels are named. If None then the index name is repeated. Returns DataFrame or None DataFrame with the new index or None if inplace=True . See Also DataFrame.set_index : Opposite of reset_index. DataFrame.reindex : Change to new indices or expand indices. DataFrame.reindex_like : Change to same indices as other DataFrame. Examples df = pd.DataFrame([('bird', 389.0), ... ('bird', 24.0), ... ('mammal', 80.5), ... ('mammal', np.nan)], ... index=['falcon', 'parrot', 'lion', 'monkey'], ... columns=('class', 'max_speed')) df class max_speed falcon bird 389.0 parrot bird 24.0 lion mammal 80.5 monkey mammal NaN When we reset the index, the old index is added as a column, and a new sequential index is used: df.reset_index() index class max_speed 0 falcon bird 389.0 1 parrot bird 24.0 2 lion mammal 80.5 3 monkey mammal NaN We can use the drop parameter to avoid the old index being added as a column: df.reset_index(drop=True) class max_speed 0 bird 389.0 1 bird 24.0 2 mammal 80.5 3 mammal NaN You can also use reset_index with MultiIndex . index = pd.MultiIndex.from_tuples([('bird', 'falcon'), ... ('bird', 'parrot'), ... ('mammal', 'lion'), ... ('mammal', 'monkey')], ... names=['class', 'name']) columns = pd.MultiIndex.from_tuples([('speed', 'max'), ... ('species', 'type')]) df = pd.DataFrame([(389.0, 'fly'), ... ( 24.0, 'fly'), ... ( 80.5, 'run'), ... (np.nan, 'jump')], ... index=index, ... columns=columns) df speed species max type class name bird falcon 389.0 fly parrot 24.0 fly mammal lion 80.5 run monkey NaN jump If the index has multiple levels, we can reset a subset of them: df.reset_index(level='class') class speed species max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump If we are not dropping the index, by default, it is placed in the top level. We can place it in another level: df.reset_index(level='class', col_level=1) speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump When the index is inserted under another level, we can specify under which one with the parameter col_fill : df.reset_index(level='class', col_level=1, col_fill='species') species speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump If we specify a nonexistent level for col_fill , it is created: df.reset_index(level='class', col_level=1, col_fill='genus') genus speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump View Source def reset_index ( self , level : Optional [ Union[Hashable, Sequence[Hashable ] ]] = None , drop : bool = False , inplace : bool = False , col_level : Hashable = 0 , col_fill : Label = \"\" , ) -> Optional [ \"DataFrame\" ] : \"\"\" Reset the index, or a level of it. Reset the index of the DataFrame, and use the default one instead. If the DataFrame has a MultiIndex, this method can remove one or more levels. Parameters ---------- level : int, str, tuple, or list, default None Only remove the given levels from the index. Removes all levels by default. drop : bool, default False Do not try to insert index into dataframe columns. This resets the index to the default integer index. inplace : bool, default False Modify the DataFrame in place (do not create a new object). col_level : int or str, default 0 If the columns have multiple levels, determines which level the labels are inserted into. By default it is inserted into the first level. col_fill : object, default '' If the columns have multiple levels, determines how the other levels are named. If None then the index name is repeated. Returns ------- DataFrame or None DataFrame with the new index or None if ``inplace=True``. See Also -------- DataFrame.set_index : Opposite of reset_index. DataFrame.reindex : Change to new indices or expand indices. DataFrame.reindex_like : Change to same indices as other DataFrame. Examples -------- >>> df = pd.DataFrame([('bird', 389.0), ... ('bird', 24.0), ... ('mammal', 80.5), ... ('mammal', np.nan)], ... index=['falcon', 'parrot', 'lion', 'monkey'], ... columns=('class', 'max_speed')) >>> df class max_speed falcon bird 389.0 parrot bird 24.0 lion mammal 80.5 monkey mammal NaN When we reset the index, the old index is added as a column, and a new sequential index is used: >>> df.reset_index() index class max_speed 0 falcon bird 389.0 1 parrot bird 24.0 2 lion mammal 80.5 3 monkey mammal NaN We can use the `drop` parameter to avoid the old index being added as a column: >>> df.reset_index(drop=True) class max_speed 0 bird 389.0 1 bird 24.0 2 mammal 80.5 3 mammal NaN You can also use `reset_index` with `MultiIndex`. >>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'), ... ('bird', 'parrot'), ... ('mammal', 'lion'), ... ('mammal', 'monkey')], ... names=['class', 'name']) >>> columns = pd.MultiIndex.from_tuples([('speed', 'max'), ... ('species', 'type')]) >>> df = pd.DataFrame([(389.0, 'fly'), ... ( 24.0, 'fly'), ... ( 80.5, 'run'), ... (np.nan, 'jump')], ... index=index, ... columns=columns) >>> df speed species max type class name bird falcon 389.0 fly parrot 24.0 fly mammal lion 80.5 run monkey NaN jump If the index has multiple levels, we can reset a subset of them: >>> df.reset_index(level='class') class speed species max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump If we are not dropping the index, by default, it is placed in the top level. We can place it in another level: >>> df.reset_index(level='class', col_level=1) speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump When the index is inserted under another level, we can specify under which one with the parameter `col_fill`: >>> df.reset_index(level='class', col_level=1, col_fill='species') species speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump If we specify a nonexistent level for `col_fill`, it is created: >>> df.reset_index(level='class', col_level=1, col_fill='genus') genus speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump \"\"\" inplace = validate_bool_kwarg ( inplace , \"inplace\" ) if inplace : new_obj = self else : new_obj = self . copy () def _maybe_casted_values ( index , labels = None ) : values = index . _values if not isinstance ( index , ( PeriodIndex , DatetimeIndex )) : if values . dtype == np . object_ : values = lib . maybe_convert_objects ( values ) # if we have the labels , extract the values with a mask if labels is not None : mask = labels == - 1 # we can have situations where the whole mask is - 1 , # meaning there is nothing found in labels , so make all nan 's if mask.size > 0 and mask.all(): dtype = index.dtype fill_value = na_value_for_dtype(dtype) values = construct_1d_arraylike_from_scalar( fill_value, len(mask), dtype ) else: values = values.take(labels) # TODO(https://github.com/pandas-dev/pandas/issues/24206) # Push this into maybe_upcast_putmask? # We can' t pass EAs there right now . Looks a bit # complicated . # So we unbox the ndarray_values , op , re - box . values_type = type ( values ) values_dtype = values . dtype if issubclass ( values_type , DatetimeLikeArray ) : values = values . _data # TODO : can we de - kludge yet ? if mask . any () : values , _ = maybe_upcast_putmask ( values , mask , np . nan ) if issubclass ( values_type , DatetimeLikeArray ) : values = values_type ( values , dtype = values_dtype ) return values new_index = ibase . default_index ( len ( new_obj )) if level is not None : if not isinstance ( level , ( tuple , list )) : level = [ level ] level = [ self.index._get_level_number(lev) for lev in level ] if len ( level ) < self . index . nlevels : new_index = self . index . droplevel ( level ) if not drop : to_insert : Iterable [ Tuple[Any, Optional[Any ] ]] if isinstance ( self . index , MultiIndex ) : names = [ (n if n is not None else f\"level_{i}\") for i, n in enumerate(self.index.names) ] to_insert = zip ( self . index . levels , self . index . codes ) else : default = \"index\" if \"index\" not in self else \"level_0\" names = [ default ] if self . index . name is None else [ self.index.name ] to_insert = (( self . index , None ),) multi_col = isinstance ( self . columns , MultiIndex ) for i , ( lev , lab ) in reversed ( list ( enumerate ( to_insert ))) : if not ( level is None or i in level ) : continue name = names [ i ] if multi_col : col_name = list ( name ) if isinstance ( name , tuple ) else [ name ] if col_fill is None : if len ( col_name ) not in ( 1 , self . columns . nlevels ) : raise ValueError ( \"col_fill=None is incompatible \" f \"with incomplete column name {name}\" ) col_fill = col_name [ 0 ] lev_num = self . columns . _get_level_number ( col_level ) name_lst = [ col_fill ] * lev_num + col_name missing = self . columns . nlevels - len ( name_lst ) name_lst += [ col_fill ] * missing name = tuple ( name_lst ) # to ndarray and maybe infer different dtype level_values = _maybe_casted_values ( lev , lab ) new_obj . insert ( 0 , name , level_values ) new_obj . index = new_index if not inplace : return new_obj return None rfloordiv def rfloordiv ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Integer division of dataframe and other, element-wise (binary operator rfloordiv ). Equivalent to other // dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, floordiv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) rmod def rmod ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Modulo of dataframe and other, element-wise (binary operator rmod ). Equivalent to other % dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, mod . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) rmul def rmul ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Multiplication of dataframe and other, element-wise (binary operator rmul ). Equivalent to other * dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, mul . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) rolling def rolling ( self , window , min_periods = None , center = False , win_type = None , on = None , axis = 0 , closed = None ) Provide rolling window calculations. Parameters window : int, offset, or BaseIndexer subclass Size of the moving window. This is the number of observations used for calculating the statistic. Each window will be a fixed size. If its an offset then this will be the time period of each window . Each window will be a variable sized based on the observations included in the time - period . This is only valid for datetimelike indexes . If a BaseIndexer subclass is passed , calculates the window boundaries based on the defined `` get_window_bounds `` method . Additional rolling keyword arguments , namely `min_periods` , `center` , and `closed` will be passed to `get_window_bounds` . min_periods : int, default None Minimum number of observations in window required to have a value (otherwise result is NA). For a window that is specified by an offset, min_periods will default to 1. Otherwise, min_periods will default to the size of the window. center : bool, default False Set the labels at the center of the window. win_type : str, default None Provide a window type. If None , all points are evenly weighted. See the notes below for further information. on : str, optional For a DataFrame, a datetime-like column or MultiIndex level on which to calculate the rolling window, rather than the DataFrame's index. Provided integer column is ignored and excluded from result since an integer index is not used to calculate the rolling window. axis : int or str, default 0 closed : str, default None Make the interval closed on the 'right', 'left', 'both' or 'neither' endpoints. For offset-based windows, it defaults to 'right'. For fixed windows, defaults to 'both'. Remaining cases not implemented for fixed windows. Returns a Window or Rolling sub-classed for the particular operation See Also expanding : Provides expanding transformations. ewm : Provides exponential weighted functions. Notes By default, the result is set to the right edge of the window. This can be changed to the center of the window by setting center=True . To learn more about the offsets & frequency strings, please see this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases> __. The recognized win_types are: boxcar triang blackman hamming bartlett parzen bohman blackmanharris nuttall barthann kaiser (needs parameter: beta) gaussian (needs parameter: std) general_gaussian (needs parameters: power, width) slepian (needs parameter: width) exponential (needs parameter: tau), center is set to None. If win_type=None all points are evenly weighted. To learn more about different window types see scipy.signal window functions <https://docs.scipy.org/doc/scipy/reference/signal.html#window-functions> __. Certain window types require additional parameters to be passed. Please see the third example below on how to add the additional parameters. Examples df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]}) df B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 Rolling sum with a window length of 2, using the 'triang' window type. df.rolling(2, win_type='triang').sum() B 0 NaN 1 0.5 2 1.5 3 NaN 4 NaN Rolling sum with a window length of 2, using the 'gaussian' window type (note how we need to specify std). df.rolling(2, win_type='gaussian').sum(std=3) B 0 NaN 1 0.986207 2 2.958621 3 NaN 4 NaN Rolling sum with a window length of 2, min_periods defaults to the window length. df.rolling(2).sum() B 0 NaN 1 1.0 2 3.0 3 NaN 4 NaN Same as above, but explicitly set the min_periods df.rolling(2, min_periods=1).sum() B 0 0.0 1 1.0 2 3.0 3 2.0 4 4.0 Same as above, but with forward-looking windows indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=2) df.rolling(window=indexer, min_periods=1).sum() B 0 1.0 1 3.0 2 2.0 3 4.0 4 4.0 A ragged (meaning not-a-regular frequency), time-indexed DataFrame df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]}, ... index = [pd.Timestamp('20130101 09:00:00'), ... pd.Timestamp('20130101 09:00:02'), ... pd.Timestamp('20130101 09:00:03'), ... pd.Timestamp('20130101 09:00:05'), ... pd.Timestamp('20130101 09:00:06')]) df B 2013-01-01 09:00:00 0.0 2013-01-01 09:00:02 1.0 2013-01-01 09:00:03 2.0 2013-01-01 09:00:05 NaN 2013-01-01 09:00:06 4.0 Contrasting to an integer rolling window, this will roll a variable length window corresponding to the time period. The default for min_periods is 1. df.rolling('2s').sum() B 2013-01-01 09:00:00 0.0 2013-01-01 09:00:02 1.0 2013-01-01 09:00:03 3.0 2013-01-01 09:00:05 NaN 2013-01-01 09:00:06 4.0 View Source @doc ( Rolling ) def rolling ( self , window , min_periods = None , center = False , win_type = None , on = None , axis = 0 , closed = None , ) : axis = self . _get_axis_number ( axis ) if win_type is not None : return Window ( self , window = window , min_periods = min_periods , center = center , win_type = win_type , on = on , axis = axis , closed = closed , ) return Rolling ( self , window = window , min_periods = min_periods , center = center , win_type = win_type , on = on , axis = axis , closed = closed , ) round def round ( self , decimals = 0 , * args , ** kwargs ) -> 'DataFrame' Round a DataFrame to a variable number of decimal places. Parameters decimals : int, dict, Series Number of decimal places to round each column to. If an int is given, round each column to the same number of places. Otherwise dict and Series round to variable numbers of places. Column names should be in the keys if decimals is a dict-like, or in the index if decimals is a Series. Any columns not included in decimals will be left as is. Elements of decimals which are not columns of the input will be ignored. args Additional keywords have no effect but might be accepted for compatibility with numpy. *kwargs Additional keywords have no effect but might be accepted for compatibility with numpy. Returns DataFrame A DataFrame with the affected columns rounded to the specified number of decimal places. See Also numpy.around : Round a numpy array to the given number of decimals. Series.round : Round a Series to the given number of decimals. Examples df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)], ... columns=['dogs', 'cats']) df dogs cats 0 0.21 0.32 1 0.01 0.67 2 0.66 0.03 3 0.21 0.18 By providing an integer each column is rounded to the same number of decimal places df.round(1) dogs cats 0 0.2 0.3 1 0.0 0.7 2 0.7 0.0 3 0.2 0.2 With a dict, the number of places for specific columns can be specified with the column names as key and the number of decimal places as value df.round({'dogs': 1, 'cats': 0}) dogs cats 0 0.2 0.0 1 0.0 1.0 2 0.7 0.0 3 0.2 0.0 Using a Series, the number of places for specific columns can be specified with the column names as index and the number of decimal places as value decimals = pd.Series([0, 1], index=['cats', 'dogs']) df.round(decimals) dogs cats 0 0.2 0.0 1 0.0 1.0 2 0.7 0.0 3 0.2 0.0 View Source def round ( self , decimals = 0 , * args , ** kwargs ) -> \"DataFrame\" : \"\"\" Round a DataFrame to a variable number of decimal places. Parameters ---------- decimals : int, dict, Series Number of decimal places to round each column to. If an int is given, round each column to the same number of places. Otherwise dict and Series round to variable numbers of places. Column names should be in the keys if `decimals` is a dict-like, or in the index if `decimals` is a Series. Any columns not included in `decimals` will be left as is. Elements of `decimals` which are not columns of the input will be ignored. *args Additional keywords have no effect but might be accepted for compatibility with numpy. **kwargs Additional keywords have no effect but might be accepted for compatibility with numpy. Returns ------- DataFrame A DataFrame with the affected columns rounded to the specified number of decimal places. See Also -------- numpy.around : Round a numpy array to the given number of decimals. Series.round : Round a Series to the given number of decimals. Examples -------- >>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)], ... columns=['dogs', 'cats']) >>> df dogs cats 0 0.21 0.32 1 0.01 0.67 2 0.66 0.03 3 0.21 0.18 By providing an integer each column is rounded to the same number of decimal places >>> df.round(1) dogs cats 0 0.2 0.3 1 0.0 0.7 2 0.7 0.0 3 0.2 0.2 With a dict, the number of places for specific columns can be specified with the column names as key and the number of decimal places as value >>> df.round({'dogs': 1, 'cats': 0}) dogs cats 0 0.2 0.0 1 0.0 1.0 2 0.7 0.0 3 0.2 0.0 Using a Series, the number of places for specific columns can be specified with the column names as index and the number of decimal places as value >>> decimals = pd.Series([0, 1], index=['cats', 'dogs']) >>> df.round(decimals) dogs cats 0 0.2 0.0 1 0.0 1.0 2 0.7 0.0 3 0.2 0.0 \"\"\" from pandas . core . reshape . concat import concat def _dict_round ( df , decimals ): for col , vals in df . items (): try : yield _series_round ( vals , decimals [ col ]) except KeyError : yield vals def _series_round ( s , decimals ): if is_integer_dtype ( s ) or is_float_dtype ( s ): return s . round ( decimals ) return s nv . validate_round ( args , kwargs ) if isinstance ( decimals , ( dict , Series )): if isinstance ( decimals , Series ): if not decimals . index . is_unique : raise ValueError ( \"Index of decimals must be unique\" ) new_cols = list ( _dict_round ( self , decimals )) elif is_integer ( decimals ): # Dispatch to Series.round new_cols = [ _series_round ( v , decimals ) for _ , v in self . items ()] else : raise TypeError ( \"decimals must be an integer, a dict-like or a Series\" ) if len ( new_cols ) > 0 : return self . _constructor ( concat ( new_cols , axis = 1 ), index = self . index , columns = self . columns ) else : return self rpow def rpow ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Exponential power of dataframe and other, element-wise (binary operator rpow ). Equivalent to other ** dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, pow . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) rsub def rsub ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Subtraction of dataframe and other, element-wise (binary operator rsub ). Equivalent to other - dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, sub . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) rtruediv def rtruediv ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Floating division of dataframe and other, element-wise (binary operator rtruediv ). Equivalent to other / dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, truediv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) sample def sample ( self : ~ FrameOrSeries , n = None , frac = None , replace = False , weights = None , random_state = None , axis = None ) -> ~ FrameOrSeries Return a random sample of items from an axis of object. You can use random_state for reproducibility. Parameters n : int, optional Number of items from axis to return. Cannot be used with frac . Default = 1 if frac = None. frac : float, optional Fraction of axis items to return. Cannot be used with n . replace : bool, default False Allow or disallow sampling of the same row more than once. weights : str or ndarray-like, optional Default 'None' results in equal probability weighting. If passed a Series, will align with target object on index. Index values in weights not found in sampled object will be ignored and index values in sampled object not in weights will be assigned weights of zero. If called on a DataFrame, will accept the name of a column when axis = 0. Unless weights are a Series, weights must be same length as axis being sampled. If weights do not sum to 1, they will be normalized to sum to 1. Missing values in the weights column will be treated as zero. Infinite values not allowed. random_state : int, array-like, BitGenerator, np.random.RandomState, optional If int, array-like, or BitGenerator (NumPy>=1.17), seed for random number generator If np.random.RandomState, use as numpy RandomState object. .. versionchanged :: 1.1.0 array-like and BitGenerator (for NumPy>=1.17) object now passed to np.random.RandomState() as seed axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None Axis to sample. Accepts axis number or name. Default is stat axis for given data type (0 for Series and DataFrames). Returns Series or DataFrame A new object of same type as caller containing n items randomly sampled from the caller object. See Also DataFrameGroupBy.sample: Generates random samples from each group of a DataFrame object. SeriesGroupBy.sample: Generates random samples from each group of a Series object. numpy.random.choice: Generates a random sample from a given 1-D numpy array. Notes If frac > 1, replacement should be set to True . Examples df = pd.DataFrame({'num_legs': [2, 4, 8, 0], ... 'num_wings': [2, 0, 0, 0], ... 'num_specimen_seen': [10, 2, 1, 8]}, ... index=['falcon', 'dog', 'spider', 'fish']) df num_legs num_wings num_specimen_seen falcon 2 2 10 dog 4 0 2 spider 8 0 1 fish 0 0 8 Extract 3 random elements from the Series df['num_legs'] : Note that we use random_state to ensure the reproducibility of the examples. df['num_legs'].sample(n=3, random_state=1) fish 0 spider 8 falcon 2 Name: num_legs, dtype: int64 A random 50% sample of the DataFrame with replacement: df.sample(frac=0.5, replace=True, random_state=1) num_legs num_wings num_specimen_seen dog 4 0 2 fish 0 0 8 An upsample sample of the DataFrame with replacement: Note that replace parameter has to be True for frac parameter > 1. df.sample(frac=2, replace=True, random_state=1) num_legs num_wings num_specimen_seen dog 4 0 2 fish 0 0 8 falcon 2 2 10 falcon 2 2 10 fish 0 0 8 dog 4 0 2 fish 0 0 8 dog 4 0 2 Using a DataFrame column as weights. Rows with larger value in the num_specimen_seen column are more likely to be sampled. df.sample(n=2, weights='num_specimen_seen', random_state=1) num_legs num_wings num_specimen_seen falcon 2 2 10 fish 0 0 8 View Source def sample ( self : FrameOrSeries , n = None , frac = None , replace = False , weights = None , random_state = None , axis = None , ) -> FrameOrSeries : \"\"\" Return a random sample of items from an axis of object. You can use `random_state` for reproducibility. Parameters ---------- n : int, optional Number of items from axis to return. Cannot be used with `frac`. Default = 1 if `frac` = None. frac : float, optional Fraction of axis items to return. Cannot be used with `n`. replace : bool, default False Allow or disallow sampling of the same row more than once. weights : str or ndarray-like, optional Default 'None' results in equal probability weighting. If passed a Series, will align with target object on index. Index values in weights not found in sampled object will be ignored and index values in sampled object not in weights will be assigned weights of zero. If called on a DataFrame, will accept the name of a column when axis = 0. Unless weights are a Series, weights must be same length as axis being sampled. If weights do not sum to 1, they will be normalized to sum to 1. Missing values in the weights column will be treated as zero. Infinite values not allowed. random_state : int, array-like, BitGenerator, np.random.RandomState, optional If int, array-like, or BitGenerator (NumPy>=1.17), seed for random number generator If np.random.RandomState, use as numpy RandomState object. .. versionchanged:: 1.1.0 array-like and BitGenerator (for NumPy>=1.17) object now passed to np.random.RandomState() as seed axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None Axis to sample. Accepts axis number or name. Default is stat axis for given data type (0 for Series and DataFrames). Returns ------- Series or DataFrame A new object of same type as caller containing `n` items randomly sampled from the caller object. See Also -------- DataFrameGroupBy.sample: Generates random samples from each group of a DataFrame object. SeriesGroupBy.sample: Generates random samples from each group of a Series object. numpy.random.choice: Generates a random sample from a given 1-D numpy array. Notes ----- If `frac` > 1, `replacement` should be set to `True`. Examples -------- >>> df = pd.DataFrame({'num_legs': [2, 4, 8, 0], ... 'num_wings': [2, 0, 0, 0], ... 'num_specimen_seen': [10, 2, 1, 8]}, ... index=['falcon', 'dog', 'spider', 'fish']) >>> df num_legs num_wings num_specimen_seen falcon 2 2 10 dog 4 0 2 spider 8 0 1 fish 0 0 8 Extract 3 random elements from the ``Series`` ``df['num_legs']``: Note that we use `random_state` to ensure the reproducibility of the examples. >>> df['num_legs'].sample(n=3, random_state=1) fish 0 spider 8 falcon 2 Name: num_legs, dtype: int64 A random 50% sample of the ``DataFrame`` with replacement: >>> df.sample(frac=0.5, replace=True, random_state=1) num_legs num_wings num_specimen_seen dog 4 0 2 fish 0 0 8 An upsample sample of the ``DataFrame`` with replacement: Note that `replace` parameter has to be `True` for `frac` parameter > 1. >>> df.sample(frac=2, replace=True, random_state=1) num_legs num_wings num_specimen_seen dog 4 0 2 fish 0 0 8 falcon 2 2 10 falcon 2 2 10 fish 0 0 8 dog 4 0 2 fish 0 0 8 dog 4 0 2 Using a DataFrame column as weights. Rows with larger value in the `num_specimen_seen` column are more likely to be sampled. >>> df.sample(n=2, weights='num_specimen_seen', random_state=1) num_legs num_wings num_specimen_seen falcon 2 2 10 fish 0 0 8 \"\"\" if axis is None : axis = self . _stat_axis_number axis = self . _get_axis_number ( axis ) axis_length = self . shape [ axis ] # Process random_state argument rs = com . random_state ( random_state ) # Check weights for compliance if weights is not None : # If a series, align with frame if isinstance ( weights , ABCSeries ): weights = weights . reindex ( self . axes [ axis ]) # Strings acceptable if a dataframe and axis = 0 if isinstance ( weights , str ): if isinstance ( self , ABCDataFrame ): if axis == 0 : try : weights = self [ weights ] except KeyError as err : raise KeyError ( \"String passed to weights not a valid column\" ) from err else : raise ValueError ( \"Strings can only be passed to \" \"weights when sampling from rows on \" \"a DataFrame\" ) else : raise ValueError ( \"Strings cannot be passed as weights \" \"when sampling from a Series.\" ) weights = pd . Series ( weights , dtype = \"float64\" ) if len ( weights ) != axis_length : raise ValueError ( \"Weights and axis to be sampled must be of same length\" ) if ( weights == np . inf ). any () or ( weights == - np . inf ). any (): raise ValueError ( \"weight vector may not include `inf` values\" ) if ( weights < 0 ). any (): raise ValueError ( \"weight vector many not include negative values\" ) # If has nan, set to zero. weights = weights . fillna ( 0 ) # Renormalize if don't sum to 1 if weights . sum () != 1 : if weights . sum () != 0 : weights = weights / weights . sum () else : raise ValueError ( \"Invalid weights: weights sum to zero\" ) weights = weights . _values # If no frac or n, default to n=1. if n is None and frac is None : n = 1 elif frac is not None and frac > 1 and not replace : raise ValueError ( \"Replace has to be set to `True` when \" \"upsampling the population `frac` > 1.\" ) elif n is not None and frac is None and n % 1 != 0 : raise ValueError ( \"Only integers accepted as `n` values\" ) elif n is None and frac is not None : n = int ( round ( frac * axis_length )) elif n is not None and frac is not None : raise ValueError ( \"Please enter a value for `frac` OR `n`, not both\" ) # Check for negative sizes if n < 0 : raise ValueError ( \"A negative number of rows requested. Please provide positive value.\" ) locs = rs . choice ( axis_length , size = n , replace = replace , p = weights ) return self . take ( locs , axis = axis ) save def save ( self ) View Source def save ( self ): try : self . lock . acquire () try : self . to_csv ( self . csv , header = None , sep = \";\" ) self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e select_dtypes def select_dtypes ( self , include = None , exclude = None ) -> 'DataFrame' Return a subset of the DataFrame's columns based on the column dtypes. Parameters include, exclude : scalar or list-like A selection of dtypes or strings to be included/excluded. At least one of these parameters must be supplied. Returns DataFrame The subset of the frame including the dtypes in include and excluding the dtypes in exclude . Raises ValueError * If both of include and exclude are empty * If include and exclude have overlapping elements * If any kind of string dtype is passed in. See Also DataFrame.dtypes: Return Series with the data type of each column. Notes To select all numeric types, use np.number or 'number' To select strings you must use the object dtype, but note that this will return all object dtype columns See the numpy dtype hierarchy <https://numpy.org/doc/stable/reference/arrays.scalars.html> __ To select datetimes, use np.datetime64 , 'datetime' or 'datetime64' To select timedeltas, use np.timedelta64 , 'timedelta' or 'timedelta64' To select Pandas categorical dtypes, use 'category' To select Pandas datetimetz dtypes, use 'datetimetz' (new in 0.20.0) or 'datetime64[ns, tz]' Examples df = pd.DataFrame({'a': [1, 2] * 3, ... 'b': [True, False] * 3, ... 'c': [1.0, 2.0] * 3}) df a b c 0 1 True 1.0 1 2 False 2.0 2 1 True 1.0 3 2 False 2.0 4 1 True 1.0 5 2 False 2.0 df.select_dtypes(include='bool') b 0 True 1 False 2 True 3 False 4 True 5 False df.select_dtypes(include=['float64']) c 0 1.0 1 2.0 2 1.0 3 2.0 4 1.0 5 2.0 df.select_dtypes(exclude=['int64']) b c 0 True 1.0 1 False 2.0 2 True 1.0 3 False 2.0 4 True 1.0 5 False 2.0 View Source def select_dtypes ( self , include = None , exclude = None ) -> \"DataFrame\" : \"\"\" Return a subset of the DataFrame's columns based on the column dtypes. Parameters ---------- include, exclude : scalar or list-like A selection of dtypes or strings to be included/excluded. At least one of these parameters must be supplied. Returns ------- DataFrame The subset of the frame including the dtypes in ``include`` and excluding the dtypes in ``exclude``. Raises ------ ValueError * If both of ``include`` and ``exclude`` are empty * If ``include`` and ``exclude`` have overlapping elements * If any kind of string dtype is passed in. See Also -------- DataFrame.dtypes: Return Series with the data type of each column. Notes ----- * To select all *numeric* types, use ``np.number`` or ``'number'`` * To select strings you must use the ``object`` dtype, but note that this will return *all* object dtype columns * See the `numpy dtype hierarchy <https://numpy.org/doc/stable/reference/arrays.scalars.html>`__ * To select datetimes, use ``np.datetime64``, ``'datetime'`` or ``'datetime64'`` * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or ``'timedelta64'`` * To select Pandas categorical dtypes, use ``'category'`` * To select Pandas datetimetz dtypes, use ``'datetimetz'`` (new in 0.20.0) or ``'datetime64[ns, tz]'`` Examples -------- >>> df = pd.DataFrame({'a': [1, 2] * 3, ... 'b': [True, False] * 3, ... 'c': [1.0, 2.0] * 3}) >>> df a b c 0 1 True 1.0 1 2 False 2.0 2 1 True 1.0 3 2 False 2.0 4 1 True 1.0 5 2 False 2.0 >>> df.select_dtypes(include='bool') b 0 True 1 False 2 True 3 False 4 True 5 False >>> df.select_dtypes(include=['float64']) c 0 1.0 1 2.0 2 1.0 3 2.0 4 1.0 5 2.0 >>> df.select_dtypes(exclude=['int64']) b c 0 True 1.0 1 False 2.0 2 True 1.0 3 False 2.0 4 True 1.0 5 False 2.0 \"\"\" if not is_list_like ( include ): include = ( include ,) if include is not None else () if not is_list_like ( exclude ): exclude = ( exclude ,) if exclude is not None else () selection = ( frozenset ( include ), frozenset ( exclude )) if not any ( selection ): raise ValueError ( \"at least one of include or exclude must be nonempty\" ) # convert the myriad valid dtypes object to a single representation include = frozenset ( infer_dtype_from_object ( x ) for x in include ) exclude = frozenset ( infer_dtype_from_object ( x ) for x in exclude ) for dtypes in ( include , exclude ): invalidate_string_dtypes ( dtypes ) # can't both include AND exclude! if not include . isdisjoint ( exclude ): raise ValueError ( f \"include and exclude overlap on {(include & exclude)}\" ) # We raise when both include and exclude are empty # Hence, we can just shrink the columns we want to keep keep_these = np . full ( self . shape [ 1 ], True ) def extract_unique_dtypes_from_dtypes_set ( dtypes_set : FrozenSet [ Dtype ], unique_dtypes : np . ndarray ) -> List [ Dtype ]: extracted_dtypes = [ unique_dtype for unique_dtype in unique_dtypes if issubclass ( unique_dtype . type , tuple ( dtypes_set )) # type: ignore ] return extracted_dtypes unique_dtypes = self . dtypes . unique () if include : included_dtypes = extract_unique_dtypes_from_dtypes_set ( include , unique_dtypes ) keep_these &= self . dtypes . isin ( included_dtypes ) if exclude : excluded_dtypes = extract_unique_dtypes_from_dtypes_set ( exclude , unique_dtypes ) keep_these &= ~ self . dtypes . isin ( excluded_dtypes ) return self . iloc [:, keep_these . values ] sem def sem ( self , axis = None , skipna = None , level = None , ddof = 1 , numeric_only = None , ** kwargs ) Return unbiased standard error of the mean over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument Parameters axis : {index (0), columns (1)} skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. ddof : int, default 1 Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. Returns Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr ) @Appender ( _num_ddof_doc ) def stat_func ( self , axis = None , skipna = None , level = None , ddof = 1 , numeric_only = None , ** kwargs ) : nv . validate_stat_ddof_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna , ddof = ddof ) return self . _reduce ( func , name , axis = axis , numeric_only = numeric_only , skipna = skipna , ddof = ddof ) set_axis def set_axis ( self , labels , axis : Union [ str , int ] = 0 , inplace : bool = False ) Assign desired index to given axis. Indexes for column or row labels can be changed by assigning a list-like or Index. Parameters labels : list-like, Index The values for the new index. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to update. The value 0 identifies the rows, and 1 identifies the columns. inplace : bool, default False Whether to return a new DataFrame instance. Returns renamed : DataFrame or None An object of type DataFrame if inplace=False, None otherwise. See Also DataFrame.rename_axis : Alter the name of the index or columns. Examples -------- >>> df = pd . DataFrame ( { \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ] } ) Change the row labels . >>> df . set_axis ([ 'a' , 'b' , 'c' ], axis = 'index' ) A B a 1 4 b 2 5 c 3 6 Change the column labels . >>> df . set_axis ([ 'I' , 'II' ], axis = 'columns' ) I II 0 1 4 1 2 5 2 3 6 Now , update the labels inplace . >>> df . set_axis ([ 'i' , 'ii' ], axis = 'columns' , inplace = True ) >>> df i ii 0 1 4 1 2 5 2 3 6 View Source @Appender ( \"\"\" Examples -------- >>> df = pd.DataFrame({\" A \": [1, 2, 3], \" B \": [4, 5, 6]}) Change the row labels. >>> df.set_axis(['a', 'b', 'c'], axis='index') A B a 1 4 b 2 5 c 3 6 Change the column labels. >>> df.set_axis(['I', 'II'], axis='columns') I II 0 1 4 1 2 5 2 3 6 Now, update the labels inplace. >>> df.set_axis(['i', 'ii'], axis='columns', inplace=True) >>> df i ii 0 1 4 1 2 5 2 3 6 \"\"\" ) @Substitution ( ** _shared_doc_kwargs , extended_summary_sub = \" column or\" , axis_description_sub = \", and 1 identifies the columns\" , see_also_sub = \" or columns\" , ) @Appender ( NDFrame . set_axis . __doc__ ) def set_axis ( self , labels , axis : Axis = 0 , inplace : bool = False ) : return super (). set_axis ( labels , axis = axis , inplace = inplace ) set_index def set_index ( self , keys , drop = True , append = False , inplace = False , verify_integrity = False ) Set the DataFrame index using existing columns. Set the DataFrame index (row labels) using one or more existing columns or arrays (of the correct length). The index can replace the existing index or expand on it. Parameters keys : label or array-like or list of labels/arrays This parameter can be either a single column key, a single array of the same length as the calling DataFrame, or a list containing an arbitrary combination of column keys and arrays. Here, \"array\" encompasses :class: Series , :class: Index , np.ndarray , and instances of :class: ~collections.abc.Iterator . drop : bool, default True Delete columns to be used as the new index. append : bool, default False Whether to append columns to existing index. inplace : bool, default False Modify the DataFrame in place (do not create a new object). verify_integrity : bool, default False Check the new index for duplicates. Otherwise defer the check until necessary. Setting to False will improve the performance of this method. Returns DataFrame Changed row labels. See Also DataFrame.reset_index : Opposite of set_index. DataFrame.reindex : Change to new indices or expand indices. DataFrame.reindex_like : Change to same indices as other DataFrame. Examples df = pd.DataFrame({'month': [1, 4, 7, 10], ... 'year': [2012, 2014, 2013, 2014], ... 'sale': [55, 40, 84, 31]}) df month year sale 0 1 2012 55 1 4 2014 40 2 7 2013 84 3 10 2014 31 Set the index to become the 'month' column: df.set_index('month') year sale month 1 2012 55 4 2014 40 7 2013 84 10 2014 31 Create a MultiIndex using columns 'year' and 'month': df.set_index(['year', 'month']) sale year month 2012 1 55 2014 4 40 2013 7 84 2014 10 31 Create a MultiIndex using an Index and a column: df.set_index([pd.Index([1, 2, 3, 4]), 'year']) month sale year 1 2012 1 55 2 2014 4 40 3 2013 7 84 4 2014 10 31 Create a MultiIndex using two Series: s = pd.Series([1, 2, 3, 4]) df.set_index([s, s**2]) month year sale 1 1 1 2012 55 2 4 4 2014 40 3 9 7 2013 84 4 16 10 2014 31 View Source def set_index ( self , keys , drop = True , append = False , inplace = False , verify_integrity = False ) : \"\"\" Set the DataFrame index using existing columns. Set the DataFrame index (row labels) using one or more existing columns or arrays (of the correct length). The index can replace the existing index or expand on it. Parameters ---------- keys : label or array-like or list of labels/arrays This parameter can be either a single column key, a single array of the same length as the calling DataFrame, or a list containing an arbitrary combination of column keys and arrays. Here, \" array \" encompasses :class:`Series`, :class:`Index`, ``np.ndarray``, and instances of :class:`~collections.abc.Iterator`. drop : bool, default True Delete columns to be used as the new index. append : bool, default False Whether to append columns to existing index. inplace : bool, default False Modify the DataFrame in place (do not create a new object). verify_integrity : bool, default False Check the new index for duplicates. Otherwise defer the check until necessary. Setting to False will improve the performance of this method. Returns ------- DataFrame Changed row labels. See Also -------- DataFrame.reset_index : Opposite of set_index. DataFrame.reindex : Change to new indices or expand indices. DataFrame.reindex_like : Change to same indices as other DataFrame. Examples -------- >>> df = pd.DataFrame({'month': [1, 4, 7, 10], ... 'year': [2012, 2014, 2013, 2014], ... 'sale': [55, 40, 84, 31]}) >>> df month year sale 0 1 2012 55 1 4 2014 40 2 7 2013 84 3 10 2014 31 Set the index to become the 'month' column: >>> df.set_index('month') year sale month 1 2012 55 4 2014 40 7 2013 84 10 2014 31 Create a MultiIndex using columns 'year' and 'month': >>> df.set_index(['year', 'month']) sale year month 2012 1 55 2014 4 40 2013 7 84 2014 10 31 Create a MultiIndex using an Index and a column: >>> df.set_index([pd.Index([1, 2, 3, 4]), 'year']) month sale year 1 2012 1 55 2 2014 4 40 3 2013 7 84 4 2014 10 31 Create a MultiIndex using two Series: >>> s = pd.Series([1, 2, 3, 4]) >>> df.set_index([s, s**2]) month year sale 1 1 1 2012 55 2 4 4 2014 40 3 9 7 2013 84 4 16 10 2014 31 \"\"\" inplace = validate_bool_kwarg ( inplace , \"inplace\" ) if not isinstance ( keys , list ) : keys = [ keys ] err_msg = ( 'The parameter \"keys\" may be a column key, one-dimensional ' \"array, or a list containing only valid column keys and \" \"one-dimensional arrays.\" ) missing : List [ Label ] = [] for col in keys : if isinstance ( col , ( Index , Series , np . ndarray , list , abc . Iterator )) : # arrays are fine as long as they are one - dimensional # iterators get converted to list below if getattr ( col , \"ndim\" , 1 ) != 1 : raise ValueError ( err_msg ) else : # everything else gets tried as a key ; see GH 24969 try : found = col in self . columns except TypeError as err : raise TypeError ( f \"{err_msg}. Received column of type {type(col)}\" ) from err else : if not found : missing . append ( col ) if missing : raise KeyError ( f \"None of {missing} are in the columns\" ) if inplace : frame = self else : frame = self . copy () arrays = [] names = [] if append : names = list ( self . index . names ) if isinstance ( self . index , MultiIndex ) : for i in range ( self . index . nlevels ) : arrays . append ( self . index . _get_level_values ( i )) else : arrays . append ( self . index ) to_remove : List [ Label ] = [] for col in keys : if isinstance ( col , MultiIndex ) : for n in range ( col . nlevels ) : arrays . append ( col . _get_level_values ( n )) names . extend ( col . names ) elif isinstance ( col , ( Index , Series )) : # if Index then not MultiIndex ( treated above ) arrays . append ( col ) names . append ( col . name ) elif isinstance ( col , ( list , np . ndarray )) : arrays . append ( col ) names . append ( None ) elif isinstance ( col , abc . Iterator ) : arrays . append ( list ( col )) names . append ( None ) # from here , col can only be a column label else : arrays . append ( frame [ col ] . _values ) names . append ( col ) if drop : to_remove . append ( col ) if len ( arrays [ -1 ] ) != len ( self ) : # check newest element against length of calling frame , since # ensure_index_from_sequences would not raise for append = False . raise ValueError ( f \"Length mismatch: Expected {len(self)} rows, \" f \"received array of length {len(arrays[-1])}\" ) index = ensure_index_from_sequences ( arrays , names ) if verify_integrity and not index . is_unique : duplicates = index [ index.duplicated() ] . unique () raise ValueError ( f \"Index has duplicate keys: {duplicates}\" ) # use set to handle duplicate column names gracefully in case of drop for c in set ( to_remove ) : del frame [ c ] # clear up memory usage index . _cleanup () frame . index = index if not inplace : return frame shift def shift ( self , periods = 1 , freq = None , axis = 0 , fill_value = None ) -> 'DataFrame' Shift index by desired number of periods with an optional time freq . When freq is not passed, shift the index without realigning the data. If freq is passed (in this case, the index must be date or datetime, or it will raise a NotImplementedError ), the index will be increased using the periods and the freq . freq can be inferred when specified as \"infer\" as long as either freq or inferred_freq attribute is set in the index. Parameters periods : int Number of periods to shift. Can be positive or negative. freq : DateOffset, tseries.offsets, timedelta, or str, optional Offset to use from the tseries module or time rule (e.g. 'EOM'). If freq is specified then the index values are shifted but the data is not realigned. That is, use freq if you would like to extend the index when shifting and preserve the original data. If freq is specified as \"infer\" then it will be inferred from the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown axis : {0 or 'index', 1 or 'columns', None}, default None Shift direction. fill_value : object, optional The scalar value to use for newly introduced missing values. the default depends on the dtype of self . For numeric data, np.nan is used. For datetime, timedelta, or period data, etc. :attr: NaT is used. For extension dtypes, self.dtype.na_value is used. .. versionchanged :: 1.1.0 Returns DataFrame Copy of input object, shifted. See Also Index.shift : Shift values of Index. DatetimeIndex.shift : Shift values of DatetimeIndex. PeriodIndex.shift : Shift values of PeriodIndex. tshift : Shift the time index, using the index's frequency if available. Examples df = pd.DataFrame({\"Col1\": [10, 20, 15, 30, 45], ... \"Col2\": [13, 23, 18, 33, 48], ... \"Col3\": [17, 27, 22, 37, 52]}, ... index=pd.date_range(\"2020-01-01\", \"2020-01-05\")) df Col1 Col2 Col3 2020-01-01 10 13 17 2020-01-02 20 23 27 2020-01-03 15 18 22 2020-01-04 30 33 37 2020-01-05 45 48 52 df.shift(periods=3) Col1 Col2 Col3 2020-01-01 NaN NaN NaN 2020-01-02 NaN NaN NaN 2020-01-03 NaN NaN NaN 2020-01-04 10.0 13.0 17.0 2020-01-05 20.0 23.0 27.0 df.shift(periods=1, axis=\"columns\") Col1 Col2 Col3 2020-01-01 NaN 10.0 13.0 2020-01-02 NaN 20.0 23.0 2020-01-03 NaN 15.0 18.0 2020-01-04 NaN 30.0 33.0 2020-01-05 NaN 45.0 48.0 df.shift(periods=3, fill_value=0) Col1 Col2 Col3 2020-01-01 0 0 0 2020-01-02 0 0 0 2020-01-03 0 0 0 2020-01-04 10 13 17 2020-01-05 20 23 27 df.shift(periods=3, freq=\"D\") Col1 Col2 Col3 2020-01-04 10 13 17 2020-01-05 20 23 27 2020-01-06 15 18 22 2020-01-07 30 33 37 2020-01-08 45 48 52 df.shift(periods=3, freq=\"infer\") Col1 Col2 Col3 2020-01-04 10 13 17 2020-01-05 20 23 27 2020-01-06 15 18 22 2020-01-07 30 33 37 2020-01-08 45 48 52 View Source @doc ( NDFrame . shift , klass = _shared_doc_kwargs [ \"klass\" ] ) def shift ( self , periods = 1 , freq = None , axis = 0 , fill_value = None ) -> \"DataFrame\" : return super (). shift ( periods = periods , freq = freq , axis = axis , fill_value = fill_value ) skew def skew ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return unbiased skew over requested axis. Normalized by N-1. Parameters axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function. Returns Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only ) slice_shift def slice_shift ( self : ~ FrameOrSeries , periods : int = 1 , axis = 0 ) -> ~ FrameOrSeries Equivalent to shift without copying data. The shifted data will not include the dropped periods and the shifted axis will be smaller than the original. Parameters periods : int Number of periods to move, can be positive or negative. Returns shifted : same type as caller Notes While the slice_shift is faster than shift , you may pay for it later during alignment. View Source def slice_shift ( self : FrameOrSeries , periods : int = 1 , axis = 0 ) -> FrameOrSeries : \"\"\" Equivalent to `shift` without copying data. The shifted data will not include the dropped periods and the shifted axis will be smaller than the original. Parameters ---------- periods : int Number of periods to move, can be positive or negative. Returns ------- shifted : same type as caller Notes ----- While the `slice_shift` is faster than `shift`, you may pay for it later during alignment. \"\"\" if periods == 0 : return self if periods > 0 : vslicer = slice ( None , - periods ) islicer = slice ( periods , None ) else : vslicer = slice ( - periods , None ) islicer = slice ( None , periods ) new_obj = self . _slice ( vslicer , axis = axis ) shifted_axis = self . _get_axis ( axis )[ islicer ] new_obj . set_axis ( shifted_axis , axis = axis , inplace = True ) return new_obj . __finalize__ ( self , method = \"slice_shift\" ) sort_index def sort_index ( self , axis = 0 , level = None , ascending : bool = True , inplace : bool = False , kind : str = 'quicksort' , na_position : str = 'last' , sort_remaining : bool = True , ignore_index : bool = False , key : Union [ Callable [[ ForwardRef ( 'Index' )], Union [ ForwardRef ( 'Index' ), ~ AnyArrayLike ]], NoneType ] = None ) Sort object by labels (along an axis). Returns a new DataFrame sorted by label if inplace argument is False , otherwise updates the original DataFrame and returns None. Parameters axis : {0 or 'index', 1 or 'columns'}, default 0 The axis along which to sort. The value 0 identifies the rows, and 1 identifies the columns. level : int or level name or list of ints or list of level names If not None, sort on values in specified index level(s). ascending : bool or list of bools, default True Sort ascending vs. descending. When the index is a MultiIndex the sort direction can be controlled for each level individually. inplace : bool, default False If True, perform operation in-place. kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort' Choice of sorting algorithm. See also ndarray.np.sort for more information. mergesort is the only stable algorithm. For DataFrames, this option is only applied when sorting on a single column or label. na_position : {'first', 'last'}, default 'last' Puts NaNs at the beginning if first ; last puts NaNs at the end. Not implemented for MultiIndex. sort_remaining : bool, default True If True and sorting by level and index is multilevel, sort by other levels too (in order) after sorting by specified level. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. .. versionadded :: 1.0.0 key : callable, optional If not None, apply the key function to the index values before sorting. This is similar to the key argument in the builtin :meth: sorted function, with the notable difference that this key function should be vectorized . It should expect an Index and return an Index of the same shape. For MultiIndex inputs, the key is applied per level . .. versionadded :: 1.1.0 Returns DataFrame The original DataFrame sorted by the labels. See Also Series.sort_index : Sort Series by the index. DataFrame.sort_values : Sort DataFrame by the value. Series.sort_values : Sort Series by the value. Examples df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150], ... columns=['A']) df.sort_index() A 1 4 29 2 100 1 150 5 234 3 By default, it sorts in ascending order, to sort in descending order, use ascending=False df.sort_index(ascending=False) A 234 3 150 5 100 1 29 2 1 4 A key function can be specified which is applied to the index before sorting. For a MultiIndex this is applied to each level separately. df = pd.DataFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd']) df.sort_index(key=lambda x: x.str.lower()) a A 1 b 2 C 3 d 4 View Source def sort_index ( self , axis = 0 , level = None , ascending : bool = True , inplace : bool = False , kind : str = \"quicksort\" , na_position : str = \"last\" , sort_remaining : bool = True , ignore_index : bool = False , key : IndexKeyFunc = None , ): \"\"\" Sort object by labels (along an axis). Returns a new DataFrame sorted by label if `inplace` argument is ``False``, otherwise updates the original DataFrame and returns None. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 The axis along which to sort. The value 0 identifies the rows, and 1 identifies the columns. level : int or level name or list of ints or list of level names If not None, sort on values in specified index level(s). ascending : bool or list of bools, default True Sort ascending vs. descending. When the index is a MultiIndex the sort direction can be controlled for each level individually. inplace : bool, default False If True, perform operation in-place. kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort' Choice of sorting algorithm. See also ndarray.np.sort for more information. `mergesort` is the only stable algorithm. For DataFrames, this option is only applied when sorting on a single column or label. na_position : {'first', 'last'}, default 'last' Puts NaNs at the beginning if `first`; `last` puts NaNs at the end. Not implemented for MultiIndex. sort_remaining : bool, default True If True and sorting by level and index is multilevel, sort by other levels too (in order) after sorting by specified level. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. .. versionadded:: 1.0.0 key : callable, optional If not None, apply the key function to the index values before sorting. This is similar to the `key` argument in the builtin :meth:`sorted` function, with the notable difference that this `key` function should be *vectorized*. It should expect an ``Index`` and return an ``Index`` of the same shape. For MultiIndex inputs, the key is applied *per level*. .. versionadded:: 1.1.0 Returns ------- DataFrame The original DataFrame sorted by the labels. See Also -------- Series.sort_index : Sort Series by the index. DataFrame.sort_values : Sort DataFrame by the value. Series.sort_values : Sort Series by the value. Examples -------- >>> df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150], ... columns=['A']) >>> df.sort_index() A 1 4 29 2 100 1 150 5 234 3 By default, it sorts in ascending order, to sort in descending order, use ``ascending=False`` >>> df.sort_index(ascending=False) A 234 3 150 5 100 1 29 2 1 4 A key function can be specified which is applied to the index before sorting. For a ``MultiIndex`` this is applied to each level separately. >>> df = pd.DataFrame({\" a \": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd']) >>> df.sort_index(key=lambda x: x.str.lower()) a A 1 b 2 C 3 d 4 \"\"\" # TODO: this can be combined with Series.sort_index impl as # almost identical inplace = validate_bool_kwarg ( inplace , \"inplace\" ) axis = self . _get_axis_number ( axis ) labels = self . _get_axis ( axis ) labels = ensure_key_mapped ( labels , key , levels = level ) # make sure that the axis is lexsorted to start # if not we need to reconstruct to get the correct indexer labels = labels . _sort_levels_monotonic () if level is not None : new_axis , indexer = labels . sortlevel ( level , ascending = ascending , sort_remaining = sort_remaining ) elif isinstance ( labels , MultiIndex ): from pandas . core . sorting import lexsort_indexer indexer = lexsort_indexer ( labels . _get_codes_for_sorting (), orders = ascending , na_position = na_position , ) else : from pandas . core . sorting import nargsort # Check monotonic-ness before sort an index # GH11080 if ( ascending and labels . is_monotonic_increasing ) or ( not ascending and labels . is_monotonic_decreasing ): if inplace : return else : return self . copy () indexer = nargsort ( labels , kind = kind , ascending = ascending , na_position = na_position ) baxis = self . _get_block_manager_axis ( axis ) new_data = self . _mgr . take ( indexer , axis = baxis , verify = False ) # reconstruct axis if needed new_data . axes [ baxis ] = new_data . axes [ baxis ]. _sort_levels_monotonic () if ignore_index : new_data . axes [ 1 ] = ibase . default_index ( len ( indexer )) result = self . _constructor ( new_data ) if inplace : return self . _update_inplace ( result ) else : return result . __finalize__ ( self , method = \"sort_index\" ) sort_values def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' , ignore_index = False , key : Union [ Callable [[ ForwardRef ( 'Series' )], Union [ ForwardRef ( 'Series' ), ~ AnyArrayLike ]], NoneType ] = None ) Sort by the values along either axis. Parameters by : str or list of str Name or list of names to sort by . - if `axis` is 0 or `'index'` then `by` may contain index levels and / or column labels . - if `axis` is 1 or `'columns'` then `by` may contain column levels and / or index labels . .. versionchanged :: 0 . 23 . 0 Allow specifying index or column level names . axis : {0 or 'index', 1 or 'columns'}, default 0 Axis to be sorted. ascending : bool or list of bool, default True Sort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by. inplace : bool, default False If True, perform operation in-place. kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort' Choice of sorting algorithm. See also ndarray.np.sort for more information. mergesort is the only stable algorithm. For DataFrames, this option is only applied when sorting on a single column or label. na_position : {'first', 'last'}, default 'last' Puts NaNs at the beginning if first ; last puts NaNs at the end. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. .. versionadded:: 1.0.0 key : callable, optional Apply the key function to the values before sorting. This is similar to the key argument in the builtin :meth: sorted function, with the notable difference that this key function should be vectorized . It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently. .. versionadded :: 1.1.0 Returns DataFrame or None DataFrame with sorted values if inplace=False, None otherwise. See Also DataFrame.sort_index : Sort a DataFrame by the index. Series.sort_values : Similar method for a Series. Examples df = pd.DataFrame({ ... 'col1': ['A', 'A', 'B', np.nan, 'D', 'C'], ... 'col2': [2, 1, 9, 8, 7, 4], ... 'col3': [0, 1, 9, 4, 2, 3], ... 'col4': ['a', 'B', 'c', 'D', 'e', 'F'] ... }) df col1 col2 col3 col4 0 A 2 0 a 1 A 1 1 B 2 B 9 9 c 3 NaN 8 4 D 4 D 7 2 e 5 C 4 3 F Sort by col1 df.sort_values(by=['col1']) col1 col2 col3 col4 0 A 2 0 a 1 A 1 1 B 2 B 9 9 c 5 C 4 3 F 4 D 7 2 e 3 NaN 8 4 D Sort by multiple columns df.sort_values(by=['col1', 'col2']) col1 col2 col3 col4 1 A 1 1 B 0 A 2 0 a 2 B 9 9 c 5 C 4 3 F 4 D 7 2 e 3 NaN 8 4 D Sort Descending df.sort_values(by='col1', ascending=False) col1 col2 col3 col4 4 D 7 2 e 5 C 4 3 F 2 B 9 9 c 0 A 2 0 a 1 A 1 1 B 3 NaN 8 4 D Putting NAs first df.sort_values(by='col1', ascending=False, na_position='first') col1 col2 col3 col4 3 NaN 8 4 D 4 D 7 2 e 5 C 4 3 F 2 B 9 9 c 0 A 2 0 a 1 A 1 1 B Sorting with a key function df.sort_values(by='col4', key=lambda col: col.str.lower()) col1 col2 col3 col4 0 A 2 0 a 1 A 1 1 B 2 B 9 9 c 3 NaN 8 4 D 4 D 7 2 e 5 C 4 3 F View Source @Substitution ( ** _shared_doc_kwargs ) @Appender ( NDFrame . sort_values . __doc__ ) def sort_values ( # type: ignore[override] # NOQA # issue 27237 self , by , axis = 0 , ascending = True , inplace = False , kind = \"quicksort\" , na_position = \"last\" , ignore_index = False , key : ValueKeyFunc = None , ): inplace = validate_bool_kwarg ( inplace , \"inplace\" ) axis = self . _get_axis_number ( axis ) if not isinstance ( by , list ): by = [ by ] if is_sequence ( ascending ) and len ( by ) != len ( ascending ): raise ValueError ( f \"Length of ascending ({len(ascending)}) != length of by ({len(by)})\" ) if len ( by ) > 1 : from pandas.core.sorting import lexsort_indexer keys = [ self . _get_label_or_level_values ( x , axis = axis ) for x in by ] # need to rewrap columns in Series to apply key function if key is not None : keys = [ Series ( k , name = name ) for ( k , name ) in zip ( keys , by )] indexer = lexsort_indexer ( keys , orders = ascending , na_position = na_position , key = key ) indexer = ensure_platform_int ( indexer ) else : from pandas.core.sorting import nargsort by = by [ 0 ] k = self . _get_label_or_level_values ( by , axis = axis ) # need to rewrap column in Series to apply key function if key is not None : k = Series ( k , name = by ) if isinstance ( ascending , ( tuple , list )): ascending = ascending [ 0 ] indexer = nargsort ( k , kind = kind , ascending = ascending , na_position = na_position , key = key ) new_data = self . _mgr . take ( indexer , axis = self . _get_block_manager_axis ( axis ), verify = False ) if ignore_index : new_data . axes [ 1 ] = ibase . default_index ( len ( indexer )) result = self . _constructor ( new_data ) if inplace : return self . _update_inplace ( result ) else : return result . __finalize__ ( self , method = \"sort_values\" ) squeeze def squeeze ( self , axis = None ) Squeeze 1 dimensional axis objects into scalars. Series or DataFrames with a single element are squeezed to a scalar. DataFrames with a single column or a single row are squeezed to a Series. Otherwise the object is unchanged. This method is most useful when you don't know if your object is a Series or DataFrame, but you do know it has just a single column. In that case you can safely call squeeze to ensure you have a Series. Parameters axis : {0 or 'index', 1 or 'columns', None}, default None A specific axis to squeeze. By default, all length-1 axes are squeezed. Returns DataFrame, Series, or scalar The projection after squeezing axis or all the axes. See Also Series.iloc : Integer-location based indexing for selecting scalars. DataFrame.iloc : Integer-location based indexing for selecting Series. Series.to_frame : Inverse of DataFrame.squeeze for a single-column DataFrame. Examples primes = pd.Series([2, 3, 5, 7]) Slicing might produce a Series with a single value: even_primes = primes[primes % 2 == 0] even_primes 0 2 dtype: int64 even_primes.squeeze() 2 Squeezing objects with more than one value in every axis does nothing: odd_primes = primes[primes % 2 == 1] odd_primes 1 3 2 5 3 7 dtype: int64 odd_primes.squeeze() 1 3 2 5 3 7 dtype: int64 Squeezing is even more effective when used with DataFrames. df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b']) df a b 0 1 2 1 3 4 Slicing a single column will produce a DataFrame with the columns having only one value: df_a = df[['a']] df_a a 0 1 1 3 So the columns can be squeezed down, resulting in a Series: df_a.squeeze('columns') 0 1 1 3 Name: a, dtype: int64 Slicing a single row from a single column will produce a single scalar DataFrame: df_0a = df.loc[df.index < 1, ['a']] df_0a a 0 1 Squeezing the rows produces a single scalar Series: df_0a.squeeze('rows') a 1 Name: 0, dtype: int64 Squeezing all axes will project directly into a scalar: df_0a.squeeze() 1 View Source def squeeze ( self , axis = None ): \"\"\" Squeeze 1 dimensional axis objects into scalars. Series or DataFrames with a single element are squeezed to a scalar. DataFrames with a single column or a single row are squeezed to a Series. Otherwise the object is unchanged. This method is most useful when you don't know if your object is a Series or DataFrame, but you do know it has just a single column. In that case you can safely call `squeeze` to ensure you have a Series. Parameters ---------- axis : {0 or 'index', 1 or 'columns', None}, default None A specific axis to squeeze. By default, all length-1 axes are squeezed. Returns ------- DataFrame, Series, or scalar The projection after squeezing `axis` or all the axes. See Also -------- Series.iloc : Integer-location based indexing for selecting scalars. DataFrame.iloc : Integer-location based indexing for selecting Series. Series.to_frame : Inverse of DataFrame.squeeze for a single-column DataFrame. Examples -------- >>> primes = pd.Series([2, 3, 5, 7]) Slicing might produce a Series with a single value: >>> even_primes = primes[primes % 2 == 0] >>> even_primes 0 2 dtype: int64 >>> even_primes.squeeze() 2 Squeezing objects with more than one value in every axis does nothing: >>> odd_primes = primes[primes % 2 == 1] >>> odd_primes 1 3 2 5 3 7 dtype: int64 >>> odd_primes.squeeze() 1 3 2 5 3 7 dtype: int64 Squeezing is even more effective when used with DataFrames. >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b']) >>> df a b 0 1 2 1 3 4 Slicing a single column will produce a DataFrame with the columns having only one value: >>> df_a = df[['a']] >>> df_a a 0 1 1 3 So the columns can be squeezed down, resulting in a Series: >>> df_a.squeeze('columns') 0 1 1 3 Name: a, dtype: int64 Slicing a single row from a single column will produce a single scalar DataFrame: >>> df_0a = df.loc[df.index < 1, ['a']] >>> df_0a a 0 1 Squeezing the rows produces a single scalar Series: >>> df_0a.squeeze('rows') a 1 Name: 0, dtype: int64 Squeezing all axes will project directly into a scalar: >>> df_0a.squeeze() 1 \"\"\" axis = range ( self . _AXIS_LEN ) if axis is None else ( self . _get_axis_number ( axis ),) return self . iloc [ tuple ( 0 if i in axis and len ( a ) == 1 else slice ( None ) for i , a in enumerate ( self . axes ) ) ] stack def stack ( self , level =- 1 , dropna = True ) Stack the prescribed level(s) from columns to index. Return a reshaped DataFrame or Series having a multi-level index with one or more new inner-most levels compared to the current DataFrame. The new inner-most levels are created by pivoting the columns of the current dataframe: if the columns have a single level, the output is a Series; if the columns have multiple levels, the new index level(s) is (are) taken from the prescribed level(s) and the output is a DataFrame. Parameters level : int, str, list, default -1 Level(s) to stack from the column axis onto the index axis, defined as one index or label, or a list of indices or labels. dropna : bool, default True Whether to drop rows in the resulting Frame/Series with missing values. Stacking a column level onto the index axis can create combinations of index and column values that are missing from the original dataframe. See Examples section. Returns DataFrame or Series Stacked dataframe or series. See Also DataFrame.unstack : Unstack prescribed level(s) from index axis onto column axis. DataFrame.pivot : Reshape dataframe from long format to wide format. DataFrame.pivot_table : Create a spreadsheet-style pivot table as a DataFrame. Notes The function is named by analogy with a collection of books being reorganized from being side by side on a horizontal position (the columns of the dataframe) to being stacked vertically on top of each other (in the index of the dataframe). Examples Single level columns df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]], ... index=['cat', 'dog'], ... columns=['weight', 'height']) Stacking a dataframe with a single level column axis returns a Series: df_single_level_cols weight height cat 0 1 dog 2 3 df_single_level_cols.stack() cat weight 0 height 1 dog weight 2 height 3 dtype: int64 Multi level columns: simple case multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'), ... ('weight', 'pounds')]) df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]], ... index=['cat', 'dog'], ... columns=multicol1) Stacking a dataframe with a multi-level column axis: df_multi_level_cols1 weight kg pounds cat 1 2 dog 2 4 df_multi_level_cols1.stack() weight cat kg 1 pounds 2 dog kg 2 pounds 4 Missing values multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'), ... ('height', 'm')]) df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]], ... index=['cat', 'dog'], ... columns=multicol2) It is common to have missing values when stacking a dataframe with multi-level columns, as the stacked dataframe typically has more values than the original dataframe. Missing values are filled with NaNs: df_multi_level_cols2 weight height kg m cat 1.0 2.0 dog 3.0 4.0 df_multi_level_cols2.stack() height weight cat kg NaN 1.0 m 2.0 NaN dog kg NaN 3.0 m 4.0 NaN Prescribing the level(s) to be stacked The first parameter controls which level or levels are stacked: df_multi_level_cols2.stack(0) kg m cat height NaN 2.0 weight 1.0 NaN dog height NaN 4.0 weight 3.0 NaN df_multi_level_cols2.stack([0, 1]) cat height m 2.0 weight kg 1.0 dog height m 4.0 weight kg 3.0 dtype: float64 Dropping missing values df_multi_level_cols3 = pd.DataFrame([[None, 1.0], [2.0, 3.0]], ... index=['cat', 'dog'], ... columns=multicol2) Note that rows where all values are missing are dropped by default but this behaviour can be controlled via the dropna keyword parameter: df_multi_level_cols3 weight height kg m cat NaN 1.0 dog 2.0 3.0 df_multi_level_cols3.stack(dropna=False) height weight cat kg NaN NaN m 1.0 NaN dog kg NaN 2.0 m 3.0 NaN df_multi_level_cols3.stack(dropna=True) height weight cat m 1.0 NaN dog kg NaN 2.0 m 3.0 NaN View Source def stack ( self , level =- 1 , dropna = True ): \"\"\" Stack the prescribed level(s) from columns to index. Return a reshaped DataFrame or Series having a multi-level index with one or more new inner-most levels compared to the current DataFrame. The new inner-most levels are created by pivoting the columns of the current dataframe: - if the columns have a single level, the output is a Series; - if the columns have multiple levels, the new index level(s) is (are) taken from the prescribed level(s) and the output is a DataFrame. Parameters ---------- level : int, str, list, default -1 Level(s) to stack from the column axis onto the index axis, defined as one index or label, or a list of indices or labels. dropna : bool, default True Whether to drop rows in the resulting Frame/Series with missing values. Stacking a column level onto the index axis can create combinations of index and column values that are missing from the original dataframe. See Examples section. Returns ------- DataFrame or Series Stacked dataframe or series. See Also -------- DataFrame.unstack : Unstack prescribed level(s) from index axis onto column axis. DataFrame.pivot : Reshape dataframe from long format to wide format. DataFrame.pivot_table : Create a spreadsheet-style pivot table as a DataFrame. Notes ----- The function is named by analogy with a collection of books being reorganized from being side by side on a horizontal position (the columns of the dataframe) to being stacked vertically on top of each other (in the index of the dataframe). Examples -------- **Single level columns** >>> df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]], ... index=['cat', 'dog'], ... columns=['weight', 'height']) Stacking a dataframe with a single level column axis returns a Series: >>> df_single_level_cols weight height cat 0 1 dog 2 3 >>> df_single_level_cols.stack() cat weight 0 height 1 dog weight 2 height 3 dtype: int64 **Multi level columns: simple case** >>> multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'), ... ('weight', 'pounds')]) >>> df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]], ... index=['cat', 'dog'], ... columns=multicol1) Stacking a dataframe with a multi-level column axis: >>> df_multi_level_cols1 weight kg pounds cat 1 2 dog 2 4 >>> df_multi_level_cols1.stack() weight cat kg 1 pounds 2 dog kg 2 pounds 4 **Missing values** >>> multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'), ... ('height', 'm')]) >>> df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]], ... index=['cat', 'dog'], ... columns=multicol2) It is common to have missing values when stacking a dataframe with multi-level columns, as the stacked dataframe typically has more values than the original dataframe. Missing values are filled with NaNs: >>> df_multi_level_cols2 weight height kg m cat 1.0 2.0 dog 3.0 4.0 >>> df_multi_level_cols2.stack() height weight cat kg NaN 1.0 m 2.0 NaN dog kg NaN 3.0 m 4.0 NaN **Prescribing the level(s) to be stacked** The first parameter controls which level or levels are stacked: >>> df_multi_level_cols2.stack(0) kg m cat height NaN 2.0 weight 1.0 NaN dog height NaN 4.0 weight 3.0 NaN >>> df_multi_level_cols2.stack([0, 1]) cat height m 2.0 weight kg 1.0 dog height m 4.0 weight kg 3.0 dtype: float64 **Dropping missing values** >>> df_multi_level_cols3 = pd.DataFrame([[None, 1.0], [2.0, 3.0]], ... index=['cat', 'dog'], ... columns=multicol2) Note that rows where all values are missing are dropped by default but this behaviour can be controlled via the dropna keyword parameter: >>> df_multi_level_cols3 weight height kg m cat NaN 1.0 dog 2.0 3.0 >>> df_multi_level_cols3.stack(dropna=False) height weight cat kg NaN NaN m 1.0 NaN dog kg NaN 2.0 m 3.0 NaN >>> df_multi_level_cols3.stack(dropna=True) height weight cat m 1.0 NaN dog kg NaN 2.0 m 3.0 NaN \"\"\" from pandas . core . reshape . reshape import stack , stack_multiple if isinstance ( level , ( tuple , list )): return stack_multiple ( self , level , dropna = dropna ) else : return stack ( self , level , dropna = dropna ) std def std ( self , axis = None , skipna = None , level = None , ddof = 1 , numeric_only = None , ** kwargs ) Return sample standard deviation over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument Parameters axis : {index (0), columns (1)} skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. ddof : int, default 1 Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. Returns Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr ) @Appender ( _num_ddof_doc ) def stat_func ( self , axis = None , skipna = None , level = None , ddof = 1 , numeric_only = None , ** kwargs ) : nv . validate_stat_ddof_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna , ddof = ddof ) return self . _reduce ( func , name , axis = axis , numeric_only = numeric_only , skipna = skipna , ddof = ddof ) sub def sub ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Subtraction of dataframe and other, element-wise (binary operator sub ). Equivalent to dataframe - other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rsub . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) subtract def subtract ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Subtraction of dataframe and other, element-wise (binary operator sub ). Equivalent to dataframe - other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rsub . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) sum def sum ( self , axis = None , skipna = None , level = None , numeric_only = None , min_count = 0 , ** kwargs ) Return the sum of the values for the requested axis. This is equivalent to the method numpy.sum . Parameters axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. min_count : int, default 0 The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. .. versionadded :: 0.22.0 Added with the default being 0. This means the sum of an all-NA or empty Series is 0, and the product of an all-NA or empty Series is 1. **kwargs Additional keyword arguments to be passed to the function. Returns Series or DataFrame (if level specified) See Also Series.sum : Return the sum. Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis. Examples idx = pd.MultiIndex.from_arrays([ ... ['warm', 'warm', 'cold', 'cold'], ... ['dog', 'falcon', 'fish', 'spider']], ... names=['blooded', 'animal']) s = pd.Series([4, 2, 0, 8], name='legs', index=idx) s blooded animal warm dog 4 falcon 2 cold fish 0 spider 8 Name: legs, dtype: int64 s.sum() 14 Sum using level names, as well as indices. s.sum(level='blooded') blooded warm 6 cold 8 Name: legs, dtype: int64 s.sum(level=0) blooded warm 6 cold 8 Name: legs, dtype: int64 By default, the sum of an empty or all-NA Series is 0 . pd.Series([]).sum() # min_count=0 is the default 0.0 This can be controlled with the min_count parameter. For example, if you'd like the sum of an empty series to be NaN, pass min_count=1 . pd.Series([]).sum(min_count=1) nan Thanks to the skipna parameter, min_count handles all-NA and empty series identically. pd.Series([np.nan]).sum() 0.0 pd.Series([np.nan]).sum(min_count=1) nan View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = _min_count_stub , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , min_count = 0 , ** kwargs , ) : if name == \"sum\" : nv . validate_sum ( tuple (), kwargs ) elif name == \"prod\" : nv . validate_prod ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna , min_count = min_count ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only , min_count = min_count , ) swapaxes def swapaxes ( self : ~ FrameOrSeries , axis1 , axis2 , copy = True ) -> ~ FrameOrSeries Interchange axes and swap values axes appropriately. Returns y : same as input View Source def swapaxes ( self : FrameOrSeries , axis1 , axis2 , copy = True ) -> FrameOrSeries : \"\"\" Interchange axes and swap values axes appropriately. Returns ------- y : same as input \"\"\" i = self . _get_axis_number ( axis1 ) j = self . _get_axis_number ( axis2 ) if i == j : if copy : return self . copy () return self mapping = { i : j , j : i } new_axes = ( self . _get_axis ( mapping . get ( k , k )) for k in range ( self . _AXIS_LEN )) new_values = self . values . swapaxes ( i , j ) if copy : new_values = new_values . copy () # ignore needed because of NDFrame constructor is different than # DataFrame / Series constructors . return self . _constructor ( new_values , * new_axes ). __finalize__ ( # type : ignore self , method = \"swapaxes\" ) swaplevel def swaplevel ( self , i =- 2 , j =- 1 , axis = 0 ) -> 'DataFrame' Swap levels i and j in a MultiIndex on a particular axis. Parameters i, j : int or str Levels of the indices to be swapped. Can pass level name as string. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to swap levels on. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. Returns DataFrame View Source def swaplevel ( self , i =- 2 , j =- 1 , axis = 0 ) -> \"DataFrame\" : \"\"\" Swap levels i and j in a MultiIndex on a particular axis. Parameters ---------- i, j : int or str Levels of the indices to be swapped. Can pass level name as string. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to swap levels on. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. Returns ------- DataFrame \"\"\" result = self . copy () axis = self . _get_axis_number ( axis ) if not isinstance ( result . _get_axis ( axis ), MultiIndex ): # pragma : no cover raise TypeError ( \"Can only swap levels on a hierarchical axis.\" ) if axis == 0 : assert isinstance ( result . index , MultiIndex ) result . index = result . index . swaplevel ( i , j ) else : assert isinstance ( result . columns , MultiIndex ) result . columns = result . columns . swaplevel ( i , j ) return result tail def tail ( self : ~ FrameOrSeries , n : int = 5 ) -> ~ FrameOrSeries Return the last n rows. This function returns last n rows from the object based on position. It is useful for quickly verifying data, for example, after sorting or appending rows. For negative values of n , this function returns all rows except the first n rows, equivalent to df[n:] . Parameters n : int, default 5 Number of rows to select. Returns type of caller The last n rows of the caller object. See Also DataFrame.head : The first n rows of the caller object. Examples df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion', ... 'monkey', 'parrot', 'shark', 'whale', 'zebra']}) df animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the last 5 lines df.tail() animal 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the last n lines (three in this case) df.tail(3) animal 6 shark 7 whale 8 zebra For negative values of n df.tail(-3) animal 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra View Source def tail ( self : FrameOrSeries , n : int = 5 ) -> FrameOrSeries : \"\"\" Return the last `n` rows. This function returns last `n` rows from the object based on position. It is useful for quickly verifying data, for example, after sorting or appending rows. For negative values of `n`, this function returns all rows except the first `n` rows, equivalent to ``df[n:]``. Parameters ---------- n : int, default 5 Number of rows to select. Returns ------- type of caller The last `n` rows of the caller object. See Also -------- DataFrame.head : The first `n` rows of the caller object. Examples -------- >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion', ... 'monkey', 'parrot', 'shark', 'whale', 'zebra']}) >>> df animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the last 5 lines >>> df.tail() animal 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the last `n` lines (three in this case) >>> df.tail(3) animal 6 shark 7 whale 8 zebra For negative values of `n` >>> df.tail(-3) animal 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra \"\"\" if n == 0 : return self . iloc [ 0 : 0 ] return self . iloc [ - n :] take def take ( self : ~ FrameOrSeries , indices , axis = 0 , is_copy : Union [ bool , NoneType ] = None , ** kwargs ) -> ~ FrameOrSeries Return the elements in the given positional indices along an axis. This means that we are not indexing according to actual values in the index attribute of the object. We are indexing according to the actual position of the element in the object. Parameters indices : array-like An array of ints indicating which positions to take. axis : {0 or 'index', 1 or 'columns', None}, default 0 The axis on which to select elements. 0 means that we are selecting rows, 1 means that we are selecting columns. is_copy : bool Before pandas 1.0, is_copy=False can be specified to ensure that the return value is an actual copy. Starting with pandas 1.0, take always returns a copy, and the keyword is therefore deprecated. .. deprecated :: 1.0.0 **kwargs For compatibility with :meth: numpy.take . Has no effect on the output. Returns taken : same type as caller An array-like containing the elements taken from the object. See Also DataFrame.loc : Select a subset of a DataFrame by labels. DataFrame.iloc : Select a subset of a DataFrame by positions. numpy.take : Take elements from an array along an axis. Examples df = pd.DataFrame([('falcon', 'bird', 389.0), ... ('parrot', 'bird', 24.0), ... ('lion', 'mammal', 80.5), ... ('monkey', 'mammal', np.nan)], ... columns=['name', 'class', 'max_speed'], ... index=[0, 2, 3, 1]) df name class max_speed 0 falcon bird 389.0 2 parrot bird 24.0 3 lion mammal 80.5 1 monkey mammal NaN Take elements at positions 0 and 3 along the axis 0 (default). Note how the actual indices selected (0 and 1) do not correspond to our selected indices 0 and 3. That's because we are selecting the 0th and 3rd rows, not rows whose indices equal 0 and 3. df.take([0, 3]) name class max_speed 0 falcon bird 389.0 1 monkey mammal NaN Take elements at indices 1 and 2 along the axis 1 (column selection). df.take([1, 2], axis=1) class max_speed 0 bird 389.0 2 bird 24.0 3 mammal 80.5 1 mammal NaN We may take elements using negative integers for positive indices, starting from the end of the object, just like with Python lists. df.take([-1, -2]) name class max_speed 1 monkey mammal NaN 3 lion mammal 80.5 View Source def take ( self : FrameOrSeries , indices , axis = 0 , is_copy : Optional [ bool_t ] = None , ** kwargs ) -> FrameOrSeries : \"\"\" Return the elements in the given *positional* indices along an axis. This means that we are not indexing according to actual values in the index attribute of the object. We are indexing according to the actual position of the element in the object. Parameters ---------- indices : array-like An array of ints indicating which positions to take. axis : {0 or 'index', 1 or 'columns', None}, default 0 The axis on which to select elements. ``0`` means that we are selecting rows, ``1`` means that we are selecting columns. is_copy : bool Before pandas 1.0, ``is_copy=False`` can be specified to ensure that the return value is an actual copy. Starting with pandas 1.0, ``take`` always returns a copy, and the keyword is therefore deprecated. .. deprecated:: 1.0.0 **kwargs For compatibility with :meth:`numpy.take`. Has no effect on the output. Returns ------- taken : same type as caller An array-like containing the elements taken from the object. See Also -------- DataFrame.loc : Select a subset of a DataFrame by labels. DataFrame.iloc : Select a subset of a DataFrame by positions. numpy.take : Take elements from an array along an axis. Examples -------- >>> df = pd.DataFrame([('falcon', 'bird', 389.0), ... ('parrot', 'bird', 24.0), ... ('lion', 'mammal', 80.5), ... ('monkey', 'mammal', np.nan)], ... columns=['name', 'class', 'max_speed'], ... index=[0, 2, 3, 1]) >>> df name class max_speed 0 falcon bird 389.0 2 parrot bird 24.0 3 lion mammal 80.5 1 monkey mammal NaN Take elements at positions 0 and 3 along the axis 0 (default). Note how the actual indices selected (0 and 1) do not correspond to our selected indices 0 and 3. That's because we are selecting the 0th and 3rd rows, not rows whose indices equal 0 and 3. >>> df.take([0, 3]) name class max_speed 0 falcon bird 389.0 1 monkey mammal NaN Take elements at indices 1 and 2 along the axis 1 (column selection). >>> df.take([1, 2], axis=1) class max_speed 0 bird 389.0 2 bird 24.0 3 mammal 80.5 1 mammal NaN We may take elements using negative integers for positive indices, starting from the end of the object, just like with Python lists. >>> df.take([-1, -2]) name class max_speed 1 monkey mammal NaN 3 lion mammal 80.5 \"\"\" if is_copy is not None : warnings . warn ( \"is_copy is deprecated and will be removed in a future version. \" \"'take' always returns a copy, so there is no need to specify this.\" , FutureWarning , stacklevel = 2 , ) nv . validate_take ( tuple (), kwargs ) new_data = self . _mgr . take ( indices , axis = self . _get_block_manager_axis ( axis ), verify = True ) return self . _constructor ( new_data ). __finalize__ ( self , method = \"take\" ) to_clipboard def to_clipboard ( self , excel : bool = True , sep : Union [ str , NoneType ] = None , ** kwargs ) -> None Copy object to the system clipboard. Write a text representation of object to the system clipboard. This can be pasted into Excel, for example. Parameters excel : bool, default True Produce output in a csv format for easy pasting into excel. - True, use the provided separator for csv pasting. - False, write a string representation of the object to the clipboard. sep : str, default '\\t' Field delimiter. **kwargs These parameters will be passed to DataFrame.to_csv. See Also DataFrame.to_csv : Write a DataFrame to a comma-separated values (csv) file. read_clipboard : Read text from clipboard and pass to read_table. Notes Requirements for your platform. Linux : xclip , or xsel (with PyQt4 modules) Windows : none OS X : none Examples Copy the contents of a DataFrame to the clipboard. df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C']) df.to_clipboard(sep=',') # doctest: +SKIP ... # Wrote the following to the system clipboard: ... # ,A,B,C ... # 0,1,2,3 ... # 1,4,5,6 We can omit the index by passing the keyword index and setting it to false. df.to_clipboard(sep=',', index=False) # doctest: +SKIP ... # Wrote the following to the system clipboard: ... # A,B,C ... # 1,2,3 ... # 4,5,6 View Source def to_clipboard ( self , excel : bool_t = True , sep : Optional [ str ] = None , ** kwargs ) -> None : r \"\"\" Copy object to the system clipboard. Write a text representation of object to the system clipboard. This can be pasted into Excel, for example. Parameters ---------- excel : bool, default True Produce output in a csv format for easy pasting into excel. - True, use the provided separator for csv pasting. - False, write a string representation of the object to the clipboard. sep : str, default ``'\\t'`` Field delimiter. **kwargs These parameters will be passed to DataFrame.to_csv. See Also -------- DataFrame.to_csv : Write a DataFrame to a comma-separated values (csv) file. read_clipboard : Read text from clipboard and pass to read_table. Notes ----- Requirements for your platform. - Linux : `xclip`, or `xsel` (with `PyQt4` modules) - Windows : none - OS X : none Examples -------- Copy the contents of a DataFrame to the clipboard. >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C']) >>> df.to_clipboard(sep=',') # doctest: +SKIP ... # Wrote the following to the system clipboard: ... # ,A,B,C ... # 0,1,2,3 ... # 1,4,5,6 We can omit the index by passing the keyword `index` and setting it to false. >>> df.to_clipboard(sep=',', index=False) # doctest: +SKIP ... # Wrote the following to the system clipboard: ... # A,B,C ... # 1,2,3 ... # 4,5,6 \"\"\" from pandas . io import clipboards clipboards . to_clipboard ( self , excel = excel , sep = sep , ** kwargs ) to_csv def to_csv ( self , path_or_buf : Union [ str , pathlib . Path , IO [ ~ AnyStr ], NoneType ] = None , sep : str = ',' , na_rep : str = '' , float_format : Union [ str , NoneType ] = None , columns : Union [ Sequence [ Union [ Hashable , NoneType ]], NoneType ] = None , header : Union [ bool , List [ str ]] = True , index : bool = True , index_label : Union [ bool , str , Sequence [ Union [ Hashable , NoneType ]], NoneType ] = None , mode : str = 'w' , encoding : Union [ str , NoneType ] = None , compression : Union [ str , Mapping [ str , str ], NoneType ] = 'infer' , quoting : Union [ int , NoneType ] = None , quotechar : str = '\"' , line_terminator : Union [ str , NoneType ] = None , chunksize : Union [ int , NoneType ] = None , date_format : Union [ str , NoneType ] = None , doublequote : bool = True , escapechar : Union [ str , NoneType ] = None , decimal : Union [ str , NoneType ] = '.' , errors : str = 'strict' ) -> Union [ str , NoneType ] Write object to a comma-separated values (csv) file. .. versionchanged:: 0.24.0 The order of arguments for Series was changed. Parameters path_or_buf : str or file handle, default None File path or object, if None is provided the result is returned as a string. If a file object is passed it should be opened with newline='' , disabling universal newlines. .. versionchanged :: 0.24.0 Was previously named \"path\" for Series. sep : str, default ',' String of length 1. Field delimiter for the output file. na_rep : str, default '' Missing data representation. float_format : str, default None Format string for floating point numbers. columns : sequence, optional Columns to write. header : bool or list of str, default True Write out the column names. If a list of strings is given it is assumed to be aliases for the column names. .. versionchanged :: 0.24.0 Previously defaulted to False for Series. index : bool, default True Write row names (index). index_label : str or sequence, or False, default None Column label for index column(s) if desired. If None is given, and header and index are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R. mode : str Python write mode, default 'w'. encoding : str, optional A string representing the encoding to use in the output file, defaults to 'utf-8'. compression : str or dict, default 'infer' If str, represents compression mode. If dict, value at 'method' is the compression mode. Compression mode may be any of the following possible values: {'infer', 'gzip', 'bz2', 'zip', 'xz', None}. If compression mode is 'infer' and path_or_buf is path-like, then detect compression mode from the following extensions: '.gz', '.bz2', '.zip' or '.xz'. (otherwise no compression). If dict given and mode is one of {'zip', 'gzip', 'bz2'}, or inferred as one of the above, other entries passed as additional compression options. .. versionchanged :: 1.0.0 May now be a dict with key 'method' as compression mode and other entries as additional compression options if compression mode is 'zip'. .. versionchanged :: 1.1.0 Passing compression options as keys in dict is supported for compression modes 'gzip' and 'bz2' as well as 'zip'. quoting : optional constant from csv module Defaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric. quotechar : str, default '\\\"' String of length 1. Character used to quote fields. line_terminator : str, optional The newline character or character sequence to use in the output file. Defaults to os.linesep , which depends on the OS in which this method is called ('\\n' for linux, '\\r\\n' for Windows, i.e.). .. versionchanged :: 0.24.0 chunksize : int or None Rows to write at a time. date_format : str, default None Format string for datetime objects. doublequote : bool, default True Control quoting of quotechar inside a field. escapechar : str, default None String of length 1. Character used to escape sep and quotechar when appropriate. decimal : str, default '.' Character recognized as decimal separator. E.g. use ',' for European data. errors : str, default 'strict' Specifies how encoding and decoding errors are to be handled. See the errors argument for :func: open for a full list of options. .. versionadded :: 1.1.0 Returns None or str If path_or_buf is None, returns the resulting csv format as a string. Otherwise returns None. See Also read_csv : Load a CSV file into a DataFrame. to_excel : Write DataFrame to an Excel file. Examples df = pd.DataFrame({'name': ['Raphael', 'Donatello'], ... 'mask': ['red', 'purple'], ... 'weapon': ['sai', 'bo staff']}) df.to_csv(index=False) 'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n' Create 'out.zip' containing 'out.csv' compression_opts = dict(method='zip', ... archive_name='out.csv') # doctest: +SKIP df.to_csv('out.zip', index=False, ... compression=compression_opts) # doctest: +SKIP View Source def to_csv ( self , path_or_buf : Optional [ FilePathOrBuffer ] = None , sep : str = \",\" , na_rep : str = \"\" , float_format : Optional [ str ] = None , columns : Optional [ Sequence[Label ] ] = None , header : Union [ bool_t, List[str ] ] = True , index : bool_t = True , index_label : Optional [ Union[bool_t, str, Sequence[Label ] ]] = None , mode : str = \"w\" , encoding : Optional [ str ] = None , compression : Optional [ Union[str, Mapping[str, str ] ]] = \"infer\" , quoting : Optional [ int ] = None , quotechar : str = '\"' , line_terminator : Optional [ str ] = None , chunksize : Optional [ int ] = None , date_format : Optional [ str ] = None , doublequote : bool_t = True , escapechar : Optional [ str ] = None , decimal : Optional [ str ] = \".\" , errors : str = \"strict\" , ) -> Optional [ str ] : r \"\"\" Write object to a comma-separated values (csv) file. .. versionchanged:: 0.24.0 The order of arguments for Series was changed. Parameters ---------- path_or_buf : str or file handle, default None File path or object, if None is provided the result is returned as a string. If a file object is passed it should be opened with `newline=''`, disabling universal newlines. .. versionchanged:: 0.24.0 Was previously named \" path \" for Series. sep : str, default ',' String of length 1. Field delimiter for the output file. na_rep : str, default '' Missing data representation. float_format : str, default None Format string for floating point numbers. columns : sequence, optional Columns to write. header : bool or list of str, default True Write out the column names. If a list of strings is given it is assumed to be aliases for the column names. .. versionchanged:: 0.24.0 Previously defaulted to False for Series. index : bool, default True Write row names (index). index_label : str or sequence, or False, default None Column label for index column(s) if desired. If None is given, and `header` and `index` are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R. mode : str Python write mode, default 'w'. encoding : str, optional A string representing the encoding to use in the output file, defaults to 'utf-8'. compression : str or dict, default 'infer' If str, represents compression mode. If dict, value at 'method' is the compression mode. Compression mode may be any of the following possible values: {'infer', 'gzip', 'bz2', 'zip', 'xz', None}. If compression mode is 'infer' and `path_or_buf` is path-like, then detect compression mode from the following extensions: '.gz', '.bz2', '.zip' or '.xz'. (otherwise no compression). If dict given and mode is one of {'zip', 'gzip', 'bz2'}, or inferred as one of the above, other entries passed as additional compression options. .. versionchanged:: 1.0.0 May now be a dict with key 'method' as compression mode and other entries as additional compression options if compression mode is 'zip'. .. versionchanged:: 1.1.0 Passing compression options as keys in dict is supported for compression modes 'gzip' and 'bz2' as well as 'zip'. quoting : optional constant from csv module Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format` then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric. quotechar : str, default '\\\" ' String of length 1. Character used to quote fields. line_terminator : str, optional The newline character or character sequence to use in the output file. Defaults to `os.linesep`, which depends on the OS in which this method is called (' \\ n ' for linux, ' \\ r \\ n ' for Windows, i.e.). .. versionchanged:: 0.24.0 chunksize : int or None Rows to write at a time. date_format : str, default None Format string for datetime objects. doublequote : bool, default True Control quoting of `quotechar` inside a field. escapechar : str, default None String of length 1. Character used to escape `sep` and `quotechar` when appropriate. decimal : str, default ' . ' Character recognized as decimal separator. E.g. use ' , ' for European data. errors : str, default ' strict ' Specifies how encoding and decoding errors are to be handled. See the errors argument for :func:`open` for a full list of options. .. versionadded:: 1.1.0 Returns ------- None or str If path_or_buf is None, returns the resulting csv format as a string. Otherwise returns None. See Also -------- read_csv : Load a CSV file into a DataFrame. to_excel : Write DataFrame to an Excel file. Examples -------- >>> df = pd.DataFrame({' name ': [' Raphael ', ' Donatello '], ... ' mask ': [' red ', ' purple '], ... ' weapon ': [' sai ', ' bo staff ']}) >>> df.to_csv(index=False) ' name , mask , weapon \\ nRaphael , red , sai \\ nDonatello , purple , bo staff \\ n ' Create ' out . zip ' containing ' out . csv ' >>> compression_opts = dict(method=' zip ', ... archive_name=' out . csv ') # doctest: +SKIP >>> df.to_csv(' out . zip ' , index = False , ... compression = compression_opts ) # doctest : + SKIP \"\" \" df = self if isinstance ( self , ABCDataFrame ) else self . to_frame () from pandas . io . formats . csvs import CSVFormatter formatter = CSVFormatter ( df , path_or_buf , line_terminator = line_terminator , sep = sep , encoding = encoding , errors = errors , compression = compression , quoting = quoting , na_rep = na_rep , float_format = float_format , cols = columns , header = header , index = index , index_label = index_label , mode = mode , chunksize = chunksize , quotechar = quotechar , date_format = date_format , doublequote = doublequote , escapechar = escapechar , decimal = decimal , ) formatter . save () if path_or_buf is None : return formatter . path_or_buf . getvalue () return None to_dict def to_dict ( self , orient = 'dict' , into =< class ' dict '> ) Convert the DataFrame to a dictionary. The type of the key-value pairs can be customized with the parameters (see below). Parameters orient : str {'dict', 'list', 'series', 'split', 'records', 'index'} Determines the type of the values of the dictionary. - 'dict' ( default ) : dict like { column -> { index -> value }} - 'list' : dict like { column -> [ values ] } - 'series' : dict like { column -> Series ( values ) } - 'split' : dict like { 'index' -> [ index ], 'columns' -> [ columns ], 'data' -> [ values ] } - 'records' : list like [ { column -> value } , ... , { column -> value } ] - 'index' : dict like { index -> { column -> value }} Abbreviations are allowed . `s` indicates `series` and `sp` indicates `split` . into : class, default dict The collections.abc.Mapping subclass used for all Mappings in the return value. Can be the actual class or an empty instance of the mapping type you want. If you want a collections.defaultdict, you must pass it initialized. Returns dict, list or collections.abc.Mapping Return a collections.abc.Mapping object representing the DataFrame. The resulting transformation depends on the orient parameter. See Also DataFrame.from_dict: Create a DataFrame from a dictionary. DataFrame.to_json: Convert a DataFrame to JSON format. Examples df = pd.DataFrame({'col1': [1, 2], ... 'col2': [0.5, 0.75]}, ... index=['row1', 'row2']) df col1 col2 row1 1 0.50 row2 2 0.75 df.to_dict() {'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}} You can specify the return orientation. df.to_dict('series') {'col1': row1 1 row2 2 Name: col1, dtype: int64, 'col2': row1 0.50 row2 0.75 Name: col2, dtype: float64} df.to_dict('split') {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'], 'data': [[1, 0.5], [2, 0.75]]} df.to_dict('records') [{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}] df.to_dict('index') {'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}} You can also specify the mapping type. from collections import OrderedDict, defaultdict df.to_dict(into=OrderedDict) OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])), ('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))]) If you want a defaultdict , you need to initialize it: dd = defaultdict(list) df.to_dict('records', into=dd) [defaultdict( , {'col1': 1, 'col2': 0.5}), defaultdict( , {'col1': 2, 'col2': 0.75})] View Source def to_dict ( self , orient = \"dict\" , into = dict ): \"\"\" Convert the DataFrame to a dictionary. The type of the key-value pairs can be customized with the parameters (see below). Parameters ---------- orient : str {'dict', 'list', 'series', 'split', 'records', 'index'} Determines the type of the values of the dictionary. - 'dict' (default) : dict like {column -> {index -> value}} - 'list' : dict like {column -> [values]} - 'series' : dict like {column -> Series(values)} - 'split' : dict like {'index' -> [index], 'columns' -> [columns], 'data' -> [values]} - 'records' : list like [{column -> value}, ... , {column -> value}] - 'index' : dict like {index -> {column -> value}} Abbreviations are allowed. `s` indicates `series` and `sp` indicates `split`. into : class, default dict The collections.abc.Mapping subclass used for all Mappings in the return value. Can be the actual class or an empty instance of the mapping type you want. If you want a collections.defaultdict, you must pass it initialized. Returns ------- dict, list or collections.abc.Mapping Return a collections.abc.Mapping object representing the DataFrame. The resulting transformation depends on the `orient` parameter. See Also -------- DataFrame.from_dict: Create a DataFrame from a dictionary. DataFrame.to_json: Convert a DataFrame to JSON format. Examples -------- >>> df = pd.DataFrame({'col1': [1, 2], ... 'col2': [0.5, 0.75]}, ... index=['row1', 'row2']) >>> df col1 col2 row1 1 0.50 row2 2 0.75 >>> df.to_dict() {'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}} You can specify the return orientation. >>> df.to_dict('series') {'col1': row1 1 row2 2 Name: col1, dtype: int64, 'col2': row1 0.50 row2 0.75 Name: col2, dtype: float64} >>> df.to_dict('split') {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'], 'data': [[1, 0.5], [2, 0.75]]} >>> df.to_dict('records') [{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}] >>> df.to_dict('index') {'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}} You can also specify the mapping type. >>> from collections import OrderedDict, defaultdict >>> df.to_dict(into=OrderedDict) OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])), ('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))]) If you want a `defaultdict`, you need to initialize it: >>> dd = defaultdict(list) >>> df.to_dict('records', into=dd) [defaultdict(<class 'list'>, {'col1': 1, 'col2': 0.5}), defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})] \"\"\" if not self . columns . is_unique : warnings . warn ( \"DataFrame columns are not unique, some columns will be omitted.\" , UserWarning , stacklevel = 2 , ) # GH16122 into_c = com . standardize_mapping ( into ) orient = orient . lower () # GH32515 if orient . startswith (( \"d\" , \"l\" , \"s\" , \"r\" , \"i\" )) and orient not in { \"dict\" , \"list\" , \"series\" , \"split\" , \"records\" , \"index\" , } : warnings . warn ( \"Using short name for 'orient' is deprecated. Only the \" \"options: ('dict', list, 'series', 'split', 'records', 'index') \" \"will be used in a future version. Use one of the above \" \"to silence this warning.\" , FutureWarning , ) if orient . startswith ( \"d\" ): orient = \"dict\" elif orient . startswith ( \"l\" ): orient = \"list\" elif orient . startswith ( \"sp\" ): orient = \"split\" elif orient . startswith ( \"s\" ): orient = \"series\" elif orient . startswith ( \"r\" ): orient = \"records\" elif orient . startswith ( \"i\" ): orient = \"index\" if orient == \"dict\" : return into_c (( k , v . to_dict ( into )) for k , v in self . items ()) elif orient == \"list\" : return into_c (( k , v . tolist ()) for k , v in self . items ()) elif orient == \"split\" : return into_c ( ( ( \"index\" , self . index . tolist ()), ( \"columns\" , self . columns . tolist ()), ( \"data\" , [ list ( map ( com . maybe_box_datetimelike , t )) for t in self . itertuples ( index = False , name = None ) ], ), ) ) elif orient == \"series\" : return into_c (( k , com . maybe_box_datetimelike ( v )) for k , v in self . items ()) elif orient == \"records\" : columns = self . columns . tolist () rows = ( dict ( zip ( columns , row )) for row in self . itertuples ( index = False , name = None ) ) return [ into_c (( k , com . maybe_box_datetimelike ( v )) for k , v in row . items ()) for row in rows ] elif orient == \"index\" : if not self . index . is_unique : raise ValueError ( \"DataFrame index must be unique for orient='index'.\" ) return into_c ( ( t [ 0 ], dict ( zip ( self . columns , t [ 1 :]))) for t in self . itertuples ( name = None ) ) else : raise ValueError ( f \"orient '{orient}' not understood\" ) to_excel def to_excel ( self , excel_writer , sheet_name = 'Sheet1' , na_rep = '' , float_format = None , columns = None , header = True , index = True , index_label = None , startrow = 0 , startcol = 0 , engine = None , merge_cells = True , encoding = None , inf_rep = 'inf' , verbose = True , freeze_panes = None ) -> None Write object to an Excel sheet. To write a single object to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an ExcelWriter object with a target file name, and specify a sheet in the file to write to. Multiple sheets may be written to by specifying unique sheet_name . With all data written to the file it is necessary to save the changes. Note that creating an ExcelWriter object with a file name that already exists will result in the contents of the existing file being erased. Parameters excel_writer : str or ExcelWriter object File path or existing ExcelWriter. sheet_name : str, default 'Sheet1' Name of sheet which will contain DataFrame. na_rep : str, default '' Missing data representation. float_format : str, optional Format string for floating point numbers. For example float_format=\"%.2f\" will format 0.1234 to 0.12. columns : sequence or list of str, optional Columns to write. header : bool or list of str, default True Write out the column names. If a list of string is given it is assumed to be aliases for the column names. index : bool, default True Write row names (index). index_label : str or sequence, optional Column label for index column(s) if desired. If not specified, and header and index are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. startrow : int, default 0 Upper left cell row to dump data frame. startcol : int, default 0 Upper left cell column to dump data frame. engine : str, optional Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this via the options io.excel.xlsx.writer , io.excel.xls.writer , and io.excel.xlsm.writer . merge_cells : bool, default True Write MultiIndex and Hierarchical Rows as merged cells. encoding : str, optional Encoding of the resulting excel file. Only necessary for xlwt, other writers support unicode natively. inf_rep : str, default 'inf' Representation for infinity (there is no native representation for infinity in Excel). verbose : bool, default True Display more information in the error logs. freeze_panes : tuple of int (length 2), optional Specifies the one-based bottommost row and rightmost column that is to be frozen. See Also to_csv : Write DataFrame to a comma-separated values (csv) file. ExcelWriter : Class for writing DataFrame objects into excel sheets. read_excel : Read an Excel file into a pandas DataFrame. read_csv : Read a comma-separated values (csv) file into DataFrame. Notes For compatibility with :meth: ~DataFrame.to_csv , to_excel serializes lists and dicts to strings before writing. Once a workbook has been saved it is not possible write further data without rewriting the whole workbook. Examples Create, write to and save a workbook: df1 = pd.DataFrame([['a', 'b'], ['c', 'd']], ... index=['row 1', 'row 2'], ... columns=['col 1', 'col 2']) df1.to_excel(\"output.xlsx\") # doctest: +SKIP To specify the sheet name: df1.to_excel(\"output.xlsx\", ... sheet_name='Sheet_name_1') # doctest: +SKIP If you wish to write to more than one sheet in the workbook, it is necessary to specify an ExcelWriter object: df2 = df1.copy() with pd.ExcelWriter('output.xlsx') as writer: # doctest: +SKIP ... df1.to_excel(writer, sheet_name='Sheet_name_1') ... df2.to_excel(writer, sheet_name='Sheet_name_2') ExcelWriter can also be used to append to an existing Excel file: with pd.ExcelWriter('output.xlsx', ... mode='a') as writer: # doctest: +SKIP ... df.to_excel(writer, sheet_name='Sheet_name_3') To set the library that is used to write the Excel file, you can pass the engine keyword (the default engine is automatically chosen depending on the file extension): df1.to_excel('output1.xlsx', engine='xlsxwriter') # doctest: +SKIP View Source @ doc ( klass = \"object\" ) def to_excel ( self , excel_writer , sheet_name = \"Sheet1\" , na_rep = \"\" , float_format = None , columns = None , header = True , index = True , index_label = None , startrow = 0 , startcol = 0 , engine = None , merge_cells = True , encoding = None , inf_rep = \"inf\" , verbose = True , freeze_panes = None , ) -> None : \"\"\" Write {klass} to an Excel sheet. To write a single {klass} to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an `ExcelWriter` object with a target file name, and specify a sheet in the file to write to. Multiple sheets may be written to by specifying unique `sheet_name`. With all data written to the file it is necessary to save the changes. Note that creating an `ExcelWriter` object with a file name that already exists will result in the contents of the existing file being erased. Parameters ---------- excel_writer : str or ExcelWriter object File path or existing ExcelWriter. sheet_name : str, default 'Sheet1' Name of sheet which will contain DataFrame. na_rep : str, default '' Missing data representation. float_format : str, optional Format string for floating point numbers. For example ``float_format=\" % . 2 f \"`` will format 0.1234 to 0.12. columns : sequence or list of str, optional Columns to write. header : bool or list of str, default True Write out the column names. If a list of string is given it is assumed to be aliases for the column names. index : bool, default True Write row names (index). index_label : str or sequence, optional Column label for index column(s) if desired. If not specified, and `header` and `index` are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. startrow : int, default 0 Upper left cell row to dump data frame. startcol : int, default 0 Upper left cell column to dump data frame. engine : str, optional Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this via the options ``io.excel.xlsx.writer``, ``io.excel.xls.writer``, and ``io.excel.xlsm.writer``. merge_cells : bool, default True Write MultiIndex and Hierarchical Rows as merged cells. encoding : str, optional Encoding of the resulting excel file. Only necessary for xlwt, other writers support unicode natively. inf_rep : str, default 'inf' Representation for infinity (there is no native representation for infinity in Excel). verbose : bool, default True Display more information in the error logs. freeze_panes : tuple of int (length 2), optional Specifies the one-based bottommost row and rightmost column that is to be frozen. See Also -------- to_csv : Write DataFrame to a comma-separated values (csv) file. ExcelWriter : Class for writing DataFrame objects into excel sheets. read_excel : Read an Excel file into a pandas DataFrame. read_csv : Read a comma-separated values (csv) file into DataFrame. Notes ----- For compatibility with :meth:`~DataFrame.to_csv`, to_excel serializes lists and dicts to strings before writing. Once a workbook has been saved it is not possible write further data without rewriting the whole workbook. Examples -------- Create, write to and save a workbook: >>> df1 = pd.DataFrame([['a', 'b'], ['c', 'd']], ... index=['row 1', 'row 2'], ... columns=['col 1', 'col 2']) >>> df1.to_excel(\" output . xlsx \") # doctest: +SKIP To specify the sheet name: >>> df1.to_excel(\" output . xlsx \", ... sheet_name='Sheet_name_1') # doctest: +SKIP If you wish to write to more than one sheet in the workbook, it is necessary to specify an ExcelWriter object: >>> df2 = df1.copy() >>> with pd.ExcelWriter('output.xlsx') as writer: # doctest: +SKIP ... df1.to_excel(writer, sheet_name='Sheet_name_1') ... df2.to_excel(writer, sheet_name='Sheet_name_2') ExcelWriter can also be used to append to an existing Excel file: >>> with pd.ExcelWriter('output.xlsx', ... mode='a') as writer: # doctest: +SKIP ... df.to_excel(writer, sheet_name='Sheet_name_3') To set the library that is used to write the Excel file, you can pass the `engine` keyword (the default engine is automatically chosen depending on the file extension): >>> df1.to_excel('output1.xlsx', engine='xlsxwriter') # doctest: +SKIP \"\"\" df = self if isinstance ( self , ABCDataFrame ) else self . to_frame () from pandas . io . formats . excel import ExcelFormatter formatter = ExcelFormatter ( df , na_rep = na_rep , cols = columns , header = header , float_format = float_format , index = index , index_label = index_label , merge_cells = merge_cells , inf_rep = inf_rep , ) formatter . write ( excel_writer , sheet_name = sheet_name , startrow = startrow , startcol = startcol , freeze_panes = freeze_panes , engine = engine , ) to_feather def to_feather ( self , path , ** kwargs ) -> None Write a DataFrame to the binary Feather format. Parameters path : str String file path. **kwargs : Additional keywords passed to :func: pyarrow.feather.write_feather . Starting with pyarrow 0.17, this includes the compression , compression_level , chunksize and version keywords. .. versionadded :: 1.1.0 View Source @deprecate_kwarg ( old_arg_name = \"fname\" , new_arg_name = \"path\" ) def to_feather ( self , path , ** kwargs ) -> None : \"\"\" Write a DataFrame to the binary Feather format. Parameters ---------- path : str String file path. **kwargs : Additional keywords passed to :func:`pyarrow.feather.write_feather`. Starting with pyarrow 0.17, this includes the `compression`, `compression_level`, `chunksize` and `version` keywords. .. versionadded:: 1.1.0 \"\"\" from pandas.io.feather_format import to_feather to_feather ( self , path , ** kwargs ) to_gbq def to_gbq ( self , destination_table , project_id = None , chunksize = None , reauth = False , if_exists = 'fail' , auth_local_webserver = False , table_schema = None , location = None , progress_bar = True , credentials = None ) -> None Write a DataFrame to a Google BigQuery table. This function requires the pandas-gbq package <https://pandas-gbq.readthedocs.io> __. See the How to authenticate with Google BigQuery <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html> __ guide for authentication instructions. Parameters destination_table : str Name of table to be written, in the form dataset.tablename . project_id : str, optional Google BigQuery Account project ID. Optional when available from the environment. chunksize : int, optional Number of rows to be inserted in each chunk from the dataframe. Set to None to load the whole dataframe at once. reauth : bool, default False Force Google BigQuery to re-authenticate the user. This is useful if multiple accounts are used. if_exists : str, default 'fail' Behavior when the destination table exists. Value can be one of: ``'fail'`` If table exists raise pandas_gbq.gbq.TableCreationError. ``'replace'`` If table exists, drop it, recreate it, and insert data. ``'append'`` If table exists, insert data. Create if does not exist. auth_local_webserver : bool, default False Use the local webserver flow instead of the console flow when getting user credentials. .. _local webserver flow: https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server .. _console flow: https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console *New in version 0.2.0 of pandas-gbq* . table_schema : list of dicts, optional List of BigQuery table fields to which according DataFrame columns conform to, e.g. [{'name': 'col1', 'type': 'STRING'},...] . If schema is not provided, it will be generated according to dtypes of DataFrame columns. See BigQuery API documentation on available names of a field. *New in version 0.3.1 of pandas-gbq*. location : str, optional Location where the load job should run. See the BigQuery locations documentation <https://cloud.google.com/bigquery/docs/dataset-locations> __ for a list of available locations. The location must match that of the target dataset. *New in version 0.5.0 of pandas-gbq*. progress_bar : bool, default True Use the library tqdm to show the progress bar for the upload, chunk by chunk. *New in version 0.5.0 of pandas-gbq*. credentials : google.auth.credentials.Credentials, optional Credentials for accessing Google APIs. Use this parameter to override default credentials, such as to use Compute Engine :class: google.auth.compute_engine.Credentials or Service Account :class: google.oauth2.service_account.Credentials directly. * New in version 0 . 8 . 0 of pandas - gbq * . .. versionadded :: 0 . 24 . 0 See Also pandas_gbq.to_gbq : This function in the pandas-gbq library. read_gbq : Read a DataFrame from Google BigQuery. View Source def to_gbq ( self , destination_table , project_id = None , chunksize = None , reauth = False , if_exists = \"fail\" , auth_local_webserver = False , table_schema = None , location = None , progress_bar = True , credentials = None , ) -> None : \"\"\" Write a DataFrame to a Google BigQuery table. This function requires the `pandas-gbq package <https://pandas-gbq.readthedocs.io>`__. See the `How to authenticate with Google BigQuery <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html>`__ guide for authentication instructions. Parameters ---------- destination_table : str Name of table to be written, in the form ``dataset.tablename``. project_id : str, optional Google BigQuery Account project ID. Optional when available from the environment. chunksize : int, optional Number of rows to be inserted in each chunk from the dataframe. Set to ``None`` to load the whole dataframe at once. reauth : bool, default False Force Google BigQuery to re-authenticate the user. This is useful if multiple accounts are used. if_exists : str, default 'fail' Behavior when the destination table exists. Value can be one of: ``'fail'`` If table exists raise pandas_gbq.gbq.TableCreationError. ``'replace'`` If table exists, drop it, recreate it, and insert data. ``'append'`` If table exists, insert data. Create if does not exist. auth_local_webserver : bool, default False Use the `local webserver flow`_ instead of the `console flow`_ when getting user credentials. .. _local webserver flow: https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server .. _console flow: https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console *New in version 0.2.0 of pandas-gbq*. table_schema : list of dicts, optional List of BigQuery table fields to which according DataFrame columns conform to, e.g. ``[{'name': 'col1', 'type': 'STRING'},...]``. If schema is not provided, it will be generated according to dtypes of DataFrame columns. See BigQuery API documentation on available names of a field. *New in version 0.3.1 of pandas-gbq*. location : str, optional Location where the load job should run. See the `BigQuery locations documentation <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a list of available locations. The location must match that of the target dataset. *New in version 0.5.0 of pandas-gbq*. progress_bar : bool, default True Use the library `tqdm` to show the progress bar for the upload, chunk by chunk. *New in version 0.5.0 of pandas-gbq*. credentials : google.auth.credentials.Credentials, optional Credentials for accessing Google APIs. Use this parameter to override default credentials, such as to use Compute Engine :class:`google.auth.compute_engine.Credentials` or Service Account :class:`google.oauth2.service_account.Credentials` directly. *New in version 0.8.0 of pandas-gbq*. .. versionadded:: 0.24.0 See Also -------- pandas_gbq.to_gbq : This function in the pandas-gbq library. read_gbq : Read a DataFrame from Google BigQuery. \"\"\" from pandas . io import gbq gbq . to_gbq ( self , destination_table , project_id = project_id , chunksize = chunksize , reauth = reauth , if_exists = if_exists , auth_local_webserver = auth_local_webserver , table_schema = table_schema , location = location , progress_bar = progress_bar , credentials = credentials , ) to_hdf def to_hdf ( self , path_or_buf , key : str , mode : str = 'a' , complevel : Union [ int , NoneType ] = None , complib : Union [ str , NoneType ] = None , append : bool = False , format : Union [ str , NoneType ] = None , index : bool = True , min_itemsize : Union [ int , Dict [ str , int ], NoneType ] = None , nan_rep = None , dropna : Union [ bool , NoneType ] = None , data_columns : Union [ bool , List [ str ], NoneType ] = None , errors : str = 'strict' , encoding : str = 'UTF-8' ) -> None Write the contained data to an HDF5 file using HDFStore. Hierarchical Data Format (HDF) is self-describing, allowing an application to interpret the structure and contents of a file with no outside information. One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects. In order to add another DataFrame or Series to an existing HDF file please use append mode and a different a key. For more information see the :ref: user guide <io.hdf5> . Parameters path_or_buf : str or pandas.HDFStore File path or HDFStore object. key : str Identifier for the group in the store. mode : {'a', 'w', 'r+'}, default 'a' Mode to open file: - 'w': write, a new file is created (an existing file with the same name would be deleted). - 'a': append, an existing file is opened for reading and writing, and if the file does not exist it is created. - 'r+': similar to 'a', but the file must already exist. complevel : {0-9}, optional Specifies a compression level for data. A value of 0 disables compression. complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib' Specifies the compression library to be used. As of v0.20.2 these additional compressors for Blosc are supported (default if no compressor specified: 'blosc:blosclz'): {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy', 'blosc:zlib', 'blosc:zstd'}. Specifying a compression library which is not available issues a ValueError. append : bool, default False For Table formats, append the input data to the existing. format : {'fixed', 'table', None}, default 'fixed' Possible values: - 'fixed': Fixed format. Fast writing/reading. Not-appendable, nor searchable. - 'table': Table format. Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data. - If None, pd.get_option('io.hdf.default_format') is checked, followed by fallback to \"fixed\" errors : str, default 'strict' Specifies how encoding and decoding errors are to be handled. See the errors argument for :func: open for a full list of options. encoding : str, default \"UTF-8\" min_itemsize : dict or int, optional Map column names to minimum string sizes for columns. nan_rep : Any, optional How to represent null values as str. Not allowed with append=True. data_columns : list of columns or True, optional List of columns to create as indexed data columns for on-disk queries, or True to use all columns. By default only the axes of the object are indexed. See :ref: io.hdf5-query-data-columns . Applicable only to format='table'. See Also DataFrame.read_hdf : Read from HDF file. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. DataFrame.to_sql : Write to a sql table. DataFrame.to_feather : Write out feather-format for DataFrames. DataFrame.to_csv : Write out to a csv file. Examples df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, ... index=['a', 'b', 'c']) df.to_hdf('data.h5', key='df', mode='w') We can add another object to the same file: s = pd.Series([1, 2, 3, 4]) s.to_hdf('data.h5', key='s') Reading from HDF file: pd.read_hdf('data.h5', 'df') A B a 1 4 b 2 5 c 3 6 pd.read_hdf('data.h5', 's') 0 1 1 2 2 3 3 4 dtype: int64 Deleting file with data: import os os.remove('data.h5') View Source def to_hdf ( self , path_or_buf , key : str , mode : str = \"a\" , complevel : Optional [ int ] = None , complib : Optional [ str ] = None , append : bool_t = False , format : Optional [ str ] = None , index : bool_t = True , min_itemsize : Optional [ Union[int, Dict[str, int ] ]] = None , nan_rep = None , dropna : Optional [ bool_t ] = None , data_columns : Optional [ Union[bool_t, List[str ] ]] = None , errors : str = \"strict\" , encoding : str = \"UTF-8\" , ) -> None : \"\"\" Write the contained data to an HDF5 file using HDFStore. Hierarchical Data Format (HDF) is self-describing, allowing an application to interpret the structure and contents of a file with no outside information. One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects. In order to add another DataFrame or Series to an existing HDF file please use append mode and a different a key. For more information see the :ref:`user guide <io.hdf5>`. Parameters ---------- path_or_buf : str or pandas.HDFStore File path or HDFStore object. key : str Identifier for the group in the store. mode : {'a', 'w', 'r+'}, default 'a' Mode to open file: - 'w': write, a new file is created (an existing file with the same name would be deleted). - 'a': append, an existing file is opened for reading and writing, and if the file does not exist it is created. - 'r+': similar to 'a', but the file must already exist. complevel : {0-9}, optional Specifies a compression level for data. A value of 0 disables compression. complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib' Specifies the compression library to be used. As of v0.20.2 these additional compressors for Blosc are supported (default if no compressor specified: 'blosc:blosclz'): {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy', 'blosc:zlib', 'blosc:zstd'}. Specifying a compression library which is not available issues a ValueError. append : bool, default False For Table formats, append the input data to the existing. format : {'fixed', 'table', None}, default 'fixed' Possible values: - 'fixed': Fixed format. Fast writing/reading. Not-appendable, nor searchable. - 'table': Table format. Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data. - If None, pd.get_option('io.hdf.default_format') is checked, followed by fallback to \" fixed \" errors : str, default 'strict' Specifies how encoding and decoding errors are to be handled. See the errors argument for :func:`open` for a full list of options. encoding : str, default \" UTF - 8 \" min_itemsize : dict or int, optional Map column names to minimum string sizes for columns. nan_rep : Any, optional How to represent null values as str. Not allowed with append=True. data_columns : list of columns or True, optional List of columns to create as indexed data columns for on-disk queries, or True to use all columns. By default only the axes of the object are indexed. See :ref:`io.hdf5-query-data-columns`. Applicable only to format='table'. See Also -------- DataFrame.read_hdf : Read from HDF file. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. DataFrame.to_sql : Write to a sql table. DataFrame.to_feather : Write out feather-format for DataFrames. DataFrame.to_csv : Write out to a csv file. Examples -------- >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, ... index=['a', 'b', 'c']) >>> df.to_hdf('data.h5', key='df', mode='w') We can add another object to the same file: >>> s = pd.Series([1, 2, 3, 4]) >>> s.to_hdf('data.h5', key='s') Reading from HDF file: >>> pd.read_hdf('data.h5', 'df') A B a 1 4 b 2 5 c 3 6 >>> pd.read_hdf('data.h5', 's') 0 1 1 2 2 3 3 4 dtype: int64 Deleting file with data: >>> import os >>> os.remove('data.h5') \"\"\" from pandas . io import pytables pytables . to_hdf ( path_or_buf , key , self , mode = mode , complevel = complevel , complib = complib , append = append , format = format , index = index , min_itemsize = min_itemsize , nan_rep = nan_rep , dropna = dropna , data_columns = data_columns , errors = errors , encoding = encoding , ) to_html def to_html ( self , buf = None , columns = None , col_space = None , header = True , index = True , na_rep = 'NaN' , formatters = None , float_format = None , sparsify = None , index_names = True , justify = None , max_rows = None , max_cols = None , show_dimensions = False , decimal = '.' , bold_rows = True , classes = None , escape = True , notebook = False , border = None , table_id = None , render_links = False , encoding = None ) Render a DataFrame as an HTML table. Parameters buf : str, Path or StringIO-like, optional, default None Buffer to write to. If None, the output is returned as a string. columns : sequence, optional, default None The subset of columns to write. Writes all columns by default. col_space : str or int, list or dict of int or str, optional The minimum width of each column in CSS length units. An int is assumed to be px units. .. versionadded :: 0.25.0 Ability to use str. header : bool, optional Whether to print column labels, default True. index : bool, optional, default True Whether to print index (row) labels. na_rep : str, optional, default 'NaN' String representation of NAN to use. formatters : list, tuple or dict of one-param. functions, optional Formatter functions to apply to columns' elements by position or name. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns. float_format : one-parameter function, optional, default None Formatter function to apply to columns' elements if they are floats. The result of this function must be a unicode string. sparsify : bool, optional, default True Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row. index_names : bool, optional, default True Prints the names of the indexes. justify : str, default None How to justify the column labels. If None uses the option from the print configuration (controlled by set_option), 'right' out of the box. Valid values are * left * right * center * justify * justify-all * start * end * inherit * match-parent * initial * unset. max_rows : int, optional Maximum number of rows to display in the console. min_rows : int, optional The number of rows to display in the console in a truncated repr (when number of rows is above max_rows ). max_cols : int, optional Maximum number of columns to display in the console. show_dimensions : bool, default False Display DataFrame dimensions (number of rows by number of columns). decimal : str, default '.' Character recognized as decimal separator, e.g. ',' in Europe. bold_rows : bool, default True Make the row labels bold in the output. classes : str or list or tuple, default None CSS class(es) to apply to the resulting html table. escape : bool, default True Convert the characters <, >, and & to HTML-safe sequences. notebook : {True, False}, default False Whether the generated HTML is for IPython Notebook. border : int A border=border attribute is included in the opening <table> tag. Default pd.options.display.html.border . encoding : str, default \"utf-8\" Set character encoding. .. versionadded :: 1.0 table_id : str, optional A css id is included in the opening <table> tag if specified. .. versionadded :: 0.23.0 render_links : bool, default False Convert URLs to HTML links. .. versionadded :: 0.24.0 Returns str or None If buf is None, returns the result as a string. Otherwise returns None. See Also to_string : Convert DataFrame to a string. View Source @ Substitution ( header_type = \"bool\" , header = \"Whether to print column labels, default True\" , col_space_type = \"str or int, list or dict of int or str\" , col_space = \"The minimum width of each column in CSS length \" \"units. An int is assumed to be px units.\\n\\n\" \" .. versionadded:: 0.25.0\\n\" \" Ability to use str\" , ) @ Substitution ( shared_params = fmt . common_docstring , returns = fmt . return_docstring ) def to_html( self , buf = None , columns = None , col_space = None , header = True , index = True , na_rep = \"NaN\" , formatters = None , float_format = None , sparsify = None , index_names = True , justify = None , max_rows = None , max_cols = None , show_dimensions = False , decimal = \".\" , bold_rows = True , classes = None , escape = True , notebook = False , border = None , table_id = None , render_links = False , encoding = None , ): \"\"\" Render a DataFrame as an HTML table. %(shared_params)s bold_rows : bool, default True Make the row labels bold in the output. classes : str or list or tuple, default None CSS class(es) to apply to the resulting html table. escape : bool, default True Convert the characters <, >, and & to HTML-safe sequences. notebook : {True, False}, default False Whether the generated HTML is for IPython Notebook. border : int A ``border=border`` attribute is included in the opening `<table>` tag. Default ``pd.options.display.html.border``. encoding : str, default \" utf - 8 \" Set character encoding. .. versionadded:: 1.0 table_id : str, optional A css id is included in the opening `<table>` tag if specified. .. versionadded:: 0.23.0 render_links : bool, default False Convert URLs to HTML links. .. versionadded:: 0.24.0 %(returns)s See Also -------- to_string : Convert DataFrame to a string. \"\"\" if justify is not None and justify not in fmt._VALID_JUSTIFY_PARAMETERS: raise ValueError(\"Invalid value for justify parameter\") formatter = fmt . DataFrameFormatter ( self , columns = columns , col_space = col_space , na_rep = na_rep , formatters = formatters , float_format = float_format , sparsify = sparsify , justify = justify , index_names = index_names , header = header , index = index , bold_rows = bold_rows , escape = escape , max_rows = max_rows , max_cols = max_cols , show_dimensions = show_dimensions , decimal = decimal , table_id = table_id , render_links = render_links , ) # TODO : a generic formatter wld b in DataFrameFormatter return formatter.to_html( buf = buf , classes = classes , notebook = notebook , border = border , encoding = encoding , ) to_json def to_json ( self , path_or_buf : Union [ str , pathlib . Path , IO [ ~ AnyStr ], NoneType ] = None , orient : Union [ str , NoneType ] = None , date_format : Union [ str , NoneType ] = None , double_precision : int = 10 , force_ascii : bool = True , date_unit : str = 'ms' , default_handler : Union [ Callable [[ Any ], Union [ str , int , float , bool , List , Dict , NoneType ]], NoneType ] = None , lines : bool = False , compression : Union [ str , NoneType ] = 'infer' , index : bool = True , indent : Union [ int , NoneType ] = None ) -> Union [ str , NoneType ] Convert the object to a JSON string. Note NaN's and None will be converted to null and datetime objects will be converted to UNIX timestamps. Parameters path_or_buf : str or file handle, optional File path or object. If not specified, the result is returned as a string. orient : str Indication of expected JSON string format. * Series : - default is 'index' - allowed values are : { 'split' , 'records' , 'index' , 'table' } . * DataFrame : - default is 'columns' - allowed values are : { 'split' , 'records' , 'index' , 'columns' , 'values' , 'table' } . * The format of the JSON string : - 'split' : dict like { 'index' -> [ index ] , 'columns' -> [ columns ] , 'data' -> [ values ] } - 'records' : list like [ {column -> value}, ... , {column -> value} ] - 'index' : dict like { index -> { column -> value }} - 'columns' : dict like { column -> { index -> value }} - 'values' : just the values array - 'table' : dict like { 'schema' : { schema } , 'data' : { data }} Describing the data , where data component is like `` orient = 'records' `` . .. versionchanged : : 0.20.0 date_format : {None, 'epoch', 'iso'} Type of date conversion. 'epoch' = epoch milliseconds, 'iso' = ISO8601. The default depends on the orient . For orient='table' , the default is 'iso'. For all other orients, the default is 'epoch'. double_precision : int, default 10 The number of decimal places to use when encoding floating point values. force_ascii : bool, default True Force encoded string to be ASCII. date_unit : str, default 'ms' (milliseconds) The time unit to encode to, governs timestamp and ISO8601 precision. One of 's', 'ms', 'us', 'ns' for second, millisecond, microsecond, and nanosecond respectively. default_handler : callable, default None Handler to call if object cannot otherwise be converted to a suitable format for JSON. Should receive a single argument which is the object to convert and return a serialisable object. lines : bool, default False If 'orient' is 'records' write out line delimited json format. Will throw ValueError if incorrect 'orient' since others are not list like. compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None} A string representing the compression to use in the output file , only used when the first argument is a filename . By default , the compression is inferred from the filename . .. versionchanged :: 0 . 24 . 0 'infer' option added and set to default index : bool, default True Whether to include the index values in the JSON string. Not including the index ( index=False ) is only supported when orient is 'split' or 'table'. .. versionadded :: 0.23.0 indent : int, optional Length of whitespace used to indent each record. .. versionadded:: 1.0.0 Returns None or str If path_or_buf is None, returns the resulting json format as a string. Otherwise returns None. See Also read_json : Convert a JSON string to pandas object. Notes The behavior of indent=0 varies from the stdlib, which does not indent the output but does insert newlines. Currently, indent=0 and the default indent=None are equivalent in pandas, though this may change in a future release. Examples import json df = pd.DataFrame( ... [[\"a\", \"b\"], [\"c\", \"d\"]], ... index=[\"row 1\", \"row 2\"], ... columns=[\"col 1\", \"col 2\"], ... ) result = df.to_json(orient=\"split\") parsed = json.loads(result) json.dumps(parsed, indent=4) # doctest: +SKIP { \"columns\": [ \"col 1\", \"col 2\" ], \"index\": [ \"row 1\", \"row 2\" ], \"data\": [ [ \"a\", \"b\" ], [ \"c\", \"d\" ] ] } Encoding/decoding a Dataframe using 'records' formatted JSON. Note that index labels are not preserved with this encoding. result = df.to_json(orient=\"records\") parsed = json.loads(result) json.dumps(parsed, indent=4) # doctest: +SKIP [ { \"col 1\": \"a\", \"col 2\": \"b\" }, { \"col 1\": \"c\", \"col 2\": \"d\" } ] Encoding/decoding a Dataframe using 'index' formatted JSON: result = df.to_json(orient=\"index\") parsed = json.loads(result) json.dumps(parsed, indent=4) # doctest: +SKIP { \"row 1\": { \"col 1\": \"a\", \"col 2\": \"b\" }, \"row 2\": { \"col 1\": \"c\", \"col 2\": \"d\" } } Encoding/decoding a Dataframe using 'columns' formatted JSON: result = df.to_json(orient=\"columns\") parsed = json.loads(result) json.dumps(parsed, indent=4) # doctest: +SKIP { \"col 1\": { \"row 1\": \"a\", \"row 2\": \"c\" }, \"col 2\": { \"row 1\": \"b\", \"row 2\": \"d\" } } Encoding/decoding a Dataframe using 'values' formatted JSON: result = df.to_json(orient=\"values\") parsed = json.loads(result) json.dumps(parsed, indent=4) # doctest: +SKIP [ [ \"a\", \"b\" ], [ \"c\", \"d\" ] ] Encoding with Table Schema: result = df.to_json(orient=\"table\") parsed = json.loads(result) json.dumps(parsed, indent=4) # doctest: +SKIP { \"schema\": { \"fields\": [ { \"name\": \"index\", \"type\": \"string\" }, { \"name\": \"col 1\", \"type\": \"string\" }, { \"name\": \"col 2\", \"type\": \"string\" } ], \"primaryKey\": [ \"index\" ], \"pandas_version\": \"0.20.0\" }, \"data\": [ { \"index\": \"row 1\", \"col 1\": \"a\", \"col 2\": \"b\" }, { \"index\": \"row 2\", \"col 1\": \"c\", \"col 2\": \"d\" } ] } View Source def to_json ( self , path_or_buf : Optional [ FilePathOrBuffer ] = None , orient : Optional [ str ] = None , date_format : Optional [ str ] = None , double_precision : int = 10 , force_ascii : bool_t = True , date_unit : str = \"ms\" , default_handler : Optional [ Callable[[Any ] , JSONSerializable ]] = None , lines : bool_t = False , compression : Optional [ str ] = \"infer\" , index : bool_t = True , indent : Optional [ int ] = None , ) -> Optional [ str ] : \"\"\" Convert the object to a JSON string. Note NaN's and None will be converted to null and datetime objects will be converted to UNIX timestamps. Parameters ---------- path_or_buf : str or file handle, optional File path or object. If not specified, the result is returned as a string. orient : str Indication of expected JSON string format. * Series: - default is 'index' - allowed values are: {'split','records','index','table'}. * DataFrame: - default is 'columns' - allowed values are: {'split', 'records', 'index', 'columns', 'values', 'table'}. * The format of the JSON string: - 'split' : dict like {'index' -> [index], 'columns' -> [columns], 'data' -> [values]} - 'records' : list like [{column -> value}, ... , {column -> value}] - 'index' : dict like {index -> {column -> value}} - 'columns' : dict like {column -> {index -> value}} - 'values' : just the values array - 'table' : dict like {'schema': {schema}, 'data': {data}} Describing the data, where data component is like ``orient='records'``. .. versionchanged:: 0.20.0 date_format : {None, 'epoch', 'iso'} Type of date conversion. 'epoch' = epoch milliseconds, 'iso' = ISO8601. The default depends on the `orient`. For ``orient='table'``, the default is 'iso'. For all other orients, the default is 'epoch'. double_precision : int, default 10 The number of decimal places to use when encoding floating point values. force_ascii : bool, default True Force encoded string to be ASCII. date_unit : str, default 'ms' (milliseconds) The time unit to encode to, governs timestamp and ISO8601 precision. One of 's', 'ms', 'us', 'ns' for second, millisecond, microsecond, and nanosecond respectively. default_handler : callable, default None Handler to call if object cannot otherwise be converted to a suitable format for JSON. Should receive a single argument which is the object to convert and return a serialisable object. lines : bool, default False If 'orient' is 'records' write out line delimited json format. Will throw ValueError if incorrect 'orient' since others are not list like. compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None} A string representing the compression to use in the output file, only used when the first argument is a filename. By default, the compression is inferred from the filename. .. versionchanged:: 0.24.0 'infer' option added and set to default index : bool, default True Whether to include the index values in the JSON string. Not including the index (``index=False``) is only supported when orient is 'split' or 'table'. .. versionadded:: 0.23.0 indent : int, optional Length of whitespace used to indent each record. .. versionadded:: 1.0.0 Returns ------- None or str If path_or_buf is None, returns the resulting json format as a string. Otherwise returns None. See Also -------- read_json : Convert a JSON string to pandas object. Notes ----- The behavior of ``indent=0`` varies from the stdlib, which does not indent the output but does insert newlines. Currently, ``indent=0`` and the default ``indent=None`` are equivalent in pandas, though this may change in a future release. Examples -------- >>> import json >>> df = pd.DataFrame( ... [[\" a \", \" b \"], [\" c \", \" d \"]], ... index=[\" row 1 \", \" row 2 \"], ... columns=[\" col 1 \", \" col 2 \"], ... ) >>> result = df.to_json(orient=\" split \") >>> parsed = json.loads(result) >>> json.dumps(parsed, indent=4) # doctest: +SKIP { \" columns \": [ \" col 1 \", \" col 2 \" ], \" index \": [ \" row 1 \", \" row 2 \" ], \" data \": [ [ \" a \", \" b \" ], [ \" c \", \" d \" ] ] } Encoding/decoding a Dataframe using ``'records'`` formatted JSON. Note that index labels are not preserved with this encoding. >>> result = df.to_json(orient=\" records \") >>> parsed = json.loads(result) >>> json.dumps(parsed, indent=4) # doctest: +SKIP [ { \" col 1 \": \" a \", \" col 2 \": \" b \" }, { \" col 1 \": \" c \", \" col 2 \": \" d \" } ] Encoding/decoding a Dataframe using ``'index'`` formatted JSON: >>> result = df.to_json(orient=\" index \") >>> parsed = json.loads(result) >>> json.dumps(parsed, indent=4) # doctest: +SKIP { \" row 1 \": { \" col 1 \": \" a \", \" col 2 \": \" b \" }, \" row 2 \": { \" col 1 \": \" c \", \" col 2 \": \" d \" } } Encoding/decoding a Dataframe using ``'columns'`` formatted JSON: >>> result = df.to_json(orient=\" columns \") >>> parsed = json.loads(result) >>> json.dumps(parsed, indent=4) # doctest: +SKIP { \" col 1 \": { \" row 1 \": \" a \", \" row 2 \": \" c \" }, \" col 2 \": { \" row 1 \": \" b \", \" row 2 \": \" d \" } } Encoding/decoding a Dataframe using ``'values'`` formatted JSON: >>> result = df.to_json(orient=\" values \") >>> parsed = json.loads(result) >>> json.dumps(parsed, indent=4) # doctest: +SKIP [ [ \" a \", \" b \" ], [ \" c \", \" d \" ] ] Encoding with Table Schema: >>> result = df.to_json(orient=\" table \") >>> parsed = json.loads(result) >>> json.dumps(parsed, indent=4) # doctest: +SKIP { \" schema \": { \" fields \": [ { \" name \": \" index \", \" type \": \" string \" }, { \" name \": \" col 1 \", \" type \": \" string \" }, { \" name \": \" col 2 \", \" type \": \" string \" } ], \" primaryKey \": [ \" index \" ], \" pandas_version \": \" 0.20.0 \" }, \" data \": [ { \" index \": \" row 1 \", \" col 1 \": \" a \", \" col 2 \": \" b \" }, { \" index \": \" row 2 \", \" col 1 \": \" c \", \" col 2 \": \" d \" } ] } \"\"\" from pandas . io import json if date_format is None and orient == \"table\" : date_format = \"iso\" elif date_format is None : date_format = \"epoch\" config . is_nonnegative_int ( indent ) indent = indent or 0 return json . to_json ( path_or_buf = path_or_buf , obj = self , orient = orient , date_format = date_format , double_precision = double_precision , force_ascii = force_ascii , date_unit = date_unit , default_handler = default_handler , lines = lines , compression = compression , index = index , indent = indent , ) to_latex def to_latex ( self , buf = None , columns = None , col_space = None , header = True , index = True , na_rep = 'NaN' , formatters = None , float_format = None , sparsify = None , index_names = True , bold_rows = False , column_format = None , longtable = None , escape = None , encoding = None , decimal = '.' , multicolumn = None , multicolumn_format = None , multirow = None , caption = None , label = None ) Render object to a LaTeX tabular, longtable, or nested table/tabular. Requires \\usepackage{booktabs} . The output can be copy/pasted into a main LaTeX document or read from an external file with \\input{table.tex} . .. versionchanged:: 0.20.2 Added to Series. .. versionchanged:: 1.0.0 Added caption and label arguments. Parameters buf : str, Path or StringIO-like, optional, default None Buffer to write to. If None, the output is returned as a string. columns : list of label, optional The subset of columns to write. Writes all columns by default. col_space : int, optional The minimum width of each column. header : bool or list of str, default True Write out the column names. If a list of strings is given, it is assumed to be aliases for the column names. index : bool, default True Write row names (index). na_rep : str, default 'NaN' Missing data representation. formatters : list of functions or dict of {str: function}, optional Formatter functions to apply to columns' elements by position or name. The result of each function must be a unicode string. List must be of length equal to the number of columns. float_format : one-parameter function or str, optional, default None Formatter for floating point numbers. For example float_format=\"%.2f\" and float_format=\"{:0.2f}\".format will both result in 0.1234 being formatted as 0.12. sparsify : bool, optional Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row. By default, the value will be read from the config module. index_names : bool, default True Prints the names of the indexes. bold_rows : bool, default False Make the row labels bold in the output. column_format : str, optional The columns format as specified in LaTeX table format <https://en.wikibooks.org/wiki/LaTeX/Tables> __ e.g. 'rcl' for 3 columns. By default, 'l' will be used for all columns except columns of numbers, which default to 'r'. longtable : bool, optional By default, the value will be read from the pandas config module. Use a longtable environment instead of tabular. Requires adding a \\usepackage{longtable} to your LaTeX preamble. escape : bool, optional By default, the value will be read from the pandas config module. When set to False prevents from escaping latex special characters in column names. encoding : str, optional A string representing the encoding to use in the output file, defaults to 'utf-8'. decimal : str, default '.' Character recognized as decimal separator, e.g. ',' in Europe. multicolumn : bool, default True Use \\multicolumn to enhance MultiIndex columns. The default will be read from the config module. multicolumn_format : str, default 'l' The alignment for multicolumns, similar to column_format The default will be read from the config module. multirow : bool, default False Use \\multirow to enhance MultiIndex rows. Requires adding a \\usepackage{multirow} to your LaTeX preamble. Will print centered labels (instead of top-aligned) across the contained rows, separating groups via clines. The default will be read from the pandas config module. caption : str, optional The LaTeX caption to be placed inside \\caption{} in the output. .. versionadded :: 1.0.0 label : str, optional The LaTeX label to be placed inside \\label{} in the output. This is used with \\ref{} in the main .tex file. .. versionadded :: 1.0.0 Returns str or None If buf is None, returns the result as a string. Otherwise returns None. See Also DataFrame.to_string : Render a DataFrame to a console-friendly tabular output. DataFrame.to_html : Render a DataFrame as an HTML table. Examples df = pd.DataFrame({'name': ['Raphael', 'Donatello'], ... 'mask': ['red', 'purple'], ... 'weapon': ['sai', 'bo staff']}) print(df.to_latex(index=False)) # doctest: +NORMALIZE_WHITESPACE \\begin{tabular}{lll} \\toprule name & mask & weapon \\ \\midrule Raphael & red & sai \\ Donatello & purple & bo staff \\ \\bottomrule \\end{tabular} View Source @ Substitution ( returns = fmt . return_docstring ) def to_latex ( self , buf = None , columns = None , col_space = None , header = True , index = True , na_rep = \"NaN\" , formatters = None , float_format = None , sparsify = None , index_names = True , bold_rows = False , column_format = None , longtable = None , escape = None , encoding = None , decimal = \".\" , multicolumn = None , multicolumn_format = None , multirow = None , caption = None , label = None , ): r \"\"\" Render object to a LaTeX tabular, longtable, or nested table/tabular. Requires ``\\usepackage{booktabs}``. The output can be copy/pasted into a main LaTeX document or read from an external file with ``\\input{table.tex}``. .. versionchanged:: 0.20.2 Added to Series. .. versionchanged:: 1.0.0 Added caption and label arguments. Parameters ---------- buf : str, Path or StringIO-like, optional, default None Buffer to write to. If None, the output is returned as a string. columns : list of label, optional The subset of columns to write. Writes all columns by default. col_space : int, optional The minimum width of each column. header : bool or list of str, default True Write out the column names. If a list of strings is given, it is assumed to be aliases for the column names. index : bool, default True Write row names (index). na_rep : str, default 'NaN' Missing data representation. formatters : list of functions or dict of {str: function}, optional Formatter functions to apply to columns' elements by position or name. The result of each function must be a unicode string. List must be of length equal to the number of columns. float_format : one-parameter function or str, optional, default None Formatter for floating point numbers. For example ``float_format=\" %% . 2 f \"`` and ``float_format=\" { : 0 . 2 f } \".format`` will both result in 0.1234 being formatted as 0.12. sparsify : bool, optional Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row. By default, the value will be read from the config module. index_names : bool, default True Prints the names of the indexes. bold_rows : bool, default False Make the row labels bold in the output. column_format : str, optional The columns format as specified in `LaTeX table format <https://en.wikibooks.org/wiki/LaTeX/Tables>`__ e.g. 'rcl' for 3 columns. By default, 'l' will be used for all columns except columns of numbers, which default to 'r'. longtable : bool, optional By default, the value will be read from the pandas config module. Use a longtable environment instead of tabular. Requires adding a \\usepackage{longtable} to your LaTeX preamble. escape : bool, optional By default, the value will be read from the pandas config module. When set to False prevents from escaping latex special characters in column names. encoding : str, optional A string representing the encoding to use in the output file, defaults to 'utf-8'. decimal : str, default '.' Character recognized as decimal separator, e.g. ',' in Europe. multicolumn : bool, default True Use \\multicolumn to enhance MultiIndex columns. The default will be read from the config module. multicolumn_format : str, default 'l' The alignment for multicolumns, similar to `column_format` The default will be read from the config module. multirow : bool, default False Use \\multirow to enhance MultiIndex rows. Requires adding a \\usepackage{multirow} to your LaTeX preamble. Will print centered labels (instead of top-aligned) across the contained rows, separating groups via clines. The default will be read from the pandas config module. caption : str, optional The LaTeX caption to be placed inside ``\\caption{}`` in the output. .. versionadded:: 1.0.0 label : str, optional The LaTeX label to be placed inside ``\\label{}`` in the output. This is used with ``\\ref{}`` in the main ``.tex`` file. .. versionadded:: 1.0.0 %(returns)s See Also -------- DataFrame.to_string : Render a DataFrame to a console-friendly tabular output. DataFrame.to_html : Render a DataFrame as an HTML table. Examples -------- >>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'], ... 'mask': ['red', 'purple'], ... 'weapon': ['sai', 'bo staff']}) >>> print(df.to_latex(index=False)) # doctest: +NORMALIZE_WHITESPACE \\begin{tabular}{lll} \\toprule name & mask & weapon \\\\ \\midrule Raphael & red & sai \\\\ Donatello & purple & bo staff \\\\ \\bottomrule \\end{tabular} \"\"\" # Get defaults from the pandas config if self . ndim == 1 : self = self . to_frame () if longtable is None : longtable = config . get_option ( \"display.latex.longtable\" ) if escape is None : escape = config . get_option ( \"display.latex.escape\" ) if multicolumn is None : multicolumn = config . get_option ( \"display.latex.multicolumn\" ) if multicolumn_format is None : multicolumn_format = config . get_option ( \"display.latex.multicolumn_format\" ) if multirow is None : multirow = config . get_option ( \"display.latex.multirow\" ) formatter = DataFrameFormatter ( self , columns = columns , col_space = col_space , na_rep = na_rep , header = header , index = index , formatters = formatters , float_format = float_format , bold_rows = bold_rows , sparsify = sparsify , index_names = index_names , escape = escape , decimal = decimal , ) return formatter . to_latex ( buf = buf , column_format = column_format , longtable = longtable , encoding = encoding , multicolumn = multicolumn , multicolumn_format = multicolumn_format , multirow = multirow , caption = caption , label = label , ) to_markdown def to_markdown ( self , buf : Union [ IO [ str ], NoneType ] = None , mode : Union [ str , NoneType ] = None , index : bool = True , ** kwargs ) -> Union [ str , NoneType ] Print DataFrame in Markdown-friendly format. .. versionadded:: 1.0.0 Parameters buf : str, Path or StringIO-like, optional, default None Buffer to write to. If None, the output is returned as a string. mode : str, optional Mode in which file is opened. index : bool, optional, default True Add index (row) labels. .. versionadded :: 1.1.0 **kwargs These parameters will be passed to tabulate <https://pypi.org/project/tabulate> _. Returns str DataFrame in Markdown-friendly format. Examples s = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\") print(s.to_markdown()) | | animal | |---:|:---------| | 0 | elk | | 1 | pig | | 2 | dog | | 3 | quetzal | Output markdown with a tabulate option. print(s.to_markdown(tablefmt=\"grid\")) +----+----------+ | | animal | +====+==========+ | 0 | elk | +----+----------+ | 1 | pig | +----+----------+ | 2 | dog | +----+----------+ | 3 | quetzal | +----+----------+ View Source @ doc ( Series . to_markdown , klass= _ shared_doc_kwargs [ \"klass\" ], examples= \"\"\"Examples -------- >>> df = pd.DataFrame( ... data={\" animal_1 \": [\" elk \", \" pig \"], \" animal_2 \": [\" dog \", \" quetzal \"]} ... ) >>> print(df.to_markdown()) | | animal_1 | animal_2 | |---:|:-----------|:-----------| | 0 | elk | dog | | 1 | pig | quetzal | Output markdown with a tabulate option. >>> print(df.to_markdown(tablefmt=\" grid \")) +----+------------+------------+ | | animal_1 | animal_2 | +====+============+============+ | 0 | elk | dog | +----+------------+------------+ | 1 | pig | quetzal | +----+------------+------------+ \"\"\" , ) def to_markdown ( self , buf : Optional [ IO [ str ]] = None , mode : Optional [ str ] = None , index : bool = True , **kwargs , ) -> Optional [ str ] : if \"showindex\" in kwargs : warnings . warn ( \"'showindex' is deprecated. Only 'index' will be used \" \"in a future version. Use 'index' to silence this warning.\" , FutureWarning , stacklevel = 2 , ) kwargs . setdefault ( \"headers\" , \"keys\" ) kwargs . setdefault ( \"tablefmt\" , \"pipe\" ) kwargs . setdefault ( \"showindex\" , index ) tabulate = import_optional_dependency ( \"tabulate\" ) result = tabulate . tabulate ( self , **kwargs ) if buf is None : return result buf , _ , _ , _ = get_filepath_or_buffer ( buf , mode = mode ) assert buf is not None # Help mypy . buf . writelines ( result ) return None to_numpy def to_numpy ( self , dtype = None , copy : bool = False , na_value =< object object at 0x7fb7e907ade0 > ) -> numpy . ndarray Convert the DataFrame to a NumPy array. .. versionadded:: 0.24.0 By default, the dtype of the returned array will be the common NumPy dtype of all types in the DataFrame. For example, if the dtypes are float16 and float32 , the results dtype will be float32 . This may require copying data and coercing values, which may be expensive. Parameters dtype : str or numpy.dtype, optional The dtype to pass to :meth: numpy.asarray . copy : bool, default False Whether to ensure that the returned value is not a view on another array. Note that copy=False does not ensure that to_numpy() is no-copy. Rather, copy=True ensure that a copy is made, even if not strictly necessary. na_value : Any, optional The value to use for missing values. The default value depends on dtype and the dtypes of the DataFrame columns. .. versionadded :: 1.1.0 Returns numpy.ndarray See Also Series.to_numpy : Similar method for Series. Examples pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy() array([[1, 3], [2, 4]]) With heterogeneous data, the lowest common type will have to be used. df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]}) df.to_numpy() array([[1. , 3. ], [2. , 4.5]]) For a mix of numeric and non-numeric types, the output array will have object dtype. df['C'] = pd.date_range('2000', periods=2) df.to_numpy() array([[1, 3.0, Timestamp('2000-01-01 00:00:00')], [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object) View Source def to_numpy ( self , dtype = None , copy : bool = False , na_value = lib . no_default ) -> np . ndarray : \"\"\" Convert the DataFrame to a NumPy array. .. versionadded:: 0.24.0 By default, the dtype of the returned array will be the common NumPy dtype of all types in the DataFrame. For example, if the dtypes are ``float16`` and ``float32``, the results dtype will be ``float32``. This may require copying data and coercing values, which may be expensive. Parameters ---------- dtype : str or numpy.dtype, optional The dtype to pass to :meth:`numpy.asarray`. copy : bool, default False Whether to ensure that the returned value is not a view on another array. Note that ``copy=False`` does not *ensure* that ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that a copy is made, even if not strictly necessary. na_value : Any, optional The value to use for missing values. The default value depends on `dtype` and the dtypes of the DataFrame columns. .. versionadded:: 1.1.0 Returns ------- numpy.ndarray See Also -------- Series.to_numpy : Similar method for Series. Examples -------- >>> pd.DataFrame({\" A \": [1, 2], \" B \": [3, 4]}).to_numpy() array([[1, 3], [2, 4]]) With heterogeneous data, the lowest common type will have to be used. >>> df = pd.DataFrame({\" A \": [1, 2], \" B \": [3.0, 4.5]}) >>> df.to_numpy() array([[1. , 3. ], [2. , 4.5]]) For a mix of numeric and non-numeric types, the output array will have object dtype. >>> df['C'] = pd.date_range('2000', periods=2) >>> df.to_numpy() array([[1, 3.0, Timestamp('2000-01-01 00:00:00')], [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object) \"\"\" self . _consolidate_inplace () result = self . _mgr . as_array ( transpose = self . _AXIS_REVERSED , dtype = dtype , copy = copy , na_value = na_value ) if result . dtype is not dtype : result = np . array ( result , dtype = dtype , copy = False ) return result to_parquet def to_parquet ( self , path : Union [ str , pathlib . Path , IO [ ~ AnyStr ]], engine : str = 'auto' , compression : Union [ str , NoneType ] = 'snappy' , index : Union [ bool , NoneType ] = None , partition_cols : Union [ List [ str ], NoneType ] = None , ** kwargs ) -> None Write a DataFrame to the binary parquet format. This function writes the dataframe as a parquet file <https://parquet.apache.org/> _. You can choose different parquet backends, and have the option of compression. See :ref: the user guide <io.parquet> for more details. Parameters path : str or file-like object If a string, it will be used as Root Directory path when writing a partitioned dataset. By file-like object, we refer to objects with a write() method, such as a file handler (e.g. via builtin open function) or io.BytesIO. The engine fastparquet does not accept file-like objects. .. versionchanged :: 1.0.0 Previously this was \"fname\" engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto' Parquet library to use. If 'auto', then the option io.parquet.engine is used. The default io.parquet.engine behavior is to try 'pyarrow', falling back to 'fastparquet' if 'pyarrow' is unavailable. compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy' Name of the compression to use. Use None for no compression. index : bool, default None If True , include the dataframe's index(es) in the file output. If False , they will not be written to the file. If None , similar to True the dataframe's index(es) will be saved. However, instead of being saved as values, the RangeIndex will be stored as a range in the metadata so it doesn't require much space and is faster. Other indexes will be included as columns in the file output. .. versionadded :: 0.24.0 partition_cols : list, optional, default None Column names by which to partition the dataset. Columns are partitioned in the order they are given. Must be None if path is not a string. .. versionadded :: 0.24.0 **kwargs Additional arguments passed to the parquet library. See :ref: pandas io <io.parquet> for more details. See Also read_parquet : Read a parquet file. DataFrame.to_csv : Write a csv file. DataFrame.to_sql : Write to a sql table. DataFrame.to_hdf : Write to hdf. Notes This function requires either the fastparquet <https://pypi.org/project/fastparquet> or pyarrow <https://arrow.apache.org/docs/python/> library. Examples df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]}) df.to_parquet('df.parquet.gzip', ... compression='gzip') # doctest: +SKIP pd.read_parquet('df.parquet.gzip') # doctest: +SKIP col1 col2 0 1 3 1 2 4 If you want to get a buffer to the parquet content you can use a io.BytesIO object, as long as you don't use partition_cols, which creates multiple files. import io f = io.BytesIO() df.to_parquet(f) f.seek(0) 0 content = f.read() View Source @ deprecate_kwarg ( old_arg_name = \"fname\" , new_arg_name = \"path\" ) def to_parquet ( self , path : FilePathOrBuffer [ AnyStr ], engine : str = \"auto\" , compression : Optional [ str ] = \"snappy\" , index : Optional [ bool ] = None , partition_cols : Optional [ List [ str ]] = None , ** kwargs , ) -> None : \"\"\" Write a DataFrame to the binary parquet format. This function writes the dataframe as a `parquet file <https://parquet.apache.org/>`_. You can choose different parquet backends, and have the option of compression. See :ref:`the user guide <io.parquet>` for more details. Parameters ---------- path : str or file-like object If a string, it will be used as Root Directory path when writing a partitioned dataset. By file-like object, we refer to objects with a write() method, such as a file handler (e.g. via builtin open function) or io.BytesIO. The engine fastparquet does not accept file-like objects. .. versionchanged:: 1.0.0 Previously this was \" fname \" engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto' Parquet library to use. If 'auto', then the option ``io.parquet.engine`` is used. The default ``io.parquet.engine`` behavior is to try 'pyarrow', falling back to 'fastparquet' if 'pyarrow' is unavailable. compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy' Name of the compression to use. Use ``None`` for no compression. index : bool, default None If ``True``, include the dataframe's index(es) in the file output. If ``False``, they will not be written to the file. If ``None``, similar to ``True`` the dataframe's index(es) will be saved. However, instead of being saved as values, the RangeIndex will be stored as a range in the metadata so it doesn't require much space and is faster. Other indexes will be included as columns in the file output. .. versionadded:: 0.24.0 partition_cols : list, optional, default None Column names by which to partition the dataset. Columns are partitioned in the order they are given. Must be None if path is not a string. .. versionadded:: 0.24.0 **kwargs Additional arguments passed to the parquet library. See :ref:`pandas io <io.parquet>` for more details. See Also -------- read_parquet : Read a parquet file. DataFrame.to_csv : Write a csv file. DataFrame.to_sql : Write to a sql table. DataFrame.to_hdf : Write to hdf. Notes ----- This function requires either the `fastparquet <https://pypi.org/project/fastparquet>`_ or `pyarrow <https://arrow.apache.org/docs/python/>`_ library. Examples -------- >>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]}) >>> df.to_parquet('df.parquet.gzip', ... compression='gzip') # doctest: +SKIP >>> pd.read_parquet('df.parquet.gzip') # doctest: +SKIP col1 col2 0 1 3 1 2 4 If you want to get a buffer to the parquet content you can use a io.BytesIO object, as long as you don't use partition_cols, which creates multiple files. >>> import io >>> f = io.BytesIO() >>> df.to_parquet(f) >>> f.seek(0) 0 >>> content = f.read() \"\"\" from pandas . io . parquet import to_parquet to_parquet ( self , path , engine , compression = compression , index = index , partition_cols = partition_cols , ** kwargs , ) to_period def to_period ( self , freq = None , axis : Union [ str , int ] = 0 , copy : bool = True ) -> 'DataFrame' Convert DataFrame from DatetimeIndex to PeriodIndex. Convert DataFrame from DatetimeIndex to PeriodIndex with desired frequency (inferred from index if not passed). Parameters freq : str, default Frequency of the PeriodIndex. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to convert (the index by default). copy : bool, default True If False then underlying input data is not copied. Returns DataFrame with PeriodIndex View Source def to_period ( self , freq = None , axis : Axis = 0 , copy : bool = True ) -> \"DataFrame\" : \"\"\" Convert DataFrame from DatetimeIndex to PeriodIndex. Convert DataFrame from DatetimeIndex to PeriodIndex with desired frequency (inferred from index if not passed). Parameters ---------- freq : str, default Frequency of the PeriodIndex. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to convert (the index by default). copy : bool, default True If False then underlying input data is not copied. Returns ------- DataFrame with PeriodIndex \"\"\" new_obj = self . copy ( deep = copy ) axis_name = self . _get_axis_name ( axis ) old_ax = getattr ( self , axis_name ) new_ax = old_ax . to_period ( freq = freq ) setattr ( new_obj , axis_name , new_ax ) return new_obj to_pickle def to_pickle ( self , path , compression : Union [ str , NoneType ] = 'infer' , protocol : int = 5 ) -> None Pickle (serialize) object to file. Parameters path : str File path where the pickled object will be stored. compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer' A string representing the compression to use in the output file. By default, infers from the file extension in specified path. protocol : int Int which indicates which protocol should be used by the pickler, default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible values are 0, 1, 2, 3, 4. A negative value for the protocol parameter is equivalent to setting its value to HIGHEST_PROTOCOL. .. [1] https://docs.python.org/3/library/pickle.html. See Also read_pickle : Load pickled pandas object (or any object) from file. DataFrame.to_hdf : Write DataFrame to an HDF5 file. DataFrame.to_sql : Write DataFrame to a SQL database. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. Examples original_df = pd.DataFrame({\"foo\": range(5), \"bar\": range(5, 10)}) original_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 original_df.to_pickle(\"./dummy.pkl\") unpickled_df = pd.read_pickle(\"./dummy.pkl\") unpickled_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 import os os.remove(\"./dummy.pkl\") View Source def to_pickle ( self , path , compression : Optional [ str ] = \"infer\" , protocol : int = pickle . HIGHEST_PROTOCOL , ) -> None : \"\"\" Pickle (serialize) object to file. Parameters ---------- path : str File path where the pickled object will be stored. compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, \\ default 'infer' A string representing the compression to use in the output file. By default, infers from the file extension in specified path. protocol : int Int which indicates which protocol should be used by the pickler, default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible values are 0, 1, 2, 3, 4. A negative value for the protocol parameter is equivalent to setting its value to HIGHEST_PROTOCOL. .. [1] https://docs.python.org/3/library/pickle.html. See Also -------- read_pickle : Load pickled pandas object (or any object) from file. DataFrame.to_hdf : Write DataFrame to an HDF5 file. DataFrame.to_sql : Write DataFrame to a SQL database. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. Examples -------- >>> original_df = pd.DataFrame({\" foo \": range(5), \" bar \": range(5, 10)}) >>> original_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> original_df.to_pickle(\" . / dummy . pkl \") >>> unpickled_df = pd.read_pickle(\" . / dummy . pkl \") >>> unpickled_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> import os >>> os.remove(\" . / dummy . pkl \") \"\"\" from pandas . io . pickle import to_pickle to_pickle ( self , path , compression = compression , protocol = protocol ) to_records def to_records ( self , index = True , column_dtypes = None , index_dtypes = None ) -> numpy . recarray Convert DataFrame to a NumPy record array. Index will be included as the first field of the record array if requested. Parameters index : bool, default True Include index in resulting record array, stored in 'index' field or using the index label, if set. column_dtypes : str, type, dict, default None .. versionadded:: 0.24.0 If a string or type, the data type to store all columns. If a dictionary, a mapping of column names and indices (zero-indexed) to specific data types. index_dtypes : str, type, dict, default None .. versionadded:: 0.24.0 If a string or type , the data type to store all index levels . If a dictionary , a mapping of index level names and indices ( zero - indexed ) to specific data types . This mapping is applied only if ` index = True ` . Returns numpy.recarray NumPy ndarray with the DataFrame labels as fields and each row of the DataFrame as entries. See Also DataFrame.from_records: Convert structured or record ndarray to DataFrame. numpy.recarray: An ndarray that allows field access using attributes, analogous to typed columns in a spreadsheet. Examples df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]}, ... index=['a', 'b']) df A B a 1 0.50 b 2 0.75 df.to_records() rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)], dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')]) If the DataFrame index has no label then the recarray field name is set to 'index'. If the index has a label then this is used as the field name: df.index = df.index.rename(\"I\") df.to_records() rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)], dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')]) The index can be excluded from the record array: df.to_records(index=False) rec.array([(1, 0.5 ), (2, 0.75)], dtype=[('A', '<i8'), ('B', '<f8')]) Data types can be specified for the columns: df.to_records(column_dtypes={\"A\": \"int32\"}) rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)], dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')]) As well as for the index: df.to_records(index_dtypes=\"<S2\") rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)], dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')]) index_dtypes = f\"<S{df.index.str.len().max()}\" df.to_records(index_dtypes=index_dtypes) rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)], dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')]) View Source def to_records ( self , index = True , column_dtypes = None , index_dtypes = None ) -> np . recarray : \"\"\" Convert DataFrame to a NumPy record array. Index will be included as the first field of the record array if requested. Parameters ---------- index : bool, default True Include index in resulting record array, stored in 'index' field or using the index label, if set. column_dtypes : str, type, dict, default None .. versionadded:: 0.24.0 If a string or type, the data type to store all columns. If a dictionary, a mapping of column names and indices (zero-indexed) to specific data types. index_dtypes : str, type, dict, default None .. versionadded:: 0.24.0 If a string or type, the data type to store all index levels. If a dictionary, a mapping of index level names and indices (zero-indexed) to specific data types. This mapping is applied only if `index=True`. Returns ------- numpy.recarray NumPy ndarray with the DataFrame labels as fields and each row of the DataFrame as entries. See Also -------- DataFrame.from_records: Convert structured or record ndarray to DataFrame. numpy.recarray: An ndarray that allows field access using attributes, analogous to typed columns in a spreadsheet. Examples -------- >>> df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]}, ... index=['a', 'b']) >>> df A B a 1 0.50 b 2 0.75 >>> df.to_records() rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)], dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')]) If the DataFrame index has no label then the recarray field name is set to 'index'. If the index has a label then this is used as the field name: >>> df.index = df.index.rename(\" I \") >>> df.to_records() rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)], dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')]) The index can be excluded from the record array: >>> df.to_records(index=False) rec.array([(1, 0.5 ), (2, 0.75)], dtype=[('A', '<i8'), ('B', '<f8')]) Data types can be specified for the columns: >>> df.to_records(column_dtypes={\" A \": \" int32 \"}) rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)], dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')]) As well as for the index: >>> df.to_records(index_dtypes=\" < S2 \") rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)], dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')]) >>> index_dtypes = f\" < S { df . index . str . len (). max () } \" >>> df.to_records(index_dtypes=index_dtypes) rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)], dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')]) \"\"\" if index : if isinstance ( self . index , MultiIndex ) : # array of tuples to numpy cols . copy copy copy ix_vals = list ( map ( np . array , zip ( * self . index . _values ))) else : ix_vals = [ self.index.values ] arrays = ix_vals + [ np.asarray(self.iloc[:, i ] ) for i in range ( len ( self . columns )) ] count = 0 index_names = list ( self . index . names ) if isinstance ( self . index , MultiIndex ) : for i , n in enumerate ( index_names ) : if n is None : index_names [ i ] = f \"level_{count}\" count += 1 elif index_names [ 0 ] is None : index_names = [ \"index\" ] names = [ str(name) for name in itertools.chain(index_names, self.columns) ] else : arrays = [ np.asarray(self.iloc[:, i ] ) for i in range ( len ( self . columns )) ] names = [ str(c) for c in self.columns ] index_names = [] index_len = len ( index_names ) formats = [] for i , v in enumerate ( arrays ) : index = i # When the names and arrays are collected , we # first collect those in the DataFrame 's index, # followed by those in its columns. # # Thus, the total length of the array is: # len(index_names) + len(DataFrame.columns). # # This check allows us to see whether we are # handling a name / array in the index or column. if index < index_len: dtype_mapping = index_dtypes name = index_names[index] else: index -= index_len dtype_mapping = column_dtypes name = self.columns[index] # We have a dictionary, so we get the data type # associated with the index or column (which can # be denoted by its name in the DataFrame or its # position in DataFrame' s array of indices or # columns , whichever is applicable . if is_dict_like ( dtype_mapping ) : if name in dtype_mapping : dtype_mapping = dtype_mapping [ name ] elif index in dtype_mapping : dtype_mapping = dtype_mapping [ index ] else : dtype_mapping = None # If no mapping can be found , use the array ' s # dtype attribute for formatting . # # A valid dtype must either be a type or # string naming a type . if dtype_mapping is None : formats . append ( v . dtype ) elif isinstance ( dtype_mapping , ( type , np . dtype , str )) : formats . append ( dtype_mapping ) else : element = \"row\" if i < index_len else \"column\" msg = f \"Invalid dtype {dtype_mapping} specified for {element} {name}\" raise ValueError ( msg ) return np . rec . fromarrays ( arrays , dtype = { \"names\" : names , \"formats\" : formats } ) to_sql def to_sql ( self , name : str , con , schema = None , if_exists : str = 'fail' , index : bool = True , index_label = None , chunksize = None , dtype = None , method = None ) -> None Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1]_ are supported. Tables can be newly created, appended to, or overwritten. Parameters name : str Name of SQL table. con : sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable See here <https://docs.sqlalchemy.org/en/13/core/connections.html> _. schema : str, optional Specify the schema (if database flavor supports this). If None, use default schema. if_exists : {'fail', 'replace', 'append'}, default 'fail' How to behave if the table already exists. * fail: Raise a ValueError. * replace: Drop the table before inserting new values. * append: Insert new values to the existing table. index : bool, default True Write DataFrame index as a column. Uses index_label as the column name in the table. index_label : str or sequence, default None Column label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksize : int, optional Specify the number of rows in each batch to be written at a time. By default, all rows will be written at once. dtype : dict or scalar, optional Specifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns. method : {None, 'multi', callable}, optional Controls the SQL insertion clause used: * None : Uses standard SQL `` INSERT `` clause ( one per row ). * 'multi' : Pass multiple values in a single `` INSERT `` clause . * callable with signature `` ( pd_table , conn , keys , data_iter ) `` . Details and a sample callable implementation can be found in the section : ref : `insert method <io.sql.method>` . .. versionadded :: 0 . 24 . 0 Raises ValueError When the table already exists and if_exists is 'fail' (the default). See Also read_sql : Read a DataFrame from a table. Notes Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. .. versionadded:: 0.24.0 References .. [1] https://docs.sqlalchemy.org .. [2] https://www.python.org/dev/peps/pep-0249/ Examples Create an in-memory SQLite database. from sqlalchemy import create_engine engine = create_engine('sqlite://', echo=False) Create a table from scratch with 3 rows. df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']}) df name 0 User 1 1 User 2 2 User 3 df.to_sql('users', con=engine) engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')] An sqlalchemy.engine.Connection can also be passed to to con : with engine.begin() as connection: ... df1 = pd.DataFrame({'name' : ['User 4', 'User 5']}) ... df1.to_sql('users', con=connection, if_exists='append') This is allowed to support operations that require that the same DBAPI connection is used for the entire operation. df2 = pd.DataFrame({'name' : ['User 6', 'User 7']}) df2.to_sql('users', con=engine, if_exists='append') engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'), (0, 'User 4'), (1, 'User 5'), (0, 'User 6'), (1, 'User 7')] Overwrite the table with just df2 . df2.to_sql('users', con=engine, if_exists='replace', ... index_label='id') engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 6'), (1, 'User 7')] Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. df = pd.DataFrame({\"A\": [1, None, 2]}) df A 0 1.0 1 NaN 2 2.0 from sqlalchemy.types import Integer df.to_sql('integers', con=engine, index=False, ... dtype={\"A\": Integer()}) engine.execute(\"SELECT * FROM integers\").fetchall() [(1,), (None,), (2,)] View Source def to_sql ( self , name : str , con , schema = None , if_exists : str = \"fail\" , index : bool_t = True , index_label = None , chunksize = None , dtype = None , method = None , ) -> None : \"\"\" Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1]_ are supported. Tables can be newly created, appended to, or overwritten. Parameters ---------- name : str Name of SQL table. con : sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable See `here \\ <https://docs.sqlalchemy.org/en/13/core/connections.html>`_. schema : str, optional Specify the schema (if database flavor supports this). If None, use default schema. if_exists : {'fail', 'replace', 'append'}, default 'fail' How to behave if the table already exists. * fail: Raise a ValueError. * replace: Drop the table before inserting new values. * append: Insert new values to the existing table. index : bool, default True Write DataFrame index as a column. Uses `index_label` as the column name in the table. index_label : str or sequence, default None Column label for index column(s). If None is given (default) and `index` is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksize : int, optional Specify the number of rows in each batch to be written at a time. By default, all rows will be written at once. dtype : dict or scalar, optional Specifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns. method : {None, 'multi', callable}, optional Controls the SQL insertion clause used: * None : Uses standard SQL ``INSERT`` clause (one per row). * 'multi': Pass multiple values in a single ``INSERT`` clause. * callable with signature ``(pd_table, conn, keys, data_iter)``. Details and a sample callable implementation can be found in the section :ref:`insert method <io.sql.method>`. .. versionadded:: 0.24.0 Raises ------ ValueError When the table already exists and `if_exists` is 'fail' (the default). See Also -------- read_sql : Read a DataFrame from a table. Notes ----- Timezone aware datetime columns will be written as ``Timestamp with timezone`` type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. .. versionadded:: 0.24.0 References ---------- .. [1] https://docs.sqlalchemy.org .. [2] https://www.python.org/dev/peps/pep-0249/ Examples -------- Create an in-memory SQLite database. >>> from sqlalchemy import create_engine >>> engine = create_engine('sqlite://', echo=False) Create a table from scratch with 3 rows. >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']}) >>> df name 0 User 1 1 User 2 2 User 3 >>> df.to_sql('users', con=engine) >>> engine.execute(\" SELECT * FROM users \").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')] An `sqlalchemy.engine.Connection` can also be passed to to `con`: >>> with engine.begin() as connection: ... df1 = pd.DataFrame({'name' : ['User 4', 'User 5']}) ... df1.to_sql('users', con=connection, if_exists='append') This is allowed to support operations that require that the same DBAPI connection is used for the entire operation. >>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']}) >>> df2.to_sql('users', con=engine, if_exists='append') >>> engine.execute(\" SELECT * FROM users \").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'), (0, 'User 4'), (1, 'User 5'), (0, 'User 6'), (1, 'User 7')] Overwrite the table with just ``df2``. >>> df2.to_sql('users', con=engine, if_exists='replace', ... index_label='id') >>> engine.execute(\" SELECT * FROM users \").fetchall() [(0, 'User 6'), (1, 'User 7')] Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. >>> df = pd.DataFrame({\" A \": [1, None, 2]}) >>> df A 0 1.0 1 NaN 2 2.0 >>> from sqlalchemy.types import Integer >>> df.to_sql('integers', con=engine, index=False, ... dtype={\" A \": Integer()}) >>> engine.execute(\" SELECT * FROM integers \").fetchall() [(1,), (None,), (2,)] \"\"\" from pandas . io import sql sql . to_sql ( self , name , con , schema = schema , if_exists = if_exists , index = index , index_label = index_label , chunksize = chunksize , dtype = dtype , method = method , ) to_stata def to_stata ( self , path : Union [ str , pathlib . Path , IO [ ~ AnyStr ]], convert_dates : Union [ Dict [ Union [ Hashable , NoneType ], str ], NoneType ] = None , write_index : bool = True , byteorder : Union [ str , NoneType ] = None , time_stamp : Union [ datetime . datetime , NoneType ] = None , data_label : Union [ str , NoneType ] = None , variable_labels : Union [ Dict [ Union [ Hashable , NoneType ], str ], NoneType ] = None , version : Union [ int , NoneType ] = 114 , convert_strl : Union [ Sequence [ Union [ Hashable , NoneType ]], NoneType ] = None , compression : Union [ str , Mapping [ str , str ], NoneType ] = 'infer' ) -> None Export DataFrame object to Stata dta format. Writes the DataFrame to a Stata dataset file. \"dta\" files contain a Stata dataset. Parameters path : str, buffer or path object String, path object (pathlib.Path or py._path.local.LocalPath) or object implementing a binary write() function. If using a buffer then the buffer will not be automatically closed after the file data has been written. .. versionchanged :: 1.0.0 Previously this was \"fname\" convert_dates : dict Dictionary mapping columns containing datetime types to stata internal format to use when writing the dates. Options are 'tc', 'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer or a name. Datetime columns that do not have a conversion type specified will be converted to 'tc'. Raises NotImplementedError if a datetime column has timezone information. write_index : bool Write the index to Stata dataset. byteorder : str Can be \">\", \"<\", \"little\", or \"big\". default is sys.byteorder . time_stamp : datetime A datetime to use as file creation date. Default is the current time. data_label : str, optional A label for the data set. Must be 80 characters or smaller. variable_labels : dict Dictionary containing columns as keys and variable labels as values. Each label must be 80 characters or smaller. version : {114, 117, 118, 119, None}, default 114 Version to use in the output dta file. Set to None to let pandas decide between 118 or 119 formats depending on the number of columns in the frame. Version 114 can be read by Stata 10 and later. Version 117 can be read by Stata 13 or later. Version 118 is supported in Stata 14 and later. Version 119 is supported in Stata 15 and later. Version 114 limits string variables to 244 characters or fewer while versions 117 and later allow strings with lengths up to 2,000,000 characters. Versions 118 and 119 support Unicode characters, and version 119 supports more than 32,767 variables. .. versionadded :: 0.23.0 .. versionchanged :: 1.0.0 Added support for formats 118 and 119. convert_strl : list, optional List of column names to convert to string columns to Stata StrL format. Only available if version is 117. Storing strings in the StrL format can produce smaller dta files if strings have more than 8 characters and values are repeated. .. versionadded :: 0.23.0 compression : str or dict, default 'infer' For on-the-fly compression of the output dta. If string, specifies compression mode. If dict, value at key 'method' specifies compression mode. Compression mode must be one of {'infer', 'gzip', 'bz2', 'zip', 'xz', None}. If compression mode is 'infer' and fname is path-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no compression). If dict and compression mode is one of {'zip', 'gzip', 'bz2'}, or inferred as one of the above, other entries passed as additional compression options. .. versionadded :: 1.1.0 Raises NotImplementedError * If datetimes contain timezone information * Column dtype is not representable in Stata ValueError * Columns listed in convert_dates are neither datetime64[ns] or datetime.datetime * Column listed in convert_dates is not in DataFrame * Categorical label contains more than 32,000 characters See Also read_stata : Import Stata data files. io.stata.StataWriter : Low-level writer for Stata data files. io.stata.StataWriter117 : Low-level writer for version 117 files. Examples df = pd.DataFrame({'animal': ['falcon', 'parrot', 'falcon', ... 'parrot'], ... 'speed': [350, 18, 361, 15]}) df.to_stata('animals.dta') # doctest: +SKIP View Source @deprecate_kwarg ( old_arg_name = \"fname\" , new_arg_name = \"path\" ) def to_stata ( self , path : FilePathOrBuffer , convert_dates : Optional [ Dict[Label, str ] ] = None , write_index : bool = True , byteorder : Optional [ str ] = None , time_stamp : Optional [ datetime.datetime ] = None , data_label : Optional [ str ] = None , variable_labels : Optional [ Dict[Label, str ] ] = None , version : Optional [ int ] = 114 , convert_strl : Optional [ Sequence[Label ] ] = None , compression : Union [ str, Mapping[str, str ] , None ] = \"infer\" , ) -> None : \"\"\" Export DataFrame object to Stata dta format. Writes the DataFrame to a Stata dataset file. \" dta \" files contain a Stata dataset. Parameters ---------- path : str, buffer or path object String, path object (pathlib.Path or py._path.local.LocalPath) or object implementing a binary write() function. If using a buffer then the buffer will not be automatically closed after the file data has been written. .. versionchanged:: 1.0.0 Previously this was \" fname \" convert_dates : dict Dictionary mapping columns containing datetime types to stata internal format to use when writing the dates. Options are 'tc', 'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer or a name. Datetime columns that do not have a conversion type specified will be converted to 'tc'. Raises NotImplementedError if a datetime column has timezone information. write_index : bool Write the index to Stata dataset. byteorder : str Can be \" > \", \" < \", \" little \", or \" big \". default is `sys.byteorder`. time_stamp : datetime A datetime to use as file creation date. Default is the current time. data_label : str, optional A label for the data set. Must be 80 characters or smaller. variable_labels : dict Dictionary containing columns as keys and variable labels as values. Each label must be 80 characters or smaller. version : {114, 117, 118, 119, None}, default 114 Version to use in the output dta file. Set to None to let pandas decide between 118 or 119 formats depending on the number of columns in the frame. Version 114 can be read by Stata 10 and later. Version 117 can be read by Stata 13 or later. Version 118 is supported in Stata 14 and later. Version 119 is supported in Stata 15 and later. Version 114 limits string variables to 244 characters or fewer while versions 117 and later allow strings with lengths up to 2,000,000 characters. Versions 118 and 119 support Unicode characters, and version 119 supports more than 32,767 variables. .. versionadded:: 0.23.0 .. versionchanged:: 1.0.0 Added support for formats 118 and 119. convert_strl : list, optional List of column names to convert to string columns to Stata StrL format. Only available if version is 117. Storing strings in the StrL format can produce smaller dta files if strings have more than 8 characters and values are repeated. .. versionadded:: 0.23.0 compression : str or dict, default 'infer' For on-the-fly compression of the output dta. If string, specifies compression mode. If dict, value at key 'method' specifies compression mode. Compression mode must be one of {'infer', 'gzip', 'bz2', 'zip', 'xz', None}. If compression mode is 'infer' and `fname` is path-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no compression). If dict and compression mode is one of {'zip', 'gzip', 'bz2'}, or inferred as one of the above, other entries passed as additional compression options. .. versionadded:: 1.1.0 Raises ------ NotImplementedError * If datetimes contain timezone information * Column dtype is not representable in Stata ValueError * Columns listed in convert_dates are neither datetime64[ns] or datetime.datetime * Column listed in convert_dates is not in DataFrame * Categorical label contains more than 32,000 characters See Also -------- read_stata : Import Stata data files. io.stata.StataWriter : Low-level writer for Stata data files. io.stata.StataWriter117 : Low-level writer for version 117 files. Examples -------- >>> df = pd.DataFrame({'animal': ['falcon', 'parrot', 'falcon', ... 'parrot'], ... 'speed': [350, 18, 361, 15]}) >>> df.to_stata('animals.dta') # doctest: +SKIP \"\"\" if version not in ( 114 , 117 , 118 , 119 , None ) : raise ValueError ( \"Only formats 114, 117, 118 and 119 are supported.\" ) if version == 114 : if convert_strl is not None : raise ValueError ( \"strl is not supported in format 114\" ) from pandas . io . stata import StataWriter as statawriter elif version == 117 : # mypy : Name 'statawriter' already defined ( possibly by an import ) from pandas . io . stata import StataWriter117 as statawriter # type : ignore else : # versions 118 and 119 # mypy : Name 'statawriter' already defined ( possibly by an import ) from pandas . io . stata import StataWriterUTF8 as statawriter # type : ignore kwargs : Dict [ str, Any ] = {} if version is None or version >= 117 : # strl conversion is only supported >= 117 kwargs [ \"convert_strl\" ] = convert_strl if version is None or version >= 118 : # Specifying the version is only supported for UTF8 ( 118 or 119 ) kwargs [ \"version\" ] = version # mypy : Too many arguments for \"StataWriter\" writer = statawriter ( # type : ignore path , self , convert_dates = convert_dates , byteorder = byteorder , time_stamp = time_stamp , data_label = data_label , write_index = write_index , variable_labels = variable_labels , compression = compression , ** kwargs , ) writer . write_file () to_string def to_string ( self , buf : Union [ str , pathlib . Path , IO [ str ], NoneType ] = None , columns : Union [ Sequence [ str ], NoneType ] = None , col_space : Union [ int , NoneType ] = None , header : Union [ bool , Sequence [ str ]] = True , index : bool = True , na_rep : str = 'NaN' , formatters : Union [ List [ Callable ], Tuple [ Callable , ... ], Mapping [ Union [ str , int ], Callable ], NoneType ] = None , float_format : Union [ str , Callable , ForwardRef ( 'EngFormatter' ), NoneType ] = None , sparsify : Union [ bool , NoneType ] = None , index_names : bool = True , justify : Union [ str , NoneType ] = None , max_rows : Union [ int , NoneType ] = None , min_rows : Union [ int , NoneType ] = None , max_cols : Union [ int , NoneType ] = None , show_dimensions : bool = False , decimal : str = '.' , line_width : Union [ int , NoneType ] = None , max_colwidth : Union [ int , NoneType ] = None , encoding : Union [ str , NoneType ] = None ) -> Union [ str , NoneType ] Render a DataFrame to a console-friendly tabular output. Parameters buf : str, Path or StringIO-like, optional, default None Buffer to write to. If None, the output is returned as a string. columns : sequence, optional, default None The subset of columns to write. Writes all columns by default. col_space : int, list or dict of int, optional The minimum width of each column. header : bool or sequence, optional Write out the column names. If a list of strings is given, it is assumed to be aliases for the column names. index : bool, optional, default True Whether to print index (row) labels. na_rep : str, optional, default 'NaN' String representation of NAN to use. formatters : list, tuple or dict of one-param. functions, optional Formatter functions to apply to columns' elements by position or name. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns. float_format : one-parameter function, optional, default None Formatter function to apply to columns' elements if they are floats. The result of this function must be a unicode string. sparsify : bool, optional, default True Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row. index_names : bool, optional, default True Prints the names of the indexes. justify : str, default None How to justify the column labels. If None uses the option from the print configuration (controlled by set_option), 'right' out of the box. Valid values are * left * right * center * justify * justify-all * start * end * inherit * match-parent * initial * unset. max_rows : int, optional Maximum number of rows to display in the console. min_rows : int, optional The number of rows to display in the console in a truncated repr (when number of rows is above max_rows ). max_cols : int, optional Maximum number of columns to display in the console. show_dimensions : bool, default False Display DataFrame dimensions (number of rows by number of columns). decimal : str, default '.' Character recognized as decimal separator, e.g. ',' in Europe. line_width : int, optional Width to wrap a line in characters. max_colwidth : int, optional Max width to truncate each column in characters. By default, no limit. .. versionadded :: 1.0.0 encoding : str, default \"utf-8\" Set character encoding. .. versionadded :: 1.0 Returns str or None If buf is None, returns the result as a string. Otherwise returns None. See Also to_html : Convert DataFrame to HTML. Examples d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]} df = pd.DataFrame(d) print(df.to_string()) col1 col2 0 1 4 1 2 5 2 3 6 View Source @Substitution ( header_type = \"bool or sequence\" , header = \"Write out the column names. If a list of strings \" \"is given, it is assumed to be aliases for the \" \"column names\" , col_space_type = \"int, list or dict of int\" , col_space = \"The minimum width of each column\" , ) @Substitution ( shared_params = fmt . common_docstring , returns = fmt . return_docstring ) def to_string ( self , buf : Optional [ FilePathOrBuffer[str ] ] = None , columns : Optional [ Sequence[str ] ] = None , col_space : Optional [ int ] = None , header : Union [ bool, Sequence[str ] ] = True , index : bool = True , na_rep : str = \"NaN\" , formatters : Optional [ fmt.FormattersType ] = None , float_format : Optional [ fmt.FloatFormatType ] = None , sparsify : Optional [ bool ] = None , index_names : bool = True , justify : Optional [ str ] = None , max_rows : Optional [ int ] = None , min_rows : Optional [ int ] = None , max_cols : Optional [ int ] = None , show_dimensions : bool = False , decimal : str = \".\" , line_width : Optional [ int ] = None , max_colwidth : Optional [ int ] = None , encoding : Optional [ str ] = None , ) -> Optional [ str ] : \"\"\" Render a DataFrame to a console-friendly tabular output. %(shared_params)s line_width : int, optional Width to wrap a line in characters. max_colwidth : int, optional Max width to truncate each column in characters. By default, no limit. .. versionadded:: 1.0.0 encoding : str, default \" utf - 8 \" Set character encoding. .. versionadded:: 1.0 %(returns)s See Also -------- to_html : Convert DataFrame to HTML. Examples -------- >>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]} >>> df = pd.DataFrame(d) >>> print(df.to_string()) col1 col2 0 1 4 1 2 5 2 3 6 \"\"\" from pandas import option_context with option_context ( \"display.max_colwidth\" , max_colwidth ) : formatter = fmt . DataFrameFormatter ( self , columns = columns , col_space = col_space , na_rep = na_rep , formatters = formatters , float_format = float_format , sparsify = sparsify , justify = justify , index_names = index_names , header = header , index = index , min_rows = min_rows , max_rows = max_rows , max_cols = max_cols , show_dimensions = show_dimensions , decimal = decimal , line_width = line_width , ) return formatter . to_string ( buf = buf , encoding = encoding ) to_timestamp def to_timestamp ( self , freq = None , how : str = 'start' , axis : Union [ str , int ] = 0 , copy : bool = True ) -> 'DataFrame' Cast to DatetimeIndex of timestamps, at beginning of period. Parameters freq : str, default frequency of PeriodIndex Desired frequency. how : {'s', 'e', 'start', 'end'} Convention for converting period to timestamp; start of period vs. end. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to convert (the index by default). copy : bool, default True If False then underlying input data is not copied. Returns DataFrame with DatetimeIndex View Source def to_timestamp ( self , freq = None , how : str = \"start\" , axis : Axis = 0 , copy : bool = True ) -> \"DataFrame\" : \"\"\" Cast to DatetimeIndex of timestamps, at *beginning* of period. Parameters ---------- freq : str, default frequency of PeriodIndex Desired frequency. how : {'s', 'e', 'start', 'end'} Convention for converting period to timestamp; start of period vs. end. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to convert (the index by default). copy : bool, default True If False then underlying input data is not copied. Returns ------- DataFrame with DatetimeIndex \"\"\" new_obj = self . copy ( deep = copy ) axis_name = self . _get_axis_name ( axis ) old_ax = getattr ( self , axis_name ) new_ax = old_ax . to_timestamp ( freq = freq , how = how ) setattr ( new_obj , axis_name , new_ax ) return new_obj to_xarray def to_xarray ( self ) Return an xarray object from the pandas object. Returns xarray.DataArray or xarray.Dataset Data in the pandas structure converted to Dataset if the object is a DataFrame, or a DataArray if the object is a Series. See Also DataFrame.to_hdf : Write DataFrame to an HDF5 file. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. Notes See the xarray docs <https://xarray.pydata.org/en/stable/> __ Examples df = pd.DataFrame([('falcon', 'bird', 389.0, 2), ... ('parrot', 'bird', 24.0, 2), ... ('lion', 'mammal', 80.5, 4), ... ('monkey', 'mammal', np.nan, 4)], ... columns=['name', 'class', 'max_speed', ... 'num_legs']) df name class max_speed num_legs 0 falcon bird 389.0 2 1 parrot bird 24.0 2 2 lion mammal 80.5 4 3 monkey mammal NaN 4 df.to_xarray() Dimensions: (index: 4) Coordinates: * index (index) int64 0 1 2 3 Data variables: name (index) object 'falcon' 'parrot' 'lion' 'monkey' class (index) object 'bird' 'bird' 'mammal' 'mammal' max_speed (index) float64 389.0 24.0 80.5 nan num_legs (index) int64 2 2 4 4 df['max_speed'].to_xarray() array([389. , 24. , 80.5, nan]) Coordinates: * index (index) int64 0 1 2 3 dates = pd.to_datetime(['2018-01-01', '2018-01-01', ... '2018-01-02', '2018-01-02']) df_multiindex = pd.DataFrame({'date': dates, ... 'animal': ['falcon', 'parrot', ... 'falcon', 'parrot'], ... 'speed': [350, 18, 361, 15]}) df_multiindex = df_multiindex.set_index(['date', 'animal']) df_multiindex speed date animal 2018-01-01 falcon 350 parrot 18 2018-01-02 falcon 361 parrot 15 df_multiindex.to_xarray() Dimensions: (animal: 2, date: 2) Coordinates: * date (date) datetime64[ns] 2018-01-01 2018-01-02 * animal (animal) object 'falcon' 'parrot' Data variables: speed (date, animal) int64 350 18 361 15 View Source def to_xarray ( self ) : \"\"\" Return an xarray object from the pandas object. Returns ------- xarray.DataArray or xarray.Dataset Data in the pandas structure converted to Dataset if the object is a DataFrame, or a DataArray if the object is a Series. See Also -------- DataFrame.to_hdf : Write DataFrame to an HDF5 file. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. Notes ----- See the `xarray docs <https://xarray.pydata.org/en/stable/>`__ Examples -------- >>> df = pd.DataFrame([('falcon', 'bird', 389.0, 2), ... ('parrot', 'bird', 24.0, 2), ... ('lion', 'mammal', 80.5, 4), ... ('monkey', 'mammal', np.nan, 4)], ... columns=['name', 'class', 'max_speed', ... 'num_legs']) >>> df name class max_speed num_legs 0 falcon bird 389.0 2 1 parrot bird 24.0 2 2 lion mammal 80.5 4 3 monkey mammal NaN 4 >>> df.to_xarray() <xarray.Dataset> Dimensions: (index: 4) Coordinates: * index (index) int64 0 1 2 3 Data variables: name (index) object 'falcon' 'parrot' 'lion' 'monkey' class (index) object 'bird' 'bird' 'mammal' 'mammal' max_speed (index) float64 389.0 24.0 80.5 nan num_legs (index) int64 2 2 4 4 >>> df['max_speed'].to_xarray() <xarray.DataArray 'max_speed' (index: 4)> array([389. , 24. , 80.5, nan]) Coordinates: * index (index) int64 0 1 2 3 >>> dates = pd.to_datetime(['2018-01-01', '2018-01-01', ... '2018-01-02', '2018-01-02']) >>> df_multiindex = pd.DataFrame({'date': dates, ... 'animal': ['falcon', 'parrot', ... 'falcon', 'parrot'], ... 'speed': [350, 18, 361, 15]}) >>> df_multiindex = df_multiindex.set_index(['date', 'animal']) >>> df_multiindex speed date animal 2018-01-01 falcon 350 parrot 18 2018-01-02 falcon 361 parrot 15 >>> df_multiindex.to_xarray() <xarray.Dataset> Dimensions: (animal: 2, date: 2) Coordinates: * date (date) datetime64[ns] 2018-01-01 2018-01-02 * animal (animal) object 'falcon' 'parrot' Data variables: speed (date, animal) int64 350 18 361 15 \"\"\" xarray = import_optional_dependency ( \"xarray\" ) if self . ndim == 1 : return xarray . DataArray . from_series ( self ) else : return xarray . Dataset . from_dataframe ( self ) transform def transform ( self , func , axis = 0 , * args , ** kwargs ) -> 'DataFrame' Call func on self producing a DataFrame with transformed values. Produced DataFrame will have same axis length as self. Parameters func : function, str, list or dict Function to use for transforming the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are : - function - string function name - list of functions and / or function names , e . g . `` [ np . exp . 'sqrt' ] `` - dict of axis labels -> functions , function names or list of such . axis : {0 or 'index', 1 or 'columns'}, default 0 If 0 or 'index': apply function to each column. If 1 or 'columns': apply function to each row. args Positional arguments to pass to func . *kwargs Keyword arguments to pass to func . Returns DataFrame A DataFrame that must have the same length as self. Raises ValueError : If the returned DataFrame has a different length than self. See Also DataFrame.agg : Only perform aggregating type operations. DataFrame.apply : Invoke function on a DataFrame. Examples df = pd.DataFrame({'A': range(3), 'B': range(1, 4)}) df A B 0 0 1 1 1 2 2 2 3 df.transform(lambda x: x + 1) A B 0 1 2 1 2 3 2 3 4 Even though the resulting DataFrame must have the same length as the input DataFrame, it is possible to provide several input functions: s = pd.Series(range(3)) s 0 0 1 1 2 2 dtype: int64 s.transform([np.sqrt, np.exp]) sqrt exp 0 0.000000 1.000000 1 1.000000 2.718282 2 1.414214 7.389056 View Source @doc ( NDFrame . transform , klass = _shared_doc_kwargs [ \"klass\" ] , axis = _shared_doc_kwargs [ \"axis\" ] , ) def transform ( self , func , axis = 0 , * args , ** kwargs ) -> \"DataFrame\" : axis = self . _get_axis_number ( axis ) if axis == 1 : return self . T . transform ( func , * args , ** kwargs ). T return super (). transform ( func , * args , ** kwargs ) transpose def transpose ( self , * args , copy : bool = False ) -> 'DataFrame' Transpose index and columns. Reflect the DataFrame over its main diagonal by writing rows as columns and vice-versa. The property :attr: .T is an accessor to the method :meth: transpose . Parameters *args : tuple, optional Accepted for compatibility with NumPy. copy : bool, default False Whether to copy the data after transposing, even for DataFrames with a single dtype. Note that a copy is always required for mixed dtype DataFrames, or for DataFrames with any extension types. Returns DataFrame The transposed DataFrame. See Also numpy.transpose : Permute the dimensions of a given array. Notes Transposing a DataFrame with mixed dtypes will result in a homogeneous DataFrame with the object dtype. In such a case, a copy of the data is always made. Examples Square DataFrame with homogeneous dtype d1 = {'col1': [1, 2], 'col2': [3, 4]} df1 = pd.DataFrame(data=d1) df1 col1 col2 0 1 3 1 2 4 df1_transposed = df1.T # or df1.transpose() df1_transposed 0 1 col1 1 2 col2 3 4 When the dtype is homogeneous in the original DataFrame, we get a transposed DataFrame with the same dtype: df1.dtypes col1 int64 col2 int64 dtype: object df1_transposed.dtypes 0 int64 1 int64 dtype: object Non-square DataFrame with mixed dtypes d2 = {'name': ['Alice', 'Bob'], ... 'score': [9.5, 8], ... 'employed': [False, True], ... 'kids': [0, 0]} df2 = pd.DataFrame(data=d2) df2 name score employed kids 0 Alice 9.5 False 0 1 Bob 8.0 True 0 df2_transposed = df2.T # or df2.transpose() df2_transposed 0 1 name Alice Bob score 9.5 8 employed False True kids 0 0 When the DataFrame has mixed dtypes, we get a transposed DataFrame with the object dtype: df2.dtypes name object score float64 employed bool kids int64 dtype: object df2_transposed.dtypes 0 object 1 object dtype: object View Source def transpose ( self , * args , copy : bool = False ) -> \"DataFrame\" : \"\"\" Transpose index and columns. Reflect the DataFrame over its main diagonal by writing rows as columns and vice-versa. The property :attr:`.T` is an accessor to the method :meth:`transpose`. Parameters ---------- *args : tuple, optional Accepted for compatibility with NumPy. copy : bool, default False Whether to copy the data after transposing, even for DataFrames with a single dtype. Note that a copy is always required for mixed dtype DataFrames, or for DataFrames with any extension types. Returns ------- DataFrame The transposed DataFrame. See Also -------- numpy.transpose : Permute the dimensions of a given array. Notes ----- Transposing a DataFrame with mixed dtypes will result in a homogeneous DataFrame with the `object` dtype. In such a case, a copy of the data is always made. Examples -------- **Square DataFrame with homogeneous dtype** >>> d1 = {'col1': [1, 2], 'col2': [3, 4]} >>> df1 = pd.DataFrame(data=d1) >>> df1 col1 col2 0 1 3 1 2 4 >>> df1_transposed = df1.T # or df1.transpose() >>> df1_transposed 0 1 col1 1 2 col2 3 4 When the dtype is homogeneous in the original DataFrame, we get a transposed DataFrame with the same dtype: >>> df1.dtypes col1 int64 col2 int64 dtype: object >>> df1_transposed.dtypes 0 int64 1 int64 dtype: object **Non-square DataFrame with mixed dtypes** >>> d2 = {'name': ['Alice', 'Bob'], ... 'score': [9.5, 8], ... 'employed': [False, True], ... 'kids': [0, 0]} >>> df2 = pd.DataFrame(data=d2) >>> df2 name score employed kids 0 Alice 9.5 False 0 1 Bob 8.0 True 0 >>> df2_transposed = df2.T # or df2.transpose() >>> df2_transposed 0 1 name Alice Bob score 9.5 8 employed False True kids 0 0 When the DataFrame has mixed dtypes, we get a transposed DataFrame with the `object` dtype: >>> df2.dtypes name object score float64 employed bool kids int64 dtype: object >>> df2_transposed.dtypes 0 object 1 object dtype: object \"\"\" nv . validate_transpose ( args , dict ()) # construct the args dtypes = list ( self . dtypes ) if self . _is_homogeneous_type and dtypes and is_extension_array_dtype ( dtypes [ 0 ]): # We have EAs with the same dtype. We can preserve that dtype in transpose. dtype = dtypes [ 0 ] arr_type = dtype . construct_array_type () values = self . values new_values = [ arr_type . _from_sequence ( row , dtype = dtype ) for row in values ] result = self . _constructor ( dict ( zip ( self . index , new_values )), index = self . columns ) else : new_values = self . values . T if copy : new_values = new_values . copy () result = self . _constructor ( new_values , index = self . columns , columns = self . index ) return result . __finalize__ ( self , method = \"transpose\" ) truediv def truediv ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Floating division of dataframe and other, element-wise (binary operator truediv ). Equivalent to dataframe / other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** . Parameters other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing. Returns DataFrame Result of the arithmetic operation. See Also DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power. Notes Mismatched indices will be unioned together. Examples df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data ) truncate def truncate ( self : ~ FrameOrSeries , before = None , after = None , axis = None , copy : bool = True ) -> ~ FrameOrSeries Truncate a Series or DataFrame before and after some index value. This is a useful shorthand for boolean indexing based on index values above or below certain thresholds. Parameters before : date, str, int Truncate all rows before this index value. after : date, str, int Truncate all rows after this index value. axis : {0 or 'index', 1 or 'columns'}, optional Axis to truncate. Truncates the index (rows) by default. copy : bool, default is True, Return a copy of the truncated section. Returns type of caller The truncated Series or DataFrame. See Also DataFrame.loc : Select a subset of a DataFrame by label. DataFrame.iloc : Select a subset of a DataFrame by position. Notes If the index being truncated contains only datetime values, before and after may be specified as strings instead of Timestamps. Examples df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'], ... 'B': ['f', 'g', 'h', 'i', 'j'], ... 'C': ['k', 'l', 'm', 'n', 'o']}, ... index=[1, 2, 3, 4, 5]) df A B C 1 a f k 2 b g l 3 c h m 4 d i n 5 e j o df.truncate(before=2, after=4) A B C 2 b g l 3 c h m 4 d i n The columns of a DataFrame can be truncated. df.truncate(before=\"A\", after=\"B\", axis=\"columns\") A B 1 a f 2 b g 3 c h 4 d i 5 e j For Series, only rows can be truncated. df['A'].truncate(before=2, after=4) 2 b 3 c 4 d Name: A, dtype: object The index values in truncate can be datetimes or string dates. dates = pd.date_range('2016-01-01', '2016-02-01', freq='s') df = pd.DataFrame(index=dates, data={'A': 1}) df.tail() A 2016-01-31 23:59:56 1 2016-01-31 23:59:57 1 2016-01-31 23:59:58 1 2016-01-31 23:59:59 1 2016-02-01 00:00:00 1 df.truncate(before=pd.Timestamp('2016-01-05'), ... after=pd.Timestamp('2016-01-10')).tail() A 2016-01-09 23:59:56 1 2016-01-09 23:59:57 1 2016-01-09 23:59:58 1 2016-01-09 23:59:59 1 2016-01-10 00:00:00 1 Because the index is a DatetimeIndex containing only dates, we can specify before and after as strings. They will be coerced to Timestamps before truncation. df.truncate('2016-01-05', '2016-01-10').tail() A 2016-01-09 23:59:56 1 2016-01-09 23:59:57 1 2016-01-09 23:59:58 1 2016-01-09 23:59:59 1 2016-01-10 00:00:00 1 Note that truncate assumes a 0 value for any unspecified time component (midnight). This differs from partial string slicing, which returns any partially matching dates. df.loc['2016-01-05':'2016-01-10', :].tail() A 2016-01-10 23:59:55 1 2016-01-10 23:59:56 1 2016-01-10 23:59:57 1 2016-01-10 23:59:58 1 2016-01-10 23:59:59 1 View Source def truncate ( self : FrameOrSeries , before = None , after = None , axis = None , copy : bool_t = True ) -> FrameOrSeries : \"\"\" Truncate a Series or DataFrame before and after some index value. This is a useful shorthand for boolean indexing based on index values above or below certain thresholds. Parameters ---------- before : date, str, int Truncate all rows before this index value. after : date, str, int Truncate all rows after this index value. axis : {0 or 'index', 1 or 'columns'}, optional Axis to truncate. Truncates the index (rows) by default. copy : bool, default is True, Return a copy of the truncated section. Returns ------- type of caller The truncated Series or DataFrame. See Also -------- DataFrame.loc : Select a subset of a DataFrame by label. DataFrame.iloc : Select a subset of a DataFrame by position. Notes ----- If the index being truncated contains only datetime values, `before` and `after` may be specified as strings instead of Timestamps. Examples -------- >>> df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'], ... 'B': ['f', 'g', 'h', 'i', 'j'], ... 'C': ['k', 'l', 'm', 'n', 'o']}, ... index=[1, 2, 3, 4, 5]) >>> df A B C 1 a f k 2 b g l 3 c h m 4 d i n 5 e j o >>> df.truncate(before=2, after=4) A B C 2 b g l 3 c h m 4 d i n The columns of a DataFrame can be truncated. >>> df.truncate(before=\" A \", after=\" B \", axis=\" columns \") A B 1 a f 2 b g 3 c h 4 d i 5 e j For Series, only rows can be truncated. >>> df['A'].truncate(before=2, after=4) 2 b 3 c 4 d Name: A, dtype: object The index values in ``truncate`` can be datetimes or string dates. >>> dates = pd.date_range('2016-01-01', '2016-02-01', freq='s') >>> df = pd.DataFrame(index=dates, data={'A': 1}) >>> df.tail() A 2016-01-31 23:59:56 1 2016-01-31 23:59:57 1 2016-01-31 23:59:58 1 2016-01-31 23:59:59 1 2016-02-01 00:00:00 1 >>> df.truncate(before=pd.Timestamp('2016-01-05'), ... after=pd.Timestamp('2016-01-10')).tail() A 2016-01-09 23:59:56 1 2016-01-09 23:59:57 1 2016-01-09 23:59:58 1 2016-01-09 23:59:59 1 2016-01-10 00:00:00 1 Because the index is a DatetimeIndex containing only dates, we can specify `before` and `after` as strings. They will be coerced to Timestamps before truncation. >>> df.truncate('2016-01-05', '2016-01-10').tail() A 2016-01-09 23:59:56 1 2016-01-09 23:59:57 1 2016-01-09 23:59:58 1 2016-01-09 23:59:59 1 2016-01-10 00:00:00 1 Note that ``truncate`` assumes a 0 value for any unspecified time component (midnight). This differs from partial string slicing, which returns any partially matching dates. >>> df.loc['2016-01-05':'2016-01-10', :].tail() A 2016-01-10 23:59:55 1 2016-01-10 23:59:56 1 2016-01-10 23:59:57 1 2016-01-10 23:59:58 1 2016-01-10 23:59:59 1 \"\"\" if axis is None : axis = self . _stat_axis_number axis = self . _get_axis_number ( axis ) ax = self . _get_axis ( axis ) # GH 17935 # Check that index is sorted if not ax . is_monotonic_increasing and not ax . is_monotonic_decreasing : raise ValueError ( \"truncate requires a sorted index\" ) # if we have a date index, convert to dates, otherwise # treat like a slice if ax . is_all_dates : from pandas . core . tools . datetimes import to_datetime before = to_datetime ( before ) after = to_datetime ( after ) if before is not None and after is not None : if before > after : raise ValueError ( f \"Truncate: {after} must be after {before}\" ) if len ( ax ) > 1 and ax . is_monotonic_decreasing : before , after = after , before slicer = [ slice ( None , None )] * self . _AXIS_LEN slicer [ axis ] = slice ( before , after ) result = self . loc [ tuple ( slicer )] if isinstance ( ax , MultiIndex ): setattr ( result , self . _get_axis_name ( axis ), ax . truncate ( before , after )) if copy : result = result . copy () return result tshift def tshift ( self : ~ FrameOrSeries , periods : int = 1 , freq = None , axis : Union [ str , int ] = 0 ) -> ~ FrameOrSeries Shift the time index, using the index's frequency if available. .. deprecated:: 1.1.0 Use shift instead. Parameters periods : int Number of periods to move, can be positive or negative. freq : DateOffset, timedelta, or str, default None Increment to use from the tseries module or time rule expressed as a string (e.g. 'EOM'). axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default 0 Corresponds to the axis that contains the Index. Returns shifted : Series/DataFrame Notes If freq is not specified then tries to use the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown View Source def tshift ( self : FrameOrSeries , periods : int = 1 , freq = None , axis : Axis = 0 ) -> FrameOrSeries : \"\"\" Shift the time index, using the index's frequency if available. .. deprecated:: 1.1.0 Use `shift` instead. Parameters ---------- periods : int Number of periods to move, can be positive or negative. freq : DateOffset, timedelta, or str, default None Increment to use from the tseries module or time rule expressed as a string (e.g. 'EOM'). axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default 0 Corresponds to the axis that contains the Index. Returns ------- shifted : Series/DataFrame Notes ----- If freq is not specified then tries to use the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown \"\"\" warnings . warn ( ( \"tshift is deprecated and will be removed in a future version. \" \"Please use shift instead.\" ), FutureWarning , stacklevel = 2 , ) if freq is None : freq = \"infer\" return self . shift ( periods , freq , axis ) tz_convert def tz_convert ( self : ~ FrameOrSeries , tz , axis = 0 , level = None , copy : bool = True ) -> ~ FrameOrSeries Convert tz-aware axis to target time zone. Parameters tz : str or tzinfo object axis : the axis to convert level : int, str, default None If axis is a MultiIndex, convert a specific level. Otherwise must be None. copy : bool, default True Also make a copy of the underlying data. Returns {klass} Object with time zone converted axis. Raises TypeError If the axis is tz-naive. View Source def tz_convert ( self : FrameOrSeries , tz , axis = 0 , level = None , copy : bool_t = True ) -> FrameOrSeries : \"\"\" Convert tz-aware axis to target time zone. Parameters ---------- tz : str or tzinfo object axis : the axis to convert level : int, str, default None If axis is a MultiIndex, convert a specific level. Otherwise must be None. copy : bool, default True Also make a copy of the underlying data. Returns ------- {klass} Object with time zone converted axis. Raises ------ TypeError If the axis is tz-naive. \"\"\" axis = self . _get_axis_number ( axis ) ax = self . _get_axis ( axis ) def _tz_convert ( ax , tz ) : if not hasattr ( ax , \"tz_convert\" ) : if len ( ax ) > 0 : ax_name = self . _get_axis_name ( axis ) raise TypeError ( f \"{ax_name} is not a valid DatetimeIndex or PeriodIndex\" ) else : ax = DatetimeIndex ( [] , tz = tz ) else : ax = ax . tz_convert ( tz ) return ax # if a level is given it must be a MultiIndex level or # equivalent to the axis name if isinstance ( ax , MultiIndex ) : level = ax . _get_level_number ( level ) new_level = _tz_convert ( ax . levels [ level ] , tz ) ax = ax . set_levels ( new_level , level = level ) else : if level not in ( None , 0 , ax . name ) : raise ValueError ( f \"The level {level} is not valid\" ) ax = _tz_convert ( ax , tz ) result = self . copy ( deep = copy ) result = result . set_axis ( ax , axis = axis , inplace = False ) return result . __finalize__ ( self , method = \"tz_convert\" ) tz_localize def tz_localize ( self : ~ FrameOrSeries , tz , axis = 0 , level = None , copy : bool = True , ambiguous = 'raise' , nonexistent : str = 'raise' ) -> ~ FrameOrSeries Localize tz-naive index of a Series or DataFrame to target time zone. This operation localizes the Index. To localize the values in a timezone-naive Series, use :meth: Series.dt.tz_localize . Parameters tz : str or tzinfo axis : the axis to localize level : int, str, default None If axis ia a MultiIndex, localize a specific level. Otherwise must be None. copy : bool, default True Also make a copy of the underlying data. ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise' When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled. - 'infer' will attempt to infer fall dst-transition hours based on order - bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) - 'NaT' will return NaT where there are ambiguous times - 'raise' will raise an AmbiguousTimeError if there are ambiguous times. nonexistent : str, default 'raise' A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. Valid values are: - 'shift_forward' will shift the nonexistent time forward to the closest existing time - 'shift_backward' will shift the nonexistent time backward to the closest existing time - 'NaT' will return NaT where there are nonexistent times - timedelta objects will shift nonexistent times by the timedelta - 'raise' will raise an NonExistentTimeError if there are nonexistent times . .. versionadded :: 0 . 24 . 0 Returns Series or DataFrame Same type as the input. Raises TypeError If the TimeSeries is tz-aware and tz is not None. Examples Localize local times: s = pd.Series([1], ... index=pd.DatetimeIndex(['2018-09-15 01:30:00'])) s.tz_localize('CET') 2018-09-15 01:30:00+02:00 1 dtype: int64 Be careful with DST changes. When there is sequential data, pandas can infer the DST time: s = pd.Series(range(7), ... index=pd.DatetimeIndex(['2018-10-28 01:30:00', ... '2018-10-28 02:00:00', ... '2018-10-28 02:30:00', ... '2018-10-28 02:00:00', ... '2018-10-28 02:30:00', ... '2018-10-28 03:00:00', ... '2018-10-28 03:30:00'])) s.tz_localize('CET', ambiguous='infer') 2018-10-28 01:30:00+02:00 0 2018-10-28 02:00:00+02:00 1 2018-10-28 02:30:00+02:00 2 2018-10-28 02:00:00+01:00 3 2018-10-28 02:30:00+01:00 4 2018-10-28 03:00:00+01:00 5 2018-10-28 03:30:00+01:00 6 dtype: int64 In some cases, inferring the DST is impossible. In such cases, you can pass an ndarray to the ambiguous parameter to set the DST explicitly s = pd.Series(range(3), ... index=pd.DatetimeIndex(['2018-10-28 01:20:00', ... '2018-10-28 02:36:00', ... '2018-10-28 03:46:00'])) s.tz_localize('CET', ambiguous=np.array([True, True, False])) 2018-10-28 01:20:00+02:00 0 2018-10-28 02:36:00+02:00 1 2018-10-28 03:46:00+01:00 2 dtype: int64 If the DST transition causes nonexistent times, you can shift these dates forward or backward with a timedelta object or 'shift_forward' or 'shift_backward' . s = pd.Series(range(2), ... index=pd.DatetimeIndex(['2015-03-29 02:30:00', ... '2015-03-29 03:30:00'])) s.tz_localize('Europe/Warsaw', nonexistent='shift_forward') 2015-03-29 03:00:00+02:00 0 2015-03-29 03:30:00+02:00 1 dtype: int64 s.tz_localize('Europe/Warsaw', nonexistent='shift_backward') 2015-03-29 01:59:59.999999999+01:00 0 2015-03-29 03:30:00+02:00 1 dtype: int64 s.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H')) 2015-03-29 03:30:00+02:00 0 2015-03-29 03:30:00+02:00 1 dtype: int64 View Source def tz_localize ( self : FrameOrSeries , tz , axis = 0 , level = None , copy : bool_t = True , ambiguous = \"raise\" , nonexistent : str = \"raise\" , ) -> FrameOrSeries : \"\"\" Localize tz-naive index of a Series or DataFrame to target time zone. This operation localizes the Index. To localize the values in a timezone-naive Series, use :meth:`Series.dt.tz_localize`. Parameters ---------- tz : str or tzinfo axis : the axis to localize level : int, str, default None If axis ia a MultiIndex, localize a specific level. Otherwise must be None. copy : bool, default True Also make a copy of the underlying data. ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise' When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the `ambiguous` parameter dictates how ambiguous times should be handled. - 'infer' will attempt to infer fall dst-transition hours based on order - bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) - 'NaT' will return NaT where there are ambiguous times - 'raise' will raise an AmbiguousTimeError if there are ambiguous times. nonexistent : str, default 'raise' A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. Valid values are: - 'shift_forward' will shift the nonexistent time forward to the closest existing time - 'shift_backward' will shift the nonexistent time backward to the closest existing time - 'NaT' will return NaT where there are nonexistent times - timedelta objects will shift nonexistent times by the timedelta - 'raise' will raise an NonExistentTimeError if there are nonexistent times. .. versionadded:: 0.24.0 Returns ------- Series or DataFrame Same type as the input. Raises ------ TypeError If the TimeSeries is tz-aware and tz is not None. Examples -------- Localize local times: >>> s = pd.Series([1], ... index=pd.DatetimeIndex(['2018-09-15 01:30:00'])) >>> s.tz_localize('CET') 2018-09-15 01:30:00+02:00 1 dtype: int64 Be careful with DST changes. When there is sequential data, pandas can infer the DST time: >>> s = pd.Series(range(7), ... index=pd.DatetimeIndex(['2018-10-28 01:30:00', ... '2018-10-28 02:00:00', ... '2018-10-28 02:30:00', ... '2018-10-28 02:00:00', ... '2018-10-28 02:30:00', ... '2018-10-28 03:00:00', ... '2018-10-28 03:30:00'])) >>> s.tz_localize('CET', ambiguous='infer') 2018-10-28 01:30:00+02:00 0 2018-10-28 02:00:00+02:00 1 2018-10-28 02:30:00+02:00 2 2018-10-28 02:00:00+01:00 3 2018-10-28 02:30:00+01:00 4 2018-10-28 03:00:00+01:00 5 2018-10-28 03:30:00+01:00 6 dtype: int64 In some cases, inferring the DST is impossible. In such cases, you can pass an ndarray to the ambiguous parameter to set the DST explicitly >>> s = pd.Series(range(3), ... index=pd.DatetimeIndex(['2018-10-28 01:20:00', ... '2018-10-28 02:36:00', ... '2018-10-28 03:46:00'])) >>> s.tz_localize('CET', ambiguous=np.array([True, True, False])) 2018-10-28 01:20:00+02:00 0 2018-10-28 02:36:00+02:00 1 2018-10-28 03:46:00+01:00 2 dtype: int64 If the DST transition causes nonexistent times, you can shift these dates forward or backward with a timedelta object or `'shift_forward'` or `'shift_backward'`. >>> s = pd.Series(range(2), ... index=pd.DatetimeIndex(['2015-03-29 02:30:00', ... '2015-03-29 03:30:00'])) >>> s.tz_localize('Europe/Warsaw', nonexistent='shift_forward') 2015-03-29 03:00:00+02:00 0 2015-03-29 03:30:00+02:00 1 dtype: int64 >>> s.tz_localize('Europe/Warsaw', nonexistent='shift_backward') 2015-03-29 01:59:59.999999999+01:00 0 2015-03-29 03:30:00+02:00 1 dtype: int64 >>> s.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H')) 2015-03-29 03:30:00+02:00 0 2015-03-29 03:30:00+02:00 1 dtype: int64 \"\"\" nonexistent_options = ( \"raise\" , \"NaT\" , \"shift_forward\" , \"shift_backward\" ) if nonexistent not in nonexistent_options and not isinstance ( nonexistent , timedelta ): raise ValueError ( \"The nonexistent argument must be one of 'raise', \" \"'NaT', 'shift_forward', 'shift_backward' or \" \"a timedelta object\" ) axis = self . _get_axis_number ( axis ) ax = self . _get_axis ( axis ) def _tz_localize ( ax , tz , ambiguous , nonexistent ): if not hasattr ( ax , \"tz_localize\" ): if len ( ax ) > 0 : ax_name = self . _get_axis_name ( axis ) raise TypeError ( f \"{ax_name} is not a valid DatetimeIndex or PeriodIndex\" ) else : ax = DatetimeIndex ([], tz = tz ) else : ax = ax . tz_localize ( tz , ambiguous = ambiguous , nonexistent = nonexistent ) return ax # if a level is given it must be a MultiIndex level or # equivalent to the axis name if isinstance ( ax , MultiIndex ): level = ax . _get_level_number ( level ) new_level = _tz_localize ( ax . levels [ level ], tz , ambiguous , nonexistent ) ax = ax . set_levels ( new_level , level = level ) else : if level not in ( None , 0 , ax . name ): raise ValueError ( f \"The level {level} is not valid\" ) ax = _tz_localize ( ax , tz , ambiguous , nonexistent ) result = self . copy ( deep = copy ) result = result . set_axis ( ax , axis = axis , inplace = False ) return result . __finalize__ ( self , method = \"tz_localize\" ) unstack def unstack ( self , level =- 1 , fill_value = None ) Pivot a level of the (necessarily hierarchical) index labels. Returns a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels. If the index is not a MultiIndex, the output will be a Series (the analogue of stack when the columns are not a MultiIndex). Parameters level : int, str, or list of these, default -1 (last level) Level(s) of index to unstack, can pass level name. fill_value : int, str or dict Replace NaN with this value if the unstack produces missing values. Returns Series or DataFrame See Also DataFrame.pivot : Pivot a table based on column values. DataFrame.stack : Pivot a level of the column labels (inverse operation from unstack ). Examples index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'), ... ('two', 'a'), ('two', 'b')]) s = pd.Series(np.arange(1.0, 5.0), index=index) s one a 1.0 b 2.0 two a 3.0 b 4.0 dtype: float64 s.unstack(level=-1) a b one 1.0 2.0 two 3.0 4.0 s.unstack(level=0) one two a 1.0 3.0 b 2.0 4.0 df = s.unstack(level=0) df.unstack() one a 1.0 b 2.0 two a 3.0 b 4.0 dtype: float64 View Source def unstack ( self , level =- 1 , fill_value = None ): \"\"\" Pivot a level of the (necessarily hierarchical) index labels. Returns a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels. If the index is not a MultiIndex, the output will be a Series (the analogue of stack when the columns are not a MultiIndex). Parameters ---------- level : int, str, or list of these, default -1 (last level) Level(s) of index to unstack, can pass level name. fill_value : int, str or dict Replace NaN with this value if the unstack produces missing values. Returns ------- Series or DataFrame See Also -------- DataFrame.pivot : Pivot a table based on column values. DataFrame.stack : Pivot a level of the column labels (inverse operation from `unstack`). Examples -------- >>> index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'), ... ('two', 'a'), ('two', 'b')]) >>> s = pd.Series(np.arange(1.0, 5.0), index=index) >>> s one a 1.0 b 2.0 two a 3.0 b 4.0 dtype: float64 >>> s.unstack(level=-1) a b one 1.0 2.0 two 3.0 4.0 >>> s.unstack(level=0) one two a 1.0 3.0 b 2.0 4.0 >>> df = s.unstack(level=0) >>> df.unstack() one a 1.0 b 2.0 two a 3.0 b 4.0 dtype: float64 \"\"\" from pandas . core . reshape . reshape import unstack return unstack ( self , level , fill_value ) update def update ( self , value ) Needs to lock for writing json-database View Source def update ( self , value ): \"\"\" Needs to lock for writing json-database \"\"\" try : value = value . to_frame () except AttributeError as e : pass value = value . reset_index () value . columns = list ( range ( value . columns . __len__ ())) value = value . set_index ( value . columns [ 0 ]) error = None try : self . lock . acquire () try : self . __chk_and_reload_cache () self . drop ( value . index , axis = 0 , errors = 'ignore' , inplace = True ) self . append ( value , inplace = True ). sort_index ( inplace = True ) self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error value_counts def value_counts ( self , subset : Union [ Sequence [ Union [ Hashable , NoneType ]], NoneType ] = None , normalize : bool = False , sort : bool = True , ascending : bool = False ) Return a Series containing counts of unique rows in the DataFrame. .. versionadded:: 1.1.0 Parameters subset : list-like, optional Columns to use when counting unique combinations. normalize : bool, default False Return proportions rather than frequencies. sort : bool, default True Sort by frequencies. ascending : bool, default False Sort in ascending order. Returns Series See Also Series.value_counts: Equivalent method on Series. Notes The returned Series will have a MultiIndex with one level per input column. By default, rows that contain any NA values are omitted from the result. By default, the resulting Series will be in descending order so that the first element is the most frequently-occurring row. Examples df = pd.DataFrame({'num_legs': [2, 4, 4, 6], ... 'num_wings': [2, 0, 0, 0]}, ... index=['falcon', 'dog', 'cat', 'ant']) df num_legs num_wings falcon 2 2 dog 4 0 cat 4 0 ant 6 0 df.value_counts() num_legs num_wings 4 0 2 6 0 1 2 2 1 dtype: int64 df.value_counts(sort=False) num_legs num_wings 2 2 1 4 0 2 6 0 1 dtype: int64 df.value_counts(ascending=True) num_legs num_wings 2 2 1 6 0 1 4 0 2 dtype: int64 df.value_counts(normalize=True) num_legs num_wings 4 0 0.50 6 0 0.25 2 2 0.25 dtype: float64 View Source def value_counts ( self , subset : Optional [ Sequence[Label ] ] = None , normalize : bool = False , sort : bool = True , ascending : bool = False , ) : \"\"\" Return a Series containing counts of unique rows in the DataFrame. .. versionadded:: 1.1.0 Parameters ---------- subset : list-like, optional Columns to use when counting unique combinations. normalize : bool, default False Return proportions rather than frequencies. sort : bool, default True Sort by frequencies. ascending : bool, default False Sort in ascending order. Returns ------- Series See Also -------- Series.value_counts: Equivalent method on Series. Notes ----- The returned Series will have a MultiIndex with one level per input column. By default, rows that contain any NA values are omitted from the result. By default, the resulting Series will be in descending order so that the first element is the most frequently-occurring row. Examples -------- >>> df = pd.DataFrame({'num_legs': [2, 4, 4, 6], ... 'num_wings': [2, 0, 0, 0]}, ... index=['falcon', 'dog', 'cat', 'ant']) >>> df num_legs num_wings falcon 2 2 dog 4 0 cat 4 0 ant 6 0 >>> df.value_counts() num_legs num_wings 4 0 2 6 0 1 2 2 1 dtype: int64 >>> df.value_counts(sort=False) num_legs num_wings 2 2 1 4 0 2 6 0 1 dtype: int64 >>> df.value_counts(ascending=True) num_legs num_wings 2 2 1 6 0 1 4 0 2 dtype: int64 >>> df.value_counts(normalize=True) num_legs num_wings 4 0 0.50 6 0 0.25 2 2 0.25 dtype: float64 \"\"\" if subset is None : subset = self . columns . tolist () counts = self . groupby ( subset ). grouper . size () if sort : counts = counts . sort_values ( ascending = ascending ) if normalize : counts /= counts . sum () # Force MultiIndex for single column if len ( subset ) == 1 : counts . index = MultiIndex . from_arrays ( [ counts.index ] , names =[ counts.index.name ] ) return counts var def var ( self , axis = None , skipna = None , level = None , ddof = 1 , numeric_only = None , ** kwargs ) Return unbiased variance over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument Parameters axis : {index (0), columns (1)} skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. ddof : int, default 1 Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. Returns Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr ) @Appender ( _num_ddof_doc ) def stat_func ( self , axis = None , skipna = None , level = None , ddof = 1 , numeric_only = None , ** kwargs ) : nv . validate_stat_ddof_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna , ddof = ddof ) return self . _reduce ( func , name , axis = axis , numeric_only = numeric_only , skipna = skipna , ddof = ddof ) where def where ( self , cond , other = nan , inplace = False , axis = None , level = None , errors = 'raise' , try_cast = False ) Replace values where the condition is False. Parameters cond : bool Series/DataFrame, array-like, or callable Where cond is True, keep the original value. Where False, replace with corresponding value from other . If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn't check it). other : scalar, Series/DataFrame, or callable Entries where cond is False are replaced with corresponding value from other . If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn't check it). inplace : bool, default False Whether to perform the operation in place on the data. axis : int, default None Alignment axis if needed. level : int, default None Alignment level if needed. errors : str, {'raise', 'ignore'}, default 'raise' Note that currently this parameter won't affect the results and will always coerce to a suitable dtype. - 'raise' : allow exceptions to be raised. - 'ignore' : suppress exceptions. On error return original object. try_cast : bool, default False Try to cast the result back to the input type (if possible). Returns Same type as caller See Also :func: DataFrame.mask : Return an object of same shape as self. Notes The where method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is True the element is used; otherwise the corresponding element from the DataFrame other is used. The signature for :func: DataFrame.where differs from :func: numpy.where . Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2) . For further details and examples see the where documentation in :ref: indexing <indexing.where_mask> . Examples s = pd.Series(range(5)) s.where(s > 0) 0 NaN 1 1.0 2 2.0 3 3.0 4 4.0 dtype: float64 s.mask(s > 0) 0 0.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 s.where(s > 1, 10) 0 10 1 10 2 2 3 3 4 4 dtype: int64 df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B']) df A B 0 0 1 1 2 3 2 4 5 3 6 7 4 8 9 m = df % 3 == 0 df.where(m, -df) A B 0 0 -1 1 -2 3 2 -4 -5 3 6 -7 4 -8 9 df.where(m, -df) == np.where(m, df, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True df.where(m, -df) == df.mask(~m, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True View Source @ doc ( klass = _shared_doc_kwargs [ \"klass\" ], cond = \"True\" , cond_rev = \"False\" , name = \"where\" , name_other = \"mask\" , ) def where ( self , cond , other = np . nan , inplace = False , axis = None , level = None , errors = \"raise\" , try_cast = False , ): \"\"\" Replace values where the condition is {cond_rev}. Parameters ---------- cond : bool {klass}, array-like, or callable Where `cond` is {cond}, keep the original value. Where {cond_rev}, replace with corresponding value from `other`. If `cond` is callable, it is computed on the {klass} and should return boolean {klass} or array. The callable must not change input {klass} (though pandas doesn't check it). other : scalar, {klass}, or callable Entries where `cond` is {cond_rev} are replaced with corresponding value from `other`. If other is callable, it is computed on the {klass} and should return scalar or {klass}. The callable must not change input {klass} (though pandas doesn't check it). inplace : bool, default False Whether to perform the operation in place on the data. axis : int, default None Alignment axis if needed. level : int, default None Alignment level if needed. errors : str, {{'raise', 'ignore'}}, default 'raise' Note that currently this parameter won't affect the results and will always coerce to a suitable dtype. - 'raise' : allow exceptions to be raised. - 'ignore' : suppress exceptions. On error return original object. try_cast : bool, default False Try to cast the result back to the input type (if possible). Returns ------- Same type as caller See Also -------- :func:`DataFrame.{name_other}` : Return an object of same shape as self. Notes ----- The {name} method is an application of the if-then idiom. For each element in the calling DataFrame, if ``cond`` is ``{cond}`` the element is used; otherwise the corresponding element from the DataFrame ``other`` is used. The signature for :func:`DataFrame.where` differs from :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to ``np.where(m, df1, df2)``. For further details and examples see the ``{name}`` documentation in :ref:`indexing <indexing.where_mask>`. Examples -------- >>> s = pd.Series(range(5)) >>> s.where(s > 0) 0 NaN 1 1.0 2 2.0 3 3.0 4 4.0 dtype: float64 >>> s.mask(s > 0) 0 0.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 >>> s.where(s > 1, 10) 0 10 1 10 2 2 3 3 4 4 dtype: int64 >>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B']) >>> df A B 0 0 1 1 2 3 2 4 5 3 6 7 4 8 9 >>> m = df % 3 == 0 >>> df.where(m, -df) A B 0 0 -1 1 -2 3 2 -4 -5 3 6 -7 4 -8 9 >>> df.where(m, -df) == np.where(m, df, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True >>> df.where(m, -df) == df.mask(~m, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True \"\"\" other = com . apply_if_callable ( other , self ) return self . _where ( cond , other , inplace , axis , level , errors = errors , try_cast = try_cast ) xs def xs ( self , key , axis = 0 , level = None , drop_level : bool = True ) Return cross-section from the Series/DataFrame. This method takes a key argument to select data at a particular level of a MultiIndex. Parameters key : label or tuple of label Label contained in the index, or partially in a MultiIndex. axis : {0 or 'index', 1 or 'columns'}, default 0 Axis to retrieve cross-section on. level : object, defaults to first n levels (n=1 or len(key)) In case of a key partially contained in a MultiIndex, indicate which levels are used. Levels can be referred by label or position. drop_level : bool, default True If False, returns object with same levels as self. Returns Series or DataFrame Cross-section from the original Series or DataFrame corresponding to the selected index levels. See Also DataFrame.loc : Access a group of rows and columns by label(s) or a boolean array. DataFrame.iloc : Purely integer-location based indexing for selection by position. Notes xs can not be used to set values. MultiIndex Slicers is a generic way to get/set values on any level or levels. It is a superset of xs functionality, see :ref: MultiIndex Slicers <advanced.mi_slicers> . Examples d = {'num_legs': [4, 4, 2, 2], ... 'num_wings': [0, 0, 2, 2], ... 'class': ['mammal', 'mammal', 'mammal', 'bird'], ... 'animal': ['cat', 'dog', 'bat', 'penguin'], ... 'locomotion': ['walks', 'walks', 'flies', 'walks']} df = pd.DataFrame(data=d) df = df.set_index(['class', 'animal', 'locomotion']) df num_legs num_wings class animal locomotion mammal cat walks 4 0 dog walks 4 0 bat flies 2 2 bird penguin walks 2 2 Get values at specified index df.xs('mammal') num_legs num_wings animal locomotion cat walks 4 0 dog walks 4 0 bat flies 2 2 Get values at several indexes df.xs(('mammal', 'dog')) num_legs num_wings locomotion walks 4 0 Get values at specified index and level df.xs('cat', level=1) num_legs num_wings class locomotion mammal walks 4 0 Get values at several indexes and levels df.xs(('bird', 'walks'), ... level=[0, 'locomotion']) num_legs num_wings animal penguin 2 2 Get values at specified column and axis df.xs('num_wings', axis=1) class animal locomotion mammal cat walks 0 dog walks 0 bat flies 2 bird penguin walks 2 Name: num_wings, dtype: int64 View Source def xs ( self , key , axis = 0 , level = None , drop_level : bool_t = True ) : \"\"\" Return cross-section from the Series/DataFrame. This method takes a `key` argument to select data at a particular level of a MultiIndex. Parameters ---------- key : label or tuple of label Label contained in the index, or partially in a MultiIndex. axis : {0 or 'index', 1 or 'columns'}, default 0 Axis to retrieve cross-section on. level : object, defaults to first n levels (n=1 or len(key)) In case of a key partially contained in a MultiIndex, indicate which levels are used. Levels can be referred by label or position. drop_level : bool, default True If False, returns object with same levels as self. Returns ------- Series or DataFrame Cross-section from the original Series or DataFrame corresponding to the selected index levels. See Also -------- DataFrame.loc : Access a group of rows and columns by label(s) or a boolean array. DataFrame.iloc : Purely integer-location based indexing for selection by position. Notes ----- `xs` can not be used to set values. MultiIndex Slicers is a generic way to get/set values on any level or levels. It is a superset of `xs` functionality, see :ref:`MultiIndex Slicers <advanced.mi_slicers>`. Examples -------- >>> d = {'num_legs': [4, 4, 2, 2], ... 'num_wings': [0, 0, 2, 2], ... 'class': ['mammal', 'mammal', 'mammal', 'bird'], ... 'animal': ['cat', 'dog', 'bat', 'penguin'], ... 'locomotion': ['walks', 'walks', 'flies', 'walks']} >>> df = pd.DataFrame(data=d) >>> df = df.set_index(['class', 'animal', 'locomotion']) >>> df num_legs num_wings class animal locomotion mammal cat walks 4 0 dog walks 4 0 bat flies 2 2 bird penguin walks 2 2 Get values at specified index >>> df.xs('mammal') num_legs num_wings animal locomotion cat walks 4 0 dog walks 4 0 bat flies 2 2 Get values at several indexes >>> df.xs(('mammal', 'dog')) num_legs num_wings locomotion walks 4 0 Get values at specified index and level >>> df.xs('cat', level=1) num_legs num_wings class locomotion mammal walks 4 0 Get values at several indexes and levels >>> df.xs(('bird', 'walks'), ... level=[0, 'locomotion']) num_legs num_wings animal penguin 2 2 Get values at specified column and axis >>> df.xs('num_wings', axis=1) class animal locomotion mammal cat walks 0 dog walks 0 bat flies 2 bird penguin walks 2 Name: num_wings, dtype: int64 \"\"\" axis = self . _get_axis_number ( axis ) labels = self . _get_axis ( axis ) if level is not None : if not isinstance ( labels , MultiIndex ) : raise TypeError ( \"Index must be a MultiIndex\" ) loc , new_ax = labels . get_loc_level ( key , level = level , drop_level = drop_level ) # create the tuple of the indexer _indexer = [ slice(None) ] * self . ndim _indexer [ axis ] = loc indexer = tuple ( _indexer ) result = self . iloc [ indexer ] setattr ( result , result . _get_axis_name ( axis ), new_ax ) return result if axis == 1 : return self [ key ] index = self . index if isinstance ( index , MultiIndex ) : loc , new_index = self . index . get_loc_level ( key , drop_level = drop_level ) else : loc = self . index . get_loc ( key ) if isinstance ( loc , np . ndarray ) : if loc . dtype == np . bool_ : ( inds ,) = loc . nonzero () return self . _take_with_is_copy ( inds , axis = axis ) else : return self . _take_with_is_copy ( loc , axis = axis ) if not is_scalar ( loc ) : new_index = self . index [ loc ] if is_scalar ( loc ) : # In this case loc should be an integer if self . ndim == 1 : # if we encounter an array - like and we only have 1 dim # that means that their are list / ndarrays inside the Series ! # so just return them ( GH 6394 ) return self . _values [ loc ] new_values = self . _mgr . fast_xs ( loc ) result = self . _constructor_sliced ( new_values , index = self . columns , name = self . index [ loc ] , dtype = new_values . dtype , ) else : result = self . iloc [ loc ] result . index = new_index # this could be a view # but only in a single - dtyped view sliceable case result . _set_is_copy ( self , copy = not result . _is_view ) return result DB class DB ( connection ) Helper class that provides a standard way to create an ABC using inheritance. View Source class DB ( ABC ) : @abstractmethod def __init__ ( self , connection ) : pass @abstractmethod def __getitem__ ( self , key ) : pass @abstractmethod def __setitem__ ( self , key , value ) : pass @abstractmethod def pop ( self , key ) : pass Ancestors (in MRO) abc.ABC Descendants hielen2.datalink_prova_df.JsonDB Methods pop def pop ( self , key ) View Source @abstractmethod def pop ( self , key ) : pass JsonDB class JsonDB ( connection , schema , lock_timeout_seconds = 10 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class JsonDB ( DB ) : def __init__ ( self , connection , schema , lock_timeout_seconds = 10 ) : self . jsonfile = connection self . lock = FileLock ( f \"{connection}.lock\" , timeout = lock_timeout_seconds ) self . md5file = f \"{connection}.md5\" self . md5 = None self . schema = schema self . __chk_and_reload_jsondb ( force = True ) def __brute_load_jsondb ( self ) : try : self . db = read_json ( self . jsonfile , orient = 'table' , convert_dates = False ) except Exception as e : self . db = DataFrame () if self . db . empty : self . db = DataFrame ( {} , columns = self . schema [ 'columns' ] ) self . db = self . db . set_index ( self . schema [ 'primary_key' ] ) def __chk_and_reload_jsondb ( self , force = False ) : \"\"\" Needs to check for json-database file changes in a thread safe way!! \"\"\" md5 = None error = None try : self . lock . acquire () try : if force : raise FileNotFoundError () with open ( self . md5file ) as o : md5 = o . read () if not md5 == self . md5 : self . md5 = md5 self . __brute_load_jsondb () except FileNotFoundError as e : ## refershing hash self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) self . __brute_load_jsondb () finally : self . lock . release () except Timeout : pass def save ( self ) : try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def __write_jsondb ( self , key , value ) : \"\"\" Needs to lock for writing json-database \"\"\" item = None error = None try : self . lock . acquire () try : self . __chk_and_reload_jsondb () if value is None : # Request to remove key , raises KeyError item = self . __getitem__ ( key ) try : self . db = self . db . drop ( key , axis = 0 ) except KeyError : raise KeyError ( f \"key {key} to remove does not exist\" ) else : # Request to insert key , raises ValueError primarykey = self . schema [ 'primary_key' ] if not isinstance ( key ,( list , set , tuple )) : key =[ key ] if key . __len__ () < primarykey . __len__ () : raise ValueError ( f \"key {key!r} is not fully determinated\" ) keydict = dict ( zip ( self . schema [ 'primary_key' ] , key )) value . update ( keydict ) df = DataFrame ( [ value.values() ] ) df . columns = value . keys () df = df . set_index ( self . schema [ 'primary_key' ] ) try : self . db = self . db . append ( df , verify_integrity = True ). sort_index () except ValueError : raise ValueError ( f \"key {key} to insert exists\" ) self . db . replace ( { nan : None , NaT : None } , inplace = True ) item = self . __brute_getitem ( key ) self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error return item def __brute_getitem ( self , key = None ) : out = None if key is None : out = self . db else : out = self . db . loc [ key ] if isinstance ( out , Series ) : out = out . to_frame (). T out . index . names = self . schema [ 'primary_key' ] out = out . reset_index (). to_dict ( orient = 'records' ) if out . __len__ () == 1 : out = out [ 0 ] return out def __getitem__ ( self , key = None ) : self . __chk_and_reload_jsondb () if isinstance ( key , list ) : try : key = list ( filter ( None , key )) except TypeError : pass return self . __brute_getitem ( key ) def pop ( self , key ) : return self . __write_jsondb ( key , None ) def __setitem__ ( self , key = None , value = None ) : self . __write_jsondb ( key , value ) Ancestors (in MRO) hielen2.datalink_prova_df.DB abc.ABC Descendants hielen2.datalink_prova_df.fsHielenCache Methods pop def pop ( self , key ) View Source def pop ( self , key ): return self . __write_jsondb ( key , None ) save def save ( self ) View Source def save ( self ): try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e fsHielenCache class fsHielenCache ( connection , lock_timeout_seconds = 10 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class fsHielenCache ( JsonDB ) : def __init__ ( self , connection , lock_timeout_seconds = 10 ) : self . cachepath = connection self . lts = lock_timeout_seconds schema = { \"columns\" : [ \"uid\",\"info\" ] , \"primary_key\" : [ \"uid\" ] } connfile = str ( Path ( connection ) / \"index.json\" ) super (). __init__ ( connfile , schema , self . lts ) def __getitem__ ( self , key ) : info = super (). __getitem__ ( key ) return CsvCache ( self . cachepath , key , self . lts ). get ( force_reload = True ) def __setitem__ ( self , key , value ) : if value is not None and not isinstance ( value ,( DataFrame , Series )) : raise ValueError ( \"pandas.DataFrame or pandas.Series required\" ) try : assert isinstance ( key , str ) assert key . __len__ () == 32 except AssertionError as e : raise ValueError ( f \"key {key} doesn't seems to match requirement format\" ) if value is not None : super (). __setitem__ ( key , {} ) item = CsvCache ( self . cachepath , key , self . lts ) os . makedirs ( item . cachepath , exist_ok = True ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ]= statistics else : super (). __setitem__ ( key , None ) try : item = CsvCache ( self . cachepath , key , self . lts ) item . cleanfs () except FileNotFoundError as e : pass def update ( self , key , value ) : if value is not None and not isinstance ( value ,( DataFrame , Series )) : #if value is not None and not isinstance ( value , DataFrame ) : raise ValueError ( \"pandas.DataFrame or pandas.Series required\" ) if value is not None : info = super (). __getitem__ ( key ) item = CsvCache ( self . cachepath , key , self . lts ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ]= statistics Ancestors (in MRO) hielen2.datalink_prova_df.JsonDB hielen2.datalink_prova_df.DB abc.ABC Methods pop def pop ( self , key ) View Source def pop ( self , key ): return self . __write_jsondb ( key , None ) save def save ( self ) View Source def save ( self ): try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e update def update ( self , key , value ) View Source def update ( self , key , value ) : if value is not None and not isinstance ( value ,( DataFrame , Series )) : #if value is not None and not isinstance ( value , DataFrame ) : raise ValueError ( \"pandas.DataFrame or pandas.Series required\" ) if value is not None : info = super (). __getitem__ ( key ) item = CsvCache ( self . cachepath , key , self . lts ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ]= statistics seriescode class seriescode ( * args , ** kwargs ) View Source class seriescode (): def __init__ ( self ,* args ,** kwargs ): self . h =[ * args ] self . h . extend ( list ( kwargs . values ())) self . h = '' . join ([ str ( a ) for a in self . h ]) self . h = md5 ( f' { self . h }'. encode () ). hexdigest () def __repr__ ( self ): return self . h","title":"Datalink Prova Df"},{"location":"reference/hielen2/datalink_prova_df/#module-hielen2datalink_prova_df","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647 648 649 650 651 #!/usr/bin/env python # coding=utf-8 from pandas import DataFrame , Series , read_json , NaT , read_csv from abc import ABC , abstractmethod from hielen2.utils import loadjsonfile , savejsonfile , newinstanceof , hashfile from filelock import Timeout , FileLock from numpy import nan from pathlib import Path from hashlib import md5 from shutil import rmtree import os def dbinit ( conf ): return { k : newinstanceof ( w . pop ( \"klass\" ), ** w ) for k , w in conf [ \"db\" ] . items () } class DB ( ABC ): @abstractmethod def __init__ ( self , connection ): pass @abstractmethod def __getitem__ ( self , key ): pass @abstractmethod def __setitem__ ( self , key , value ): pass @abstractmethod def pop ( self , key ): pass class JsonDB ( DB ): def __init__ ( self , connection , schema , lock_timeout_seconds = 10 ): self . jsonfile = connection self . lock = FileLock ( f \" { connection } .lock\" , timeout = lock_timeout_seconds ) self . md5file = f \" { connection } .md5\" self . md5 = None self . schema = schema self . __chk_and_reload_jsondb ( force = True ) def __brute_load_jsondb ( self ): try : self . db = read_json ( self . jsonfile , orient = 'table' , convert_dates = False ) except Exception as e : self . db = DataFrame () if self . db . empty : self . db = DataFrame ({}, columns = self . schema [ 'columns' ]) self . db = self . db . set_index ( self . schema [ 'primary_key' ]) def __chk_and_reload_jsondb ( self , force = False ): \"\"\" Needs to check for json-database file changes in a thread safe way!! \"\"\" md5 = None error = None try : self . lock . acquire () try : if force : raise FileNotFoundError () with open ( self . md5file ) as o : md5 = o . read () if not md5 == self . md5 : self . md5 = md5 self . __brute_load_jsondb () except FileNotFoundError as e : ## refershing hash self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) self . __brute_load_jsondb () finally : self . lock . release () except Timeout : pass def save ( self ): try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def __write_jsondb ( self , key , value ): \"\"\" Needs to lock for writing json-database \"\"\" item = None error = None try : self . lock . acquire () try : self . __chk_and_reload_jsondb () if value is None : # Request to remove key, raises KeyError item = self . __getitem__ ( key ) try : self . db = self . db . drop ( key , axis = 0 ) except KeyError : raise KeyError ( f \"key { key } to remove does not exist\" ) else : # Request to insert key, raises ValueError primarykey = self . schema [ 'primary_key' ] if not isinstance ( key ,( list , set , tuple )): key = [ key ] if key . __len__ () < primarykey . __len__ (): raise ValueError ( f \"key { key !r} is not fully determinated\" ) keydict = dict ( zip ( self . schema [ 'primary_key' ], key )) value . update ( keydict ) df = DataFrame ([ value . values ()]) df . columns = value . keys () df = df . set_index ( self . schema [ 'primary_key' ]) try : self . db = self . db . append ( df , verify_integrity = True ) . sort_index () except ValueError : raise ValueError ( f \"key { key } to insert exists\" ) self . db . replace ({ nan : None , NaT : None }, inplace = True ) item = self . __brute_getitem ( key ) self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error return item def __brute_getitem ( self , key = None ): out = None if key is None : out = self . db else : out = self . db . loc [ key ] if isinstance ( out , Series ): out = out . to_frame () . T out . index . names = self . schema [ 'primary_key' ] out = out . reset_index () . to_dict ( orient = 'records' ) if out . __len__ () == 1 : out = out [ 0 ] return out def __getitem__ ( self , key = None ): self . __chk_and_reload_jsondb () if isinstance ( key , list ): try : key = list ( filter ( None , key )) except TypeError : pass return self . __brute_getitem ( key ) def pop ( self , key ): return self . __write_jsondb ( key , None ) def __setitem__ ( self , key = None , value = None ): self . __write_jsondb ( key , value ) ''' class JsonCache(DB): def __init__(self, connection): self.cache = ( read_json(connection, convert_dates=False) .set_index([\"uid\", \"timestamp\"])[\"value\"] .sort_index() ) self.filename = connection def __getitem__(self, key): return self.cache[key] def __setitem__(self, key, value): pass def pop(self, key): pass def save(self): self.cache.reset_index().to_json(self.filename, orient=\"records\") ''' class seriescode (): def __init__ ( self , * args , ** kwargs ): self . h = [ * args ] self . h . extend ( list ( kwargs . values ())) self . h = '' . join ([ str ( a ) for a in self . h ]) self . h = md5 ( f ' { self . h } ' . encode () ) . hexdigest () def __repr__ ( self ): return self . h class fsHielenCache ( JsonDB ): def __init__ ( self , connection , lock_timeout_seconds = 10 ): self . cachepath = connection self . lts = lock_timeout_seconds schema = { \"columns\" :[ \"uid\" , \"info\" ], \"primary_key\" :[ \"uid\" ]} connfile = str ( Path ( connection ) / \"index.json\" ) super () . __init__ ( connfile , schema , self . lts ) def __getitem__ ( self , key ): info = super () . __getitem__ ( key ) return CsvCache ( self . cachepath , key , self . lts ) . get ( force_reload = True ) def __setitem__ ( self , key , value ): if value is not None and not isinstance ( value ,( DataFrame , Series )): raise ValueError ( \"pandas.DataFrame or pandas.Series required\" ) try : assert isinstance ( key , str ) assert key . __len__ () == 32 except AssertionError as e : raise ValueError ( f \"key { key } doesn't seems to match requirement format\" ) if value is not None : super () . __setitem__ ( key ,{}) item = CsvCache ( self . cachepath , key , self . lts ) os . makedirs ( item . cachepath , exist_ok = True ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ] = statistics else : super () . __setitem__ ( key , None ) try : item = CsvCache ( self . cachepath , key , self . lts ) item . cleanfs () except FileNotFoundError as e : pass def update ( self , key , value ): if value is not None and not isinstance ( value ,( DataFrame , Series )): #if value is not None and not isinstance(value,DataFrame): raise ValueError ( \"pandas.DataFrame or pandas.Series required\" ) if value is not None : info = super () . __getitem__ ( key ) item = CsvCache ( self . cachepath , key , self . lts ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ] = statistics class CsvCache ( DataFrame ): def __init__ ( self , cachepath , item , lock_timeout_seconds = 10 ): super () . __init__ ({}) self . cachepath = Path ( cachepath ) / item [ 0 : 8 ] / item [ 8 : 16 ] / item [ 16 : 24 ] / item [ 24 : 32 ] self . db = None self . csv = str ( self . cachepath / f \" { item } .csv\" ) self . lock = FileLock ( f \" { self . csv } .lock\" , timeout = lock_timeout_seconds ) self . md5file = f \" { self . csv } .md5\" self . md5 = None #self.__chk_and_reload_cache(force=True) def __brute_load_cache ( self ): try : tmpdf = read_csv ( self . csv , header = None , sep = \";\" , parse_dates = True ) super () . __init__ ( tmpdf ) print ( 'STOCAZZO' ) print ( self ) print ( 'CETTESEFREGE' ) except Exception as e : pass def __chk_and_reload_cache ( self , force = False ): \"\"\" Needs to check for cache file changes in a thread safe way!! \"\"\" md5 = None error = None try : self . lock . acquire () try : if force : raise FileNotFoundError () with open ( self . md5file ) as o : md5 = o . read () if not md5 == self . md5 : self . md5 = md5 self . __brute_load_cache () except FileNotFoundError as e : ## refershing hash try : self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) except FileNotFoundError as e : pass self . __brute_load_cache () finally : self . lock . release () except Timeout : pass def save ( self ): try : self . lock . acquire () try : self . to_csv ( self . csv , header = None , sep = \";\" ) self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def cleanfs ( self ): try : self . lock . acquire () try : os . unlink ( self . csv ) os . unlink ( self . md5file ) self . drop ( self . index , axis = 1 , inplace = True ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def update ( self , value ): \"\"\" Needs to lock for writing json-database \"\"\" try : value = value . to_frame () except AttributeError as e : pass value = value . reset_index () value . columns = list ( range ( value . columns . __len__ ())) value = value . set_index ( value . columns [ 0 ]) error = None try : self . lock . acquire () try : self . __chk_and_reload_cache () self . drop ( value . index , axis = 0 , errors = 'ignore' , inplace = True ) self . append ( value , inplace = True ) . sort_index ( inplace = True ) self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error def get ( self , force_reload = False ): self . __chk_and_reload_cache ( force = force_reload ) return self . db","title":"Module hielen2.datalink_prova_df"},{"location":"reference/hielen2/datalink_prova_df/#variables","text":"nan","title":"Variables"},{"location":"reference/hielen2/datalink_prova_df/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/datalink_prova_df/#dbinit","text":"def dbinit ( conf ) View Source def dbinit ( conf ): return { k : newinstanceof ( w . pop ( \"klass\" ), ** w ) for k , w in conf [ \"db\" ]. items () }","title":"dbinit"},{"location":"reference/hielen2/datalink_prova_df/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/datalink_prova_df/#csvcache","text":"class CsvCache ( cachepath , item , lock_timeout_seconds = 10 ) Two-dimensional, size-mutable, potentially heterogeneous tabular data. Data structure also contains labeled axes (rows and columns). Arithmetic operations align on both row and column labels. Can be thought of as a dict-like container for Series objects. The primary pandas data structure.","title":"CsvCache"},{"location":"reference/hielen2/datalink_prova_df/#parameters","text":"data : ndarray (structured or homogeneous), Iterable, dict, or DataFrame Dict can contain Series, arrays, constants, or list-like objects. .. versionchanged :: 0.23.0 If data is a dict, column order follows insertion-order for Python 3.6 and later. .. versionchanged :: 0.25.0 If data is a list of dicts, column order follows insertion-order for Python 3.6 and later. index : Index or array-like Index to use for resulting frame. Will default to RangeIndex if no indexing information part of input data and no index provided. columns : Index or array-like Column labels to use for resulting frame. Will default to RangeIndex (0, 1, 2, ..., n) if no column labels are provided. dtype : dtype, default None Data type to force. Only a single dtype is allowed. If None, infer. copy : bool, default False Copy data from inputs. Only affects DataFrame / 2d ndarray input.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#see-also","text":"DataFrame.from_records : Constructor from tuples, also record arrays. DataFrame.from_dict : From dicts of Series, arrays, or dicts. read_csv : Read a comma-separated values (csv) file into DataFrame. read_table : Read general delimited file into DataFrame. read_clipboard : Read text from clipboard into DataFrame.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples","text":"Constructing DataFrame from a dictionary. d = {'col1': [1, 2], 'col2': [3, 4]} df = pd.DataFrame(data=d) df col1 col2 0 1 3 1 2 4 Notice that the inferred dtype is int64. df.dtypes col1 int64 col2 int64 dtype: object To enforce a single dtype: df = pd.DataFrame(data=d, dtype=np.int8) df.dtypes col1 int8 col2 int8 dtype: object Constructing DataFrame from numpy ndarray: df2 = pd.DataFrame(np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9]]), ... columns=['a', 'b', 'c']) df2 a b c 0 1 2 3 1 4 5 6 2 7 8 9 View Source class CsvCache ( DataFrame ): def __init__ ( self , cachepath , item , lock_timeout_seconds = 10 ): super (). __init__ ({}) self . cachepath = Path ( cachepath ) / item [ 0 : 8 ] / item [ 8 : 16 ] / item [ 16 : 24 ] / item [ 24 : 32 ] self . db = None self . csv = str ( self . cachepath / f \"{item}.csv\" ) self . lock = FileLock ( f \"{self.csv}.lock\" , timeout = lock_timeout_seconds ) self . md5file = f \"{self.csv}.md5\" self . md5 = None #self.__chk_and_reload_cache(force=True) def __brute_load_cache ( self ): try: tmpdf = read_csv ( self . csv , header = None , sep = \";\" , parse_dates = True ) super (). __init__ ( tmpdf ) print ( 'STOCAZZO' ) print ( self ) print ( 'CETTESEFREGE' ) except Exception as e: pass def __chk_and_reload_cache ( self , force = False ): \"\"\" Needs to check for cache file changes in a thread safe way!! \"\"\" md5 = None error = None try: self . lock . acquire () try: if force: raise FileNotFoundError () with open ( self . md5file ) as o: md5 = o . read () if not md5 == self . md5: self . md5 = md5 self . __brute_load_cache () except FileNotFoundError as e: ## refershing hash try: self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o: o . write ( self . md5 ) except FileNotFoundError as e: pass self . __brute_load_cache () finally: self . lock . release () except Timeout: pass def save ( self ): try: self . lock . acquire () try: self . to_csv ( self . csv , header = None , sep = \";\" ) self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o: o . write ( self . md5 ) finally: self . lock . release () except Timeout as e: # Just to remind Timout error here raise e def cleanfs ( self ): try: self . lock . acquire () try: os . unlink ( self . csv ) os . unlink ( self . md5file ) self . drop ( self . index , axis = 1 , inplace = True ) finally: self . lock . release () except Timeout as e: # Just to remind Timout error here raise e def update ( self , value ): \"\"\" Needs to lock for writing json-database \"\"\" try: value = value . to_frame () except AttributeError as e: pass value = value . reset_index () value . columns = list ( range ( value . columns . __len__ ())) value = value . set_index ( value . columns [ 0 ]) error = None try: self . lock . acquire () try: self . __chk_and_reload_cache () self . drop ( value . index , axis = 0 , errors = 'ignore' , inplace = True ) self . append ( value , inplace = True ). sort_index ( inplace = True ) self . save () except Exception as e: error = e finally: self . lock . release () except Timeout as e: error = e if error is not None: raise error def get ( self , force_reload = False ): self . __chk_and_reload_cache ( force = force_reload ) return self . db","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#ancestors-in-mro","text":"pandas.core.frame.DataFrame pandas.core.generic.NDFrame pandas.core.base.PandasObject pandas.core.accessor.DirNamesMixin pandas.core.base.SelectionMixin pandas.core.indexing.IndexingMixin","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/datalink_prova_df/#class-variables","text":"columns index plot sparse","title":"Class variables"},{"location":"reference/hielen2/datalink_prova_df/#static-methods","text":"","title":"Static methods"},{"location":"reference/hielen2/datalink_prova_df/#from_dict","text":"def from_dict ( data , orient = 'columns' , dtype = None , columns = None ) -> 'DataFrame' Construct DataFrame from dict of array-like or dicts. Creates DataFrame object from dictionary by columns or by index allowing dtype specification.","title":"from_dict"},{"location":"reference/hielen2/datalink_prova_df/#parameters_1","text":"data : dict Of the form {field : array-like} or {field : dict}. orient : {'columns', 'index'}, default 'columns' The \"orientation\" of the data. If the keys of the passed dict should be the columns of the resulting DataFrame, pass 'columns' (default). Otherwise if the keys should be rows, pass 'index'. dtype : dtype, default None Data type to force, otherwise infer. columns : list, default None Column labels to use when orient='index' . Raises a ValueError if used with orient='columns' . .. versionadded :: 0.23.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns","text":"DataFrame","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_1","text":"DataFrame.from_records : DataFrame from structured ndarray, sequence of tuples or dicts, or DataFrame. DataFrame : DataFrame object creation using constructor.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_1","text":"By default the keys of the dict become the DataFrame columns: data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']} pd.DataFrame.from_dict(data) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Specify orient='index' to create the DataFrame using dictionary keys as rows: data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']} pd.DataFrame.from_dict(data, orient='index') 0 1 2 3 row_1 3 2 1 0 row_2 a b c d When using the 'index' orientation, the column names can be specified manually: pd.DataFrame.from_dict(data, orient='index', ... columns=['A', 'B', 'C', 'D']) A B C D row_1 3 2 1 0 row_2 a b c d View Source @classmethod def from_dict ( cls , data , orient = \"columns\" , dtype = None , columns = None ) -> \"DataFrame\" : \"\"\" Construct DataFrame from dict of array-like or dicts. Creates DataFrame object from dictionary by columns or by index allowing dtype specification. Parameters ---------- data : dict Of the form {field : array-like} or {field : dict}. orient : {'columns', 'index'}, default 'columns' The \" orientation \" of the data. If the keys of the passed dict should be the columns of the resulting DataFrame, pass 'columns' (default). Otherwise if the keys should be rows, pass 'index'. dtype : dtype, default None Data type to force, otherwise infer. columns : list, default None Column labels to use when ``orient='index'``. Raises a ValueError if used with ``orient='columns'``. .. versionadded:: 0.23.0 Returns ------- DataFrame See Also -------- DataFrame.from_records : DataFrame from structured ndarray, sequence of tuples or dicts, or DataFrame. DataFrame : DataFrame object creation using constructor. Examples -------- By default the keys of the dict become the DataFrame columns: >>> data = {'col_1': [3, 2, 1, 0], 'col_2': ['a', 'b', 'c', 'd']} >>> pd.DataFrame.from_dict(data) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Specify ``orient='index'`` to create the DataFrame using dictionary keys as rows: >>> data = {'row_1': [3, 2, 1, 0], 'row_2': ['a', 'b', 'c', 'd']} >>> pd.DataFrame.from_dict(data, orient='index') 0 1 2 3 row_1 3 2 1 0 row_2 a b c d When using the 'index' orientation, the column names can be specified manually: >>> pd.DataFrame.from_dict(data, orient='index', ... columns=['A', 'B', 'C', 'D']) A B C D row_1 3 2 1 0 row_2 a b c d \"\"\" index = None orient = orient . lower () if orient == \"index\" : if len ( data ) > 0 : # TODO speed up Series case if isinstance ( list ( data . values ()) [ 0 ] , ( Series , dict )) : data = _from_nested_dict ( data ) else : data , index = list ( data . values ()), list ( data . keys ()) elif orient == \"columns\" : if columns is not None : raise ValueError ( \"cannot use columns parameter with orient='columns'\" ) else : # pragma : no cover raise ValueError ( \"only recognize index or columns for orient\" ) return cls ( data , index = index , columns = columns , dtype = dtype )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#from_records","text":"def from_records ( data , index = None , exclude = None , columns = None , coerce_float = False , nrows = None ) -> 'DataFrame' Convert structured or record ndarray to DataFrame. Creates a DataFrame object from a structured ndarray, sequence of tuples or dicts, or DataFrame.","title":"from_records"},{"location":"reference/hielen2/datalink_prova_df/#parameters_2","text":"data : structured ndarray, sequence of tuples or dicts, or DataFrame Structured input data. index : str, list of fields, array-like Field of array to use as the index, alternately a specific set of input labels to use. exclude : sequence, default None Columns or fields to exclude. columns : sequence, default None Column names to use. If the passed data do not have names associated with them, this argument provides names for the columns. Otherwise this argument indicates the order of the columns in the result (any names not found in the data will become all-NA columns). coerce_float : bool, default False Attempt to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point, useful for SQL result sets. nrows : int, default None Number of rows to read if data is an iterator.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_1","text":"DataFrame","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_2","text":"DataFrame.from_dict : DataFrame from dict of array-like or dicts. DataFrame : DataFrame object creation using constructor.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_2","text":"Data can be provided as a structured ndarray: data = np.array([(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')], ... dtype=[('col_1', 'i4'), ('col_2', 'U1')]) pd.DataFrame.from_records(data) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Data can be provided as a list of dicts: data = [{'col_1': 3, 'col_2': 'a'}, ... {'col_1': 2, 'col_2': 'b'}, ... {'col_1': 1, 'col_2': 'c'}, ... {'col_1': 0, 'col_2': 'd'}] pd.DataFrame.from_records(data) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Data can be provided as a list of tuples with corresponding columns: data = [(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')] pd.DataFrame.from_records(data, columns=['col_1', 'col_2']) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d View Source @classmethod def from_records ( cls , data , index = None , exclude = None , columns = None , coerce_float = False , nrows = None , ) -> \"DataFrame\" : \"\"\" Convert structured or record ndarray to DataFrame. Creates a DataFrame object from a structured ndarray, sequence of tuples or dicts, or DataFrame. Parameters ---------- data : structured ndarray, sequence of tuples or dicts, or DataFrame Structured input data. index : str, list of fields, array-like Field of array to use as the index, alternately a specific set of input labels to use. exclude : sequence, default None Columns or fields to exclude. columns : sequence, default None Column names to use. If the passed data do not have names associated with them, this argument provides names for the columns. Otherwise this argument indicates the order of the columns in the result (any names not found in the data will become all-NA columns). coerce_float : bool, default False Attempt to convert values of non-string, non-numeric objects (like decimal.Decimal) to floating point, useful for SQL result sets. nrows : int, default None Number of rows to read if data is an iterator. Returns ------- DataFrame See Also -------- DataFrame.from_dict : DataFrame from dict of array-like or dicts. DataFrame : DataFrame object creation using constructor. Examples -------- Data can be provided as a structured ndarray: >>> data = np.array([(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')], ... dtype=[('col_1', 'i4'), ('col_2', 'U1')]) >>> pd.DataFrame.from_records(data) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Data can be provided as a list of dicts: >>> data = [{'col_1': 3, 'col_2': 'a'}, ... {'col_1': 2, 'col_2': 'b'}, ... {'col_1': 1, 'col_2': 'c'}, ... {'col_1': 0, 'col_2': 'd'}] >>> pd.DataFrame.from_records(data) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d Data can be provided as a list of tuples with corresponding columns: >>> data = [(3, 'a'), (2, 'b'), (1, 'c'), (0, 'd')] >>> pd.DataFrame.from_records(data, columns=['col_1', 'col_2']) col_1 col_2 0 3 a 1 2 b 2 1 c 3 0 d \"\"\" # Make a copy of the input columns so we can modify it if columns is not None : columns = ensure_index ( columns ) if is_iterator ( data ) : if nrows == 0 : return cls () try : first_row = next ( data ) except StopIteration : return cls ( index = index , columns = columns ) dtype = None if hasattr ( first_row , \"dtype\" ) and first_row . dtype . names : dtype = first_row . dtype values = [ first_row ] if nrows is None : values += data else : values . extend ( itertools . islice ( data , nrows - 1 )) if dtype is not None : data = np . array ( values , dtype = dtype ) else : data = values if isinstance ( data , dict ) : if columns is None : columns = arr_columns = ensure_index ( sorted ( data )) arrays = [ data[k ] for k in columns ] else : arrays = [] arr_columns = [] for k , v in data . items () : if k in columns : arr_columns . append ( k ) arrays . append ( v ) arrays , arr_columns = reorder_arrays ( arrays , arr_columns , columns ) elif isinstance ( data , ( np . ndarray , DataFrame )) : arrays , columns = to_arrays ( data , columns ) if columns is not None : columns = ensure_index ( columns ) arr_columns = columns else : arrays , arr_columns = to_arrays ( data , columns , coerce_float = coerce_float ) arr_columns = ensure_index ( arr_columns ) if columns is not None : columns = ensure_index ( columns ) else : columns = arr_columns if exclude is None : exclude = set () else : exclude = set ( exclude ) result_index = None if index is not None : if isinstance ( index , str ) or not hasattr ( index , \"__iter__\" ) : i = columns . get_loc ( index ) exclude . add ( index ) if len ( arrays ) > 0 : result_index = Index ( arrays [ i ] , name = index ) else : result_index = Index ( [] , name = index ) else : try : index_data = [ arrays[arr_columns.get_loc(field) ] for field in index ] except ( KeyError , TypeError ) : # raised by get_loc , see GH#29258 result_index = index else : result_index = ensure_index_from_sequences ( index_data , names = index ) exclude . update ( index ) if any ( exclude ) : arr_exclude = [ x for x in exclude if x in arr_columns ] to_remove = [ arr_columns.get_loc(col) for col in arr_exclude ] arrays = [ v for i, v in enumerate(arrays) if i not in to_remove ] arr_columns = arr_columns . drop ( arr_exclude ) columns = columns . drop ( exclude ) mgr = arrays_to_mgr ( arrays , arr_columns , result_index , columns ) return cls ( mgr )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#instance-variables","text":"T at Access a single value for a row/column label pair. Similar to loc , in that both provide label-based lookups. Use at if you only need to get or set a single value in a DataFrame or Series.","title":"Instance variables"},{"location":"reference/hielen2/datalink_prova_df/#raises","text":"KeyError If 'label' does not exist in DataFrame.","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_3","text":"DataFrame.iat : Access a single value for a row/column pair by integer position. DataFrame.loc : Access a group of rows and columns by label(s). Series.at : Access a single value using a label.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_3","text":"df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]], ... index=[4, 5, 6], columns=['A', 'B', 'C']) df A B C 4 0 2 3 5 0 4 1 6 10 20 30 Get value at specified row/column pair df.at[4, 'B'] 2 Set value at specified row/column pair df.at[4, 'B'] = 10 df.at[4, 'B'] 10 Get value within a Series df.loc[5].at['B'] 4 attrs Dictionary of global attributes on this object. .. warning:: attrs is experimental and may change without warning. axes Return a list representing the axes of the DataFrame. It has the row axis labels and column axis labels as the only members. They are returned in that order.","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#examples_4","text":"df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) df.axes [RangeIndex(start=0, stop=2, step=1), Index(['col1', 'col2'], dtype='object')] dtypes Return the dtypes in the DataFrame. This returns a Series with the data type of each column. The result's index is the original DataFrame's columns. Columns with mixed types are stored with the object dtype. See :ref: the User Guide <basics.dtypes> for more.","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#returns_2","text":"pandas.Series The data type of each column.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#examples_5","text":"df = pd.DataFrame({'float': [1.0], ... 'int': [1], ... 'datetime': [pd.Timestamp('20180310')], ... 'string': ['foo']}) df.dtypes float float64 int int64 datetime datetime64[ns] string object dtype: object empty Indicator whether DataFrame is empty. True if DataFrame is entirely empty (no items), meaning any of the axes are of length 0.","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#returns_3","text":"bool If DataFrame is empty, return True, if not return False.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_4","text":"Series.dropna : Return series without null values. DataFrame.dropna : Return DataFrame with labels on given axis omitted where (all or any) data are missing.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes","text":"If DataFrame contains only NaNs, it is still not considered empty. See the example below.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_6","text":"An example of an actual empty DataFrame. Notice the index is empty: df_empty = pd.DataFrame({'A' : []}) df_empty Empty DataFrame Columns: [A] Index: [] df_empty.empty True If we only have NaNs in our DataFrame, it is not considered empty! We will need to drop the NaNs to make the DataFrame empty: df = pd.DataFrame({'A' : [np.nan]}) df A 0 NaN df.empty False df.dropna().empty True iat Access a single value for a row/column pair by integer position. Similar to iloc , in that both provide integer-based lookups. Use iat if you only need to get or set a single value in a DataFrame or Series.","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#raises_1","text":"IndexError When integer position is out of bounds.","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_5","text":"DataFrame.at : Access a single value for a row/column label pair. DataFrame.loc : Access a group of rows and columns by label(s). DataFrame.iloc : Access a group of rows and columns by integer position(s).","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_7","text":"df = pd.DataFrame([[0, 2, 3], [0, 4, 1], [10, 20, 30]], ... columns=['A', 'B', 'C']) df A B C 0 0 2 3 1 0 4 1 2 10 20 30 Get value at specified row/column pair df.iat[1, 2] 1 Set value at specified row/column pair df.iat[1, 2] = 10 df.iat[1, 2] 10 Get value within a series df.loc[0].iat[1] 2 iloc Purely integer-location based indexing for selection by position. .iloc[] is primarily integer position based (from 0 to length-1 of the axis), but may also be used with a boolean array. Allowed inputs are: An integer, e.g. 5 . A list or array of integers, e.g. [4, 3, 0] . A slice object with ints, e.g. 1:7 . A boolean array. A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above). This is useful in method chains, when you don't have a reference to the calling object, but would like to base your selection on some value. .iloc will raise IndexError if a requested indexer is out-of-bounds, except slice indexers which allow out-of-bounds indexing (this conforms with python/numpy slice semantics). See more at :ref: Selection by Position <indexing.integer> .","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#see-also_6","text":"DataFrame.iat : Fast integer location scalar accessor. DataFrame.loc : Purely label-location based indexer for selection by label. Series.iloc : Purely integer-location based indexing for selection by position.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_8","text":"mydict = [{'a': 1, 'b': 2, 'c': 3, 'd': 4}, ... {'a': 100, 'b': 200, 'c': 300, 'd': 400}, ... {'a': 1000, 'b': 2000, 'c': 3000, 'd': 4000 }] df = pd.DataFrame(mydict) df a b c d 0 1 2 3 4 1 100 200 300 400 2 1000 2000 3000 4000 Indexing just the rows With a scalar integer. type(df.iloc[0]) df.iloc[0] a 1 b 2 c 3 d 4 Name: 0, dtype: int64 With a list of integers. df.iloc[[0]] a b c d 0 1 2 3 4 type(df.iloc[[0]]) df.iloc[[0, 1]] a b c d 0 1 2 3 4 1 100 200 300 400 With a slice object. df.iloc[:3] a b c d 0 1 2 3 4 1 100 200 300 400 2 1000 2000 3000 4000 With a boolean mask the same length as the index. df.iloc[[True, False, True]] a b c d 0 1 2 3 4 2 1000 2000 3000 4000 With a callable, useful in method chains. The x passed to the lambda is the DataFrame being sliced. This selects the rows whose index label even. df.iloc[lambda x: x.index % 2 == 0] a b c d 0 1 2 3 4 2 1000 2000 3000 4000 Indexing both axes You can mix the indexer types for the index and columns. Use : to select the entire axis. With scalar integers. df.iloc[0, 1] 2 With lists of integers. df.iloc[[0, 2], [1, 3]] b d 0 2 4 2 2000 4000 With slice objects. df.iloc[1:3, 0:3] a b c 1 100 200 300 2 1000 2000 3000 With a boolean array whose length matches the columns. df.iloc[:, [True, False, True, False]] a c 0 1 3 1 100 300 2 1000 3000 With a callable function that expects the Series or DataFrame. df.iloc[:, lambda df: [0, 2]] a c 0 1 3 1 100 300 2 1000 3000 loc Access a group of rows and columns by label(s) or a boolean array. .loc[] is primarily label based, but may also be used with a boolean array. Allowed inputs are: A single label, e.g. 5 or 'a' , (note that 5 is interpreted as a label of the index, and never as an integer position along the index). A list or array of labels, e.g. ['a', 'b', 'c'] . A slice object with labels, e.g. 'a':'f' . .. warning:: Note that contrary to usual python slices, both the start and the stop are included A boolean array of the same length as the axis being sliced, e.g. [True, False, True] . A callable function with one argument (the calling Series or DataFrame) and that returns valid output for indexing (one of the above) See more at :ref: Selection by Label <indexing.label>","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#raises_2","text":"KeyError If any items are not found.","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_7","text":"DataFrame.at : Access a single value for a row/column label pair. DataFrame.iloc : Access group of rows and columns by integer position(s). DataFrame.xs : Returns a cross-section (row(s) or column(s)) from the Series/DataFrame. Series.loc : Access group of values using labels.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_9","text":"Getting values df = pd.DataFrame([[1, 2], [4, 5], [7, 8]], ... index=['cobra', 'viper', 'sidewinder'], ... columns=['max_speed', 'shield']) df max_speed shield cobra 1 2 viper 4 5 sidewinder 7 8 Single label. Note this returns the row as a Series. df.loc['viper'] max_speed 4 shield 5 Name: viper, dtype: int64 List of labels. Note using [[]] returns a DataFrame. df.loc[['viper', 'sidewinder']] max_speed shield viper 4 5 sidewinder 7 8 Single label for row and column df.loc['cobra', 'shield'] 2 Slice with labels for row and single label for column. As mentioned above, note that both the start and stop of the slice are included. df.loc['cobra':'viper', 'max_speed'] cobra 1 viper 4 Name: max_speed, dtype: int64 Boolean list with the same length as the row axis df.loc[[False, False, True]] max_speed shield sidewinder 7 8 Conditional that returns a boolean Series df.loc[df['shield'] > 6] max_speed shield sidewinder 7 8 Conditional that returns a boolean Series with column labels specified df.loc[df['shield'] > 6, ['max_speed']] max_speed sidewinder 7 Callable that returns a boolean Series df.loc[lambda df: df['shield'] == 8] max_speed shield sidewinder 7 8 Setting values Set value for all items matching the list of labels df.loc[['viper', 'sidewinder'], ['shield']] = 50 df max_speed shield cobra 1 2 viper 4 50 sidewinder 7 50 Set value for an entire row df.loc['cobra'] = 10 df max_speed shield cobra 10 10 viper 4 50 sidewinder 7 50 Set value for an entire column df.loc[:, 'max_speed'] = 30 df max_speed shield cobra 30 10 viper 30 50 sidewinder 30 50 Set value for rows matching callable condition df.loc[df['shield'] > 35] = 0 df max_speed shield cobra 30 10 viper 0 0 sidewinder 0 0 Getting values on a DataFrame with an index that has integer labels Another example using integers for the index df = pd.DataFrame([[1, 2], [4, 5], [7, 8]], ... index=[7, 8, 9], columns=['max_speed', 'shield']) df max_speed shield 7 1 2 8 4 5 9 7 8 Slice with integer labels for rows. As mentioned above, note that both the start and stop of the slice are included. df.loc[7:9] max_speed shield 7 1 2 8 4 5 9 7 8 Getting values with a MultiIndex A number of examples using a DataFrame with a MultiIndex tuples = [ ... ('cobra', 'mark i'), ('cobra', 'mark ii'), ... ('sidewinder', 'mark i'), ('sidewinder', 'mark ii'), ... ('viper', 'mark ii'), ('viper', 'mark iii') ... ] index = pd.MultiIndex.from_tuples(tuples) values = [[12, 2], [0, 4], [10, 20], ... [1, 4], [7, 1], [16, 36]] df = pd.DataFrame(values, columns=['max_speed', 'shield'], index=index) df max_speed shield cobra mark i 12 2 mark ii 0 4 sidewinder mark i 10 20 mark ii 1 4 viper mark ii 7 1 mark iii 16 36 Single label. Note this returns a DataFrame with a single index. df.loc['cobra'] max_speed shield mark i 12 2 mark ii 0 4 Single index tuple. Note this returns a Series. df.loc[('cobra', 'mark ii')] max_speed 0 shield 4 Name: (cobra, mark ii), dtype: int64 Single label for row and column. Similar to passing in a tuple, this returns a Series. df.loc['cobra', 'mark i'] max_speed 12 shield 2 Name: (cobra, mark i), dtype: int64 Single tuple. Note using [[]] returns a DataFrame. df.loc[[('cobra', 'mark ii')]] max_speed shield cobra mark ii 0 4 Single tuple for the index with a single label for the column df.loc[('cobra', 'mark i'), 'shield'] 2 Slice from index tuple to single label df.loc[('cobra', 'mark i'):'viper'] max_speed shield cobra mark i 12 2 mark ii 0 4 sidewinder mark i 10 20 mark ii 1 4 viper mark ii 7 1 mark iii 16 36 Slice from index tuple to index tuple df.loc[('cobra', 'mark i'):('viper', 'mark ii')] max_speed shield cobra mark i 12 2 mark ii 0 4 sidewinder mark i 10 20 mark ii 1 4 viper mark ii 7 1 ndim Return an int representing the number of axes / array dimensions. Return 1 if Series. Otherwise return 2 if DataFrame.","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#see-also_8","text":"ndarray.ndim : Number of array dimensions.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_10","text":"s = pd.Series({'a': 1, 'b': 2, 'c': 3}) s.ndim 1 df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) df.ndim 2 shape Return a tuple representing the dimensionality of the DataFrame.","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#see-also_9","text":"ndarray.shape","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_11","text":"df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) df.shape (2, 2) df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4], ... 'col3': [5, 6]}) df.shape (2, 3) size Return an int representing the number of elements in this object. Return the number of rows if Series. Otherwise return the number of rows times number of columns if DataFrame.","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#see-also_10","text":"ndarray.size : Number of elements in the array.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_12","text":"s = pd.Series({'a': 1, 'b': 2, 'c': 3}) s.size 3 df = pd.DataFrame({'col1': [1, 2], 'col2': [3, 4]}) df.size 4 style Returns a Styler object. Contains methods for building a styled HTML representation of the DataFrame.","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#see-also_11","text":"io.formats.style.Styler : Helps style a DataFrame or Series according to the data with HTML and CSS. values Return a Numpy representation of the DataFrame. .. warning:: We recommend using :meth: DataFrame.to_numpy instead. Only the values in the DataFrame will be returned, the axes labels will be removed.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#returns_4","text":"numpy.ndarray The values of the DataFrame.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_12","text":"DataFrame.to_numpy : Recommended alternative to this method. DataFrame.index : Retrieve the index labels. DataFrame.columns : Retrieving the column names.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_1","text":"The dtype will be a lower-common-denominator dtype (implicit upcasting); that is to say if the dtypes (even of numeric types) are mixed, the one that accommodates all will be chosen. Use this with care if you are not dealing with the blocks. e.g. If the dtypes are float16 and float32, dtype will be upcast to float32. If dtypes are int32 and uint8, dtype will be upcast to int32. By :func: numpy.find_common_type convention, mixing int64 and uint64 will result in a float64 dtype.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_13","text":"A DataFrame where all columns are the same type (e.g., int64) results in an array of the same type. df = pd.DataFrame({'age': [ 3, 29], ... 'height': [94, 170], ... 'weight': [31, 115]}) df age height weight 0 3 94 31 1 29 170 115 df.dtypes age int64 height int64 weight int64 dtype: object df.values array([[ 3, 94, 31], [ 29, 170, 115]]) A DataFrame with mixed type columns(e.g., str/object, int64, float32) results in an ndarray of the broadest type that accommodates these mixed types (e.g., object). df2 = pd.DataFrame([('parrot', 24.0, 'second'), ... ('lion', 80.5, 1), ... ('monkey', np.nan, None)], ... columns=('name', 'max_speed', 'rank')) df2.dtypes name object max_speed float64 rank object dtype: object df2.values array([['parrot', 24.0, 'second'], ['lion', 80.5, 1], ['monkey', nan, None]], dtype=object)","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/datalink_prova_df/#abs","text":"def abs ( self : ~ FrameOrSeries ) -> ~ FrameOrSeries Return a Series/DataFrame with absolute numeric value of each element. This function only applies to elements that are all numeric.","title":"abs"},{"location":"reference/hielen2/datalink_prova_df/#returns_5","text":"abs Series/DataFrame containing the absolute value of each element.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_13","text":"numpy.absolute : Calculate the absolute value element-wise.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_2","text":"For complex inputs, 1.2 + 1j , the absolute value is :math: \\sqrt{ a^2 + b^2 } .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_14","text":"Absolute numeric values in a Series. s = pd.Series([-1.10, 2, -3.33, 4]) s.abs() 0 1.10 1 2.00 2 3.33 3 4.00 dtype: float64 Absolute numeric values in a Series with complex numbers. s = pd.Series([1.2 + 1j]) s.abs() 0 1.56205 dtype: float64 Absolute numeric values in a Series with a Timedelta element. s = pd.Series([pd.Timedelta('1 days')]) s.abs() 0 1 days dtype: timedelta64[ns] Select rows with data closest to certain value using argsort (from StackOverflow <https://stackoverflow.com/a/17758115> __). df = pd.DataFrame({ ... 'a': [4, 5, 6, 7], ... 'b': [10, 20, 30, 40], ... 'c': [100, 50, -30, -50] ... }) df a b c 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 df.loc[(df.c - 43).abs().argsort()] a b c 1 5 20 50 0 4 10 100 2 6 30 -30 3 7 40 -50 View Source def abs ( self : FrameOrSeries ) -> FrameOrSeries : \"\"\" Return a Series/DataFrame with absolute numeric value of each element. This function only applies to elements that are all numeric. Returns ------- abs Series/DataFrame containing the absolute value of each element. See Also -------- numpy.absolute : Calculate the absolute value element-wise. Notes ----- For ``complex`` inputs, ``1.2 + 1j``, the absolute value is :math:`\\\\sqrt{ a^2 + b^2 }`. Examples -------- Absolute numeric values in a Series. >>> s = pd.Series([-1.10, 2, -3.33, 4]) >>> s.abs() 0 1.10 1 2.00 2 3.33 3 4.00 dtype: float64 Absolute numeric values in a Series with complex numbers. >>> s = pd.Series([1.2 + 1j]) >>> s.abs() 0 1.56205 dtype: float64 Absolute numeric values in a Series with a Timedelta element. >>> s = pd.Series([pd.Timedelta('1 days')]) >>> s.abs() 0 1 days dtype: timedelta64[ns] Select rows with data closest to certain value using argsort (from `StackOverflow <https://stackoverflow.com/a/17758115>`__). >>> df = pd.DataFrame({ ... 'a': [4, 5, 6, 7], ... 'b': [10, 20, 30, 40], ... 'c': [100, 50, -30, -50] ... }) >>> df a b c 0 4 10 100 1 5 20 50 2 6 30 -30 3 7 40 -50 >>> df.loc[(df.c - 43).abs().argsort()] a b c 1 5 20 50 0 4 10 100 2 6 30 -30 3 7 40 -50 \"\"\" return np . abs ( self )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#add","text":"def add ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Addition of dataframe and other, element-wise (binary operator add ). Equivalent to dataframe + other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, radd . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"add"},{"location":"reference/hielen2/datalink_prova_df/#parameters_3","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_6","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_14","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_3","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_15","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#add_prefix","text":"def add_prefix ( self : ~ FrameOrSeries , prefix : str ) -> ~ FrameOrSeries Prefix labels with string prefix . For Series, the row labels are prefixed. For DataFrame, the column labels are prefixed.","title":"add_prefix"},{"location":"reference/hielen2/datalink_prova_df/#parameters_4","text":"prefix : str The string to add before each label.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_7","text":"Series or DataFrame New Series or DataFrame with updated labels.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_15","text":"Series.add_suffix: Suffix row labels with string suffix . DataFrame.add_suffix: Suffix column labels with string suffix .","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_16","text":"s = pd.Series([1, 2, 3, 4]) s 0 1 1 2 2 3 3 4 dtype: int64 s.add_prefix('item_') item_0 1 item_1 2 item_2 3 item_3 4 dtype: int64 df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]}) df A B 0 1 3 1 2 4 2 3 5 3 4 6 df.add_prefix('col_') col_A col_B 0 1 3 1 2 4 2 3 5 3 4 6 View Source def add_prefix ( self : FrameOrSeries , prefix : str ) -> FrameOrSeries : \"\"\" Prefix labels with string `prefix`. For Series, the row labels are prefixed. For DataFrame, the column labels are prefixed. Parameters ---------- prefix : str The string to add before each label. Returns ------- Series or DataFrame New Series or DataFrame with updated labels. See Also -------- Series.add_suffix: Suffix row labels with string `suffix`. DataFrame.add_suffix: Suffix column labels with string `suffix`. Examples -------- >>> s = pd.Series([1, 2, 3, 4]) >>> s 0 1 1 2 2 3 3 4 dtype: int64 >>> s.add_prefix('item_') item_0 1 item_1 2 item_2 3 item_3 4 dtype: int64 >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]}) >>> df A B 0 1 3 1 2 4 2 3 5 3 4 6 >>> df.add_prefix('col_') col_A col_B 0 1 3 1 2 4 2 3 5 3 4 6 \"\"\" f = functools . partial ( \"{prefix}{}\" . format , prefix = prefix ) mapper = { self . _info_axis_name : f } return self . rename ( ** mapper ) # type: ignore","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#add_suffix","text":"def add_suffix ( self : ~ FrameOrSeries , suffix : str ) -> ~ FrameOrSeries Suffix labels with string suffix . For Series, the row labels are suffixed. For DataFrame, the column labels are suffixed.","title":"add_suffix"},{"location":"reference/hielen2/datalink_prova_df/#parameters_5","text":"suffix : str The string to add after each label.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_8","text":"Series or DataFrame New Series or DataFrame with updated labels.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_16","text":"Series.add_prefix: Prefix row labels with string prefix . DataFrame.add_prefix: Prefix column labels with string prefix .","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_17","text":"s = pd.Series([1, 2, 3, 4]) s 0 1 1 2 2 3 3 4 dtype: int64 s.add_suffix('_item') 0_item 1 1_item 2 2_item 3 3_item 4 dtype: int64 df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]}) df A B 0 1 3 1 2 4 2 3 5 3 4 6 df.add_suffix('_col') A_col B_col 0 1 3 1 2 4 2 3 5 3 4 6 View Source def add_suffix ( self : FrameOrSeries , suffix : str ) -> FrameOrSeries : \"\"\" Suffix labels with string `suffix`. For Series, the row labels are suffixed. For DataFrame, the column labels are suffixed. Parameters ---------- suffix : str The string to add after each label. Returns ------- Series or DataFrame New Series or DataFrame with updated labels. See Also -------- Series.add_prefix: Prefix row labels with string `prefix`. DataFrame.add_prefix: Prefix column labels with string `prefix`. Examples -------- >>> s = pd.Series([1, 2, 3, 4]) >>> s 0 1 1 2 2 3 3 4 dtype: int64 >>> s.add_suffix('_item') 0_item 1 1_item 2 2_item 3 3_item 4 dtype: int64 >>> df = pd.DataFrame({'A': [1, 2, 3, 4], 'B': [3, 4, 5, 6]}) >>> df A B 0 1 3 1 2 4 2 3 5 3 4 6 >>> df.add_suffix('_col') A_col B_col 0 1 3 1 2 4 2 3 5 3 4 6 \"\"\" f = functools . partial ( \"{}{suffix}\" . format , suffix = suffix ) mapper = { self . _info_axis_name : f } return self . rename ( ** mapper ) # type: ignore","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#agg","text":"def agg ( self , func = None , axis = 0 , * args , ** kwargs ) Aggregate using one or more operations over the specified axis. .. versionadded:: 0.20.0","title":"agg"},{"location":"reference/hielen2/datalink_prova_df/#parameters_6","text":"func : function, str, list or dict Function to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are : - function - string function name - list of functions and / or function names , e . g . `` [ np . sum , 'mean' ] `` - dict of axis labels -> functions , function names or list of such . axis : {0 or 'index', 1 or 'columns'}, default 0 If 0 or 'index': apply function to each column. If 1 or 'columns': apply function to each row. args Positional arguments to pass to func . *kwargs Keyword arguments to pass to func .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_9","text":"scalar, Series or DataFrame The return can be : * scalar : when Series . agg is called with single function * Series : when DataFrame . agg is called with a single function * DataFrame : when DataFrame . agg is called with several functions Return scalar , Series or DataFrame . The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions ( mean , median , prod , sum , std , var ), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0) . agg is an alias for aggregate . Use the alias.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_17","text":"DataFrame.apply : Perform any type of operations. DataFrame.transform : Perform transformation type operations. core.groupby.GroupBy : Perform operations over groups. core.resample.Resampler : Perform operations over resampled bins. core.window.Rolling : Perform operations over rolling window. core.window.Expanding : Perform operations over expanding window. core.window.ExponentialMovingWindow : Perform operation over exponential weighted window.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_4","text":"agg is an alias for aggregate . Use the alias. A passed user-defined-function will be passed a Series for evaluation.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_18","text":"df = pd.DataFrame([[1, 2, 3], ... [4, 5, 6], ... [7, 8, 9], ... [np.nan, np.nan, np.nan]], ... columns=['A', 'B', 'C']) Aggregate these functions over the rows. df.agg(['sum', 'min']) A B C sum 12.0 15.0 18.0 min 1.0 2.0 3.0 Different aggregations per column. df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']}) A B max NaN 8.0 min 1.0 2.0 sum 12.0 NaN Aggregate over the columns. df.agg(\"mean\", axis=\"columns\") 0 2.0 1 5.0 2 8.0 3 NaN dtype: float64 View Source @doc ( _shared_docs [ \"aggregate\" ] , klass = _shared_doc_kwargs [ \"klass\" ] , axis = _shared_doc_kwargs [ \"axis\" ] , see_also = _agg_summary_and_see_also_doc , examples = _agg_examples_doc , versionadded = \"\\n.. versionadded:: 0.20.0\\n\" , ) def aggregate ( self , func = None , axis = 0 , * args , ** kwargs ) : axis = self . _get_axis_number ( axis ) relabeling , func , columns , order = reconstruct_func ( func , ** kwargs ) result = None try : result , how = self . _aggregate ( func , axis = axis , * args , ** kwargs ) except TypeError as err : exc = TypeError ( \"DataFrame constructor called with \" f \"incompatible data and dtype: {err}\" ) raise exc from err if result is None : return self . apply ( func , axis = axis , args = args , ** kwargs ) if relabeling : # This is to keep the order to columns occurrence unchanged , and also # keep the order of new columns occurrence unchanged result_in_dict = relabel_result ( result , func , columns , order ) result = DataFrame ( result_in_dict , index = columns ) return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#aggregate","text":"def aggregate ( self , func = None , axis = 0 , * args , ** kwargs ) Aggregate using one or more operations over the specified axis. .. versionadded:: 0.20.0","title":"aggregate"},{"location":"reference/hielen2/datalink_prova_df/#parameters_7","text":"func : function, str, list or dict Function to use for aggregating the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are : - function - string function name - list of functions and / or function names , e . g . `` [ np . sum , 'mean' ] `` - dict of axis labels -> functions , function names or list of such . axis : {0 or 'index', 1 or 'columns'}, default 0 If 0 or 'index': apply function to each column. If 1 or 'columns': apply function to each row. args Positional arguments to pass to func . *kwargs Keyword arguments to pass to func .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_10","text":"scalar, Series or DataFrame The return can be : * scalar : when Series . agg is called with single function * Series : when DataFrame . agg is called with a single function * DataFrame : when DataFrame . agg is called with several functions Return scalar , Series or DataFrame . The aggregation operations are always performed over an axis, either the index (default) or the column axis. This behavior is different from numpy aggregation functions ( mean , median , prod , sum , std , var ), where the default is to compute the aggregation of the flattened array, e.g., numpy.mean(arr_2d) as opposed to numpy.mean(arr_2d, axis=0) . agg is an alias for aggregate . Use the alias.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_18","text":"DataFrame.apply : Perform any type of operations. DataFrame.transform : Perform transformation type operations. core.groupby.GroupBy : Perform operations over groups. core.resample.Resampler : Perform operations over resampled bins. core.window.Rolling : Perform operations over rolling window. core.window.Expanding : Perform operations over expanding window. core.window.ExponentialMovingWindow : Perform operation over exponential weighted window.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_5","text":"agg is an alias for aggregate . Use the alias. A passed user-defined-function will be passed a Series for evaluation.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_19","text":"df = pd.DataFrame([[1, 2, 3], ... [4, 5, 6], ... [7, 8, 9], ... [np.nan, np.nan, np.nan]], ... columns=['A', 'B', 'C']) Aggregate these functions over the rows. df.agg(['sum', 'min']) A B C sum 12.0 15.0 18.0 min 1.0 2.0 3.0 Different aggregations per column. df.agg({'A' : ['sum', 'min'], 'B' : ['min', 'max']}) A B max NaN 8.0 min 1.0 2.0 sum 12.0 NaN Aggregate over the columns. df.agg(\"mean\", axis=\"columns\") 0 2.0 1 5.0 2 8.0 3 NaN dtype: float64 View Source @doc ( _shared_docs [ \"aggregate\" ] , klass = _shared_doc_kwargs [ \"klass\" ] , axis = _shared_doc_kwargs [ \"axis\" ] , see_also = _agg_summary_and_see_also_doc , examples = _agg_examples_doc , versionadded = \"\\n.. versionadded:: 0.20.0\\n\" , ) def aggregate ( self , func = None , axis = 0 , * args , ** kwargs ) : axis = self . _get_axis_number ( axis ) relabeling , func , columns , order = reconstruct_func ( func , ** kwargs ) result = None try : result , how = self . _aggregate ( func , axis = axis , * args , ** kwargs ) except TypeError as err : exc = TypeError ( \"DataFrame constructor called with \" f \"incompatible data and dtype: {err}\" ) raise exc from err if result is None : return self . apply ( func , axis = axis , args = args , ** kwargs ) if relabeling : # This is to keep the order to columns occurrence unchanged , and also # keep the order of new columns occurrence unchanged result_in_dict = relabel_result ( result , func , columns , order ) result = DataFrame ( result_in_dict , index = columns ) return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#align","text":"def align ( self , other , join = 'outer' , axis = None , level = None , copy = True , fill_value = None , method = None , limit = None , fill_axis = 0 , broadcast_axis = None ) -> 'DataFrame' Align two objects on their axes with the specified join method. Join method is specified for each axis Index.","title":"align"},{"location":"reference/hielen2/datalink_prova_df/#parameters_8","text":"other : DataFrame or Series join : {'outer', 'inner', 'left', 'right'}, default 'outer' axis : allowed axis of the other object, default None Align on index (0), columns (1), or both (None). level : int or level name, default None Broadcast across a level, matching Index values on the passed MultiIndex level. copy : bool, default True Always returns new objects. If copy=False and no reindexing is required then original objects are returned. fill_value : scalar, default np.NaN Value to use for missing values. Defaults to NaN, but can be any \"compatible\" value. method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None Method to use for filling holes in reindexed Series: - pad / ffill: propagate last valid observation forward to next valid. - backfill / bfill: use NEXT valid observation to fill gap. limit : int, default None If method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. fill_axis : {0 or 'index', 1 or 'columns'}, default 0 Filling axis, method and limit. broadcast_axis : {0 or 'index', 1 or 'columns'}, default None Broadcast values along this axis, if aligning two objects of different dimensions.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_11","text":"(left, right) : (DataFrame, type of other) Aligned objects. View Source @doc ( NDFrame . align , ** _shared_doc_kwargs ) def align ( self , other , join = \"outer\" , axis = None , level = None , copy = True , fill_value = None , method = None , limit = None , fill_axis = 0 , broadcast_axis = None , ) -> \"DataFrame\" : return super (). align ( other , join = join , axis = axis , level = level , copy = copy , fill_value = fill_value , method = method , limit = limit , fill_axis = fill_axis , broadcast_axis = broadcast_axis , )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#all","text":"def all ( self , axis = 0 , bool_only = None , skipna = True , level = None , ** kwargs ) Return whether all elements are True, potentially over an axis. Returns True unless there at least one element within a series or along a Dataframe axis that is False or equivalent (e.g. zero or empty).","title":"all"},{"location":"reference/hielen2/datalink_prova_df/#parameters_9","text":"axis : {0 or 'index', 1 or 'columns', None}, default 0 Indicate which axis or axes should be reduced. * 0 / 'index' : reduce the index, return a Series whose index is the original column labels. * 1 / 'columns' : reduce the columns, return a Series whose index is the original index. * None : reduce all axes, return a scalar. bool_only : bool, default None Include only boolean columns. If None, will attempt to use everything, then use only boolean data. Not implemented for Series. skipna : bool, default True Exclude NA/null values. If the entire row/column is NA and skipna is True, then the result will be True, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. **kwargs : any, default None Additional keywords have no effect but might be accepted for compatibility with NumPy.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_12","text":"Series or DataFrame If level is specified, then, DataFrame is returned; otherwise, Series is returned.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_19","text":"Series.all : Return True if all elements are True. DataFrame.any : Return True if one (or more) elements are True.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_20","text":"Series pd.Series([True, True]).all() True pd.Series([True, False]).all() False pd.Series([]).all() True pd.Series([np.nan]).all() True pd.Series([np.nan]).all(skipna=False) True DataFrames Create a dataframe from a dictionary. df = pd.DataFrame({'col1': [True, True], 'col2': [True, False]}) df col1 col2 0 True True 1 True False Default behaviour checks if column-wise values all return True. df.all() col1 True col2 False dtype: bool Specify axis='columns' to check if row-wise values all return True. df.all(axis='columns') 0 True 1 False dtype: bool Or axis=None for whether every value is True. df.all(axis=None) False View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , see_also = see_also , examples = examples , empty_value = empty_value , ) @Appender ( _bool_doc ) def logical_func ( self , axis = 0 , bool_only = None , skipna = True , level = None , ** kwargs ) : nv . validate_logical_func ( tuple (), kwargs , fname = name ) if level is not None : if bool_only is not None : raise NotImplementedError ( \"Option bool_only is not implemented with option level.\" ) return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = bool_only , filter_type = \"bool\" , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#any","text":"def any ( self , axis = 0 , bool_only = None , skipna = True , level = None , ** kwargs ) Return whether any element is True, potentially over an axis. Returns False unless there at least one element within a series or along a Dataframe axis that is True or equivalent (e.g. non-zero or non-empty).","title":"any"},{"location":"reference/hielen2/datalink_prova_df/#parameters_10","text":"axis : {0 or 'index', 1 or 'columns', None}, default 0 Indicate which axis or axes should be reduced. * 0 / 'index' : reduce the index, return a Series whose index is the original column labels. * 1 / 'columns' : reduce the columns, return a Series whose index is the original index. * None : reduce all axes, return a scalar. bool_only : bool, default None Include only boolean columns. If None, will attempt to use everything, then use only boolean data. Not implemented for Series. skipna : bool, default True Exclude NA/null values. If the entire row/column is NA and skipna is True, then the result will be False, as for an empty row/column. If skipna is False, then NA are treated as True, because these are not equal to zero. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. **kwargs : any, default None Additional keywords have no effect but might be accepted for compatibility with NumPy.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_13","text":"Series or DataFrame If level is specified, then, DataFrame is returned; otherwise, Series is returned.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_20","text":"numpy.any : Numpy version of this method. Series.any : Return whether any element is True. Series.all : Return whether all elements are True. DataFrame.any : Return whether any element is True over requested axis. DataFrame.all : Return whether all elements are True over requested axis.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_21","text":"Series For Series input, the output is a scalar indicating whether any element is True. pd.Series([False, False]).any() False pd.Series([True, False]).any() True pd.Series([]).any() False pd.Series([np.nan]).any() False pd.Series([np.nan]).any(skipna=False) True DataFrame Whether each column contains at least one True element (the default). df = pd.DataFrame({\"A\": [1, 2], \"B\": [0, 2], \"C\": [0, 0]}) df A B C 0 1 0 0 1 2 2 0 df.any() A True B True C False dtype: bool Aggregating over the columns. df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 2]}) df A B 0 True 1 1 False 2 df.any(axis='columns') 0 True 1 True dtype: bool df = pd.DataFrame({\"A\": [True, False], \"B\": [1, 0]}) df A B 0 True 1 1 False 0 df.any(axis='columns') 0 True 1 False dtype: bool Aggregating over the entire DataFrame with axis=None . df.any(axis=None) True any for an empty DataFrame is an empty Series. pd.DataFrame([]).any() Series([], dtype: bool) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , see_also = see_also , examples = examples , empty_value = empty_value , ) @Appender ( _bool_doc ) def logical_func ( self , axis = 0 , bool_only = None , skipna = True , level = None , ** kwargs ) : nv . validate_logical_func ( tuple (), kwargs , fname = name ) if level is not None : if bool_only is not None : raise NotImplementedError ( \"Option bool_only is not implemented with option level.\" ) return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = bool_only , filter_type = \"bool\" , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#append","text":"def append ( self , other , ignore_index = False , verify_integrity = False , sort = False ) -> 'DataFrame' Append rows of other to the end of caller, returning a new object. Columns in other that are not in the caller are added as new columns.","title":"append"},{"location":"reference/hielen2/datalink_prova_df/#parameters_11","text":"other : DataFrame or Series/dict-like object, or list of these The data to append. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. verify_integrity : bool, default False If True, raise ValueError on creating index with duplicates. sort : bool, default False Sort columns if the columns of self and other are not aligned. .. versionadded :: 0.23.0 .. versionchanged :: 1.0.0 Changed to not sort by default.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_14","text":"DataFrame","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_21","text":"concat : General function to concatenate DataFrame or Series objects.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_6","text":"If a list of dict/series is passed and the keys are all contained in the DataFrame's index, the order of the columns in the resulting DataFrame will be unchanged. Iteratively appending rows to a DataFrame can be more computationally intensive than a single concatenate. A better solution is to append those rows to a list and then concatenate the list with the original DataFrame all at once.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_22","text":"df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB')) df A B 0 1 2 1 3 4 df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB')) df.append(df2) A B 0 1 2 1 3 4 0 5 6 1 7 8 With ignore_index set to True: df.append(df2, ignore_index=True) A B 0 1 2 1 3 4 2 5 6 3 7 8 The following, while not recommended methods for generating DataFrames, show two ways to generate a DataFrame from multiple data sources. Less efficient: df = pd.DataFrame(columns=['A']) for i in range(5): ... df = df.append({'A': i}, ignore_index=True) df A 0 0 1 1 2 2 3 3 4 4 More efficient: pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)], ... ignore_index=True) A 0 0 1 1 2 2 3 3 4 4 View Source def append ( self , other , ignore_index = False , verify_integrity = False , sort = False ) -> \"DataFrame\" : \"\"\" Append rows of `other` to the end of caller, returning a new object. Columns in `other` that are not in the caller are added as new columns. Parameters ---------- other : DataFrame or Series/dict-like object, or list of these The data to append. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. verify_integrity : bool, default False If True, raise ValueError on creating index with duplicates. sort : bool, default False Sort columns if the columns of `self` and `other` are not aligned. .. versionadded:: 0.23.0 .. versionchanged:: 1.0.0 Changed to not sort by default. Returns ------- DataFrame See Also -------- concat : General function to concatenate DataFrame or Series objects. Notes ----- If a list of dict/series is passed and the keys are all contained in the DataFrame's index, the order of the columns in the resulting DataFrame will be unchanged. Iteratively appending rows to a DataFrame can be more computationally intensive than a single concatenate. A better solution is to append those rows to a list and then concatenate the list with the original DataFrame all at once. Examples -------- >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=list('AB')) >>> df A B 0 1 2 1 3 4 >>> df2 = pd.DataFrame([[5, 6], [7, 8]], columns=list('AB')) >>> df.append(df2) A B 0 1 2 1 3 4 0 5 6 1 7 8 With `ignore_index` set to True: >>> df.append(df2, ignore_index=True) A B 0 1 2 1 3 4 2 5 6 3 7 8 The following, while not recommended methods for generating DataFrames, show two ways to generate a DataFrame from multiple data sources. Less efficient: >>> df = pd.DataFrame(columns=['A']) >>> for i in range(5): ... df = df.append({'A': i}, ignore_index=True) >>> df A 0 0 1 1 2 2 3 3 4 4 More efficient: >>> pd.concat([pd.DataFrame([i], columns=['A']) for i in range(5)], ... ignore_index=True) A 0 0 1 1 2 2 3 3 4 4 \"\"\" if isinstance ( other , ( Series , dict )): if isinstance ( other , dict ): if not ignore_index : raise TypeError ( \"Can only append a dict if ignore_index=True\" ) other = Series ( other ) if other . name is None and not ignore_index : raise TypeError ( \"Can only append a Series if ignore_index=True \" \"or if the Series has a name\" ) index = Index ([ other . name ], name = self . index . name ) idx_diff = other . index . difference ( self . columns ) try : combined_columns = self . columns . append ( idx_diff ) except TypeError : combined_columns = self . columns . astype ( object ). append ( idx_diff ) other = ( other . reindex ( combined_columns , copy = False ) . to_frame () . T . infer_objects () . rename_axis ( index . names , copy = False ) ) if not self . columns . equals ( combined_columns ): self = self . reindex ( columns = combined_columns ) elif isinstance ( other , list ): if not other : pass elif not isinstance ( other [ 0 ], DataFrame ): other = DataFrame ( other ) if ( self . columns . get_indexer ( other . columns ) >= 0 ). all (): other = other . reindex ( columns = self . columns ) from pandas . core . reshape . concat import concat if isinstance ( other , ( list , tuple )): to_concat = [ self , * other ] else : to_concat = [ self , other ] return concat ( to_concat , ignore_index = ignore_index , verify_integrity = verify_integrity , sort = sort , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#apply","text":"def apply ( self , func , axis = 0 , raw = False , result_type = None , args = (), ** kwds ) Apply a function along an axis of the DataFrame. Objects passed to the function are Series objects whose index is either the DataFrame's index ( axis=0 ) or the DataFrame's columns ( axis=1 ). By default ( result_type=None ), the final return type is inferred from the return type of the applied function. Otherwise, it depends on the result_type argument.","title":"apply"},{"location":"reference/hielen2/datalink_prova_df/#parameters_12","text":"func : function Function to apply to each column or row. axis : {0 or 'index', 1 or 'columns'}, default 0 Axis along which the function is applied: * 0 or 'index': apply function to each column. * 1 or 'columns': apply function to each row. raw : bool, default False Determines if row or column is passed as a Series or ndarray object: * `` False `` : passes each row or column as a Series to the function . * `` True `` : the passed function will receive ndarray objects instead . If you are just applying a NumPy reduction function this will achieve much better performance . result_type : {'expand', 'reduce', 'broadcast', None}, default None These only act when axis=1 (columns): * 'expand' : list - like results will be turned into columns . * 'reduce' : returns a Series if possible rather than expanding list - like results . This is the opposite of 'expand' . * 'broadcast' : results will be broadcast to the original shape of the DataFrame , the original index and columns will be retained . The default behaviour ( None ) depends on the return value of the applied function: list - like results will be returned as a Series of those . However if the apply function returns a Series these are expanded to columns . .. versionadded:: 0.23.0 args : tuple Positional arguments to pass to func in addition to the array/series. **kwds Additional keyword arguments to pass as keywords arguments to func .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_15","text":"Series or DataFrame Result of applying func along the given axis of the DataFrame.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_22","text":"DataFrame.applymap: For elementwise operations. DataFrame.aggregate: Only perform aggregating type operations. DataFrame.transform: Only perform transforming type operations.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_23","text":"df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B']) df A B 0 4 9 1 4 9 2 4 9 Using a numpy universal function (in this case the same as np.sqrt(df) ): df.apply(np.sqrt) A B 0 2.0 3.0 1 2.0 3.0 2 2.0 3.0 Using a reducing function on either axis df.apply(np.sum, axis=0) A 12 B 27 dtype: int64 df.apply(np.sum, axis=1) 0 13 1 13 2 13 dtype: int64 Returning a list-like will result in a Series df.apply(lambda x: [1, 2], axis=1) 0 [1, 2] 1 [1, 2] 2 [1, 2] dtype: object Passing result_type='expand' will expand list-like results to columns of a Dataframe df.apply(lambda x: [1, 2], axis=1, result_type='expand') 0 1 0 1 2 1 1 2 2 1 2 Returning a Series inside the function is similar to passing result_type='expand' . The resulting column names will be the Series index. df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1) foo bar 0 1 2 1 1 2 2 1 2 Passing result_type='broadcast' will ensure the same shape result, whether list-like or scalar is returned by the function, and broadcast it along the axis. The resulting column names will be the originals. df.apply(lambda x: [1, 2], axis=1, result_type='broadcast') A B 0 1 2 1 1 2 2 1 2 View Source def apply ( self , func , axis = 0 , raw = False , result_type = None , args = (), ** kwds ): \"\"\" Apply a function along an axis of the DataFrame. Objects passed to the function are Series objects whose index is either the DataFrame's index (``axis=0``) or the DataFrame's columns (``axis=1``). By default (``result_type=None``), the final return type is inferred from the return type of the applied function. Otherwise, it depends on the `result_type` argument. Parameters ---------- func : function Function to apply to each column or row. axis : {0 or 'index', 1 or 'columns'}, default 0 Axis along which the function is applied: * 0 or 'index': apply function to each column. * 1 or 'columns': apply function to each row. raw : bool, default False Determines if row or column is passed as a Series or ndarray object: * ``False`` : passes each row or column as a Series to the function. * ``True`` : the passed function will receive ndarray objects instead. If you are just applying a NumPy reduction function this will achieve much better performance. result_type : {'expand', 'reduce', 'broadcast', None}, default None These only act when ``axis=1`` (columns): * 'expand' : list-like results will be turned into columns. * 'reduce' : returns a Series if possible rather than expanding list-like results. This is the opposite of 'expand'. * 'broadcast' : results will be broadcast to the original shape of the DataFrame, the original index and columns will be retained. The default behaviour (None) depends on the return value of the applied function: list-like results will be returned as a Series of those. However if the apply function returns a Series these are expanded to columns. .. versionadded:: 0.23.0 args : tuple Positional arguments to pass to `func` in addition to the array/series. **kwds Additional keyword arguments to pass as keywords arguments to `func`. Returns ------- Series or DataFrame Result of applying ``func`` along the given axis of the DataFrame. See Also -------- DataFrame.applymap: For elementwise operations. DataFrame.aggregate: Only perform aggregating type operations. DataFrame.transform: Only perform transforming type operations. Examples -------- >>> df = pd.DataFrame([[4, 9]] * 3, columns=['A', 'B']) >>> df A B 0 4 9 1 4 9 2 4 9 Using a numpy universal function (in this case the same as ``np.sqrt(df)``): >>> df.apply(np.sqrt) A B 0 2.0 3.0 1 2.0 3.0 2 2.0 3.0 Using a reducing function on either axis >>> df.apply(np.sum, axis=0) A 12 B 27 dtype: int64 >>> df.apply(np.sum, axis=1) 0 13 1 13 2 13 dtype: int64 Returning a list-like will result in a Series >>> df.apply(lambda x: [1, 2], axis=1) 0 [1, 2] 1 [1, 2] 2 [1, 2] dtype: object Passing ``result_type='expand'`` will expand list-like results to columns of a Dataframe >>> df.apply(lambda x: [1, 2], axis=1, result_type='expand') 0 1 0 1 2 1 1 2 2 1 2 Returning a Series inside the function is similar to passing ``result_type='expand'``. The resulting column names will be the Series index. >>> df.apply(lambda x: pd.Series([1, 2], index=['foo', 'bar']), axis=1) foo bar 0 1 2 1 1 2 2 1 2 Passing ``result_type='broadcast'`` will ensure the same shape result, whether list-like or scalar is returned by the function, and broadcast it along the axis. The resulting column names will be the originals. >>> df.apply(lambda x: [1, 2], axis=1, result_type='broadcast') A B 0 1 2 1 1 2 2 1 2 \"\"\" from pandas . core . apply import frame_apply op = frame_apply ( self , func = func , axis = axis , raw = raw , result_type = result_type , args = args , kwds = kwds , ) return op . get_result ()","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#applymap","text":"def applymap ( self , func ) -> 'DataFrame' Apply a function to a Dataframe elementwise. This method applies a function that accepts and returns a scalar to every element of a DataFrame.","title":"applymap"},{"location":"reference/hielen2/datalink_prova_df/#parameters_13","text":"func : callable Python function, returns a single value from a single value.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_16","text":"DataFrame Transformed DataFrame.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_23","text":"DataFrame.apply : Apply a function along input axis of DataFrame.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_24","text":"df = pd.DataFrame([[1, 2.12], [3.356, 4.567]]) df 0 1 0 1.000 2.120 1 3.356 4.567 df.applymap(lambda x: len(str(x))) 0 1 0 3 4 1 5 5 Note that a vectorized version of func often exists, which will be much faster. You could square each number elementwise. df.applymap(lambda x: x**2) 0 1 0 1.000000 4.494400 1 11.262736 20.857489 But it's better to avoid applymap in that case. df ** 2 0 1 0 1.000000 4.494400 1 11.262736 20.857489 View Source def applymap ( self , func ) -> \"DataFrame\" : \"\"\" Apply a function to a Dataframe elementwise. This method applies a function that accepts and returns a scalar to every element of a DataFrame. Parameters ---------- func : callable Python function, returns a single value from a single value. Returns ------- DataFrame Transformed DataFrame. See Also -------- DataFrame.apply : Apply a function along input axis of DataFrame. Examples -------- >>> df = pd.DataFrame([[1, 2.12], [3.356, 4.567]]) >>> df 0 1 0 1.000 2.120 1 3.356 4.567 >>> df.applymap(lambda x: len(str(x))) 0 1 0 3 4 1 5 5 Note that a vectorized version of `func` often exists, which will be much faster. You could square each number elementwise. >>> df.applymap(lambda x: x**2) 0 1 0 1.000000 4.494400 1 11.262736 20.857489 But it's better to avoid applymap in that case. >>> df ** 2 0 1 0 1.000000 4.494400 1 11.262736 20.857489 \"\"\" # if we have a dtype == 'M8[ns]', provide boxed values def infer ( x ): if x . empty : return lib . map_infer ( x , func ) return lib . map_infer ( x . astype ( object ). _values , func ) return self . apply ( infer )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#asfreq","text":"def asfreq ( self : ~ FrameOrSeries , freq , method = None , how : Union [ str , NoneType ] = None , normalize : bool = False , fill_value = None ) -> ~ FrameOrSeries Convert TimeSeries to specified frequency. Optionally provide filling method to pad/backfill missing values. Returns the original data conformed to a new index with the specified frequency. resample is more appropriate if an operation, such as summarization, is necessary to represent the data at the new frequency.","title":"asfreq"},{"location":"reference/hielen2/datalink_prova_df/#parameters_14","text":"freq : DateOffset or str Frequency DateOffset or string. method : {'backfill'/'bfill', 'pad'/'ffill'}, default None Method to use for filling holes in reindexed Series (note this does not fill NaNs that already were present): * 'pad' / 'ffill': propagate last valid observation forward to next valid * 'backfill' / 'bfill': use NEXT valid observation to fill. how : {'start', 'end'}, default end For PeriodIndex only (see PeriodIndex.asfreq). normalize : bool, default False Whether to reset output index to midnight. fill_value : scalar, optional Value to use for missing values, applied during upsampling (note this does not fill NaNs that already were present).","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_17","text":"Same type as caller Object converted to the specified frequency.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_24","text":"reindex : Conform DataFrame to new index with optional filling logic.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_7","text":"To learn more about the frequency strings, please see this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases> __.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_25","text":"Start by creating a series with 4 one minute timestamps. index = pd.date_range('1/1/2000', periods=4, freq='T') series = pd.Series([0.0, None, 2.0, 3.0], index=index) df = pd.DataFrame({'s':series}) df s 2000-01-01 00:00:00 0.0 2000-01-01 00:01:00 NaN 2000-01-01 00:02:00 2.0 2000-01-01 00:03:00 3.0 Upsample the series into 30 second bins. df.asfreq(freq='30S') s 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 NaN 2000-01-01 00:01:00 NaN 2000-01-01 00:01:30 NaN 2000-01-01 00:02:00 2.0 2000-01-01 00:02:30 NaN 2000-01-01 00:03:00 3.0 Upsample again, providing a fill value . df.asfreq(freq='30S', fill_value=9.0) s 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 9.0 2000-01-01 00:01:00 NaN 2000-01-01 00:01:30 9.0 2000-01-01 00:02:00 2.0 2000-01-01 00:02:30 9.0 2000-01-01 00:03:00 3.0 Upsample again, providing a method . df.asfreq(freq='30S', method='bfill') s 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 NaN 2000-01-01 00:01:00 NaN 2000-01-01 00:01:30 2.0 2000-01-01 00:02:00 2.0 2000-01-01 00:02:30 3.0 2000-01-01 00:03:00 3.0 View Source def asfreq ( self : FrameOrSeries , freq , method = None , how : Optional [ str ] = None , normalize : bool_t = False , fill_value = None , ) -> FrameOrSeries : \"\"\" Convert TimeSeries to specified frequency. Optionally provide filling method to pad/backfill missing values. Returns the original data conformed to a new index with the specified frequency. ``resample`` is more appropriate if an operation, such as summarization, is necessary to represent the data at the new frequency. Parameters ---------- freq : DateOffset or str Frequency DateOffset or string. method : {'backfill'/'bfill', 'pad'/'ffill'}, default None Method to use for filling holes in reindexed Series (note this does not fill NaNs that already were present): * 'pad' / 'ffill': propagate last valid observation forward to next valid * 'backfill' / 'bfill': use NEXT valid observation to fill. how : {'start', 'end'}, default end For PeriodIndex only (see PeriodIndex.asfreq). normalize : bool, default False Whether to reset output index to midnight. fill_value : scalar, optional Value to use for missing values, applied during upsampling (note this does not fill NaNs that already were present). Returns ------- Same type as caller Object converted to the specified frequency. See Also -------- reindex : Conform DataFrame to new index with optional filling logic. Notes ----- To learn more about the frequency strings, please see `this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases>`__. Examples -------- Start by creating a series with 4 one minute timestamps. >>> index = pd.date_range('1/1/2000', periods=4, freq='T') >>> series = pd.Series([0.0, None, 2.0, 3.0], index=index) >>> df = pd.DataFrame({'s':series}) >>> df s 2000-01-01 00:00:00 0.0 2000-01-01 00:01:00 NaN 2000-01-01 00:02:00 2.0 2000-01-01 00:03:00 3.0 Upsample the series into 30 second bins. >>> df.asfreq(freq='30S') s 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 NaN 2000-01-01 00:01:00 NaN 2000-01-01 00:01:30 NaN 2000-01-01 00:02:00 2.0 2000-01-01 00:02:30 NaN 2000-01-01 00:03:00 3.0 Upsample again, providing a ``fill value``. >>> df.asfreq(freq='30S', fill_value=9.0) s 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 9.0 2000-01-01 00:01:00 NaN 2000-01-01 00:01:30 9.0 2000-01-01 00:02:00 2.0 2000-01-01 00:02:30 9.0 2000-01-01 00:03:00 3.0 Upsample again, providing a ``method``. >>> df.asfreq(freq='30S', method='bfill') s 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 NaN 2000-01-01 00:01:00 NaN 2000-01-01 00:01:30 2.0 2000-01-01 00:02:00 2.0 2000-01-01 00:02:30 3.0 2000-01-01 00:03:00 3.0 \"\"\" from pandas . core . resample import asfreq return asfreq ( self , freq , method = method , how = how , normalize = normalize , fill_value = fill_value , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#asof","text":"def asof ( self , where , subset = None ) Return the last row(s) without any NaNs before where . The last row (for each element in where , if list) without any NaN is taken. In case of a :class: ~pandas.DataFrame , the last row without NaN considering only the subset of columns (if not None ) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame","title":"asof"},{"location":"reference/hielen2/datalink_prova_df/#parameters_15","text":"where : date or array-like of dates Date(s) before which the last row(s) are returned. subset : str or array-like of str, default None For DataFrame, if not None , only use these columns to check for NaNs.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_18","text":"scalar, Series, or DataFrame The return can be : * scalar : when `self` is a Series and `where` is a scalar * Series : when `self` is a Series and `where` is an array - like , or when `self` is a DataFrame and `where` is a scalar * DataFrame : when `self` is a DataFrame and `where` is an array - like Return scalar , Series , or DataFrame .","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_25","text":"merge_asof : Perform an asof merge. Similar to left join.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_8","text":"Dates are assumed to be sorted. Raises if this is not the case.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_26","text":"A Series and a scalar where . s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40]) s 10 1.0 20 2.0 30 NaN 40 4.0 dtype: float64 s.asof(20) 2.0 For a sequence where , a Series is returned. The first value is NaN, because the first element of where is before the first index value. s.asof([5, 20]) 5 NaN 20 2.0 dtype: float64 Missing values are not considered. The following is 2.0 , not NaN, even though NaN is at the index location for 30 . s.asof(30) 2.0 Take all columns into consideration df = pd.DataFrame({'a': [10, 20, 30, 40, 50], ... 'b': [None, None, None, None, 500]}, ... index=pd.DatetimeIndex(['2018-02-27 09:01:00', ... '2018-02-27 09:02:00', ... '2018-02-27 09:03:00', ... '2018-02-27 09:04:00', ... '2018-02-27 09:05:00'])) df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30', ... '2018-02-27 09:04:30'])) a b 2018-02-27 09:03:30 NaN NaN 2018-02-27 09:04:30 NaN NaN Take a single column into consideration df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30', ... '2018-02-27 09:04:30']), ... subset=['a']) a b 2018-02-27 09:03:30 30.0 NaN 2018-02-27 09:04:30 40.0 NaN View Source def asof ( self , where , subset = None ): \"\"\" Return the last row(s) without any NaNs before `where`. The last row (for each element in `where`, if list) without any NaN is taken. In case of a :class:`~pandas.DataFrame`, the last row without NaN considering only the subset of columns (if not `None`) If there is no good value, NaN is returned for a Series or a Series of NaN values for a DataFrame Parameters ---------- where : date or array-like of dates Date(s) before which the last row(s) are returned. subset : str or array-like of str, default `None` For DataFrame, if not `None`, only use these columns to check for NaNs. Returns ------- scalar, Series, or DataFrame The return can be: * scalar : when `self` is a Series and `where` is a scalar * Series: when `self` is a Series and `where` is an array-like, or when `self` is a DataFrame and `where` is a scalar * DataFrame : when `self` is a DataFrame and `where` is an array-like Return scalar, Series, or DataFrame. See Also -------- merge_asof : Perform an asof merge. Similar to left join. Notes ----- Dates are assumed to be sorted. Raises if this is not the case. Examples -------- A Series and a scalar `where`. >>> s = pd.Series([1, 2, np.nan, 4], index=[10, 20, 30, 40]) >>> s 10 1.0 20 2.0 30 NaN 40 4.0 dtype: float64 >>> s.asof(20) 2.0 For a sequence `where`, a Series is returned. The first value is NaN, because the first element of `where` is before the first index value. >>> s.asof([5, 20]) 5 NaN 20 2.0 dtype: float64 Missing values are not considered. The following is ``2.0``, not NaN, even though NaN is at the index location for ``30``. >>> s.asof(30) 2.0 Take all columns into consideration >>> df = pd.DataFrame({'a': [10, 20, 30, 40, 50], ... 'b': [None, None, None, None, 500]}, ... index=pd.DatetimeIndex(['2018-02-27 09:01:00', ... '2018-02-27 09:02:00', ... '2018-02-27 09:03:00', ... '2018-02-27 09:04:00', ... '2018-02-27 09:05:00'])) >>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30', ... '2018-02-27 09:04:30'])) a b 2018-02-27 09:03:30 NaN NaN 2018-02-27 09:04:30 NaN NaN Take a single column into consideration >>> df.asof(pd.DatetimeIndex(['2018-02-27 09:03:30', ... '2018-02-27 09:04:30']), ... subset=['a']) a b 2018-02-27 09:03:30 30.0 NaN 2018-02-27 09:04:30 40.0 NaN \"\"\" if isinstance ( where , str ): where = Timestamp ( where ) if not self . index . is_monotonic : raise ValueError ( \"asof requires a sorted index\" ) is_series = isinstance ( self , ABCSeries ) if is_series : if subset is not None : raise ValueError ( \"subset is not valid for Series\" ) else : if subset is None : subset = self . columns if not is_list_like ( subset ): subset = [ subset ] is_list = is_list_like ( where ) if not is_list : start = self . index [ 0 ] if isinstance ( self . index , PeriodIndex ): where = Period ( where , freq = self . index . freq ) if where < start : if not is_series : return self . _constructor_sliced ( index = self . columns , name = where , dtype = np . float64 ) return np . nan # It's always much faster to use a *while* loop here for # Series than pre-computing all the NAs. However a # *while* loop is extremely expensive for DataFrame # so we later pre-compute all the NAs and use the same # code path whether *where* is a scalar or list. # See PR: https://github.com/pandas-dev/pandas/pull/14476 if is_series : loc = self . index . searchsorted ( where , side = \"right\" ) if loc > 0 : loc -= 1 values = self . _values while loc > 0 and isna ( values [ loc ]): loc -= 1 return values [ loc ] if not isinstance ( where , Index ): where = Index ( where ) if is_list else Index ([ where ]) null s = self . isna () if is_series else self [ subset ]. isna (). any ( 1 ) if null s . all (): if is_series : return self . _constructor ( np . nan , index = where , name = self . name ) elif is_list : return self . _constructor ( np . nan , index = where , columns = self . columns ) else : return self . _constructor_sliced ( np . nan , index = self . columns , name = where [ 0 ] ) locs = self . index . asof_locs ( where , ~ ( null s . _values )) # mask the missing missing = locs == - 1 data = self . take ( locs ) data . index = where data . loc [ missing ] = np . nan return data if is_list else data . iloc [ - 1 ]","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#assign","text":"def assign ( self , ** kwargs ) -> 'DataFrame' Assign new columns to a DataFrame. Returns a new object with all original columns in addition to new ones. Existing columns that are re-assigned will be overwritten.","title":"assign"},{"location":"reference/hielen2/datalink_prova_df/#parameters_16","text":"**kwargs : dict of {str: callable or Series} The column names are keywords. If the values are callable, they are computed on the DataFrame and assigned to the new columns. The callable must not change input DataFrame (though pandas doesn't check it). If the values are not callable, (e.g. a Series, scalar, or array), they are simply assigned.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_19","text":"DataFrame A new DataFrame with the new columns in addition to all the existing columns.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#notes_9","text":"Assigning multiple columns within the same assign is possible. Later items in '**kwargs' may refer to newly created or modified columns in 'df'; items are computed and assigned into 'df' in order. .. versionchanged:: 0.23.0 Keyword argument order is maintained.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_27","text":"df = pd.DataFrame({'temp_c': [17.0, 25.0]}, ... index=['Portland', 'Berkeley']) df temp_c Portland 17.0 Berkeley 25.0 Where the value is a callable, evaluated on df : df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32) temp_c temp_f Portland 17.0 62.6 Berkeley 25.0 77.0 Alternatively, the same behavior can be achieved by directly referencing an existing Series or sequence: df.assign(temp_f=df['temp_c'] * 9 / 5 + 32) temp_c temp_f Portland 17.0 62.6 Berkeley 25.0 77.0 You can create multiple columns within the same assign where one of the columns depends on another one defined within the same assign: df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32, ... temp_k=lambda x: (x['temp_f'] + 459.67) * 5 / 9) temp_c temp_f temp_k Portland 17.0 62.6 290.15 Berkeley 25.0 77.0 298.15 View Source def assign ( self , ** kwargs ) -> \"DataFrame\" : r \"\"\" Assign new columns to a DataFrame. Returns a new object with all original columns in addition to new ones. Existing columns that are re-assigned will be overwritten. Parameters ---------- **kwargs : dict of {str: callable or Series} The column names are keywords. If the values are callable, they are computed on the DataFrame and assigned to the new columns. The callable must not change input DataFrame (though pandas doesn't check it). If the values are not callable, (e.g. a Series, scalar, or array), they are simply assigned. Returns ------- DataFrame A new DataFrame with the new columns in addition to all the existing columns. Notes ----- Assigning multiple columns within the same ``assign`` is possible. Later items in '\\*\\*kwargs' may refer to newly created or modified columns in 'df'; items are computed and assigned into 'df' in order. .. versionchanged:: 0.23.0 Keyword argument order is maintained. Examples -------- >>> df = pd.DataFrame({'temp_c': [17.0, 25.0]}, ... index=['Portland', 'Berkeley']) >>> df temp_c Portland 17.0 Berkeley 25.0 Where the value is a callable, evaluated on `df`: >>> df.assign(temp_f=lambda x: x.temp_c * 9 / 5 + 32) temp_c temp_f Portland 17.0 62.6 Berkeley 25.0 77.0 Alternatively, the same behavior can be achieved by directly referencing an existing Series or sequence: >>> df.assign(temp_f=df['temp_c'] * 9 / 5 + 32) temp_c temp_f Portland 17.0 62.6 Berkeley 25.0 77.0 You can create multiple columns within the same assign where one of the columns depends on another one defined within the same assign: >>> df.assign(temp_f=lambda x: x['temp_c'] * 9 / 5 + 32, ... temp_k=lambda x: (x['temp_f'] + 459.67) * 5 / 9) temp_c temp_f temp_k Portland 17.0 62.6 290.15 Berkeley 25.0 77.0 298.15 \"\"\" data = self . copy () for k , v in kwargs . items (): data [ k ] = com . apply_if_callable ( v , data ) return data","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#astype","text":"def astype ( self : ~ FrameOrSeries , dtype , copy : bool = True , errors : str = 'raise' ) -> ~ FrameOrSeries Cast a pandas object to a specified dtype dtype .","title":"astype"},{"location":"reference/hielen2/datalink_prova_df/#parameters_17","text":"dtype : data type, or dict of column name -> data type Use a numpy.dtype or Python type to cast entire pandas object to the same type. Alternatively, use {col: dtype, ...}, where col is a column label and dtype is a numpy.dtype or Python type to cast one or more of the DataFrame's columns to column-specific types. copy : bool, default True Return a copy when copy=True (be very careful setting copy=False as changes to values then may propagate to other pandas objects). errors : {'raise', 'ignore'}, default 'raise' Control raising of exceptions on invalid data for provided dtype. - `` raise `` : allow exceptions to be raised - `` ignore `` : suppress exceptions . On error return original object .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_20","text":"casted : same type as caller","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_26","text":"to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type. numpy.ndarray.astype : Cast a numpy array to a specified type.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_28","text":"Create a DataFrame: d = {'col1': [1, 2], 'col2': [3, 4]} df = pd.DataFrame(data=d) df.dtypes col1 int64 col2 int64 dtype: object Cast all columns to int32: df.astype('int32').dtypes col1 int32 col2 int32 dtype: object Cast col1 to int32 using a dictionary: df.astype({'col1': 'int32'}).dtypes col1 int32 col2 int64 dtype: object Create a series: ser = pd.Series([1, 2], dtype='int32') ser 0 1 1 2 dtype: int32 ser.astype('int64') 0 1 1 2 dtype: int64 Convert to categorical type: ser.astype('category') 0 1 1 2 dtype: category Categories (2, int64): [1, 2] Convert to ordered categorical type with custom ordering: cat_dtype = pd.api.types.CategoricalDtype( ... categories=[2, 1], ordered=True) ser.astype(cat_dtype) 0 1 1 2 dtype: category Categories (2, int64): [2 < 1] Note that using copy=False and changing data on a new pandas object may propagate changes: s1 = pd.Series([1, 2]) s2 = s1.astype('int64', copy=False) s2[0] = 10 s1 # note that s1[0] has changed too 0 10 1 2 dtype: int64 Create a series of dates: ser_date = pd.Series(pd.date_range('20200101', periods=3)) ser_date 0 2020-01-01 1 2020-01-02 2 2020-01-03 dtype: datetime64[ns] Datetimes are localized to UTC first before converting to the specified timezone: ser_date.astype('datetime64[ns, US/Eastern]') 0 2019-12-31 19:00:00-05:00 1 2020-01-01 19:00:00-05:00 2 2020-01-02 19:00:00-05:00 dtype: datetime64[ns, US/Eastern] View Source def astype ( self : FrameOrSeries , dtype , copy : bool_t = True , errors : str = \"raise\" ) -> FrameOrSeries : \"\"\" Cast a pandas object to a specified dtype ``dtype``. Parameters ---------- dtype : data type, or dict of column name -> data type Use a numpy.dtype or Python type to cast entire pandas object to the same type. Alternatively, use {col: dtype, ...}, where col is a column label and dtype is a numpy.dtype or Python type to cast one or more of the DataFrame's columns to column-specific types. copy : bool, default True Return a copy when ``copy=True`` (be very careful setting ``copy=False`` as changes to values then may propagate to other pandas objects). errors : {'raise', 'ignore'}, default 'raise' Control raising of exceptions on invalid data for provided dtype. - ``raise`` : allow exceptions to be raised - ``ignore`` : suppress exceptions. On error return original object. Returns ------- casted : same type as caller See Also -------- to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type. numpy.ndarray.astype : Cast a numpy array to a specified type. Examples -------- Create a DataFrame: >>> d = {'col1': [1, 2], 'col2': [3, 4]} >>> df = pd.DataFrame(data=d) >>> df.dtypes col1 int64 col2 int64 dtype: object Cast all columns to int32: >>> df.astype('int32').dtypes col1 int32 col2 int32 dtype: object Cast col1 to int32 using a dictionary: >>> df.astype({'col1': 'int32'}).dtypes col1 int32 col2 int64 dtype: object Create a series: >>> ser = pd.Series([1, 2], dtype='int32') >>> ser 0 1 1 2 dtype: int32 >>> ser.astype('int64') 0 1 1 2 dtype: int64 Convert to categorical type: >>> ser.astype('category') 0 1 1 2 dtype: category Categories (2, int64): [1, 2] Convert to ordered categorical type with custom ordering: >>> cat_dtype = pd.api.types.CategoricalDtype( ... categories=[2, 1], ordered=True) >>> ser.astype(cat_dtype) 0 1 1 2 dtype: category Categories (2, int64): [2 < 1] Note that using ``copy=False`` and changing data on a new pandas object may propagate changes: >>> s1 = pd.Series([1, 2]) >>> s2 = s1.astype('int64', copy=False) >>> s2[0] = 10 >>> s1 # note that s1[0] has changed too 0 10 1 2 dtype: int64 Create a series of dates: >>> ser_date = pd.Series(pd.date_range('20200101', periods=3)) >>> ser_date 0 2020-01-01 1 2020-01-02 2 2020-01-03 dtype: datetime64[ns] Datetimes are localized to UTC first before converting to the specified timezone: >>> ser_date.astype('datetime64[ns, US/Eastern]') 0 2019-12-31 19:00:00-05:00 1 2020-01-01 19:00:00-05:00 2 2020-01-02 19:00:00-05:00 dtype: datetime64[ns, US/Eastern] \"\"\" if is_dict_like ( dtype ): if self . ndim == 1 : # i.e. Series if len ( dtype ) > 1 or self . name not in dtype : raise KeyError ( \"Only the Series name can be used for \" \"the key in Series dtype mappings.\" ) new_type = dtype [ self . name ] return self . astype ( new_type , copy , errors ) for col_name in dtype . keys (): if col_name not in self : raise KeyError ( \"Only a column name can be used for the \" \"key in a dtype mappings argument.\" ) results = [] for col_name , col in self . items (): if col_name in dtype : results . append ( col . astype ( dtype = dtype [ col_name ], copy = copy , errors = errors ) ) else : results . append ( col . copy () if copy else col ) elif is_extension_array_dtype ( dtype ) and self . ndim > 1 : # GH 18099/22869: columnwise conversion to extension dtype # GH 24704: use iloc to handle duplicate column names results = [ self . iloc [:, i ]. astype ( dtype , copy = copy ) for i in range ( len ( self . columns )) ] else : # else, only a single dtype is given new_data = self . _mgr . astype ( dtype = dtype , copy = copy , errors = errors ,) return self . _constructor ( new_data ). __finalize__ ( self , method = \"astype\" ) # GH 33113: handle empty frame or series if not results : return self . copy () # GH 19920: retain column metadata after concat result = pd . concat ( results , axis = 1 , copy = False ) result . columns = self . columns return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#at_time","text":"def at_time ( self : ~ FrameOrSeries , time , asof : bool = False , axis = None ) -> ~ FrameOrSeries Select values at particular time of day (e.g., 9:30AM).","title":"at_time"},{"location":"reference/hielen2/datalink_prova_df/#parameters_18","text":"time : datetime.time or str axis : {0 or 'index', 1 or 'columns'}, default 0 .. versionadded :: 0.24.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_21","text":"Series or DataFrame","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_3","text":"TypeError If the index is not a :class: DatetimeIndex","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_27","text":"between_time : Select values between particular times of the day. first : Select initial periods of time series based on a date offset. last : Select final periods of time series based on a date offset. DatetimeIndex.indexer_at_time : Get just the index locations for values at particular time of the day.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_29","text":"i = pd.date_range('2018-04-09', periods=4, freq='12H') ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) ts A 2018-04-09 00:00:00 1 2018-04-09 12:00:00 2 2018-04-10 00:00:00 3 2018-04-10 12:00:00 4 ts.at_time('12:00') A 2018-04-09 12:00:00 2 2018-04-10 12:00:00 4 View Source def at_time ( self : FrameOrSeries , time , asof : bool_t = False , axis = None ) -> FrameOrSeries : \"\"\" Select values at particular time of day (e.g., 9:30AM). Parameters ---------- time : datetime.time or str axis : {0 or 'index', 1 or 'columns'}, default 0 .. versionadded:: 0.24.0 Returns ------- Series or DataFrame Raises ------ TypeError If the index is not a :class:`DatetimeIndex` See Also -------- between_time : Select values between particular times of the day. first : Select initial periods of time series based on a date offset. last : Select final periods of time series based on a date offset. DatetimeIndex.indexer_at_time : Get just the index locations for values at particular time of the day. Examples -------- >>> i = pd.date_range('2018-04-09', periods=4, freq='12H') >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) >>> ts A 2018-04-09 00:00:00 1 2018-04-09 12:00:00 2 2018-04-10 00:00:00 3 2018-04-10 12:00:00 4 >>> ts.at_time('12:00') A 2018-04-09 12:00:00 2 2018-04-10 12:00:00 4 \"\"\" if axis is None : axis = self . _stat_axis_number axis = self . _get_axis_number ( axis ) index = self . _get_axis ( axis ) if not isinstance ( index , DatetimeIndex ): raise TypeError ( \"Index must be DatetimeIndex\" ) indexer = index . indexer_at_time ( time , asof = asof ) return self . _take_with_is_copy ( indexer , axis = axis )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#backfill","text":"def backfill ( self : ~ FrameOrSeries , axis = None , inplace : bool = False , limit = None , downcast = None ) -> Union [ ~ FrameOrSeries , NoneType ] Synonym for :meth: DataFrame.fillna with method='bfill' .","title":"backfill"},{"location":"reference/hielen2/datalink_prova_df/#returns_22","text":"{klass} or None Object with missing values filled or None if inplace=True . View Source def bfill ( self : FrameOrSeries , axis = None , inplace : bool_t = False , limit = None , downcast = None , ) -> Optional [ FrameOrSeries ] : \"\"\" Synonym for :meth:`DataFrame.fillna` with ``method='bfill'``. Returns ------- {klass} or None Object with missing values filled or None if ``inplace=True``. \"\"\" return self . fillna ( method = \"bfill\" , axis = axis , inplace = inplace , limit = limit , downcast = downcast )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#between_time","text":"def between_time ( self : ~ FrameOrSeries , start_time , end_time , include_start : bool = True , include_end : bool = True , axis = None ) -> ~ FrameOrSeries Select values between particular times of the day (e.g., 9:00-9:30 AM). By setting start_time to be later than end_time , you can get the times that are not between the two times.","title":"between_time"},{"location":"reference/hielen2/datalink_prova_df/#parameters_19","text":"start_time : datetime.time or str Initial time as a time filter limit. end_time : datetime.time or str End time as a time filter limit. include_start : bool, default True Whether the start time needs to be included in the result. include_end : bool, default True Whether the end time needs to be included in the result. axis : {0 or 'index', 1 or 'columns'}, default 0 Determine range time on index or columns value. .. versionadded :: 0.24.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_23","text":"Series or DataFrame Data from the original object filtered to the specified dates range.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_4","text":"TypeError If the index is not a :class: DatetimeIndex","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_28","text":"at_time : Select values at a particular time of the day. first : Select initial periods of time series based on a date offset. last : Select final periods of time series based on a date offset. DatetimeIndex.indexer_between_time : Get just the index locations for values between particular times of the day.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_30","text":"i = pd.date_range('2018-04-09', periods=4, freq='1D20min') ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) ts A 2018-04-09 00:00:00 1 2018-04-10 00:20:00 2 2018-04-11 00:40:00 3 2018-04-12 01:00:00 4 ts.between_time('0:15', '0:45') A 2018-04-10 00:20:00 2 2018-04-11 00:40:00 3 You get the times that are not between two times by setting start_time later than end_time : ts.between_time('0:45', '0:15') A 2018-04-09 00:00:00 1 2018-04-12 01:00:00 4 View Source def between_time ( self : FrameOrSeries , start_time , end_time , include_start : bool_t = True , include_end : bool_t = True , axis = None , ) -> FrameOrSeries : \"\"\" Select values between particular times of the day (e.g., 9:00-9:30 AM). By setting ``start_time`` to be later than ``end_time``, you can get the times that are *not* between the two times. Parameters ---------- start_time : datetime.time or str Initial time as a time filter limit. end_time : datetime.time or str End time as a time filter limit. include_start : bool, default True Whether the start time needs to be included in the result. include_end : bool, default True Whether the end time needs to be included in the result. axis : {0 or 'index', 1 or 'columns'}, default 0 Determine range time on index or columns value. .. versionadded:: 0.24.0 Returns ------- Series or DataFrame Data from the original object filtered to the specified dates range. Raises ------ TypeError If the index is not a :class:`DatetimeIndex` See Also -------- at_time : Select values at a particular time of the day. first : Select initial periods of time series based on a date offset. last : Select final periods of time series based on a date offset. DatetimeIndex.indexer_between_time : Get just the index locations for values between particular times of the day. Examples -------- >>> i = pd.date_range('2018-04-09', periods=4, freq='1D20min') >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) >>> ts A 2018-04-09 00:00:00 1 2018-04-10 00:20:00 2 2018-04-11 00:40:00 3 2018-04-12 01:00:00 4 >>> ts.between_time('0:15', '0:45') A 2018-04-10 00:20:00 2 2018-04-11 00:40:00 3 You get the times that are *not* between two times by setting ``start_time`` later than ``end_time``: >>> ts.between_time('0:45', '0:15') A 2018-04-09 00:00:00 1 2018-04-12 01:00:00 4 \"\"\" if axis is None : axis = self . _stat_axis_number axis = self . _get_axis_number ( axis ) index = self . _get_axis ( axis ) if not isinstance ( index , DatetimeIndex ): raise TypeError ( \"Index must be DatetimeIndex\" ) indexer = index . indexer_between_time ( start_time , end_time , include_start = include_start , include_end = include_end , ) return self . _take_with_is_copy ( indexer , axis = axis )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#bfill","text":"def bfill ( self : ~ FrameOrSeries , axis = None , inplace : bool = False , limit = None , downcast = None ) -> Union [ ~ FrameOrSeries , NoneType ] Synonym for :meth: DataFrame.fillna with method='bfill' .","title":"bfill"},{"location":"reference/hielen2/datalink_prova_df/#returns_24","text":"{klass} or None Object with missing values filled or None if inplace=True . View Source def bfill ( self : FrameOrSeries , axis = None , inplace : bool_t = False , limit = None , downcast = None , ) -> Optional [ FrameOrSeries ] : \"\"\" Synonym for :meth:`DataFrame.fillna` with ``method='bfill'``. Returns ------- {klass} or None Object with missing values filled or None if ``inplace=True``. \"\"\" return self . fillna ( method = \"bfill\" , axis = axis , inplace = inplace , limit = limit , downcast = downcast )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#bool","text":"def bool ( self ) Return the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception).","title":"bool"},{"location":"reference/hielen2/datalink_prova_df/#returns_25","text":"bool The value in the Series or DataFrame.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_29","text":"Series.astype : Change the data type of a Series, including to boolean. DataFrame.astype : Change the data type of a DataFrame, including to boolean. numpy.bool_ : NumPy boolean data type, used by pandas for boolean values.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_31","text":"The method will only work for single element objects with a boolean value: pd.Series([True]).bool() True pd.Series([False]).bool() False pd.DataFrame({'col': [True]}).bool() True pd.DataFrame({'col': [False]}).bool() False View Source def bool ( self ) : \"\"\" Return the bool of a single element Series or DataFrame. This must be a boolean scalar value, either True or False. It will raise a ValueError if the Series or DataFrame does not have exactly 1 element, or that element is not boolean (integer values 0 and 1 will also raise an exception). Returns ------- bool The value in the Series or DataFrame. See Also -------- Series.astype : Change the data type of a Series, including to boolean. DataFrame.astype : Change the data type of a DataFrame, including to boolean. numpy.bool_ : NumPy boolean data type, used by pandas for boolean values. Examples -------- The method will only work for single element objects with a boolean value: >>> pd.Series([True]).bool() True >>> pd.Series([False]).bool() False >>> pd.DataFrame({'col': [True]}).bool() True >>> pd.DataFrame({'col': [False]}).bool() False \"\"\" v = self . squeeze () if isinstance ( v , ( bool , np . bool_ )) : return bool ( v ) elif is_scalar ( v ) : raise ValueError ( \"bool cannot act on a non-boolean single element \" f \"{type(self).__name__}\" ) self . __nonzero__ ()","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#boxplot","text":"def boxplot ( self , column = None , by = None , ax = None , fontsize = None , rot = 0 , grid = True , figsize = None , layout = None , return_type = None , backend = None , ** kwargs ) Make a box plot from DataFrame columns. Make a box-and-whisker plot from DataFrame columns, optionally grouped by some other columns. A box plot is a method for graphically depicting groups of numerical data through their quartiles. The box extends from the Q1 to Q3 quartile values of the data, with a line at the median (Q2). The whiskers extend from the edges of box to show the range of the data. By default, they extend no more than 1.5 * IQR (IQR = Q3 - Q1) from the edges of the box, ending at the farthest data point within that interval. Outliers are plotted as separate dots. For further details see Wikipedia's entry for boxplot <https://en.wikipedia.org/wiki/Box_plot> _.","title":"boxplot"},{"location":"reference/hielen2/datalink_prova_df/#parameters_20","text":"column : str or list of str, optional Column name or list of names, or vector. Can be any valid input to :meth: pandas.DataFrame.groupby . by : str or array-like, optional Column in the DataFrame to :meth: pandas.DataFrame.groupby . One box-plot will be done per value of columns in by . ax : object of class matplotlib.axes.Axes, optional The matplotlib axes to be used by boxplot. fontsize : float or str Tick label font size in points or as a string (e.g., large ). rot : int or float, default 0 The rotation angle of labels (in degrees) with respect to the screen coordinate system. grid : bool, default True Setting this to True will show the grid. figsize : A tuple (width, height) in inches The size of the figure to create in matplotlib. layout : tuple (rows, columns), optional For example, (3, 5) will display the subplots using 3 columns and 5 rows, starting from the top-left. return_type : {'axes', 'dict', 'both'} or None, default 'axes' The kind of object to return. The default is axes . * 'axes' returns the matplotlib axes the boxplot is drawn on . * 'dict' returns a dictionary whose values are the matplotlib Lines of the boxplot . * 'both' returns a namedtuple with the axes and dict . * when grouping with `` by `` , a Series mapping columns to `` return_type `` is returned . If `` return_type `` is `None` , a NumPy array of axes with the same shape as `` layout `` is returned . backend : str, default None Backend to use instead of the backend specified in the option plotting.backend . For instance, 'matplotlib'. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend . .. versionadded :: 1.0.0 **kwargs All other plotting keyword arguments to be passed to :func: matplotlib.pyplot.boxplot .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_26","text":"result See Notes.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_30","text":"Series.plot.hist: Make a histogram. matplotlib.pyplot.boxplot : Matplotlib equivalent plot.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_10","text":"The return type depends on the return_type parameter: 'axes' : object of class matplotlib.axes.Axes 'dict' : dict of matplotlib.lines.Line2D objects 'both' : a namedtuple with structure (ax, lines) For data grouped with by , return a Series of the above or a numpy array: :class: ~pandas.Series :class: ~numpy.array (for return_type = None ) Use return_type='dict' when you want to tweak the appearance of the lines after plotting. In this case a dict containing the Lines making up the boxes, caps, fliers, medians, and whiskers is returned.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_32","text":"Boxplots can be created for every column in the dataframe by df.boxplot() or indicating the columns to be used: .. plot:: :context: close-figs >>> np.random.seed(1234) >>> df = pd.DataFrame(np.random.randn(10, 4), ... columns=['Col1', 'Col2', 'Col3', 'Col4']) >>> boxplot = df.boxplot(column=['Col1', 'Col2', 'Col3']) Boxplots of variables distributions grouped by the values of a third variable can be created using the option by . For instance: .. plot:: :context: close-figs >>> df = pd.DataFrame(np.random.randn(10, 2), ... columns=['Col1', 'Col2']) >>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A', ... 'B', 'B', 'B', 'B', 'B']) >>> boxplot = df.boxplot(by='X') A list of strings (i.e. ['X', 'Y'] ) can be passed to boxplot in order to group the data by combination of the variables in the x-axis: .. plot:: :context: close-figs >>> df = pd.DataFrame(np.random.randn(10, 3), ... columns=['Col1', 'Col2', 'Col3']) >>> df['X'] = pd.Series(['A', 'A', 'A', 'A', 'A', ... 'B', 'B', 'B', 'B', 'B']) >>> df['Y'] = pd.Series(['A', 'B', 'A', 'B', 'A', ... 'B', 'A', 'B', 'A', 'B']) >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by=['X', 'Y']) The layout of boxplot can be adjusted giving a tuple to layout : .. plot:: :context: close-figs >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X', ... layout=(2, 1)) Additional formatting can be done to the boxplot, like suppressing the grid ( grid=False ), rotating the labels in the x-axis (i.e. rot=45 ) or changing the fontsize (i.e. fontsize=15 ): .. plot:: :context: close-figs >>> boxplot = df.boxplot(grid=False, rot=45, fontsize=15) The parameter return_type can be used to select the type of element returned by boxplot . When return_type='axes' is selected, the matplotlib axes on which the boxplot is drawn are returned: >>> boxplot = df.boxplot(column=['Col1', 'Col2'], return_type='axes') >>> type(boxplot) <class 'matplotlib.axes._subplots.AxesSubplot'> When grouping with by , a Series mapping columns to return_type is returned: >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X', ... return_type='axes') >>> type(boxplot) <class 'pandas.core.series.Series'> If return_type is None , a NumPy array of axes with the same shape as layout is returned: >>> boxplot = df.boxplot(column=['Col1', 'Col2'], by='X', ... return_type=None) >>> type(boxplot) <class 'numpy.ndarray'> View Source @Substitution ( backend = _backend_doc ) @Appender ( _boxplot_doc ) def boxplot_frame ( self , column = None , by = None , ax = None , fontsize = None , rot = 0 , grid = True , figsize = None , layout = None , return_type = None , backend = None , ** kwargs , ) : plot_backend = _get_plot_backend ( backend ) return plot_backend . boxplot_frame ( self , column = column , by = by , ax = ax , fontsize = fontsize , rot = rot , grid = grid , figsize = figsize , layout = layout , return_type = return_type , ** kwargs , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#cleanfs","text":"def cleanfs ( self ) View Source def cleanfs ( self ): try : self . lock . acquire () try : os . unlink ( self . csv ) os . unlink ( self . md5file ) self . drop ( self . index , axis = 1 , inplace = True ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e","title":"cleanfs"},{"location":"reference/hielen2/datalink_prova_df/#clip","text":"def clip ( self : ~ FrameOrSeries , lower = None , upper = None , axis = None , inplace : bool = False , * args , ** kwargs ) -> ~ FrameOrSeries Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis.","title":"clip"},{"location":"reference/hielen2/datalink_prova_df/#parameters_21","text":"lower : float or array_like, default None Minimum threshold value. All values below this threshold will be set to it. upper : float or array_like, default None Maximum threshold value. All values above this threshold will be set to it. axis : int or str axis name, optional Align object with lower and upper along the given axis. inplace : bool, default False Whether to perform the operation in place on the data. args, *kwargs Additional keywords have no effect but might be accepted for compatibility with numpy.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_27","text":"Series or DataFrame Same type as calling object with the values outside the clip boundaries replaced.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_31","text":"Series.clip : Trim values at input threshold in series. DataFrame.clip : Trim values at input threshold in dataframe. numpy.clip : Clip (limit) the values in an array.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_33","text":"data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]} df = pd.DataFrame(data) df col_0 col_1 0 9 -2 1 -3 -7 2 0 6 3 -1 8 4 5 -5 Clips per column using lower and upper thresholds: df.clip(-4, 6) col_0 col_1 0 6 -2 1 -3 -4 2 0 6 3 -1 6 4 5 -4 Clips using specific lower and upper thresholds per column element: t = pd.Series([2, -4, -1, 6, 3]) t 0 2 1 -4 2 -1 3 6 4 3 dtype: int64 df.clip(t, t + 4, axis=0) col_0 col_1 0 6 2 1 -3 -4 2 0 3 3 6 8 4 5 3 View Source def clip ( self : FrameOrSeries , lower = None , upper = None , axis = None , inplace : bool_t = False , * args , ** kwargs , ) -> FrameOrSeries : \"\"\" Trim values at input threshold(s). Assigns values outside boundary to boundary values. Thresholds can be singular values or array like, and in the latter case the clipping is performed element-wise in the specified axis. Parameters ---------- lower : float or array_like, default None Minimum threshold value. All values below this threshold will be set to it. upper : float or array_like, default None Maximum threshold value. All values above this threshold will be set to it. axis : int or str axis name, optional Align object with lower and upper along the given axis. inplace : bool, default False Whether to perform the operation in place on the data. *args, **kwargs Additional keywords have no effect but might be accepted for compatibility with numpy. Returns ------- Series or DataFrame Same type as calling object with the values outside the clip boundaries replaced. See Also -------- Series.clip : Trim values at input threshold in series. DataFrame.clip : Trim values at input threshold in dataframe. numpy.clip : Clip (limit) the values in an array. Examples -------- >>> data = {'col_0': [9, -3, 0, -1, 5], 'col_1': [-2, -7, 6, 8, -5]} >>> df = pd.DataFrame(data) >>> df col_0 col_1 0 9 -2 1 -3 -7 2 0 6 3 -1 8 4 5 -5 Clips per column using lower and upper thresholds: >>> df.clip(-4, 6) col_0 col_1 0 6 -2 1 -3 -4 2 0 6 3 -1 6 4 5 -4 Clips using specific lower and upper thresholds per column element: >>> t = pd.Series([2, -4, -1, 6, 3]) >>> t 0 2 1 -4 2 -1 3 6 4 3 dtype: int64 >>> df.clip(t, t + 4, axis=0) col_0 col_1 0 6 2 1 -3 -4 2 0 3 3 6 8 4 5 3 \"\"\" inplace = validate_bool_kwarg ( inplace , \"inplace\" ) axis = nv . validate_clip_with_axis ( axis , args , kwargs ) if axis is not None : axis = self . _get_axis_number ( axis ) # GH 17276 # numpy doesn 't like NaN as a clip value # so ignore # GH 19992 # numpy doesn' t drop a list - like bound containing NaN if not is_list_like ( lower ) and np . any ( isna ( lower )): lower = None if not is_list_like ( upper ) and np . any ( isna ( upper )): upper = None # GH 2747 ( arguments were reversed ) if lower is not None and upper is not None : if is_scalar ( lower ) and is_scalar ( upper ): lower , upper = min ( lower , upper ), max ( lower , upper ) # fast - path for scalars if ( lower is None or ( is_scalar ( lower ) and is_number ( lower ))) and ( upper is None or ( is_scalar ( upper ) and is_number ( upper )) ): return self . _clip_with_scalar ( lower , upper , inplace = inplace ) result = self if lower is not None : result = result . _clip_with_one_bound ( lower , method = self . ge , axis = axis , inplace = inplace ) if upper is not None : if inplace : result = self result = result . _clip_with_one_bound ( upper , method = self . le , axis = axis , inplace = inplace ) return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#combine","text":"def combine ( self , other : 'DataFrame' , func , fill_value = None , overwrite = True ) -> 'DataFrame' Perform column-wise combine with another DataFrame. Combines a DataFrame with other DataFrame using func to element-wise combine columns. The row and column indexes of the resulting DataFrame will be the union of the two.","title":"combine"},{"location":"reference/hielen2/datalink_prova_df/#parameters_22","text":"other : DataFrame The DataFrame to merge column-wise. func : function Function that takes two series as inputs and return a Series or a scalar. Used to merge the two dataframes column by columns. fill_value : scalar value, default None The value to fill NaNs with prior to passing any column to the merge func. overwrite : bool, default True If True, columns in self that do not exist in other will be overwritten with NaNs.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_28","text":"DataFrame Combination of the provided DataFrames.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_32","text":"DataFrame.combine_first : Combine two DataFrame objects and default to non-null values in frame calling the method.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_34","text":"Combine using a simple function that chooses the smaller column. df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]}) df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2 df1.combine(df2, take_smaller) A B 0 0 3 1 0 3 Example using a true element-wise combine function. df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]}) df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) df1.combine(df2, np.minimum) A B 0 1 2 1 0 3 Using fill_value fills Nones prior to passing the column to the merge function. df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]}) df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) df1.combine(df2, take_smaller, fill_value=-5) A B 0 0 -5.0 1 0 4.0 However, if the same element in both dataframes is None, that None is preserved df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]}) df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]}) df1.combine(df2, take_smaller, fill_value=-5) A B 0 0 -5.0 1 0 3.0 Example that demonstrates the use of overwrite and behavior when the axis differ between the dataframes. df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]}) df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2]) df1.combine(df2, take_smaller) A B C 0 NaN NaN NaN 1 NaN 3.0 -10.0 2 NaN 3.0 1.0 df1.combine(df2, take_smaller, overwrite=False) A B C 0 0.0 NaN NaN 1 0.0 3.0 -10.0 2 NaN 3.0 1.0 Demonstrating the preference of the passed in dataframe. df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2]) df2.combine(df1, take_smaller) A B C 0 0.0 NaN NaN 1 0.0 3.0 NaN 2 NaN 3.0 NaN df2.combine(df1, take_smaller, overwrite=False) A B C 0 0.0 NaN NaN 1 0.0 3.0 1.0 2 NaN 3.0 1.0 View Source def combine ( self , other : \"DataFrame\" , func , fill_value = None , overwrite = True ) -> \"DataFrame\" : \"\"\" Perform column-wise combine with another DataFrame. Combines a DataFrame with `other` DataFrame using `func` to element-wise combine columns. The row and column indexes of the resulting DataFrame will be the union of the two. Parameters ---------- other : DataFrame The DataFrame to merge column-wise. func : function Function that takes two series as inputs and return a Series or a scalar. Used to merge the two dataframes column by columns. fill_value : scalar value, default None The value to fill NaNs with prior to passing any column to the merge func. overwrite : bool, default True If True, columns in `self` that do not exist in `other` will be overwritten with NaNs. Returns ------- DataFrame Combination of the provided DataFrames. See Also -------- DataFrame.combine_first : Combine two DataFrame objects and default to non-null values in frame calling the method. Examples -------- Combine using a simple function that chooses the smaller column. >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]}) >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) >>> take_smaller = lambda s1, s2: s1 if s1.sum() < s2.sum() else s2 >>> df1.combine(df2, take_smaller) A B 0 0 3 1 0 3 Example using a true element-wise combine function. >>> df1 = pd.DataFrame({'A': [5, 0], 'B': [2, 4]}) >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) >>> df1.combine(df2, np.minimum) A B 0 1 2 1 0 3 Using `fill_value` fills Nones prior to passing the column to the merge function. >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]}) >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) >>> df1.combine(df2, take_smaller, fill_value=-5) A B 0 0 -5.0 1 0 4.0 However, if the same element in both dataframes is None, that None is preserved >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [None, 4]}) >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [None, 3]}) >>> df1.combine(df2, take_smaller, fill_value=-5) A B 0 0 -5.0 1 0 3.0 Example that demonstrates the use of `overwrite` and behavior when the axis differ between the dataframes. >>> df1 = pd.DataFrame({'A': [0, 0], 'B': [4, 4]}) >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [-10, 1], }, index=[1, 2]) >>> df1.combine(df2, take_smaller) A B C 0 NaN NaN NaN 1 NaN 3.0 -10.0 2 NaN 3.0 1.0 >>> df1.combine(df2, take_smaller, overwrite=False) A B C 0 0.0 NaN NaN 1 0.0 3.0 -10.0 2 NaN 3.0 1.0 Demonstrating the preference of the passed in dataframe. >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1], }, index=[1, 2]) >>> df2.combine(df1, take_smaller) A B C 0 0.0 NaN NaN 1 0.0 3.0 NaN 2 NaN 3.0 NaN >>> df2.combine(df1, take_smaller, overwrite=False) A B C 0 0.0 NaN NaN 1 0.0 3.0 1.0 2 NaN 3.0 1.0 \"\"\" other_idxlen = len ( other . index ) # save for compare this , other = self . align ( other , copy = False ) new_index = this . index if other . empty and len ( new_index ) == len ( self . index ) : return self . copy () if self . empty and len ( other ) == other_idxlen : return other . copy () # sorts if possible new_columns = this . columns . union ( other . columns ) do_fill = fill_value is not None result = {} for col in new_columns : series = this [ col ] otherSeries = other [ col ] this_dtype = series . dtype other_dtype = otherSeries . dtype this_mask = isna ( series ) other_mask = isna ( otherSeries ) # don ' t overwrite columns unnecessarily # DO propagate if this column is not in the intersection if not overwrite and other_mask . all () : result [ col ] = this [ col ] . copy () continue if do_fill : series = series . copy () otherSeries = otherSeries . copy () series [ this_mask ] = fill_value otherSeries [ other_mask ] = fill_value if col not in self . columns : # If self DataFrame does not have col in other DataFrame , # try to promote series , which is all NaN , as other_dtype . new_dtype = other_dtype try : series = series . astype ( new_dtype , copy = False ) except ValueError : # e . g . new_dtype is integer types pass else : # if we have different dtypes , possibly promote new_dtype = find_common_type ( [ this_dtype, other_dtype ] ) if not is_dtype_equal ( this_dtype , new_dtype ) : series = series . astype ( new_dtype ) if not is_dtype_equal ( other_dtype , new_dtype ) : otherSeries = otherSeries . astype ( new_dtype ) arr = func ( series , otherSeries ) arr = maybe_downcast_to_dtype ( arr , this_dtype ) result [ col ] = arr # convert_objects just in case return self . _constructor ( result , index = new_index , columns = new_columns )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#combine_first","text":"def combine_first ( self , other : 'DataFrame' ) -> 'DataFrame' Update null elements with value in the same location in other . Combine two DataFrame objects by filling null values in one DataFrame with non-null values from other DataFrame. The row and column indexes of the resulting DataFrame will be the union of the two.","title":"combine_first"},{"location":"reference/hielen2/datalink_prova_df/#parameters_23","text":"other : DataFrame Provided DataFrame to use to fill null values.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_29","text":"DataFrame","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_33","text":"DataFrame.combine : Perform series-wise operation on two DataFrames using a given function.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_35","text":"df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]}) df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) df1.combine_first(df2) A B 0 1.0 3.0 1 0.0 4.0 Null values still persist if the location of that null value does not exist in other df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]}) df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2]) df1.combine_first(df2) A B C 0 NaN 4.0 NaN 1 0.0 3.0 1.0 2 NaN 3.0 1.0 View Source def combine_first ( self , other : \"DataFrame\" ) -> \"DataFrame\" : \"\"\" Update null elements with value in the same location in `other`. Combine two DataFrame objects by filling null values in one DataFrame with non-null values from other DataFrame. The row and column indexes of the resulting DataFrame will be the union of the two. Parameters ---------- other : DataFrame Provided DataFrame to use to fill null values. Returns ------- DataFrame See Also -------- DataFrame.combine : Perform series-wise operation on two DataFrames using a given function. Examples -------- >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [None, 4]}) >>> df2 = pd.DataFrame({'A': [1, 1], 'B': [3, 3]}) >>> df1.combine_first(df2) A B 0 1.0 3.0 1 0.0 4.0 Null values still persist if the location of that null value does not exist in `other` >>> df1 = pd.DataFrame({'A': [None, 0], 'B': [4, None]}) >>> df2 = pd.DataFrame({'B': [3, 3], 'C': [1, 1]}, index=[1, 2]) >>> df1.combine_first(df2) A B C 0 NaN 4.0 NaN 1 0.0 3.0 1.0 2 NaN 3.0 1.0 \"\"\" import pandas . core . computation . expressions as expressions def extract_values ( arr ): # Does two things: # 1. maybe gets the values from the Series / Index # 2. convert datelike to i8 # TODO: extract_array? if isinstance ( arr , ( Index , Series )): arr = arr . _values if needs_i8_conversion ( arr . dtype ): if is_extension_array_dtype ( arr . dtype ): arr = arr . asi8 else : arr = arr . view ( \"i8\" ) return arr def combiner ( x , y ): mask = isna ( x ) # TODO: extract_array? if isinstance ( mask , ( Index , Series )): mask = mask . _values x_values = extract_values ( x ) y_values = extract_values ( y ) # If the column y in other DataFrame is not in first DataFrame, # just return y_values. if y . name not in self . columns : return y_values return expressions . where ( mask , y_values , x_values ) return self . combine ( other , combiner , overwrite = False )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#compare","text":"def compare ( self , other : 'DataFrame' , align_axis : Union [ str , int ] = 1 , keep_shape : bool = False , keep_equal : bool = False ) -> 'DataFrame' Compare to another DataFrame and show the differences. .. versionadded:: 1.1.0","title":"compare"},{"location":"reference/hielen2/datalink_prova_df/#parameters_24","text":"other : DataFrame Object to compare with. align_axis : {0 or 'index', 1 or 'columns'}, default 1 Determine which axis to align the comparison on. * 0, or 'index' : Resulting differences are stacked vertically with rows drawn alternately from self and other. * 1, or 'columns' : Resulting differences are aligned horizontally with columns drawn alternately from self and other. keep_shape : bool, default False If true, all rows and columns are kept. Otherwise, only the ones with different values are kept. keep_equal : bool, default False If true, the result keeps values that are equal. Otherwise, equal values are shown as NaNs.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_30","text":"DataFrame DataFrame that shows the differences stacked side by side. The resulting index will be a MultiIndex with 'self' and 'other' stacked alternately at the inner level.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_34","text":"Series.compare : Compare with another Series and show differences.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_11","text":"Matching NaNs will not appear as a difference.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_36","text":"df = pd.DataFrame( ... { ... \"col1\": [\"a\", \"a\", \"b\", \"b\", \"a\"], ... \"col2\": [1.0, 2.0, 3.0, np.nan, 5.0], ... \"col3\": [1.0, 2.0, 3.0, 4.0, 5.0] ... }, ... columns=[\"col1\", \"col2\", \"col3\"], ... ) df col1 col2 col3 0 a 1.0 1.0 1 a 2.0 2.0 2 b 3.0 3.0 3 b NaN 4.0 4 a 5.0 5.0 df2 = df.copy() df2.loc[0, 'col1'] = 'c' df2.loc[2, 'col3'] = 4.0 df2 col1 col2 col3 0 c 1.0 1.0 1 a 2.0 2.0 2 b 3.0 4.0 3 b NaN 4.0 4 a 5.0 5.0 Align the differences on columns df.compare(df2) col1 col3 self other self other 0 a c NaN NaN 2 NaN NaN 3.0 4.0 Stack the differences on rows df.compare(df2, align_axis=0) col1 col3 0 self a NaN other c NaN 2 self NaN 3.0 other NaN 4.0 Keep the equal values df.compare(df2, keep_equal=True) col1 col3 self other self other 0 a c 1.0 1.0 2 b b 3.0 4.0 Keep all original rows and columns df.compare(df2, keep_shape=True) col1 col2 col3 self other self other self other 0 a c NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN NaN 2 NaN NaN NaN NaN 3.0 4.0 3 NaN NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN NaN NaN Keep all original rows and columns and also all original values df.compare(df2, keep_shape=True, keep_equal=True) col1 col2 col3 self other self other self other 0 a c 1.0 1.0 1.0 1.0 1 a a 2.0 2.0 2.0 2.0 2 b b 3.0 3.0 3.0 4.0 3 b b NaN NaN 4.0 4.0 4 a a 5.0 5.0 5.0 5.0 View Source @Appender ( \"\"\" Returns ------- DataFrame DataFrame that shows the differences stacked side by side. The resulting index will be a MultiIndex with 'self' and 'other' stacked alternately at the inner level. See Also -------- Series.compare : Compare with another Series and show differences. Notes ----- Matching NaNs will not appear as a difference. Examples -------- >>> df = pd.DataFrame( ... { ... \" col1 \": [\" a \", \" a \", \" b \", \" b \", \" a \"], ... \" col2 \": [1.0, 2.0, 3.0, np.nan, 5.0], ... \" col3 \": [1.0, 2.0, 3.0, 4.0, 5.0] ... }, ... columns=[\" col1 \", \" col2 \", \" col3 \"], ... ) >>> df col1 col2 col3 0 a 1.0 1.0 1 a 2.0 2.0 2 b 3.0 3.0 3 b NaN 4.0 4 a 5.0 5.0 >>> df2 = df.copy() >>> df2.loc[0, 'col1'] = 'c' >>> df2.loc[2, 'col3'] = 4.0 >>> df2 col1 col2 col3 0 c 1.0 1.0 1 a 2.0 2.0 2 b 3.0 4.0 3 b NaN 4.0 4 a 5.0 5.0 Align the differences on columns >>> df.compare(df2) col1 col3 self other self other 0 a c NaN NaN 2 NaN NaN 3.0 4.0 Stack the differences on rows >>> df.compare(df2, align_axis=0) col1 col3 0 self a NaN other c NaN 2 self NaN 3.0 other NaN 4.0 Keep the equal values >>> df.compare(df2, keep_equal=True) col1 col3 self other self other 0 a c 1.0 1.0 2 b b 3.0 4.0 Keep all original rows and columns >>> df.compare(df2, keep_shape=True) col1 col2 col3 self other self other self other 0 a c NaN NaN NaN NaN 1 NaN NaN NaN NaN NaN NaN 2 NaN NaN NaN NaN 3.0 4.0 3 NaN NaN NaN NaN NaN NaN 4 NaN NaN NaN NaN NaN NaN Keep all original rows and columns and also all original values >>> df.compare(df2, keep_shape=True, keep_equal=True) col1 col2 col3 self other self other self other 0 a c 1.0 1.0 1.0 1.0 1 a a 2.0 2.0 2.0 2.0 2 b b 3.0 3.0 3.0 4.0 3 b b NaN NaN 4.0 4.0 4 a a 5.0 5.0 5.0 5.0 \"\"\" ) @Appender ( _shared_docs [ \"compare\" ] % _shared_doc_kwargs ) def compare ( self , other : \"DataFrame\" , align_axis : Axis = 1 , keep_shape : bool = False , keep_equal : bool = False , ) -> \"DataFrame\" : return super (). compare ( other = other , align_axis = align_axis , keep_shape = keep_shape , keep_equal = keep_equal , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#convert_dtypes","text":"def convert_dtypes ( self : ~ FrameOrSeries , infer_objects : bool = True , convert_string : bool = True , convert_integer : bool = True , convert_boolean : bool = True ) -> ~ FrameOrSeries Convert columns to best possible dtypes using dtypes supporting pd.NA . .. versionadded:: 1.0.0","title":"convert_dtypes"},{"location":"reference/hielen2/datalink_prova_df/#parameters_25","text":"infer_objects : bool, default True Whether object dtypes should be converted to the best possible types. convert_string : bool, default True Whether object dtypes should be converted to StringDtype() . convert_integer : bool, default True Whether, if possible, conversion can be done to integer extension types. convert_boolean : bool, defaults True Whether object dtypes should be converted to BooleanDtypes() .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_31","text":"Series or DataFrame Copy of input object with new dtype.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_35","text":"infer_objects : Infer dtypes of objects. to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_12","text":"By default, convert_dtypes will attempt to convert a Series (or each Series in a DataFrame) to dtypes that support pd.NA . By using the options convert_string , convert_integer , and convert_boolean , it is possible to turn off individual conversions to StringDtype , the integer extension types or BooleanDtype , respectively. For object-dtyped columns, if infer_objects is True , use the inference rules as during normal Series/DataFrame construction. Then, if possible, convert to StringDtype , BooleanDtype or an appropriate integer extension type, otherwise leave as object . If the dtype is integer, convert to an appropriate integer extension type. If the dtype is numeric, and consists of all integers, convert to an appropriate integer extension type. In the future, as new dtypes are added that support pd.NA , the results of this method will change to support those new dtypes.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_37","text":"df = pd.DataFrame( ... { ... \"a\": pd.Series([1, 2, 3], dtype=np.dtype(\"int32\")), ... \"b\": pd.Series([\"x\", \"y\", \"z\"], dtype=np.dtype(\"O\")), ... \"c\": pd.Series([True, False, np.nan], dtype=np.dtype(\"O\")), ... \"d\": pd.Series([\"h\", \"i\", np.nan], dtype=np.dtype(\"O\")), ... \"e\": pd.Series([10, np.nan, 20], dtype=np.dtype(\"float\")), ... \"f\": pd.Series([np.nan, 100.5, 200], dtype=np.dtype(\"float\")), ... } ... ) Start with a DataFrame with default dtypes. df a b c d e f 0 1 x True h 10.0 NaN 1 2 y False i NaN 100.5 2 3 z NaN NaN 20.0 200.0 df.dtypes a int32 b object c object d object e float64 f float64 dtype: object Convert the DataFrame to use best possible dtypes. dfn = df.convert_dtypes() dfn a b c d e f 0 1 x True h 10 NaN 1 2 y False i 100.5 2 3 z 20 200.0 dfn.dtypes a Int32 b string c boolean d string e Int64 f float64 dtype: object Start with a Series of strings and missing data represented by np.nan . s = pd.Series([\"a\", \"b\", np.nan]) s 0 a 1 b 2 NaN dtype: object Obtain a Series with dtype StringDtype . s.convert_dtypes() 0 a 1 b 2 dtype: string View Source def convert_dtypes ( self : FrameOrSeries , infer_objects : bool_t = True , convert_string : bool_t = True , convert_integer : bool_t = True , convert_boolean : bool_t = True , ) -> FrameOrSeries : \"\"\" Convert columns to best possible dtypes using dtypes supporting ``pd.NA``. .. versionadded:: 1.0.0 Parameters ---------- infer_objects : bool, default True Whether object dtypes should be converted to the best possible types. convert_string : bool, default True Whether object dtypes should be converted to ``StringDtype()``. convert_integer : bool, default True Whether, if possible, conversion can be done to integer extension types. convert_boolean : bool, defaults True Whether object dtypes should be converted to ``BooleanDtypes()``. Returns ------- Series or DataFrame Copy of input object with new dtype. See Also -------- infer_objects : Infer dtypes of objects. to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to a numeric type. Notes ----- By default, ``convert_dtypes`` will attempt to convert a Series (or each Series in a DataFrame) to dtypes that support ``pd.NA``. By using the options ``convert_string``, ``convert_integer``, and ``convert_boolean``, it is possible to turn off individual conversions to ``StringDtype``, the integer extension types or ``BooleanDtype``, respectively. For object-dtyped columns, if ``infer_objects`` is ``True``, use the inference rules as during normal Series/DataFrame construction. Then, if possible, convert to ``StringDtype``, ``BooleanDtype`` or an appropriate integer extension type, otherwise leave as ``object``. If the dtype is integer, convert to an appropriate integer extension type. If the dtype is numeric, and consists of all integers, convert to an appropriate integer extension type. In the future, as new dtypes are added that support ``pd.NA``, the results of this method will change to support those new dtypes. Examples -------- >>> df = pd.DataFrame( ... { ... \" a \": pd.Series([1, 2, 3], dtype=np.dtype(\" int32 \")), ... \" b \": pd.Series([\" x \", \" y \", \" z \"], dtype=np.dtype(\" O \")), ... \" c \": pd.Series([True, False, np.nan], dtype=np.dtype(\" O \")), ... \" d \": pd.Series([\" h \", \" i \", np.nan], dtype=np.dtype(\" O \")), ... \" e \": pd.Series([10, np.nan, 20], dtype=np.dtype(\" float \")), ... \" f \": pd.Series([np.nan, 100.5, 200], dtype=np.dtype(\" float \")), ... } ... ) Start with a DataFrame with default dtypes. >>> df a b c d e f 0 1 x True h 10.0 NaN 1 2 y False i NaN 100.5 2 3 z NaN NaN 20.0 200.0 >>> df.dtypes a int32 b object c object d object e float64 f float64 dtype: object Convert the DataFrame to use best possible dtypes. >>> dfn = df.convert_dtypes() >>> dfn a b c d e f 0 1 x True h 10 NaN 1 2 y False i <NA> 100.5 2 3 z <NA> <NA> 20 200.0 >>> dfn.dtypes a Int32 b string c boolean d string e Int64 f float64 dtype: object Start with a Series of strings and missing data represented by ``np.nan``. >>> s = pd.Series([\" a \", \" b \", np.nan]) >>> s 0 a 1 b 2 NaN dtype: object Obtain a Series with dtype ``StringDtype``. >>> s.convert_dtypes() 0 a 1 b 2 <NA> dtype: string \"\"\" if self . ndim == 1 : return self . _convert_dtypes ( infer_objects , convert_string , convert_integer , convert_boolean ) else : results = [ col . _convert_dtypes ( infer_objects , convert_string , convert_integer , convert_boolean ) for col_name , col in self . items () ] result = pd . concat ( results , axis = 1 , copy = False ) return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#copy","text":"def copy ( self : ~ FrameOrSeries , deep : bool = True ) -> ~ FrameOrSeries Make a copy of this object's indices and data. When deep=True (default), a new object will be created with a copy of the calling object's data and indices. Modifications to the data or indices of the copy will not be reflected in the original object (see notes below). When deep=False , a new object will be created without copying the calling object's data or index (only references to the data and index are copied). Any changes to the data of the original will be reflected in the shallow copy (and vice versa).","title":"copy"},{"location":"reference/hielen2/datalink_prova_df/#parameters_26","text":"deep : bool, default True Make a deep copy, including a copy of the data and the indices. With deep=False neither the indices nor the data are copied.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_32","text":"copy : Series or DataFrame Object type matches caller.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#notes_13","text":"When deep=True , data is copied but actual Python objects will not be copied recursively, only the reference to the object. This is in contrast to copy.deepcopy in the Standard Library, which recursively copies object data (see examples below). While Index objects are copied when deep=True , the underlying numpy array is not copied for performance reasons. Since Index is immutable, the underlying data can be safely shared and a copy is not needed.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_38","text":"s = pd.Series([1, 2], index=[\"a\", \"b\"]) s a 1 b 2 dtype: int64 s_copy = s.copy() s_copy a 1 b 2 dtype: int64 Shallow copy versus default (deep) copy: s = pd.Series([1, 2], index=[\"a\", \"b\"]) deep = s.copy() shallow = s.copy(deep=False) Shallow copy shares data and index with original. s is shallow False s.values is shallow.values and s.index is shallow.index True Deep copy has own copy of data and index. s is deep False s.values is deep.values or s.index is deep.index False Updates to the data shared by shallow copy and original is reflected in both; deep copy remains unchanged. s[0] = 3 shallow[1] = 4 s a 3 b 4 dtype: int64 shallow a 3 b 4 dtype: int64 deep a 1 b 2 dtype: int64 Note that when copying an object containing Python objects, a deep copy will copy the data, but will not do so recursively. Updating a nested data object will be reflected in the deep copy. s = pd.Series([[1, 2], [3, 4]]) deep = s.copy() s[0][0] = 10 s 0 [10, 2] 1 [3, 4] dtype: object deep 0 [10, 2] 1 [3, 4] dtype: object View Source def copy ( self : FrameOrSeries , deep : bool_t = True ) -> FrameOrSeries : \"\"\" Make a copy of this object's indices and data. When ``deep=True`` (default), a new object will be created with a copy of the calling object's data and indices. Modifications to the data or indices of the copy will not be reflected in the original object (see notes below). When ``deep=False``, a new object will be created without copying the calling object's data or index (only references to the data and index are copied). Any changes to the data of the original will be reflected in the shallow copy (and vice versa). Parameters ---------- deep : bool, default True Make a deep copy, including a copy of the data and the indices. With ``deep=False`` neither the indices nor the data are copied. Returns ------- copy : Series or DataFrame Object type matches caller. Notes ----- When ``deep=True``, data is copied but actual Python objects will not be copied recursively, only the reference to the object. This is in contrast to `copy.deepcopy` in the Standard Library, which recursively copies object data (see examples below). While ``Index`` objects are copied when ``deep=True``, the underlying numpy array is not copied for performance reasons. Since ``Index`` is immutable, the underlying data can be safely shared and a copy is not needed. Examples -------- >>> s = pd.Series([1, 2], index=[\" a \", \" b \"]) >>> s a 1 b 2 dtype: int64 >>> s_copy = s.copy() >>> s_copy a 1 b 2 dtype: int64 **Shallow copy versus default (deep) copy:** >>> s = pd.Series([1, 2], index=[\" a \", \" b \"]) >>> deep = s.copy() >>> shallow = s.copy(deep=False) Shallow copy shares data and index with original. >>> s is shallow False >>> s.values is shallow.values and s.index is shallow.index True Deep copy has own copy of data and index. >>> s is deep False >>> s.values is deep.values or s.index is deep.index False Updates to the data shared by shallow copy and original is reflected in both; deep copy remains unchanged. >>> s[0] = 3 >>> shallow[1] = 4 >>> s a 3 b 4 dtype: int64 >>> shallow a 3 b 4 dtype: int64 >>> deep a 1 b 2 dtype: int64 Note that when copying an object containing Python objects, a deep copy will copy the data, but will not do so recursively. Updating a nested data object will be reflected in the deep copy. >>> s = pd.Series([[1, 2], [3, 4]]) >>> deep = s.copy() >>> s[0][0] = 10 >>> s 0 [10, 2] 1 [3, 4] dtype: object >>> deep 0 [10, 2] 1 [3, 4] dtype: object \"\"\" data = self . _mgr . copy ( deep = deep ) self . _clear_item_cache () return self . _constructor ( data ). __finalize__ ( self , method = \"copy\" )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#corr","text":"def corr ( self , method = 'pearson' , min_periods = 1 ) -> 'DataFrame' Compute pairwise correlation of columns, excluding NA/null values.","title":"corr"},{"location":"reference/hielen2/datalink_prova_df/#parameters_27","text":"method : {'pearson', 'kendall', 'spearman'} or callable Method of correlation: * pearson : standard correlation coefficient * kendall : Kendall Tau correlation coefficient * spearman : Spearman rank correlation * callable : callable with input two 1 d ndarrays and returning a float . Note that the returned matrix from corr will have 1 along the diagonals and will be symmetric regardless of the callable ' s behavior . .. versionadded :: 0 . 24 . 0 min_periods : int, optional Minimum number of observations required per pair of columns to have a valid result. Currently only available for Pearson and Spearman correlation.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_33","text":"DataFrame Correlation matrix.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_36","text":"DataFrame.corrwith : Compute pairwise correlation with another DataFrame or Series. Series.corr : Compute the correlation between two Series.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_39","text":"def histogram_intersection(a, b): ... v = np.minimum(a, b).sum().round(decimals=1) ... return v df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)], ... columns=['dogs', 'cats']) df.corr(method=histogram_intersection) dogs cats dogs 1.0 0.3 cats 0.3 1.0 View Source def corr ( self , method = \"pearson\" , min_periods = 1 ) -> \"DataFrame\" : \"\"\" Compute pairwise correlation of columns, excluding NA/null values. Parameters ---------- method : {'pearson', 'kendall', 'spearman'} or callable Method of correlation: * pearson : standard correlation coefficient * kendall : Kendall Tau correlation coefficient * spearman : Spearman rank correlation * callable: callable with input two 1d ndarrays and returning a float. Note that the returned matrix from corr will have 1 along the diagonals and will be symmetric regardless of the callable's behavior. .. versionadded:: 0.24.0 min_periods : int, optional Minimum number of observations required per pair of columns to have a valid result. Currently only available for Pearson and Spearman correlation. Returns ------- DataFrame Correlation matrix. See Also -------- DataFrame.corrwith : Compute pairwise correlation with another DataFrame or Series. Series.corr : Compute the correlation between two Series. Examples -------- >>> def histogram_intersection(a, b): ... v = np.minimum(a, b).sum().round(decimals=1) ... return v >>> df = pd.DataFrame([(.2, .3), (.0, .6), (.6, .0), (.2, .1)], ... columns=['dogs', 'cats']) >>> df.corr(method=histogram_intersection) dogs cats dogs 1.0 0.3 cats 0.3 1.0 \"\"\" numeric_df = self . _get_numeric_data () cols = numeric_df . columns idx = cols . copy () mat = numeric_df . to_numpy ( dtype = float , na_value = np . nan , copy = False ) if method == \"pearson\" : correl = libalgos . nancorr ( mat , minp = min_periods ) elif method == \"spearman\" : correl = libalgos . nancorr_spearman ( mat , minp = min_periods ) elif method == \"kendall\" or callable ( method ) : if min_periods is None : min_periods = 1 mat = mat . T corrf = nanops . get_corr_func ( method ) K = len ( cols ) correl = np . empty (( K , K ), dtype = float ) mask = np . isfinite ( mat ) for i , ac in enumerate ( mat ) : for j , bc in enumerate ( mat ) : if i > j : continue valid = mask [ i ] & mask [ j ] if valid . sum () < min_periods : c = np . nan elif i == j : c = 1.0 elif not valid . all () : c = corrf ( ac [ valid ] , bc [ valid ] ) else : c = corrf ( ac , bc ) correl [ i, j ] = c correl [ j, i ] = c else : raise ValueError ( \"method must be either 'pearson', \" \"'spearman', 'kendall', or a callable, \" f \"'{method}' was supplied\" ) return self . _constructor ( correl , index = idx , columns = cols )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#corrwith","text":"def corrwith ( self , other , axis = 0 , drop = False , method = 'pearson' ) -> pandas . core . series . Series Compute pairwise correlation. Pairwise correlation is computed between rows or columns of DataFrame with rows or columns of Series or DataFrame. DataFrames are first aligned along both axes before computing the correlations.","title":"corrwith"},{"location":"reference/hielen2/datalink_prova_df/#parameters_28","text":"other : DataFrame, Series Object with which to compute correlations. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' to compute column-wise, 1 or 'columns' for row-wise. drop : bool, default False Drop missing indices from result. method : {'pearson', 'kendall', 'spearman'} or callable Method of correlation: * pearson : standard correlation coefficient * kendall : Kendall Tau correlation coefficient * spearman : Spearman rank correlation * callable : callable with input two 1 d ndarrays and returning a float . .. versionadded :: 0 . 24 . 0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_34","text":"Series Pairwise correlations.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_37","text":"DataFrame.corr : Compute pairwise correlation of columns. View Source def corrwith ( self , other , axis = 0 , drop = False , method = \"pearson\" ) -> Series : \"\"\" Compute pairwise correlation. Pairwise correlation is computed between rows or columns of DataFrame with rows or columns of Series or DataFrame. DataFrames are first aligned along both axes before computing the correlations. Parameters ---------- other : DataFrame, Series Object with which to compute correlations. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' to compute column-wise, 1 or 'columns' for row-wise. drop : bool, default False Drop missing indices from result. method : {'pearson', 'kendall', 'spearman'} or callable Method of correlation: * pearson : standard correlation coefficient * kendall : Kendall Tau correlation coefficient * spearman : Spearman rank correlation * callable: callable with input two 1d ndarrays and returning a float. .. versionadded:: 0.24.0 Returns ------- Series Pairwise correlations. See Also -------- DataFrame.corr : Compute pairwise correlation of columns. \"\"\" axis = self . _get_axis_number ( axis ) this = self . _get_numeric_data () if isinstance ( other , Series ): return this . apply ( lambda x : other . corr ( x , method = method ), axis = axis ) other = other . _get_numeric_data () left , right = this . align ( other , join = \"inner\" , copy = False ) if axis == 1 : left = left . T right = right . T if method == \"pearson\" : # mask missing values left = left + right * 0 right = right + left * 0 # demeaned data ldem = left - left . mean () rdem = right - right . mean () num = ( ldem * rdem ). sum () dom = ( left . count () - 1 ) * left . std () * right . std () correl = num / dom elif method in [ \"kendall\" , \"spearman\" ] or callable ( method ): def c ( x ): return nanops . nancorr ( x [ 0 ], x [ 1 ], method = method ) correl = self . _constructor_sliced ( map ( c , zip ( left . values . T , right . values . T )), index = left . columns ) else : raise ValueError ( f \"Invalid method {method} was passed, \" \"valid methods are: 'pearson', 'kendall', \" \"'spearman', or callable\" ) if not drop : # Find non - matching labels along the given axis # and append missing correlations ( GH 22375 ) raxis = 1 if axis == 0 else 0 result_index = this . _get_axis ( raxis ). union ( other . _get_axis ( raxis )) idx_diff = result_index . difference ( correl . index ) if len ( idx_diff ) > 0 : correl = correl . append ( Series ([ np . nan ] * len ( idx_diff ), index = idx_diff )) return correl","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#count","text":"def count ( self , axis = 0 , level = None , numeric_only = False ) Count non-NA cells for each column or row. The values None , NaN , NaT , and optionally numpy.inf (depending on pandas.options.mode.use_inf_as_na ) are considered NA.","title":"count"},{"location":"reference/hielen2/datalink_prova_df/#parameters_29","text":"axis : {0 or 'index', 1 or 'columns'}, default 0 If 0 or 'index' counts are generated for each column. If 1 or 'columns' counts are generated for each row. level : int or str, optional If the axis is a MultiIndex (hierarchical), count along a particular level , collapsing into a DataFrame . A str specifies the level name. numeric_only : bool, default False Include only float , int or boolean data.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_35","text":"Series or DataFrame For each column/row the number of non-NA/null entries. If level is specified returns a DataFrame .","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_38","text":"Series.count: Number of non-NA elements in a Series. DataFrame.shape: Number of DataFrame rows and columns (including NA elements). DataFrame.isna: Boolean same-sized DataFrame showing places of NA elements.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_40","text":"Constructing DataFrame from a dictionary: df = pd.DataFrame({\"Person\": ... [\"John\", \"Myla\", \"Lewis\", \"John\", \"Myla\"], ... \"Age\": [24., np.nan, 21., 33, 26], ... \"Single\": [False, True, True, True, False]}) df Person Age Single 0 John 24.0 False 1 Myla NaN True 2 Lewis 21.0 True 3 John 33.0 True 4 Myla 26.0 False Notice the uncounted NA values: df.count() Person 5 Age 4 Single 5 dtype: int64 Counts for each row : df.count(axis='columns') 0 3 1 2 2 3 3 3 4 3 dtype: int64 Counts for one level of a MultiIndex : df.set_index([\"Person\", \"Single\"]).count(level=\"Person\") Age Person John 2 Lewis 1 Myla 1 View Source def count ( self , axis = 0 , level = None , numeric_only = False ): \"\"\" Count non-NA cells for each column or row. The values `None`, `NaN`, `NaT`, and optionally `numpy.inf` (depending on `pandas.options.mode.use_inf_as_na`) are considered NA. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 If 0 or 'index' counts are generated for each column. If 1 or 'columns' counts are generated for each row. level : int or str, optional If the axis is a `MultiIndex` (hierarchical), count along a particular `level`, collapsing into a `DataFrame`. A `str` specifies the level name. numeric_only : bool, default False Include only `float`, `int` or `boolean` data. Returns ------- Series or DataFrame For each column/row the number of non-NA/null entries. If `level` is specified returns a `DataFrame`. See Also -------- Series.count: Number of non-NA elements in a Series. DataFrame.shape: Number of DataFrame rows and columns (including NA elements). DataFrame.isna: Boolean same-sized DataFrame showing places of NA elements. Examples -------- Constructing DataFrame from a dictionary: >>> df = pd.DataFrame({\" Person \": ... [\" John \", \" Myla \", \" Lewis \", \" John \", \" Myla \"], ... \" Age \": [24., np.nan, 21., 33, 26], ... \" Single \": [False, True, True, True, False]}) >>> df Person Age Single 0 John 24.0 False 1 Myla NaN True 2 Lewis 21.0 True 3 John 33.0 True 4 Myla 26.0 False Notice the uncounted NA values: >>> df.count() Person 5 Age 4 Single 5 dtype: int64 Counts for each **row**: >>> df.count(axis='columns') 0 3 1 2 2 3 3 3 4 3 dtype: int64 Counts for one level of a `MultiIndex`: >>> df.set_index([\" Person \", \" Single \"]).count(level=\" Person \") Age Person John 2 Lewis 1 Myla 1 \"\"\" axis = self . _get_axis_number ( axis ) if level is not None : return self . _count_level ( level , axis = axis , numeric_only = numeric_only ) if numeric_only : frame = self . _get_numeric_data () else : frame = self # GH #423 if len ( frame . _get_axis ( axis )) == 0 : result = self . _constructor_sliced ( 0 , index = frame . _get_agg_axis ( axis )) else : if frame . _is_mixed_type or frame . _mgr . any_extension_types : # the or any_extension_types is really only hit for single- # column frames with an extension array result = notna ( frame ). sum ( axis = axis ) else : # GH13407 series_counts = notna ( frame ). sum ( axis = axis ) counts = series_counts . values result = self . _constructor_sliced ( counts , index = frame . _get_agg_axis ( axis ) ) return result . astype ( \"int64\" )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#cov","text":"def cov ( self , min_periods : Union [ int , NoneType ] = None , ddof : Union [ int , NoneType ] = 1 ) -> 'DataFrame' Compute pairwise covariance of columns, excluding NA/null values. Compute the pairwise covariance among the series of a DataFrame. The returned data frame is the covariance matrix <https://en.wikipedia.org/wiki/Covariance_matrix> __ of the columns of the DataFrame. Both NA and null values are automatically excluded from the calculation. (See the note below about bias from missing values.) A threshold can be set for the minimum number of observations for each value created. Comparisons with observations below this threshold will be returned as NaN . This method is generally used for the analysis of time series data to understand the relationship between different measures across time.","title":"cov"},{"location":"reference/hielen2/datalink_prova_df/#parameters_30","text":"min_periods : int, optional Minimum number of observations required per pair of columns to have a valid result. ddof : int, default 1 Delta degrees of freedom. The divisor used in calculations is N - ddof , where N represents the number of elements. .. versionadded :: 1.1.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_36","text":"DataFrame The covariance matrix of the series of the DataFrame.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_39","text":"Series.cov : Compute covariance with another Series. core.window.ExponentialMovingWindow.cov: Exponential weighted sample covariance. core.window.Expanding.cov : Expanding sample covariance. core.window.Rolling.cov : Rolling sample covariance.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_14","text":"Returns the covariance matrix of the DataFrame's time series. The covariance is normalized by N-ddof. For DataFrames that have Series that are missing data (assuming that data is missing at random <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random> __) the returned covariance matrix will be an unbiased estimate of the variance and covariance between the member Series. However, for many applications this estimate may not be acceptable because the estimate covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimate correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See Estimation of covariance matrices <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_ matrices> __ for more details.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_41","text":"df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)], ... columns=['dogs', 'cats']) df.cov() dogs cats dogs 0.666667 -1.000000 cats -1.000000 1.666667 np.random.seed(42) df = pd.DataFrame(np.random.randn(1000, 5), ... columns=['a', 'b', 'c', 'd', 'e']) df.cov() a b c d e a 0.998438 -0.020161 0.059277 -0.008943 0.014144 b -0.020161 1.059352 -0.008543 -0.024738 0.009826 c 0.059277 -0.008543 1.010670 -0.001486 -0.000271 d -0.008943 -0.024738 -0.001486 0.921297 -0.013692 e 0.014144 0.009826 -0.000271 -0.013692 0.977795 Minimum number of periods This method also supports an optional min_periods keyword that specifies the required minimum number of non-NA observations for each column pair in order to have a valid result: np.random.seed(42) df = pd.DataFrame(np.random.randn(20, 3), ... columns=['a', 'b', 'c']) df.loc[df.index[:5], 'a'] = np.nan df.loc[df.index[5:10], 'b'] = np.nan df.cov(min_periods=12) a b c a 0.316741 NaN -0.150812 b NaN 1.248003 0.191417 c -0.150812 0.191417 0.895202 View Source def cov ( self , min_periods : Optional [ int ] = None , ddof : Optional [ int ] = 1 ) -> \"DataFrame\" : \"\"\" Compute pairwise covariance of columns, excluding NA/null values. Compute the pairwise covariance among the series of a DataFrame. The returned data frame is the `covariance matrix <https://en.wikipedia.org/wiki/Covariance_matrix>`__ of the columns of the DataFrame. Both NA and null values are automatically excluded from the calculation. (See the note below about bias from missing values.) A threshold can be set for the minimum number of observations for each value created. Comparisons with observations below this threshold will be returned as ``NaN``. This method is generally used for the analysis of time series data to understand the relationship between different measures across time. Parameters ---------- min_periods : int, optional Minimum number of observations required per pair of columns to have a valid result. ddof : int, default 1 Delta degrees of freedom. The divisor used in calculations is ``N - ddof``, where ``N`` represents the number of elements. .. versionadded:: 1.1.0 Returns ------- DataFrame The covariance matrix of the series of the DataFrame. See Also -------- Series.cov : Compute covariance with another Series. core.window.ExponentialMovingWindow.cov: Exponential weighted sample covariance. core.window.Expanding.cov : Expanding sample covariance. core.window.Rolling.cov : Rolling sample covariance. Notes ----- Returns the covariance matrix of the DataFrame's time series. The covariance is normalized by N-ddof. For DataFrames that have Series that are missing data (assuming that data is `missing at random <https://en.wikipedia.org/wiki/Missing_data#Missing_at_random>`__) the returned covariance matrix will be an unbiased estimate of the variance and covariance between the member Series. However, for many applications this estimate may not be acceptable because the estimate covariance matrix is not guaranteed to be positive semi-definite. This could lead to estimate correlations having absolute values which are greater than one, and/or a non-invertible covariance matrix. See `Estimation of covariance matrices <https://en.wikipedia.org/w/index.php?title=Estimation_of_covariance_ matrices>`__ for more details. Examples -------- >>> df = pd.DataFrame([(1, 2), (0, 3), (2, 0), (1, 1)], ... columns=['dogs', 'cats']) >>> df.cov() dogs cats dogs 0.666667 -1.000000 cats -1.000000 1.666667 >>> np.random.seed(42) >>> df = pd.DataFrame(np.random.randn(1000, 5), ... columns=['a', 'b', 'c', 'd', 'e']) >>> df.cov() a b c d e a 0.998438 -0.020161 0.059277 -0.008943 0.014144 b -0.020161 1.059352 -0.008543 -0.024738 0.009826 c 0.059277 -0.008543 1.010670 -0.001486 -0.000271 d -0.008943 -0.024738 -0.001486 0.921297 -0.013692 e 0.014144 0.009826 -0.000271 -0.013692 0.977795 **Minimum number of periods** This method also supports an optional ``min_periods`` keyword that specifies the required minimum number of non-NA observations for each column pair in order to have a valid result: >>> np.random.seed(42) >>> df = pd.DataFrame(np.random.randn(20, 3), ... columns=['a', 'b', 'c']) >>> df.loc[df.index[:5], 'a'] = np.nan >>> df.loc[df.index[5:10], 'b'] = np.nan >>> df.cov(min_periods=12) a b c a 0.316741 NaN -0.150812 b NaN 1.248003 0.191417 c -0.150812 0.191417 0.895202 \"\"\" numeric_df = self . _get_numeric_data () cols = numeric_df . columns idx = cols . copy () mat = numeric_df . to_numpy ( dtype = float , na_value = np . nan , copy = False ) if notna ( mat ). all (): if min_periods is not None and min_periods > len ( mat ): base_cov = np . empty (( mat . shape [ 1 ], mat . shape [ 1 ])) base_cov . fill ( np . nan ) else : base_cov = np . cov ( mat . T , ddof = ddof ) base_cov = base_cov . reshape (( len ( cols ), len ( cols ))) else : base_cov = libalgos . nancorr ( mat , cov = True , minp = min_periods ) return self . _constructor ( base_cov , index = idx , columns = cols )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#cummax","text":"def cummax ( self , axis = None , skipna = True , * args , ** kwargs ) Return cumulative maximum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative maximum.","title":"cummax"},{"location":"reference/hielen2/datalink_prova_df/#parameters_31","text":"axis : {0 or 'index', 1 or 'columns'}, default 0 The index or the name of the axis. 0 is equivalent to None or 'index'. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. args, *kwargs Additional keywords have no effect but might be accepted for compatibility with NumPy.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_37","text":"Series or DataFrame Return cumulative maximum of Series or DataFrame.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_40","text":"core.window.Expanding.max : Similar functionality but ignores NaN values. DataFrame.max : Return the maximum over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_42","text":"Series s = pd.Series([2, np.nan, 5, -1, 0]) s 0 2.0 1 NaN 2 5.0 3 -1.0 4 0.0 dtype: float64 By default, NA values are ignored. s.cummax() 0 2.0 1 NaN 2 5.0 3 5.0 4 5.0 dtype: float64 To include NA values in the operation, use skipna=False s.cummax(skipna=False) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 DataFrame df = pd.DataFrame([[2.0, 1.0], ... [3.0, np.nan], ... [1.0, 0.0]], ... columns=list('AB')) df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the maximum in each column. This is equivalent to axis=None or axis='index' . df.cummax() A B 0 2.0 1.0 1 3.0 NaN 2 3.0 1.0 To iterate over columns and find the maximum in each row, use axis=1 df.cummax(axis=1) A B 0 2.0 2.0 1 3.0 NaN 2 1.0 1.0 View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , accum_func_name = accum_func_name , examples = examples , ) @Appender ( _cnum_doc ) def cum_func ( self , axis = None , skipna = True , * args , ** kwargs ) : skipna = nv . validate_cum_func_with_skipna ( skipna , args , kwargs , name ) if axis is None : axis = self . _stat_axis_number else : axis = self . _get_axis_number ( axis ) if axis == 1 : return cum_func ( self . T , axis = 0 , skipna = skipna , * args , ** kwargs ). T def block_accum_func ( blk_values ) : values = blk_values . T if hasattr ( blk_values , \"T\" ) else blk_values result = nanops . na_accum_func ( values , accum_func , skipna = skipna ) result = result . T if hasattr ( result , \"T\" ) else result return result result = self . _mgr . apply ( block_accum_func ) return self . _constructor ( result ). __finalize__ ( self , method = name )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#cummin","text":"def cummin ( self , axis = None , skipna = True , * args , ** kwargs ) Return cumulative minimum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative minimum.","title":"cummin"},{"location":"reference/hielen2/datalink_prova_df/#parameters_32","text":"axis : {0 or 'index', 1 or 'columns'}, default 0 The index or the name of the axis. 0 is equivalent to None or 'index'. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. args, *kwargs Additional keywords have no effect but might be accepted for compatibility with NumPy.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_38","text":"Series or DataFrame Return cumulative minimum of Series or DataFrame.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_41","text":"core.window.Expanding.min : Similar functionality but ignores NaN values. DataFrame.min : Return the minimum over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_43","text":"Series s = pd.Series([2, np.nan, 5, -1, 0]) s 0 2.0 1 NaN 2 5.0 3 -1.0 4 0.0 dtype: float64 By default, NA values are ignored. s.cummin() 0 2.0 1 NaN 2 2.0 3 -1.0 4 -1.0 dtype: float64 To include NA values in the operation, use skipna=False s.cummin(skipna=False) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 DataFrame df = pd.DataFrame([[2.0, 1.0], ... [3.0, np.nan], ... [1.0, 0.0]], ... columns=list('AB')) df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the minimum in each column. This is equivalent to axis=None or axis='index' . df.cummin() A B 0 2.0 1.0 1 2.0 NaN 2 1.0 0.0 To iterate over columns and find the minimum in each row, use axis=1 df.cummin(axis=1) A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , accum_func_name = accum_func_name , examples = examples , ) @Appender ( _cnum_doc ) def cum_func ( self , axis = None , skipna = True , * args , ** kwargs ) : skipna = nv . validate_cum_func_with_skipna ( skipna , args , kwargs , name ) if axis is None : axis = self . _stat_axis_number else : axis = self . _get_axis_number ( axis ) if axis == 1 : return cum_func ( self . T , axis = 0 , skipna = skipna , * args , ** kwargs ). T def block_accum_func ( blk_values ) : values = blk_values . T if hasattr ( blk_values , \"T\" ) else blk_values result = nanops . na_accum_func ( values , accum_func , skipna = skipna ) result = result . T if hasattr ( result , \"T\" ) else result return result result = self . _mgr . apply ( block_accum_func ) return self . _constructor ( result ). __finalize__ ( self , method = name )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#cumprod","text":"def cumprod ( self , axis = None , skipna = True , * args , ** kwargs ) Return cumulative product over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative product.","title":"cumprod"},{"location":"reference/hielen2/datalink_prova_df/#parameters_33","text":"axis : {0 or 'index', 1 or 'columns'}, default 0 The index or the name of the axis. 0 is equivalent to None or 'index'. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. args, *kwargs Additional keywords have no effect but might be accepted for compatibility with NumPy.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_39","text":"Series or DataFrame Return cumulative product of Series or DataFrame.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_42","text":"core.window.Expanding.prod : Similar functionality but ignores NaN values. DataFrame.prod : Return the product over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_44","text":"Series s = pd.Series([2, np.nan, 5, -1, 0]) s 0 2.0 1 NaN 2 5.0 3 -1.0 4 0.0 dtype: float64 By default, NA values are ignored. s.cumprod() 0 2.0 1 NaN 2 10.0 3 -10.0 4 -0.0 dtype: float64 To include NA values in the operation, use skipna=False s.cumprod(skipna=False) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 DataFrame df = pd.DataFrame([[2.0, 1.0], ... [3.0, np.nan], ... [1.0, 0.0]], ... columns=list('AB')) df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the product in each column. This is equivalent to axis=None or axis='index' . df.cumprod() A B 0 2.0 1.0 1 6.0 NaN 2 6.0 0.0 To iterate over columns and find the product in each row, use axis=1 df.cumprod(axis=1) A B 0 2.0 2.0 1 3.0 NaN 2 1.0 0.0 View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , accum_func_name = accum_func_name , examples = examples , ) @Appender ( _cnum_doc ) def cum_func ( self , axis = None , skipna = True , * args , ** kwargs ) : skipna = nv . validate_cum_func_with_skipna ( skipna , args , kwargs , name ) if axis is None : axis = self . _stat_axis_number else : axis = self . _get_axis_number ( axis ) if axis == 1 : return cum_func ( self . T , axis = 0 , skipna = skipna , * args , ** kwargs ). T def block_accum_func ( blk_values ) : values = blk_values . T if hasattr ( blk_values , \"T\" ) else blk_values result = nanops . na_accum_func ( values , accum_func , skipna = skipna ) result = result . T if hasattr ( result , \"T\" ) else result return result result = self . _mgr . apply ( block_accum_func ) return self . _constructor ( result ). __finalize__ ( self , method = name )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#cumsum","text":"def cumsum ( self , axis = None , skipna = True , * args , ** kwargs ) Return cumulative sum over a DataFrame or Series axis. Returns a DataFrame or Series of the same size containing the cumulative sum.","title":"cumsum"},{"location":"reference/hielen2/datalink_prova_df/#parameters_34","text":"axis : {0 or 'index', 1 or 'columns'}, default 0 The index or the name of the axis. 0 is equivalent to None or 'index'. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. args, *kwargs Additional keywords have no effect but might be accepted for compatibility with NumPy.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_40","text":"Series or DataFrame Return cumulative sum of Series or DataFrame.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_43","text":"core.window.Expanding.sum : Similar functionality but ignores NaN values. DataFrame.sum : Return the sum over DataFrame axis. DataFrame.cummax : Return cumulative maximum over DataFrame axis. DataFrame.cummin : Return cumulative minimum over DataFrame axis. DataFrame.cumsum : Return cumulative sum over DataFrame axis. DataFrame.cumprod : Return cumulative product over DataFrame axis.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_45","text":"Series s = pd.Series([2, np.nan, 5, -1, 0]) s 0 2.0 1 NaN 2 5.0 3 -1.0 4 0.0 dtype: float64 By default, NA values are ignored. s.cumsum() 0 2.0 1 NaN 2 7.0 3 6.0 4 6.0 dtype: float64 To include NA values in the operation, use skipna=False s.cumsum(skipna=False) 0 2.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 DataFrame df = pd.DataFrame([[2.0, 1.0], ... [3.0, np.nan], ... [1.0, 0.0]], ... columns=list('AB')) df A B 0 2.0 1.0 1 3.0 NaN 2 1.0 0.0 By default, iterates over rows and finds the sum in each column. This is equivalent to axis=None or axis='index' . df.cumsum() A B 0 2.0 1.0 1 5.0 NaN 2 6.0 1.0 To iterate over columns and find the sum in each row, use axis=1 df.cumsum(axis=1) A B 0 2.0 3.0 1 3.0 NaN 2 1.0 1.0 View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , accum_func_name = accum_func_name , examples = examples , ) @Appender ( _cnum_doc ) def cum_func ( self , axis = None , skipna = True , * args , ** kwargs ) : skipna = nv . validate_cum_func_with_skipna ( skipna , args , kwargs , name ) if axis is None : axis = self . _stat_axis_number else : axis = self . _get_axis_number ( axis ) if axis == 1 : return cum_func ( self . T , axis = 0 , skipna = skipna , * args , ** kwargs ). T def block_accum_func ( blk_values ) : values = blk_values . T if hasattr ( blk_values , \"T\" ) else blk_values result = nanops . na_accum_func ( values , accum_func , skipna = skipna ) result = result . T if hasattr ( result , \"T\" ) else result return result result = self . _mgr . apply ( block_accum_func ) return self . _constructor ( result ). __finalize__ ( self , method = name )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#describe","text":"def describe ( self : ~ FrameOrSeries , percentiles = None , include = None , exclude = None , datetime_is_numeric = False ) -> ~ FrameOrSeries Generate descriptive statistics. Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding NaN values. Analyzes both numeric and object series, as well as DataFrame column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail.","title":"describe"},{"location":"reference/hielen2/datalink_prova_df/#parameters_35","text":"percentiles : list-like of numbers, optional The percentiles to include in the output. All should fall between 0 and 1. The default is [.25, .5, .75] , which returns the 25th, 50th, and 75th percentiles. include : 'all', list-like of dtypes or None (default), optional A white list of data types to include in the result. Ignored for Series . Here are the options: - 'all' : All columns of the input will be included in the output . - A list - like of dtypes : Limits the results to the provided data types . To limit the result to numeric types submit `` numpy . number `` . To limit it instead to object columns submit the `` numpy . object `` data type . Strings can also be used in the style of `` select_dtypes `` ( e . g . `` df . describe ( include = [ 'O' ]) `` ). To select pandas categorical columns , use `` 'category' `` - None ( default ) : The result will include all numeric columns . exclude : list-like of dtypes or None (default), optional, A black list of data types to omit from the result. Ignored for Series . Here are the options: - A list - like of dtypes : Excludes the provided data types from the result . To exclude numeric types submit `` numpy . number `` . To exclude object columns submit the data type `` numpy . object `` . Strings can also be used in the style of `` select_dtypes `` ( e . g . `` df . describe ( include = [ 'O' ]) `` ). To exclude pandas categorical columns , use `` 'category' `` - None ( default ) : The result will exclude nothing . datetime_is_numeric : bool, default False Whether to treat datetime dtypes as numeric. This affects statistics calculated for the column. For DataFrame input, this also controls whether datetime columns are included by default. .. versionadded :: 1.1.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_41","text":"Series or DataFrame Summary statistics of the Series or Dataframe provided.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_44","text":"DataFrame.count: Count number of non-NA/null observations. DataFrame.max: Maximum of the values in the object. DataFrame.min: Minimum of the values in the object. DataFrame.mean: Mean of the values. DataFrame.std: Standard deviation of the observations. DataFrame.select_dtypes: Subset of a DataFrame including/excluding columns based on their dtype.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_15","text":"For numeric data, the result's index will include count , mean , std , min , max as well as lower, 50 and upper percentiles. By default the lower percentile is 25 and the upper percentile is 75 . The 50 percentile is the same as the median. For object data (e.g. strings or timestamps), the result's index will include count , unique , top , and freq . The top is the most common value. The freq is the most common value's frequency. Timestamps also include the first and last items. If multiple object values have the highest count, then the count and top results will be arbitrarily chosen from among those with the highest count. For mixed data types provided via a DataFrame , the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If include='all' is provided as an option, the result will include a union of attributes of each type. The include and exclude parameters can be used to limit which columns in a DataFrame are analyzed for the output. The parameters are ignored when analyzing a Series .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_46","text":"Describing a numeric Series . s = pd.Series([1, 2, 3]) s.describe() count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 dtype: float64 Describing a categorical Series . s = pd.Series(['a', 'a', 'b', 'c']) s.describe() count 4 unique 3 top a freq 2 dtype: object Describing a timestamp Series . s = pd.Series([ ... np.datetime64(\"2000-01-01\"), ... np.datetime64(\"2010-01-01\"), ... np.datetime64(\"2010-01-01\") ... ]) s.describe(datetime_is_numeric=True) count 3 mean 2006-09-01 08:00:00 min 2000-01-01 00:00:00 25% 2004-12-31 12:00:00 50% 2010-01-01 00:00:00 75% 2010-01-01 00:00:00 max 2010-01-01 00:00:00 dtype: object Describing a DataFrame . By default only numeric fields are returned. df = pd.DataFrame({'categorical': pd.Categorical(['d','e','f']), ... 'numeric': [1, 2, 3], ... 'object': ['a', 'b', 'c'] ... }) df.describe() numeric count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 Describing all columns of a DataFrame regardless of data type. df.describe(include='all') # doctest: +SKIP categorical numeric object count 3 3.0 3 unique 3 NaN 3 top f NaN a freq 1 NaN 1 mean NaN 2.0 NaN std NaN 1.0 NaN min NaN 1.0 NaN 25% NaN 1.5 NaN 50% NaN 2.0 NaN 75% NaN 2.5 NaN max NaN 3.0 NaN Describing a column from a DataFrame by accessing it as an attribute. df.numeric.describe() count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 Name: numeric, dtype: float64 Including only numeric columns in a DataFrame description. df.describe(include=[np.number]) numeric count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 Including only string columns in a DataFrame description. df.describe(include=[object]) # doctest: +SKIP object count 3 unique 3 top a freq 1 Including only categorical columns from a DataFrame description. df.describe(include=['category']) categorical count 3 unique 3 top f freq 1 Excluding numeric columns from a DataFrame description. df.describe(exclude=[np.number]) # doctest: +SKIP categorical object count 3 3 unique 3 3 top f a freq 1 1 Excluding object columns from a DataFrame description. df.describe(exclude=[object]) # doctest: +SKIP categorical numeric count 3 3.0 unique 3 NaN top f NaN freq 1 NaN mean NaN 2.0 std NaN 1.0 min NaN 1.0 25% NaN 1.5 50% NaN 2.0 75% NaN 2.5 max NaN 3.0 View Source def describe ( self : FrameOrSeries , percentiles = None , include = None , exclude = None , datetime_is_numeric = False , ) -> FrameOrSeries : \"\"\" Generate descriptive statistics. Descriptive statistics include those that summarize the central tendency, dispersion and shape of a dataset's distribution, excluding ``NaN`` values. Analyzes both numeric and object series, as well as ``DataFrame`` column sets of mixed data types. The output will vary depending on what is provided. Refer to the notes below for more detail. Parameters ---------- percentiles : list-like of numbers, optional The percentiles to include in the output. All should fall between 0 and 1. The default is ``[.25, .5, .75]``, which returns the 25th, 50th, and 75th percentiles. include : 'all', list-like of dtypes or None (default), optional A white list of data types to include in the result. Ignored for ``Series``. Here are the options: - 'all' : All columns of the input will be included in the output. - A list-like of dtypes : Limits the results to the provided data types. To limit the result to numeric types submit ``numpy.number``. To limit it instead to object columns submit the ``numpy.object`` data type. Strings can also be used in the style of ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To select pandas categorical columns, use ``'category'`` - None (default) : The result will include all numeric columns. exclude : list-like of dtypes or None (default), optional, A black list of data types to omit from the result. Ignored for ``Series``. Here are the options: - A list-like of dtypes : Excludes the provided data types from the result. To exclude numeric types submit ``numpy.number``. To exclude object columns submit the data type ``numpy.object``. Strings can also be used in the style of ``select_dtypes`` (e.g. ``df.describe(include=['O'])``). To exclude pandas categorical columns, use ``'category'`` - None (default) : The result will exclude nothing. datetime_is_numeric : bool, default False Whether to treat datetime dtypes as numeric. This affects statistics calculated for the column. For DataFrame input, this also controls whether datetime columns are included by default. .. versionadded:: 1.1.0 Returns ------- Series or DataFrame Summary statistics of the Series or Dataframe provided. See Also -------- DataFrame.count: Count number of non-NA/null observations. DataFrame.max: Maximum of the values in the object. DataFrame.min: Minimum of the values in the object. DataFrame.mean: Mean of the values. DataFrame.std: Standard deviation of the observations. DataFrame.select_dtypes: Subset of a DataFrame including/excluding columns based on their dtype. Notes ----- For numeric data, the result's index will include ``count``, ``mean``, ``std``, ``min``, ``max`` as well as lower, ``50`` and upper percentiles. By default the lower percentile is ``25`` and the upper percentile is ``75``. The ``50`` percentile is the same as the median. For object data (e.g. strings or timestamps), the result's index will include ``count``, ``unique``, ``top``, and ``freq``. The ``top`` is the most common value. The ``freq`` is the most common value's frequency. Timestamps also include the ``first`` and ``last`` items. If multiple object values have the highest count, then the ``count`` and ``top`` results will be arbitrarily chosen from among those with the highest count. For mixed data types provided via a ``DataFrame``, the default is to return only an analysis of numeric columns. If the dataframe consists only of object and categorical data without any numeric columns, the default is to return an analysis of both the object and categorical columns. If ``include='all'`` is provided as an option, the result will include a union of attributes of each type. The `include` and `exclude` parameters can be used to limit which columns in a ``DataFrame`` are analyzed for the output. The parameters are ignored when analyzing a ``Series``. Examples -------- Describing a numeric ``Series``. >>> s = pd.Series([1, 2, 3]) >>> s.describe() count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 dtype: float64 Describing a categorical ``Series``. >>> s = pd.Series(['a', 'a', 'b', 'c']) >>> s.describe() count 4 unique 3 top a freq 2 dtype: object Describing a timestamp ``Series``. >>> s = pd.Series([ ... np.datetime64(\" 2000 - 01 - 01 \"), ... np.datetime64(\" 2010 - 01 - 01 \"), ... np.datetime64(\" 2010 - 01 - 01 \") ... ]) >>> s.describe(datetime_is_numeric=True) count 3 mean 2006-09-01 08:00:00 min 2000-01-01 00:00:00 25% 2004-12-31 12:00:00 50% 2010-01-01 00:00:00 75% 2010-01-01 00:00:00 max 2010-01-01 00:00:00 dtype: object Describing a ``DataFrame``. By default only numeric fields are returned. >>> df = pd.DataFrame({'categorical': pd.Categorical(['d','e','f']), ... 'numeric': [1, 2, 3], ... 'object': ['a', 'b', 'c'] ... }) >>> df.describe() numeric count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 Describing all columns of a ``DataFrame`` regardless of data type. >>> df.describe(include='all') # doctest: +SKIP categorical numeric object count 3 3.0 3 unique 3 NaN 3 top f NaN a freq 1 NaN 1 mean NaN 2.0 NaN std NaN 1.0 NaN min NaN 1.0 NaN 25% NaN 1.5 NaN 50% NaN 2.0 NaN 75% NaN 2.5 NaN max NaN 3.0 NaN Describing a column from a ``DataFrame`` by accessing it as an attribute. >>> df.numeric.describe() count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 Name: numeric, dtype: float64 Including only numeric columns in a ``DataFrame`` description. >>> df.describe(include=[np.number]) numeric count 3.0 mean 2.0 std 1.0 min 1.0 25% 1.5 50% 2.0 75% 2.5 max 3.0 Including only string columns in a ``DataFrame`` description. >>> df.describe(include=[object]) # doctest: +SKIP object count 3 unique 3 top a freq 1 Including only categorical columns from a ``DataFrame`` description. >>> df.describe(include=['category']) categorical count 3 unique 3 top f freq 1 Excluding numeric columns from a ``DataFrame`` description. >>> df.describe(exclude=[np.number]) # doctest: +SKIP categorical object count 3 3 unique 3 3 top f a freq 1 1 Excluding object columns from a ``DataFrame`` description. >>> df.describe(exclude=[object]) # doctest: +SKIP categorical numeric count 3 3.0 unique 3 NaN top f NaN freq 1 NaN mean NaN 2.0 std NaN 1.0 min NaN 1.0 25% NaN 1.5 50% NaN 2.0 75% NaN 2.5 max NaN 3.0 \"\"\" if self . ndim == 2 and self . columns . size == 0 : raise ValueError ( \"Cannot describe a DataFrame without columns\" ) if percentiles is not None : # explicit conversion of `percentiles` to list percentiles = list ( percentiles ) # get them all to be in [0, 1] validate_percentile ( percentiles ) # median should always be included if 0 . 5 not in percentiles : percentiles . append ( 0 . 5 ) percentiles = np . asarray ( percentiles ) else : percentiles = np . array ([ 0 . 25 , 0 . 5 , 0 . 75 ]) # sort and check for duplicates unique_pcts = np . unique ( percentiles ) if len ( unique_pcts ) < len ( percentiles ): raise ValueError ( \"percentiles cannot contain duplicates\" ) percentiles = unique_pcts formatted_percentiles = format_percentiles ( percentiles ) def describe_numeric_1d ( series ): stat_index = ( [ \"count\" , \"mean\" , \"std\" , \"min\" ] + formatted_percentiles + [ \"max\" ] ) d = ( [ series . count (), series . mean (), series . std (), series . min ()] + series . quantile ( percentiles ). tolist () + [ series . max ()] ) return pd . Series ( d , index = stat_index , name = series . name ) def describe_categorical_1d ( data ): names = [ \"count\" , \"unique\" ] objcounts = data . value_counts () count_unique = len ( objcounts [ objcounts != 0 ]) result = [ data . count (), count_unique ] dtype = None if result [ 1 ] > 0 : top , freq = objcounts . index [ 0 ], objcounts . iloc [ 0 ] if is_datetime64_any_dtype ( data . dtype ): if self . ndim == 1 : stacklevel = 4 else : stacklevel = 5 warnings . warn ( \"Treating datetime data as categorical rather than numeric in \" \"`.describe` is deprecated and will be removed in a future \" \"version of pandas. Specify `datetime_is_numeric=True` to \" \"silence this warning and adopt the future behavior now.\" , FutureWarning , stacklevel = stacklevel , ) tz = data . dt . tz asint = data . dropna (). values . view ( \"i8\" ) top = Timestamp ( top ) if top . tzinfo is not None and tz is not None : # Don't tz_localize(None) if key is already tz-aware top = top . tz_convert ( tz ) else : top = top . tz_localize ( tz ) names += [ \"top\" , \"freq\" , \"first\" , \"last\" ] result += [ top , freq , Timestamp ( asint . min (), tz = tz ), Timestamp ( asint . max (), tz = tz ), ] else : names += [ \"top\" , \"freq\" ] result += [ top , freq ] # If the DataFrame is empty, set 'top' and 'freq' to None # to maintain output shape consistency else : names += [ \"top\" , \"freq\" ] result += [ np . nan , np . nan ] dtype = \"object\" return pd . Series ( result , index = names , name = data . name , dtype = dtype ) def describe_timestamp_1d ( data ): # GH-30164 stat_index = [ \"count\" , \"mean\" , \"min\" ] + formatted_percentiles + [ \"max\" ] d = ( [ data . count (), data . mean (), data . min ()] + data . quantile ( percentiles ). tolist () + [ data . max ()] ) return pd . Series ( d , index = stat_index , name = data . name ) def describe_1d ( data ): if is_bool_dtype ( data . dtype ): return describe_categorical_1d ( data ) elif is_numeric_dtype ( data ): return describe_numeric_1d ( data ) elif is_datetime64_any_dtype ( data . dtype ) and datetime_is_numeric : return describe_timestamp_1d ( data ) elif is_timedelta64_dtype ( data . dtype ): return describe_numeric_1d ( data ) else : return describe_categorical_1d ( data ) if self . ndim == 1 : return describe_1d ( self ) elif ( include is None ) and ( exclude is None ): # when some numerics are found, keep only numerics default_include = [ np . number ] if datetime_is_numeric : default_include . append ( \"datetime\" ) data = self . select_dtypes ( include = default_include ) if len ( data . columns ) == 0 : data = self elif include == \"all\" : if exclude is not None : msg = \"exclude must be None when include is 'all'\" raise ValueError ( msg ) data = self else : data = self . select_dtypes ( include = include , exclude = exclude ) ldesc = [ describe_1d ( s ) for _ , s in data . items ()] # set a convenient order for rows names : List [ Label ] = [] ldesc_indexes = sorted (( x . index for x in ldesc ), key = len ) for idxnames in ldesc_indexes : for name in idxnames : if name not in names : names . append ( name ) d = pd . concat ([ x . reindex ( names , copy = False ) for x in ldesc ], axis = 1 , sort = False ) d . columns = data . columns . copy () return d","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#diff","text":"def diff ( self , periods : int = 1 , axis : Union [ str , int ] = 0 ) -> 'DataFrame' First discrete difference of element. Calculates the difference of a Dataframe element compared with another element in the Dataframe (default is element in previous row).","title":"diff"},{"location":"reference/hielen2/datalink_prova_df/#parameters_36","text":"periods : int, default 1 Periods to shift for calculating difference, accepts negative values. axis : {0 or 'index', 1 or 'columns'}, default 0 Take difference over rows (0) or columns (1).","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_42","text":"Dataframe First differences of the Series.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_45","text":"Dataframe.pct_change: Percent change over given number of periods. Dataframe.shift: Shift index by desired number of periods with an optional time freq. Series.diff: First discrete difference of object.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_16","text":"For boolean dtypes, this uses :meth: operator.xor rather than :meth: operator.sub . The result is calculated according to current dtype in Dataframe, however dtype of the result is always float64.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_47","text":"Difference with previous row df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6], ... 'b': [1, 1, 2, 3, 5, 8], ... 'c': [1, 4, 9, 16, 25, 36]}) df a b c 0 1 1 1 1 2 1 4 2 3 2 9 3 4 3 16 4 5 5 25 5 6 8 36 df.diff() a b c 0 NaN NaN NaN 1 1.0 0.0 3.0 2 1.0 1.0 5.0 3 1.0 1.0 7.0 4 1.0 2.0 9.0 5 1.0 3.0 11.0 Difference with previous column df.diff(axis=1) a b c 0 NaN 0.0 0.0 1 NaN -1.0 3.0 2 NaN -1.0 7.0 3 NaN -1.0 13.0 4 NaN 0.0 20.0 5 NaN 2.0 28.0 Difference with 3rd previous row df.diff(periods=3) a b c 0 NaN NaN NaN 1 NaN NaN NaN 2 NaN NaN NaN 3 3.0 2.0 15.0 4 3.0 4.0 21.0 5 3.0 6.0 27.0 Difference with following row df.diff(periods=-1) a b c 0 -1.0 0.0 -3.0 1 -1.0 -1.0 -5.0 2 -1.0 -1.0 -7.0 3 -1.0 -2.0 -9.0 4 -1.0 -3.0 -11.0 5 NaN NaN NaN Overflow in input dtype df = pd.DataFrame({'a': [1, 0]}, dtype=np.uint8) df.diff() a 0 NaN 1 255.0 View Source @doc ( Series . diff , klass = \"Dataframe\" , extra_params = \"axis : {0 or 'index', 1 or 'columns'}, default 0\\n \" \"Take difference over rows (0) or columns (1).\\n\" , other_klass = \"Series\" , examples = dedent ( \"\"\" Difference with previous row >>> df = pd.DataFrame({'a': [1, 2, 3, 4, 5, 6], ... 'b': [1, 1, 2, 3, 5, 8], ... 'c': [1, 4, 9, 16, 25, 36]}) >>> df a b c 0 1 1 1 1 2 1 4 2 3 2 9 3 4 3 16 4 5 5 25 5 6 8 36 >>> df.diff() a b c 0 NaN NaN NaN 1 1.0 0.0 3.0 2 1.0 1.0 5.0 3 1.0 1.0 7.0 4 1.0 2.0 9.0 5 1.0 3.0 11.0 Difference with previous column >>> df.diff(axis=1) a b c 0 NaN 0.0 0.0 1 NaN -1.0 3.0 2 NaN -1.0 7.0 3 NaN -1.0 13.0 4 NaN 0.0 20.0 5 NaN 2.0 28.0 Difference with 3rd previous row >>> df.diff(periods=3) a b c 0 NaN NaN NaN 1 NaN NaN NaN 2 NaN NaN NaN 3 3.0 2.0 15.0 4 3.0 4.0 21.0 5 3.0 6.0 27.0 Difference with following row >>> df.diff(periods=-1) a b c 0 -1.0 0.0 -3.0 1 -1.0 -1.0 -5.0 2 -1.0 -1.0 -7.0 3 -1.0 -2.0 -9.0 4 -1.0 -3.0 -11.0 5 NaN NaN NaN Overflow in input dtype >>> df = pd.DataFrame({'a': [1, 0]}, dtype=np.uint8) >>> df.diff() a 0 NaN 1 255.0\"\"\" ), ) def diff ( self , periods : int = 1 , axis : Axis = 0 ) -> \"DataFrame\" : bm_axis = self . _get_block_manager_axis ( axis ) self . _consolidate_inplace () if bm_axis == 0 and periods != 0 : return self . T . diff ( periods , axis = 0 ). T new_data = self . _mgr . diff ( n = periods , axis = bm_axis ) return self . _constructor ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#div","text":"def div ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Floating division of dataframe and other, element-wise (binary operator truediv ). Equivalent to dataframe / other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"div"},{"location":"reference/hielen2/datalink_prova_df/#parameters_37","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_43","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_46","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_17","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_48","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#divide","text":"def divide ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Floating division of dataframe and other, element-wise (binary operator truediv ). Equivalent to dataframe / other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"divide"},{"location":"reference/hielen2/datalink_prova_df/#parameters_38","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_44","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_47","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_18","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_49","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#dot","text":"def dot ( self , other ) Compute the matrix multiplication between the DataFrame and other. This method computes the matrix product between the DataFrame and the values of an other Series, DataFrame or a numpy array. It can also be called using self @ other in Python >= 3.5.","title":"dot"},{"location":"reference/hielen2/datalink_prova_df/#parameters_39","text":"other : Series, DataFrame or array-like The other object to compute the matrix product with.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_45","text":"Series or DataFrame If other is a Series, return the matrix product between self and other as a Series. If other is a DataFrame or a numpy.array, return the matrix product of self and other in a DataFrame of a np.array.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_48","text":"Series.dot: Similar method for Series.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_19","text":"The dimensions of DataFrame and other must be compatible in order to compute the matrix multiplication. In addition, the column names of DataFrame and the index of other must contain the same values, as they will be aligned prior to the multiplication. The dot method for Series computes the inner product, instead of the matrix product here.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_50","text":"Here we multiply a DataFrame with a Series. df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]]) s = pd.Series([1, 1, 2, 1]) df.dot(s) 0 -4 1 5 dtype: int64 Here we multiply a DataFrame with another DataFrame. other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]]) df.dot(other) 0 1 0 1 4 1 2 2 Note that the dot method give the same result as @ df @ other 0 1 0 1 4 1 2 2 The dot method works also if other is an np.array. arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]]) df.dot(arr) 0 1 0 1 4 1 2 2 Note how shuffling of the objects does not change the result. s2 = s.reindex([1, 0, 2, 3]) df.dot(s2) 0 -4 1 5 dtype: int64 View Source def dot ( self , other ): \"\"\" Compute the matrix multiplication between the DataFrame and other. This method computes the matrix product between the DataFrame and the values of an other Series, DataFrame or a numpy array. It can also be called using ``self @ other`` in Python >= 3.5. Parameters ---------- other : Series, DataFrame or array-like The other object to compute the matrix product with. Returns ------- Series or DataFrame If other is a Series, return the matrix product between self and other as a Series. If other is a DataFrame or a numpy.array, return the matrix product of self and other in a DataFrame of a np.array. See Also -------- Series.dot: Similar method for Series. Notes ----- The dimensions of DataFrame and other must be compatible in order to compute the matrix multiplication. In addition, the column names of DataFrame and the index of other must contain the same values, as they will be aligned prior to the multiplication. The dot method for Series computes the inner product, instead of the matrix product here. Examples -------- Here we multiply a DataFrame with a Series. >>> df = pd.DataFrame([[0, 1, -2, -1], [1, 1, 1, 1]]) >>> s = pd.Series([1, 1, 2, 1]) >>> df.dot(s) 0 -4 1 5 dtype: int64 Here we multiply a DataFrame with another DataFrame. >>> other = pd.DataFrame([[0, 1], [1, 2], [-1, -1], [2, 0]]) >>> df.dot(other) 0 1 0 1 4 1 2 2 Note that the dot method give the same result as @ >>> df @ other 0 1 0 1 4 1 2 2 The dot method works also if other is an np.array. >>> arr = np.array([[0, 1], [1, 2], [-1, -1], [2, 0]]) >>> df.dot(arr) 0 1 0 1 4 1 2 2 Note how shuffling of the objects does not change the result. >>> s2 = s.reindex([1, 0, 2, 3]) >>> df.dot(s2) 0 -4 1 5 dtype: int64 \"\"\" if isinstance ( other , ( Series , DataFrame )): common = self . columns . union ( other . index ) if len ( common ) > len ( self . columns ) or len ( common ) > len ( other . index ): raise ValueError ( \"matrices are not aligned\" ) left = self . reindex ( columns = common , copy = False ) right = other . reindex ( index = common , copy = False ) lvals = left . values rvals = right . _values else : left = self lvals = self . values rvals = np . asarray ( other ) if lvals . shape [ 1 ] != rvals . shape [ 0 ]: raise ValueError ( f \"Dot product shape mismatch, {lvals.shape} vs {rvals.shape}\" ) if isinstance ( other , DataFrame ): return self . _constructor ( np . dot ( lvals , rvals ), index = left . index , columns = other . columns ) elif isinstance ( other , Series ): return self . _constructor_sliced ( np . dot ( lvals , rvals ), index = left . index ) elif isinstance ( rvals , ( np . ndarray , Index )): result = np . dot ( lvals , rvals ) if result . ndim == 2 : return self . _constructor ( result , index = left . index ) else : return self . _constructor_sliced ( result , index = left . index ) else : # pragma : no cover raise TypeError ( f \"unsupported type: {type(other)}\" )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#drop","text":"def drop ( self , labels = None , axis = 0 , index = None , columns = None , level = None , inplace = False , errors = 'raise' ) Drop specified labels from rows or columns. Remove rows or columns by specifying label names and corresponding axis, or by specifying directly index or column names. When using a multi-index, labels on different levels can be removed by specifying the level.","title":"drop"},{"location":"reference/hielen2/datalink_prova_df/#parameters_40","text":"labels : single label or list-like Index or column labels to drop. axis : {0 or 'index', 1 or 'columns'}, default 0 Whether to drop labels from the index (0 or 'index') or columns (1 or 'columns'). index : single label or list-like Alternative to specifying axis ( labels, axis=0 is equivalent to index=labels ). columns : single label or list-like Alternative to specifying axis ( labels, axis=1 is equivalent to columns=labels ). level : int or level name, optional For MultiIndex, level from which the labels will be removed. inplace : bool, default False If False, return a copy. Otherwise, do operation inplace and return None. errors : {'ignore', 'raise'}, default 'raise' If 'ignore', suppress error and only existing labels are dropped.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_46","text":"DataFrame DataFrame without the removed index or column labels.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_5","text":"KeyError If any of the labels is not found in the selected axis.","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_49","text":"DataFrame.loc : Label-location based indexer for selection by label. DataFrame.dropna : Return DataFrame with labels on given axis omitted where (all or any) data are missing. DataFrame.drop_duplicates : Return DataFrame with duplicate rows removed, optionally only considering certain columns. Series.drop : Return Series with specified index labels removed.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_51","text":"df = pd.DataFrame(np.arange(12).reshape(3, 4), ... columns=['A', 'B', 'C', 'D']) df A B C D 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 Drop columns df.drop(['B', 'C'], axis=1) A D 0 0 3 1 4 7 2 8 11 df.drop(columns=['B', 'C']) A D 0 0 3 1 4 7 2 8 11 Drop a row by index df.drop([0, 1]) A B C D 2 8 9 10 11 Drop columns and/or rows of MultiIndex DataFrame midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'], ... ['speed', 'weight', 'length']], ... codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2], ... [0, 1, 2, 0, 1, 2, 0, 1, 2]]) df = pd.DataFrame(index=midx, columns=['big', 'small'], ... data=[[45, 30], [200, 100], [1.5, 1], [30, 20], ... [250, 150], [1.5, 0.8], [320, 250], ... [1, 0.8], [0.3, 0.2]]) df big small lama speed 45.0 30.0 weight 200.0 100.0 length 1.5 1.0 cow speed 30.0 20.0 weight 250.0 150.0 length 1.5 0.8 falcon speed 320.0 250.0 weight 1.0 0.8 length 0.3 0.2 df.drop(index='cow', columns='small') big lama speed 45.0 weight 200.0 length 1.5 falcon speed 320.0 weight 1.0 length 0.3 df.drop(index='length', level=1) big small lama speed 45.0 30.0 weight 200.0 100.0 cow speed 30.0 20.0 weight 250.0 150.0 falcon speed 320.0 250.0 weight 1.0 0.8 View Source def drop ( self , labels = None , axis = 0 , index = None , columns = None , level = None , inplace = False , errors = \"raise\" , ): \"\"\" Drop specified labels from rows or columns. Remove rows or columns by specifying label names and corresponding axis, or by specifying directly index or column names. When using a multi-index, labels on different levels can be removed by specifying the level. Parameters ---------- labels : single label or list-like Index or column labels to drop. axis : {0 or 'index', 1 or 'columns'}, default 0 Whether to drop labels from the index (0 or 'index') or columns (1 or 'columns'). index : single label or list-like Alternative to specifying axis (``labels, axis=0`` is equivalent to ``index=labels``). columns : single label or list-like Alternative to specifying axis (``labels, axis=1`` is equivalent to ``columns=labels``). level : int or level name, optional For MultiIndex, level from which the labels will be removed. inplace : bool, default False If False, return a copy. Otherwise, do operation inplace and return None. errors : {'ignore', 'raise'}, default 'raise' If 'ignore', suppress error and only existing labels are dropped. Returns ------- DataFrame DataFrame without the removed index or column labels. Raises ------ KeyError If any of the labels is not found in the selected axis. See Also -------- DataFrame.loc : Label-location based indexer for selection by label. DataFrame.dropna : Return DataFrame with labels on given axis omitted where (all or any) data are missing. DataFrame.drop_duplicates : Return DataFrame with duplicate rows removed, optionally only considering certain columns. Series.drop : Return Series with specified index labels removed. Examples -------- >>> df = pd.DataFrame(np.arange(12).reshape(3, 4), ... columns=['A', 'B', 'C', 'D']) >>> df A B C D 0 0 1 2 3 1 4 5 6 7 2 8 9 10 11 Drop columns >>> df.drop(['B', 'C'], axis=1) A D 0 0 3 1 4 7 2 8 11 >>> df.drop(columns=['B', 'C']) A D 0 0 3 1 4 7 2 8 11 Drop a row by index >>> df.drop([0, 1]) A B C D 2 8 9 10 11 Drop columns and/or rows of MultiIndex DataFrame >>> midx = pd.MultiIndex(levels=[['lama', 'cow', 'falcon'], ... ['speed', 'weight', 'length']], ... codes=[[0, 0, 0, 1, 1, 1, 2, 2, 2], ... [0, 1, 2, 0, 1, 2, 0, 1, 2]]) >>> df = pd.DataFrame(index=midx, columns=['big', 'small'], ... data=[[45, 30], [200, 100], [1.5, 1], [30, 20], ... [250, 150], [1.5, 0.8], [320, 250], ... [1, 0.8], [0.3, 0.2]]) >>> df big small lama speed 45.0 30.0 weight 200.0 100.0 length 1.5 1.0 cow speed 30.0 20.0 weight 250.0 150.0 length 1.5 0.8 falcon speed 320.0 250.0 weight 1.0 0.8 length 0.3 0.2 >>> df.drop(index='cow', columns='small') big lama speed 45.0 weight 200.0 length 1.5 falcon speed 320.0 weight 1.0 length 0.3 >>> df.drop(index='length', level=1) big small lama speed 45.0 30.0 weight 200.0 100.0 cow speed 30.0 20.0 weight 250.0 150.0 falcon speed 320.0 250.0 weight 1.0 0.8 \"\"\" return super (). drop ( labels = labels , axis = axis , index = index , columns = columns , level = level , inplace = inplace , errors = errors , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#drop_duplicates","text":"def drop_duplicates ( self , subset : Union [ Hashable , Sequence [ Hashable ], NoneType ] = None , keep : Union [ str , bool ] = 'first' , inplace : bool = False , ignore_index : bool = False ) -> Union [ ForwardRef ( 'DataFrame' ), NoneType ] Return DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored.","title":"drop_duplicates"},{"location":"reference/hielen2/datalink_prova_df/#parameters_41","text":"subset : column label or sequence of labels, optional Only consider certain columns for identifying duplicates, by default use all of the columns. keep : {'first', 'last', False}, default 'first' Determines which duplicates (if any) to keep. - first : Drop duplicates except for the first occurrence. - last : Drop duplicates except for the last occurrence. - False : Drop all duplicates. inplace : bool, default False Whether to drop duplicates in place or to return a copy. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. .. versionadded :: 1.0.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_47","text":"DataFrame DataFrame with duplicates removed or None if inplace=True .","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_50","text":"DataFrame.value_counts: Count unique combinations of columns.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_52","text":"Consider dataset containing ramen rating. df = pd.DataFrame({ ... 'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'], ... 'style': ['cup', 'cup', 'cup', 'pack', 'pack'], ... 'rating': [4, 4, 3.5, 15, 5] ... }) df brand style rating 0 Yum Yum cup 4.0 1 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 By default, it removes duplicate rows based on all columns. df.drop_duplicates() brand style rating 0 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 To remove duplicates on specific column(s), use subset . df.drop_duplicates(subset=['brand']) brand style rating 0 Yum Yum cup 4.0 2 Indomie cup 3.5 To remove duplicates and keep last occurences, use keep . df.drop_duplicates(subset=['brand', 'style'], keep='last') brand style rating 1 Yum Yum cup 4.0 2 Indomie cup 3.5 4 Indomie pack 5.0 View Source def drop_duplicates ( self , subset : Optional [ Union [ Hashable , Sequence [ Hashable ]]] = None , keep : Union [ str , bool ] = \"first\" , inplace : bool = False , ignore_index : bool = False , ) -> Optional [ \"DataFrame\" ]: \"\"\" Return DataFrame with duplicate rows removed. Considering certain columns is optional. Indexes, including time indexes are ignored. Parameters ---------- subset : column label or sequence of labels, optional Only consider certain columns for identifying duplicates, by default use all of the columns. keep : {'first', 'last', False}, default 'first' Determines which duplicates (if any) to keep. - ``first`` : Drop duplicates except for the first occurrence. - ``last`` : Drop duplicates except for the last occurrence. - False : Drop all duplicates. inplace : bool, default False Whether to drop duplicates in place or to return a copy. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. .. versionadded:: 1.0.0 Returns ------- DataFrame DataFrame with duplicates removed or None if ``inplace=True``. See Also -------- DataFrame.value_counts: Count unique combinations of columns. Examples -------- Consider dataset containing ramen rating. >>> df = pd.DataFrame({ ... 'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'], ... 'style': ['cup', 'cup', 'cup', 'pack', 'pack'], ... 'rating': [4, 4, 3.5, 15, 5] ... }) >>> df brand style rating 0 Yum Yum cup 4.0 1 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 By default, it removes duplicate rows based on all columns. >>> df.drop_duplicates() brand style rating 0 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 To remove duplicates on specific column(s), use ``subset``. >>> df.drop_duplicates(subset=['brand']) brand style rating 0 Yum Yum cup 4.0 2 Indomie cup 3.5 To remove duplicates and keep last occurences, use ``keep``. >>> df.drop_duplicates(subset=['brand', 'style'], keep='last') brand style rating 1 Yum Yum cup 4.0 2 Indomie cup 3.5 4 Indomie pack 5.0 \"\"\" if self . empty : return self . copy () inplace = validate_bool_kwarg ( inplace , \"inplace\" ) duplicated = self . duplicated ( subset , keep = keep ) result = self [ - duplicated ] if ignore_index : result . index = ibase . default_index ( len ( result )) if inplace : self . _update_inplace ( result ) return None else : return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#droplevel","text":"def droplevel ( self : ~ FrameOrSeries , level , axis = 0 ) -> ~ FrameOrSeries Return DataFrame with requested index / column level(s) removed. .. versionadded:: 0.24.0","title":"droplevel"},{"location":"reference/hielen2/datalink_prova_df/#parameters_42","text":"level : int, str, or list-like If a string is given, must be the name of a level If list-like, elements must be names or positional indexes of levels. axis : {0 or 'index', 1 or 'columns'}, default 0 Axis along which the level(s) is removed: * 0 or 'index': remove level(s) in column. * 1 or 'columns': remove level(s) in row.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_48","text":"DataFrame DataFrame with requested index / column level(s) removed.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#examples_53","text":"df = pd.DataFrame([ ... [1, 2, 3, 4], ... [5, 6, 7, 8], ... [9, 10, 11, 12] ... ]).set_index([0, 1]).rename_axis(['a', 'b']) df.columns = pd.MultiIndex.from_tuples([ ... ('c', 'e'), ('d', 'f') ... ], names=['level_1', 'level_2']) df level_1 c d level_2 e f a b 1 2 3 4 5 6 7 8 9 10 11 12 df.droplevel('a') level_1 c d level_2 e f b 2 3 4 6 7 8 10 11 12 df.droplevel('level_2', axis=1) level_1 c d a b 1 2 3 4 5 6 7 8 9 10 11 12 View Source def droplevel ( self : FrameOrSeries , level , axis = 0 ) -> FrameOrSeries : \"\"\" Return DataFrame with requested index / column level(s) removed. .. versionadded:: 0.24.0 Parameters ---------- level : int, str, or list-like If a string is given, must be the name of a level If list-like, elements must be names or positional indexes of levels. axis : {0 or 'index', 1 or 'columns'}, default 0 Axis along which the level(s) is removed: * 0 or 'index': remove level(s) in column. * 1 or 'columns': remove level(s) in row. Returns ------- DataFrame DataFrame with requested index / column level(s) removed. Examples -------- >>> df = pd.DataFrame([ ... [1, 2, 3, 4], ... [5, 6, 7, 8], ... [9, 10, 11, 12] ... ]).set_index([0, 1]).rename_axis(['a', 'b']) >>> df.columns = pd.MultiIndex.from_tuples([ ... ('c', 'e'), ('d', 'f') ... ], names=['level_1', 'level_2']) >>> df level_1 c d level_2 e f a b 1 2 3 4 5 6 7 8 9 10 11 12 >>> df.droplevel('a') level_1 c d level_2 e f b 2 3 4 6 7 8 10 11 12 >>> df.droplevel('level_2', axis=1) level_1 c d a b 1 2 3 4 5 6 7 8 9 10 11 12 \"\"\" labels = self . _get_axis ( axis ) new_labels = labels . droplevel ( level ) result = self . set_axis ( new_labels , axis = axis , inplace = False ) return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#dropna","text":"def dropna ( self , axis = 0 , how = 'any' , thresh = None , subset = None , inplace = False ) Remove missing values. See the :ref: User Guide <missing_data> for more on which values are considered missing, and how to work with missing data.","title":"dropna"},{"location":"reference/hielen2/datalink_prova_df/#parameters_43","text":"axis : {0 or 'index', 1 or 'columns'}, default 0 Determine if rows or columns which contain missing values are removed. * 0 , or 'index' : Drop rows which contain missing values . * 1 , or 'columns' : Drop columns which contain missing value . .. versionchanged :: 1 . 0 . 0 Pass tuple or list to drop on multiple axes . Only a single axis is allowed . how : {'any', 'all'}, default 'any' Determine if row or column is removed from DataFrame, when we have at least one NA or all NA. * 'any' : If any NA values are present, drop that row or column. * 'all' : If all values are NA, drop that row or column. thresh : int, optional Require that many non-NA values. subset : array-like, optional Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include. inplace : bool, default False If True, do operation inplace and return None.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_49","text":"DataFrame DataFrame with NA entries dropped from it.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_51","text":"DataFrame.isna: Indicate missing values. DataFrame.notna : Indicate existing (non-missing) values. DataFrame.fillna : Replace missing values. Series.dropna : Drop missing values. Index.dropna : Drop missing indices.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_54","text":"df = pd.DataFrame({\"name\": ['Alfred', 'Batman', 'Catwoman'], ... \"toy\": [np.nan, 'Batmobile', 'Bullwhip'], ... \"born\": [pd.NaT, pd.Timestamp(\"1940-04-25\"), ... pd.NaT]}) df name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Drop the rows where at least one element is missing. df.dropna() name toy born 1 Batman Batmobile 1940-04-25 Drop the columns where at least one element is missing. df.dropna(axis='columns') name 0 Alfred 1 Batman 2 Catwoman Drop the rows where all elements are missing. df.dropna(how='all') name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Keep only the rows with at least 2 non-NA values. df.dropna(thresh=2) name toy born 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Define in which columns to look for missing values. df.dropna(subset=['name', 'born']) name toy born 1 Batman Batmobile 1940-04-25 Keep the DataFrame with valid entries in the same variable. df.dropna(inplace=True) df name toy born 1 Batman Batmobile 1940-04-25 View Source def dropna ( self , axis = 0 , how = \"any\" , thresh = None , subset = None , inplace = False ) : \"\"\" Remove missing values. See the :ref:`User Guide <missing_data>` for more on which values are considered missing, and how to work with missing data. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 Determine if rows or columns which contain missing values are removed. * 0, or 'index' : Drop rows which contain missing values. * 1, or 'columns' : Drop columns which contain missing value. .. versionchanged:: 1.0.0 Pass tuple or list to drop on multiple axes. Only a single axis is allowed. how : {'any', 'all'}, default 'any' Determine if row or column is removed from DataFrame, when we have at least one NA or all NA. * 'any' : If any NA values are present, drop that row or column. * 'all' : If all values are NA, drop that row or column. thresh : int, optional Require that many non-NA values. subset : array-like, optional Labels along other axis to consider, e.g. if you are dropping rows these would be a list of columns to include. inplace : bool, default False If True, do operation inplace and return None. Returns ------- DataFrame DataFrame with NA entries dropped from it. See Also -------- DataFrame.isna: Indicate missing values. DataFrame.notna : Indicate existing (non-missing) values. DataFrame.fillna : Replace missing values. Series.dropna : Drop missing values. Index.dropna : Drop missing indices. Examples -------- >>> df = pd.DataFrame({\" name \": ['Alfred', 'Batman', 'Catwoman'], ... \" toy \": [np.nan, 'Batmobile', 'Bullwhip'], ... \" born \": [pd.NaT, pd.Timestamp(\" 1940 - 04 - 25 \"), ... pd.NaT]}) >>> df name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Drop the rows where at least one element is missing. >>> df.dropna() name toy born 1 Batman Batmobile 1940-04-25 Drop the columns where at least one element is missing. >>> df.dropna(axis='columns') name 0 Alfred 1 Batman 2 Catwoman Drop the rows where all elements are missing. >>> df.dropna(how='all') name toy born 0 Alfred NaN NaT 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Keep only the rows with at least 2 non-NA values. >>> df.dropna(thresh=2) name toy born 1 Batman Batmobile 1940-04-25 2 Catwoman Bullwhip NaT Define in which columns to look for missing values. >>> df.dropna(subset=['name', 'born']) name toy born 1 Batman Batmobile 1940-04-25 Keep the DataFrame with valid entries in the same variable. >>> df.dropna(inplace=True) >>> df name toy born 1 Batman Batmobile 1940-04-25 \"\"\" inplace = validate_bool_kwarg ( inplace , \"inplace\" ) if isinstance ( axis , ( tuple , list )) : # GH20987 raise TypeError ( \"supplying multiple axes to axis is no longer supported.\" ) axis = self . _get_axis_number ( axis ) agg_axis = 1 - axis agg_obj = self if subset is not None : ax = self . _get_axis ( agg_axis ) indices = ax . get_indexer_for ( subset ) check = indices == - 1 if check . any () : raise KeyError ( list ( np . compress ( check , subset ))) agg_obj = self . take ( indices , axis = agg_axis ) count = agg_obj . count ( axis = agg_axis ) if thresh is not None : mask = count >= thresh elif how == \"any\" : mask = count == len ( agg_obj . _get_axis ( agg_axis )) elif how == \"all\" : mask = count > 0 else : if how is not None : raise ValueError ( f \"invalid how option: {how}\" ) else : raise TypeError ( \"must specify how or thresh\" ) result = self . loc ( axis = axis ) [ mask ] if inplace : self . _update_inplace ( result ) else : return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#duplicated","text":"def duplicated ( self , subset : Union [ Hashable , Sequence [ Hashable ], NoneType ] = None , keep : Union [ str , bool ] = 'first' ) -> 'Series' Return boolean Series denoting duplicate rows. Considering certain columns is optional.","title":"duplicated"},{"location":"reference/hielen2/datalink_prova_df/#parameters_44","text":"subset : column label or sequence of labels, optional Only consider certain columns for identifying duplicates, by default use all of the columns. keep : {'first', 'last', False}, default 'first' Determines which duplicates (if any) to mark. - `` first `` : Mark duplicates as `` True `` except for the first occurrence . - `` last `` : Mark duplicates as `` True `` except for the last occurrence . - False : Mark all duplicates as `` True `` .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_50","text":"Series Boolean series for each duplicated rows.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_52","text":"Index.duplicated : Equivalent method on index. Series.duplicated : Equivalent method on Series. Series.drop_duplicates : Remove duplicate values from Series. DataFrame.drop_duplicates : Remove duplicate values from DataFrame.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_55","text":"Consider dataset containing ramen rating. df = pd.DataFrame({ ... 'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'], ... 'style': ['cup', 'cup', 'cup', 'pack', 'pack'], ... 'rating': [4, 4, 3.5, 15, 5] ... }) df brand style rating 0 Yum Yum cup 4.0 1 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 By default, for each set of duplicated values, the first occurrence is set on False and all others on True. df.duplicated() 0 False 1 True 2 False 3 False 4 False dtype: bool By using 'last', the last occurrence of each set of duplicated values is set on False and all others on True. df.duplicated(keep='last') 0 True 1 False 2 False 3 False 4 False dtype: bool By setting keep on False, all duplicates are True. df.duplicated(keep=False) 0 True 1 True 2 False 3 False 4 False dtype: bool To find duplicates on specific column(s), use subset . df.duplicated(subset=['brand']) 0 False 1 True 2 False 3 True 4 True dtype: bool View Source def duplicated ( self , subset : Optional [ Union [ Hashable , Sequence [ Hashable ]]] = None , keep : Union [ str , bool ] = \"first\" , ) -> \"Series\" : \"\"\" Return boolean Series denoting duplicate rows. Considering certain columns is optional. Parameters ---------- subset : column label or sequence of labels, optional Only consider certain columns for identifying duplicates, by default use all of the columns. keep : {'first', 'last', False}, default 'first' Determines which duplicates (if any) to mark. - ``first`` : Mark duplicates as ``True`` except for the first occurrence. - ``last`` : Mark duplicates as ``True`` except for the last occurrence. - False : Mark all duplicates as ``True``. Returns ------- Series Boolean series for each duplicated rows. See Also -------- Index.duplicated : Equivalent method on index. Series.duplicated : Equivalent method on Series. Series.drop_duplicates : Remove duplicate values from Series. DataFrame.drop_duplicates : Remove duplicate values from DataFrame. Examples -------- Consider dataset containing ramen rating. >>> df = pd.DataFrame({ ... 'brand': ['Yum Yum', 'Yum Yum', 'Indomie', 'Indomie', 'Indomie'], ... 'style': ['cup', 'cup', 'cup', 'pack', 'pack'], ... 'rating': [4, 4, 3.5, 15, 5] ... }) >>> df brand style rating 0 Yum Yum cup 4.0 1 Yum Yum cup 4.0 2 Indomie cup 3.5 3 Indomie pack 15.0 4 Indomie pack 5.0 By default, for each set of duplicated values, the first occurrence is set on False and all others on True. >>> df.duplicated() 0 False 1 True 2 False 3 False 4 False dtype: bool By using 'last', the last occurrence of each set of duplicated values is set on False and all others on True. >>> df.duplicated(keep='last') 0 True 1 False 2 False 3 False 4 False dtype: bool By setting ``keep`` on False, all duplicates are True. >>> df.duplicated(keep=False) 0 True 1 True 2 False 3 False 4 False dtype: bool To find duplicates on specific column(s), use ``subset``. >>> df.duplicated(subset=['brand']) 0 False 1 True 2 False 3 True 4 True dtype: bool \"\"\" from pandas . _libs . hashtable import _SIZE_HINT_LIMIT , duplicated_int64 from pandas . core . sorting import get_group_index if self . empty : return self . _constructor_sliced ( dtype = bool ) def f ( vals ): labels , shape = algorithms . factorize ( vals , size_hint = min ( len ( self ), _SIZE_HINT_LIMIT ) ) return labels . astype ( \"i8\" , copy = False ), len ( shape ) if subset is None : subset = self . columns elif ( not np . iterable ( subset ) or isinstance ( subset , str ) or isinstance ( subset , tuple ) and subset in self . columns ): subset = ( subset ,) # needed for mypy since can't narrow types using np.iterable subset = cast ( Iterable , subset ) # Verify all columns in subset exist in the queried dataframe # Otherwise, raise a KeyError, same as if you try to __getitem__ with a # key that doesn't exist. diff = Index ( subset ). difference ( self . columns ) if not diff . empty : raise KeyError ( diff ) vals = ( col . values for name , col in self . items () if name in subset ) labels , shape = map ( list , zip ( * map ( f , vals ))) ids = get_group_index ( labels , shape , sort = False , xnull = False ) return self . _constructor_sliced ( duplicated_int64 ( ids , keep ), index = self . index )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#eq","text":"def eq ( self , other , axis = 'columns' , level = None ) Get Equal to of dataframe and other, element-wise (binary operator eq ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , =! , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison.","title":"eq"},{"location":"reference/hielen2/datalink_prova_df/#parameters_45","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'}, default 'columns' Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_51","text":"DataFrame of bool Result of the comparison.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_53","text":"DataFrame.eq : Compare DataFrames for equality elementwise. DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_20","text":"Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ).","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_56","text":"df = pd.DataFrame({'cost': [250, 150, 100], ... 'revenue': [100, 250, 300]}, ... index=['A', 'B', 'C']) df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: df == 100 cost revenue A False True B False False C True False df.eq(100) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: df != pd.Series([100, 250], index=[\"cost\", \"revenue\"]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index') cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : df == [250, 100] cost revenue A True True B False False C False False Use the method to control the axis: df.eq([250, 250, 100], axis='index') cost revenue A True False B False True C True False Compare to a DataFrame of different shape. other = pd.DataFrame({'revenue': [300, 250, 100, 150]}, ... index=['A', 'B', 'C', 'D']) other revenue A 300 B 250 C 100 D 150 df.gt(other) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220], ... 'revenue': [100, 250, 300, 200, 175, 225]}, ... index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'], ... ['A', 'B', 'C', 'A', 'B', 'C']]) df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 df.le(df_multindex, level=1) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None ) : axis = self . _get_axis_number ( axis ) if axis is not None else 1 self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) new_data = dispatch_to_series ( self , other , op , axis = axis ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#equals","text":"def equals ( self , other ) Test whether two objects contain the same elements. This function allows two Series or DataFrames to be compared against each other to see if they have the same shape and elements. NaNs in the same location are considered equal. The column headers do not need to have the same type, but the elements within the columns must be the same dtype.","title":"equals"},{"location":"reference/hielen2/datalink_prova_df/#parameters_46","text":"other : Series or DataFrame The other Series or DataFrame to be compared with the first.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_52","text":"bool True if all elements are the same in both objects, False otherwise.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_54","text":"Series.eq : Compare two Series objects of the same length and return a Series where each element is True if the element in each Series is equal, False otherwise. DataFrame.eq : Compare two DataFrame objects of the same shape and return a DataFrame where each element is True if the respective element in each DataFrame is equal, False otherwise. testing.assert_series_equal : Raises an AssertionError if left and right are not equal. Provides an easy interface to ignore inequality in dtypes, indexes and precision among others. testing.assert_frame_equal : Like assert_series_equal, but targets DataFrames. numpy.array_equal : Return True if two arrays have the same shape and elements, False otherwise.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_21","text":"This function requires that the elements have the same dtype as their respective elements in the other Series or DataFrame. However, the column labels do not need to have the same type, as long as they are still considered equal.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_57","text":"df = pd.DataFrame({1: [10], 2: [20]}) df 1 2 0 10 20 DataFrames df and exactly_equal have the same types and values for their elements and column labels, which will return True. exactly_equal = pd.DataFrame({1: [10], 2: [20]}) exactly_equal 1 2 0 10 20 df.equals(exactly_equal) True DataFrames df and different_column_type have the same element types and values, but have different types for the column labels, which will still return True. different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]}) different_column_type 1.0 2.0 0 10 20 df.equals(different_column_type) True DataFrames df and different_data_type have different types for the same values for their elements, and will return False even though their column labels are the same values and types. different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]}) different_data_type 1 2 0 10.0 20.0 df.equals(different_data_type) False View Source def equals ( self , other ): \"\"\" Test whether two objects contain the same elements. This function allows two Series or DataFrames to be compared against each other to see if they have the same shape and elements. NaNs in the same location are considered equal. The column headers do not need to have the same type, but the elements within the columns must be the same dtype. Parameters ---------- other : Series or DataFrame The other Series or DataFrame to be compared with the first. Returns ------- bool True if all elements are the same in both objects, False otherwise. See Also -------- Series.eq : Compare two Series objects of the same length and return a Series where each element is True if the element in each Series is equal, False otherwise. DataFrame.eq : Compare two DataFrame objects of the same shape and return a DataFrame where each element is True if the respective element in each DataFrame is equal, False otherwise. testing.assert_series_equal : Raises an AssertionError if left and right are not equal. Provides an easy interface to ignore inequality in dtypes, indexes and precision among others. testing.assert_frame_equal : Like assert_series_equal, but targets DataFrames. numpy.array_equal : Return True if two arrays have the same shape and elements, False otherwise. Notes ----- This function requires that the elements have the same dtype as their respective elements in the other Series or DataFrame. However, the column labels do not need to have the same type, as long as they are still considered equal. Examples -------- >>> df = pd.DataFrame({1: [10], 2: [20]}) >>> df 1 2 0 10 20 DataFrames df and exactly_equal have the same types and values for their elements and column labels, which will return True. >>> exactly_equal = pd.DataFrame({1: [10], 2: [20]}) >>> exactly_equal 1 2 0 10 20 >>> df.equals(exactly_equal) True DataFrames df and different_column_type have the same element types and values, but have different types for the column labels, which will still return True. >>> different_column_type = pd.DataFrame({1.0: [10], 2.0: [20]}) >>> different_column_type 1.0 2.0 0 10 20 >>> df.equals(different_column_type) True DataFrames df and different_data_type have different types for the same values for their elements, and will return False even though their column labels are the same values and types. >>> different_data_type = pd.DataFrame({1: [10.0], 2: [20.0]}) >>> different_data_type 1 2 0 10.0 20.0 >>> df.equals(different_data_type) False \"\"\" if not ( isinstance ( other , type ( self )) or isinstance ( self , type ( other ))): return False return self . _mgr . equals ( other . _mgr )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#eval","text":"def eval ( self , expr , inplace = False , ** kwargs ) Evaluate a string describing operations on DataFrame columns. Operates on columns only, not specific rows or elements. This allows eval to run arbitrary code, which can make you vulnerable to code injection if you pass user input to this function.","title":"eval"},{"location":"reference/hielen2/datalink_prova_df/#parameters_47","text":"expr : str The expression string to evaluate. inplace : bool, default False If the expression contains an assignment, whether to perform the operation inplace and mutate the existing DataFrame. Otherwise, a new DataFrame is returned. **kwargs See the documentation for :func: eval for complete details on the keyword arguments accepted by :meth: ~pandas.DataFrame.query .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_53","text":"ndarray, scalar, or pandas object The result of the evaluation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_55","text":"DataFrame.query : Evaluates a boolean expression to query the columns of a frame. DataFrame.assign : Can evaluate an expression or function to create new values for a column. eval : Evaluate a Python expression as a string using various backends.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_22","text":"For more details see the API documentation for :func: ~eval . For detailed examples see :ref: enhancing performance with eval <enhancingperf.eval> .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_58","text":"df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2)}) df A B 0 1 10 1 2 8 2 3 6 3 4 4 4 5 2 df.eval('A + B') 0 11 1 10 2 9 3 8 4 7 dtype: int64 Assignment is allowed though by default the original DataFrame is not modified. df.eval('C = A + B') A B C 0 1 10 11 1 2 8 10 2 3 6 9 3 4 4 8 4 5 2 7 df A B 0 1 10 1 2 8 2 3 6 3 4 4 4 5 2 Use inplace=True to modify the original DataFrame. df.eval('C = A + B', inplace=True) df A B C 0 1 10 11 1 2 8 10 2 3 6 9 3 4 4 8 4 5 2 7 Multiple columns can be assigned to using multi-line expressions: df.eval( ... ''' ... C = A + B ... D = A - B ... ''' ... ) A B C D 0 1 10 11 -9 1 2 8 10 -6 2 3 6 9 -3 3 4 4 8 0 4 5 2 7 3 View Source def eval ( self , expr , inplace = False , ** kwargs ): \"\"\" Evaluate a string describing operations on DataFrame columns. Operates on columns only, not specific rows or elements. This allows `eval` to run arbitrary code, which can make you vulnerable to code injection if you pass user input to this function. Parameters ---------- expr : str The expression string to evaluate. inplace : bool, default False If the expression contains an assignment, whether to perform the operation inplace and mutate the existing DataFrame. Otherwise, a new DataFrame is returned. **kwargs See the documentation for :func:`eval` for complete details on the keyword arguments accepted by :meth:`~pandas.DataFrame.query`. Returns ------- ndarray, scalar, or pandas object The result of the evaluation. See Also -------- DataFrame.query : Evaluates a boolean expression to query the columns of a frame. DataFrame.assign : Can evaluate an expression or function to create new values for a column. eval : Evaluate a Python expression as a string using various backends. Notes ----- For more details see the API documentation for :func:`~eval`. For detailed examples see :ref:`enhancing performance with eval <enhancingperf.eval>`. Examples -------- >>> df = pd.DataFrame({'A': range(1, 6), 'B': range(10, 0, -2)}) >>> df A B 0 1 10 1 2 8 2 3 6 3 4 4 4 5 2 >>> df.eval('A + B') 0 11 1 10 2 9 3 8 4 7 dtype: int64 Assignment is allowed though by default the original DataFrame is not modified. >>> df.eval('C = A + B') A B C 0 1 10 11 1 2 8 10 2 3 6 9 3 4 4 8 4 5 2 7 >>> df A B 0 1 10 1 2 8 2 3 6 3 4 4 4 5 2 Use ``inplace=True`` to modify the original DataFrame. >>> df.eval('C = A + B', inplace=True) >>> df A B C 0 1 10 11 1 2 8 10 2 3 6 9 3 4 4 8 4 5 2 7 Multiple columns can be assigned to using multi-line expressions: >>> df.eval( ... ''' ... C = A + B ... D = A - B ... ''' ... ) A B C D 0 1 10 11 -9 1 2 8 10 -6 2 3 6 9 -3 3 4 4 8 0 4 5 2 7 3 \"\"\" from pandas . core . computation . eval import eval as _eval inplace = validate_bool_kwarg ( inplace , \"inplace\" ) resolvers = kwargs . pop ( \"resolvers\" , None ) kwargs [ \"level\" ] = kwargs . pop ( \"level\" , 0 ) + 1 if resolvers is None : index_resolvers = self . _get_index_resolvers () column_resolvers = self . _get_cleaned_column_resolvers () resolvers = column_resolvers , index_resolvers if \"target\" not in kwargs : kwargs [ \"target\" ] = self kwargs [ \"resolvers\" ] = kwargs . get ( \"resolvers\" , ()) + tuple ( resolvers ) return _eval ( expr , inplace = inplace , ** kwargs )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#ewm","text":"def ewm ( self , com = None , span = None , halflife = None , alpha = None , min_periods = 0 , adjust = True , ignore_na = False , axis = 0 , times = None ) Provide exponential weighted (EW) functions. Available EW functions: mean() , var() , std() , corr() , cov() . Exactly one parameter: com , span , halflife , or alpha must be provided.","title":"ewm"},{"location":"reference/hielen2/datalink_prova_df/#parameters_48","text":"com : float, optional Specify decay in terms of center of mass, :math: \\alpha = 1 / (1 + com) , for :math: com \\geq 0 . span : float, optional Specify decay in terms of span, :math: \\alpha = 2 / (span + 1) , for :math: span \\geq 1 . halflife : float, str, timedelta, optional Specify decay in terms of half-life, :math: \\alpha = 1 - \\exp\\left(-\\ln(2) / halflife\\right) , for :math: halflife > 0 . If `` times `` is specified , the time unit ( str or timedelta ) over which an observation decays to half its value . Only applicable to `` mean () `` and halflife value will not apply to the other functions . .. versionadded :: 1 . 1 . 0 alpha : float, optional Specify smoothing factor :math: \\alpha directly, :math: 0 < \\alpha \\leq 1 . min_periods : int, default 0 Minimum number of observations in window required to have a value (otherwise result is NA). adjust : bool, default True Divide by decaying adjustment factor in beginning periods to account for imbalance in relative weightings (viewing EWMA as a moving average). - When `` adjust = True `` ( default ), the EW function is calculated using weights : math : ` w_i = ( 1 - \\ alpha ) ^ i ` . For example , the EW moving average of the series [: math : ` x_0 , x_1 , ..., x_t ` ] would be : .. math :: y_t = \\ frac { x_t + ( 1 - \\ alpha ) x_ { t - 1 } + ( 1 - \\ alpha ) ^ 2 x_ { t - 2 } + ... + ( 1 - \\ alpha ) ^ t x_0 }{ 1 + ( 1 - \\ alpha ) + ( 1 - \\ alpha ) ^ 2 + ... + ( 1 - \\ alpha ) ^ t } - When `` adjust = False `` , the exponentially weighted function is calculated recursively : .. math :: \\ begin { split } y_0 &= x_0 \\\\ y_t &= ( 1 - \\ alpha ) y_ { t - 1 } + \\ alpha x_t , \\ end { split } ignore_na : bool, default False Ignore missing values when calculating weights; specify True to reproduce pre-0.15.0 behavior. - When `` ignore_na = False `` ( default ), weights are based on absolute positions . For example , the weights of : math : `x_0` and : math : `x_2` used in calculating the final weighted average of [: math : `x_0` , None , : math : `x_2` ] are : math : `(1-\\alpha)^2` and : math : `1` if `` adjust = True `` , and : math : `(1-\\alpha)^2` and : math : `\\alpha` if `` adjust = False `` . - When `` ignore_na = True `` ( reproducing pre - 0 . 15 . 0 behavior ), weights are based on relative positions . For example , the weights of : math : `x_0` and : math : `x_2` used in calculating the final weighted average of [: math : `x_0` , None , : math : `x_2` ] are : math : `1-\\alpha` and : math : `1` if `` adjust = True `` , and : math : `1-\\alpha` and : math : `\\alpha` if `` adjust = False `` . axis : {0, 1}, default 0 The axis to use. The value 0 identifies the rows, and 1 identifies the columns. times : str, np.ndarray, Series, default None .. versionadded : : 1.1.0 Times corresponding to the observations . Must be monotonically increasing and `` datetime64 [ ns ] `` dtype . If str , the name of the column in the DataFrame representing the times . If 1 - D array like , a sequence with the same shape as the observations . Only applicable to `` mean () `` .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_54","text":"DataFrame A Window sub-classed for the particular operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_56","text":"rolling : Provides rolling window calculations. expanding : Provides expanding transformations.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_23","text":"More details can be found at: :ref: Exponentially weighted windows <stats.moments.exponentially_weighted> .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_59","text":"df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]}) df B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 df.ewm(com=0.5).mean() B 0 0.000000 1 0.750000 2 1.615385 3 1.615385 4 3.670213 Specifying times with a timedelta halflife when computing mean. times = ['2020-01-01', '2020-01-03', '2020-01-10', '2020-01-15', '2020-01-17'] df.ewm(halflife='4 days', times=pd.DatetimeIndex(times)).mean() B 0 0.000000 1 0.585786 2 1.523889 3 1.523889 4 3.233686 View Source @doc ( ExponentialMovingWindow ) def ewm ( self , com = None , span = None , halflife = None , alpha = None , min_periods = 0 , adjust = True , ignore_na = False , axis = 0 , times = None , ) : axis = self . _get_axis_number ( axis ) return ExponentialMovingWindow ( self , com = com , span = span , halflife = halflife , alpha = alpha , min_periods = min_periods , adjust = adjust , ignore_na = ignore_na , axis = axis , times = times , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#expanding","text":"def expanding ( self , min_periods = 1 , center = None , axis = 0 ) Provide expanding transformations.","title":"expanding"},{"location":"reference/hielen2/datalink_prova_df/#parameters_49","text":"min_periods : int, default 1 Minimum number of observations in window required to have a value (otherwise result is NA). center : bool, default False Set the labels at the center of the window. axis : int or str, default 0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_55","text":"a Window sub-classed for the particular operation","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_57","text":"rolling : Provides rolling window calculations. ewm : Provides exponential weighted functions.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_24","text":"By default, the result is set to the right edge of the window. This can be changed to the center of the window by setting center=True .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_60","text":"df = pd.DataFrame({\"B\": [0, 1, 2, np.nan, 4]}) df B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 df.expanding(2).sum() B 0 NaN 1 1.0 2 3.0 3 3.0 4 7.0 View Source @ doc ( Expanding ) def expanding ( self , min_periods = 1 , center = None , axis = 0 ): axis = self . _get_axis_number ( axis ) if center is not None : warnings . warn ( \"The `center` argument on `expanding` \" \"will be removed in the future\" , FutureWarning , stacklevel = 2 , ) else : center = False return Expanding ( self , min_periods = min_periods , center = center , axis = axis )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#explode","text":"def explode ( self , column : Union [ str , Tuple ], ignore_index : bool = False ) -> 'DataFrame' Transform each element of a list-like to a row, replicating index values. .. versionadded:: 0.25.0","title":"explode"},{"location":"reference/hielen2/datalink_prova_df/#parameters_50","text":"column : str or tuple Column to explode. ignore_index : bool, default False If True, the resulting index will be labeled 0, 1, \u2026, n - 1. .. versionadded :: 1.1.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_56","text":"DataFrame Exploded lists to rows of the subset columns; index will be duplicated for these rows.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_6","text":"ValueError : if columns of the frame are not unique.","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_58","text":"DataFrame.unstack : Pivot a level of the (necessarily hierarchical) index labels. DataFrame.melt : Unpivot a DataFrame from wide format to long format. Series.explode : Explode a DataFrame from list-like columns to long format.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_25","text":"This routine will explode list-likes including lists, tuples, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged. Empty list-likes will result in a np.nan for that row.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_61","text":"df = pd.DataFrame({'A': [[1, 2, 3], 'foo', [], [3, 4]], 'B': 1}) df A B 0 [1, 2, 3] 1 1 foo 1 2 [] 1 3 [3, 4] 1 df.explode('A') A B 0 1 1 0 2 1 0 3 1 1 foo 1 2 NaN 1 3 3 1 3 4 1 View Source def explode ( self , column : Union [ str, Tuple ] , ignore_index : bool = False ) -> \"DataFrame\" : \"\"\" Transform each element of a list-like to a row, replicating index values. .. versionadded:: 0.25.0 Parameters ---------- column : str or tuple Column to explode. ignore_index : bool, default False If True, the resulting index will be labeled 0, 1, \u2026, n - 1. .. versionadded:: 1.1.0 Returns ------- DataFrame Exploded lists to rows of the subset columns; index will be duplicated for these rows. Raises ------ ValueError : if columns of the frame are not unique. See Also -------- DataFrame.unstack : Pivot a level of the (necessarily hierarchical) index labels. DataFrame.melt : Unpivot a DataFrame from wide format to long format. Series.explode : Explode a DataFrame from list-like columns to long format. Notes ----- This routine will explode list-likes including lists, tuples, Series, and np.ndarray. The result dtype of the subset rows will be object. Scalars will be returned unchanged. Empty list-likes will result in a np.nan for that row. Examples -------- >>> df = pd.DataFrame({'A': [[1, 2, 3], 'foo', [], [3, 4]], 'B': 1}) >>> df A B 0 [1, 2, 3] 1 1 foo 1 2 [] 1 3 [3, 4] 1 >>> df.explode('A') A B 0 1 1 0 2 1 0 3 1 1 foo 1 2 NaN 1 3 3 1 3 4 1 \"\"\" if not ( is_scalar ( column ) or isinstance ( column , tuple )) : raise ValueError ( \"column must be a scalar\" ) if not self . columns . is_unique : raise ValueError ( \"columns must be unique\" ) df = self . reset_index ( drop = True ) # TODO : use overload to refine return type of reset_index assert df is not None # needed for mypy result = df [ column ] . explode () result = df . drop ( [ column ] , axis = 1 ). join ( result ) if ignore_index : result . index = ibase . default_index ( len ( result )) else : result . index = self . index . take ( result . index ) result = result . reindex ( columns = self . columns , copy = False ) return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#ffill","text":"def ffill ( self : ~ FrameOrSeries , axis = None , inplace : bool = False , limit = None , downcast = None ) -> Union [ ~ FrameOrSeries , NoneType ] Synonym for :meth: DataFrame.fillna with method='ffill' .","title":"ffill"},{"location":"reference/hielen2/datalink_prova_df/#returns_57","text":"{klass} or None Object with missing values filled or None if inplace=True . View Source def ffill ( self : FrameOrSeries , axis = None , inplace : bool_t = False , limit = None , downcast = None , ) -> Optional [ FrameOrSeries ] : \"\"\" Synonym for :meth:`DataFrame.fillna` with ``method='ffill'``. Returns ------- {klass} or None Object with missing values filled or None if ``inplace=True``. \"\"\" return self . fillna ( method = \"ffill\" , axis = axis , inplace = inplace , limit = limit , downcast = downcast )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#fillna","text":"def fillna ( self , value = None , method = None , axis = None , inplace = False , limit = None , downcast = None ) -> Union [ ForwardRef ( 'DataFrame' ), NoneType ] Fill NA/NaN values using the specified method.","title":"fillna"},{"location":"reference/hielen2/datalink_prova_df/#parameters_51","text":"value : scalar, dict, Series, or DataFrame Value to use to fill holes (e.g. 0), alternately a dict/Series/DataFrame of values specifying which value to use for each index (for a Series) or column (for a DataFrame). Values not in the dict/Series/DataFrame will not be filled. This value cannot be a list. method : {'backfill', 'bfill', 'pad', 'ffill', None}, default None Method to use for filling holes in reindexed Series pad / ffill: propagate last valid observation forward to next valid backfill / bfill: use next valid observation to fill gap. axis : {0 or 'index', 1 or 'columns'} Axis along which to fill missing values. inplace : bool, default False If True, fill in-place. Note: this will modify any other views on this object (e.g., a no-copy slice for a column in a DataFrame). limit : int, default None If method is specified, this is the maximum number of consecutive NaN values to forward/backward fill. In other words, if there is a gap with more than this number of consecutive NaNs, it will only be partially filled. If method is not specified, this is the maximum number of entries along the entire axis where NaNs will be filled. Must be greater than 0 if not None. downcast : dict, default is None A dict of item->dtype of what to downcast if possible, or the string 'infer' which will try to downcast to an appropriate equal type (e.g. float64 to int64 if possible).","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_58","text":"DataFrame or None Object with missing values filled or None if inplace=True .","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_59","text":"interpolate : Fill NaN values using interpolation. reindex : Conform object to new index. asfreq : Convert TimeSeries to specified frequency.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_62","text":"df = pd.DataFrame([[np.nan, 2, np.nan, 0], ... [3, 4, np.nan, 1], ... [np.nan, np.nan, np.nan, 5], ... [np.nan, 3, np.nan, 4]], ... columns=list('ABCD')) df A B C D 0 NaN 2.0 NaN 0 1 3.0 4.0 NaN 1 2 NaN NaN NaN 5 3 NaN 3.0 NaN 4 Replace all NaN elements with 0s. df.fillna(0) A B C D 0 0.0 2.0 0.0 0 1 3.0 4.0 0.0 1 2 0.0 0.0 0.0 5 3 0.0 3.0 0.0 4 We can also propagate non-null values forward or backward. df.fillna(method='ffill') A B C D 0 NaN 2.0 NaN 0 1 3.0 4.0 NaN 1 2 3.0 4.0 NaN 5 3 3.0 3.0 NaN 4 Replace all NaN elements in column 'A', 'B', 'C', and 'D', with 0, 1, 2, and 3 respectively. values = {'A': 0, 'B': 1, 'C': 2, 'D': 3} df.fillna(value=values) A B C D 0 0.0 2.0 2.0 0 1 3.0 4.0 2.0 1 2 0.0 1.0 2.0 5 3 0.0 3.0 2.0 4 Only replace the first NaN element. df.fillna(value=values, limit=1) A B C D 0 0.0 2.0 2.0 0 1 3.0 4.0 NaN 1 2 NaN 1.0 NaN 5 3 NaN 3.0 NaN 4 View Source @doc ( NDFrame . fillna , ** _shared_doc_kwargs ) def fillna ( self , value = None , method = None , axis = None , inplace = False , limit = None , downcast = None , ) -> Optional [ \"DataFrame\" ] : return super (). fillna ( value = value , method = method , axis = axis , inplace = inplace , limit = limit , downcast = downcast , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#filter","text":"def filter ( self : ~ FrameOrSeries , items = None , like : Union [ str , NoneType ] = None , regex : Union [ str , NoneType ] = None , axis = None ) -> ~ FrameOrSeries Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index.","title":"filter"},{"location":"reference/hielen2/datalink_prova_df/#parameters_52","text":"items : list-like Keep labels from axis which are in items. like : str Keep labels from axis for which \"like in label == True\". regex : str (regular expression) Keep labels from axis for which re.search(regex, label) == True. axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None The axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, 'index' for Series, 'columns' for DataFrame.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_59","text":"same type as input object","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_60","text":"DataFrame.loc : Access a group of rows and columns by label(s) or a boolean array.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_26","text":"The items , like , and regex parameters are enforced to be mutually exclusive. axis defaults to the info axis that is used when indexing with [] .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_63","text":"df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])), ... index=['mouse', 'rabbit'], ... columns=['one', 'two', 'three']) df one two three mouse 1 2 3 rabbit 4 5 6","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#select-columns-by-name","text":"df.filter(items=['one', 'three']) one three mouse 1 3 rabbit 4 6","title":"select columns by name"},{"location":"reference/hielen2/datalink_prova_df/#select-columns-by-regular-expression","text":"df.filter(regex='e$', axis=1) one three mouse 1 3 rabbit 4 6","title":"select columns by regular expression"},{"location":"reference/hielen2/datalink_prova_df/#select-rows-containing-bbi","text":"df.filter(like='bbi', axis=0) one two three rabbit 4 5 6 View Source def filter ( self : FrameOrSeries , items = None , like : Optional [ str ] = None , regex : Optional [ str ] = None , axis = None , ) -> FrameOrSeries : \"\"\" Subset the dataframe rows or columns according to the specified index labels. Note that this routine does not filter a dataframe on its contents. The filter is applied to the labels of the index. Parameters ---------- items : list-like Keep labels from axis which are in items. like : str Keep labels from axis for which \" like in label == True \". regex : str (regular expression) Keep labels from axis for which re.search(regex, label) == True. axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None The axis to filter on, expressed either as an index (int) or axis name (str). By default this is the info axis, 'index' for Series, 'columns' for DataFrame. Returns ------- same type as input object See Also -------- DataFrame.loc : Access a group of rows and columns by label(s) or a boolean array. Notes ----- The ``items``, ``like``, and ``regex`` parameters are enforced to be mutually exclusive. ``axis`` defaults to the info axis that is used when indexing with ``[]``. Examples -------- >>> df = pd.DataFrame(np.array(([1, 2, 3], [4, 5, 6])), ... index=['mouse', 'rabbit'], ... columns=['one', 'two', 'three']) >>> df one two three mouse 1 2 3 rabbit 4 5 6 >>> # select columns by name >>> df.filter(items=['one', 'three']) one three mouse 1 3 rabbit 4 6 >>> # select columns by regular expression >>> df.filter(regex='e$', axis=1) one three mouse 1 3 rabbit 4 6 >>> # select rows containing 'bbi' >>> df.filter(like='bbi', axis=0) one two three rabbit 4 5 6 \"\"\" nkw = com . count_not_none ( items , like , regex ) if nkw > 1 : raise TypeError ( \"Keyword arguments `items`, `like`, or `regex` \" \"are mutually exclusive\" ) if axis is None : axis = self . _info_axis_name labels = self . _get_axis ( axis ) if items is not None : name = self . _get_axis_name ( axis ) return self . reindex ( ** { name : [ r for r in items if r in labels ] } ) elif like : def f ( x ): return like in ensure_str ( x ) values = labels . map ( f ) return self . loc ( axis = axis )[ values ] elif regex : def f ( x ): return matcher . search ( ensure_str ( x )) is not None matcher = re . compile ( regex ) values = labels . map ( f ) return self . loc ( axis = axis )[ values ] else : raise TypeError ( \"Must pass either `items`, `like`, or `regex`\" )","title":"select rows containing 'bbi'"},{"location":"reference/hielen2/datalink_prova_df/#first","text":"def first ( self : ~ FrameOrSeries , offset ) -> ~ FrameOrSeries Select initial periods of time series data based on a date offset. When having a DataFrame with dates as index, this function can select the first few rows based on a date offset.","title":"first"},{"location":"reference/hielen2/datalink_prova_df/#parameters_53","text":"offset : str, DateOffset or dateutil.relativedelta The offset length of the data that will be selected. For instance, '1M' will display all the rows having their index within the first month.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_60","text":"Series or DataFrame A subset of the caller.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_7","text":"TypeError If the index is not a :class: DatetimeIndex","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_61","text":"last : Select final periods of time series based on a date offset. at_time : Select values at a particular time of the day. between_time : Select values between particular times of the day.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_64","text":"i = pd.date_range('2018-04-09', periods=4, freq='2D') ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) ts A 2018-04-09 1 2018-04-11 2 2018-04-13 3 2018-04-15 4 Get the rows for the first 3 days: ts.first('3D') A 2018-04-09 1 2018-04-11 2 Notice the data for 3 first calendar days were returned, not the first 3 days observed in the dataset, and therefore data for 2018-04-13 was not returned. View Source def first ( self : FrameOrSeries , offset ) -> FrameOrSeries : \"\"\" Select initial periods of time series data based on a date offset. When having a DataFrame with dates as index, this function can select the first few rows based on a date offset. Parameters ---------- offset : str, DateOffset or dateutil.relativedelta The offset length of the data that will be selected. For instance, '1M' will display all the rows having their index within the first month. Returns ------- Series or DataFrame A subset of the caller. Raises ------ TypeError If the index is not a :class:`DatetimeIndex` See Also -------- last : Select final periods of time series based on a date offset. at_time : Select values at a particular time of the day. between_time : Select values between particular times of the day. Examples -------- >>> i = pd.date_range('2018-04-09', periods=4, freq='2D') >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) >>> ts A 2018-04-09 1 2018-04-11 2 2018-04-13 3 2018-04-15 4 Get the rows for the first 3 days: >>> ts.first('3D') A 2018-04-09 1 2018-04-11 2 Notice the data for 3 first calendar days were returned, not the first 3 days observed in the dataset, and therefore data for 2018-04-13 was not returned. \"\"\" if not isinstance ( self . index , DatetimeIndex ): raise TypeError ( \"'first' only supports a DatetimeIndex index\" ) if len ( self . index ) == 0 : return self offset = to_offset ( offset ) end_date = end = self . index [ 0 ] + offset # Tick-like, e.g. 3 weeks if isinstance ( offset , Tick ): if end_date in self . index : end = self . index . searchsorted ( end_date , side = \"left\" ) return self . iloc [: end ] return self . loc [: end ]","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#first_valid_index","text":"def first_valid_index ( self ) Return index for first non-NA/null value.","title":"first_valid_index"},{"location":"reference/hielen2/datalink_prova_df/#returns_61","text":"scalar : type of index","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#notes_27","text":"If all elements are non-NA/null, returns None. Also returns None for empty Series/DataFrame. View Source @doc ( position = \"first\" , klass = _shared_doc_kwargs [ \"klass\" ] ) def first_valid_index ( self ) : \"\"\" Return index for {position} non-NA/null value. Returns ------- scalar : type of index Notes ----- If all elements are non-NA/null, returns None. Also returns None for empty {klass}. \"\"\" return self . _find_valid_index ( \"first\" )","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#floordiv","text":"def floordiv ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Integer division of dataframe and other, element-wise (binary operator floordiv ). Equivalent to dataframe // other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rfloordiv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"floordiv"},{"location":"reference/hielen2/datalink_prova_df/#parameters_54","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_62","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_62","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_28","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_65","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#ge","text":"def ge ( self , other , axis = 'columns' , level = None ) Get Greater than or equal to of dataframe and other, element-wise (binary operator ge ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , =! , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison.","title":"ge"},{"location":"reference/hielen2/datalink_prova_df/#parameters_55","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'}, default 'columns' Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_63","text":"DataFrame of bool Result of the comparison.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_63","text":"DataFrame.eq : Compare DataFrames for equality elementwise. DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_29","text":"Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ).","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_66","text":"df = pd.DataFrame({'cost': [250, 150, 100], ... 'revenue': [100, 250, 300]}, ... index=['A', 'B', 'C']) df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: df == 100 cost revenue A False True B False False C True False df.eq(100) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: df != pd.Series([100, 250], index=[\"cost\", \"revenue\"]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index') cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : df == [250, 100] cost revenue A True True B False False C False False Use the method to control the axis: df.eq([250, 250, 100], axis='index') cost revenue A True False B False True C True False Compare to a DataFrame of different shape. other = pd.DataFrame({'revenue': [300, 250, 100, 150]}, ... index=['A', 'B', 'C', 'D']) other revenue A 300 B 250 C 100 D 150 df.gt(other) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220], ... 'revenue': [100, 250, 300, 200, 175, 225]}, ... index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'], ... ['A', 'B', 'C', 'A', 'B', 'C']]) df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 df.le(df_multindex, level=1) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None ) : axis = self . _get_axis_number ( axis ) if axis is not None else 1 self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) new_data = dispatch_to_series ( self , other , op , axis = axis ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#get","text":"def get ( self , force_reload = False ) Get item from object for given key (ex: DataFrame column). Returns default value if not found.","title":"get"},{"location":"reference/hielen2/datalink_prova_df/#parameters_56","text":"key : object","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_64","text":"value : same type as items contained in object View Source def get ( self , force_reload = False ): self . __chk_and_reload_cache ( force = force_reload ) return self . db","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#groupby","text":"def groupby ( self , by = None , axis = 0 , level = None , as_index : bool = True , sort : bool = True , group_keys : bool = True , squeeze : bool = < object object at 0x7fb7e907ade0 > , observed : bool = False , dropna : bool = True ) -> 'DataFrameGroupBy' Group DataFrame using a mapper or by a Series of columns. A groupby operation involves some combination of splitting the object, applying a function, and combining the results. This can be used to group large amounts of data and compute operations on these groups.","title":"groupby"},{"location":"reference/hielen2/datalink_prova_df/#parameters_57","text":"by : mapping, function, label, or list of labels Used to determine the groups for the groupby. If by is a function, it's called on each value of the object's index. If a dict or Series is passed, the Series or dict VALUES will be used to determine the groups (the Series' values are first aligned; see .align() method). If an ndarray is passed, the values are used as-is determine the groups. A label or list of labels may be passed to group by the columns in self . Notice that a tuple is interpreted as a (single) key. axis : {0 or 'index', 1 or 'columns'}, default 0 Split along rows (0) or columns (1). level : int, level name, or sequence of such, default None If the axis is a MultiIndex (hierarchical), group by a particular level or levels. as_index : bool, default True For aggregated output, return object with group labels as the index. Only relevant for DataFrame input. as_index=False is effectively \"SQL-style\" grouped output. sort : bool, default True Sort group keys. Get better performance by turning this off. Note this does not influence the order of observations within each group. Groupby preserves the order of rows within each group. group_keys : bool, default True When calling apply, add group keys to index to identify pieces. squeeze : bool, default False Reduce the dimensionality of the return type if possible, otherwise return a consistent type. .. deprecated :: 1.1.0 observed : bool, default False This only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. .. versionadded :: 0.23.0 dropna : bool, default True If True, and if group keys contain NA values, NA values together with row/column will be dropped. If False, NA values will also be treated as the key in groups .. versionadded :: 1.1.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_65","text":"DataFrameGroupBy Returns a groupby object that contains information about the groups.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_64","text":"resample : Convenience method for frequency conversion and resampling of time series.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_30","text":"See the user guide <https://pandas.pydata.org/pandas-docs/stable/groupby.html> _ for more.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_67","text":"df = pd.DataFrame({'Animal': ['Falcon', 'Falcon', ... 'Parrot', 'Parrot'], ... 'Max Speed': [380., 370., 24., 26.]}) df Animal Max Speed 0 Falcon 380.0 1 Falcon 370.0 2 Parrot 24.0 3 Parrot 26.0 df.groupby(['Animal']).mean() Max Speed Animal Falcon 375.0 Parrot 25.0 Hierarchical Indexes We can groupby different levels of a hierarchical index using the level parameter: arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'], ... ['Captive', 'Wild', 'Captive', 'Wild']] index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type')) df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]}, ... index=index) df Max Speed Animal Type Falcon Captive 390.0 Wild 350.0 Parrot Captive 30.0 Wild 20.0 df.groupby(level=0).mean() Max Speed Animal Falcon 370.0 Parrot 25.0 df.groupby(level=\"Type\").mean() Max Speed Type Captive 210.0 Wild 185.0 We can also choose to include NA in group keys or not by setting dropna parameter, the default setting is True : l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]] df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"]) df.groupby(by=[\"b\"]).sum() a c b 1.0 2 3 2.0 2 5 df.groupby(by=[\"b\"], dropna=False).sum() a c b 1.0 2 3 2.0 2 5 NaN 1 4 l = [[\"a\", 12, 12], [None, 12.3, 33.], [\"b\", 12.3, 123], [\"a\", 1, 1]] df = pd.DataFrame(l, columns=[\"a\", \"b\", \"c\"]) df.groupby(by=\"a\").sum() b c a a 13.0 13.0 b 12.3 123.0 df.groupby(by=\"a\", dropna=False).sum() b c a a 13.0 13.0 b 12.3 123.0 NaN 12.3 33.0 View Source @ Appender ( \"\"\" Examples -------- >>> df = pd.DataFrame({'Animal': ['Falcon', 'Falcon', ... 'Parrot', 'Parrot'], ... 'Max Speed': [380., 370., 24., 26.]}) >>> df Animal Max Speed 0 Falcon 380.0 1 Falcon 370.0 2 Parrot 24.0 3 Parrot 26.0 >>> df.groupby(['Animal']).mean() Max Speed Animal Falcon 375.0 Parrot 25.0 **Hierarchical Indexes** We can groupby different levels of a hierarchical index using the `level` parameter: >>> arrays = [['Falcon', 'Falcon', 'Parrot', 'Parrot'], ... ['Captive', 'Wild', 'Captive', 'Wild']] >>> index = pd.MultiIndex.from_arrays(arrays, names=('Animal', 'Type')) >>> df = pd.DataFrame({'Max Speed': [390., 350., 30., 20.]}, ... index=index) >>> df Max Speed Animal Type Falcon Captive 390.0 Wild 350.0 Parrot Captive 30.0 Wild 20.0 >>> df.groupby(level=0).mean() Max Speed Animal Falcon 370.0 Parrot 25.0 >>> df.groupby(level=\" Type \").mean() Max Speed Type Captive 210.0 Wild 185.0 We can also choose to include NA in group keys or not by setting `dropna` parameter, the default setting is `True`: >>> l = [[1, 2, 3], [1, None, 4], [2, 1, 3], [1, 2, 2]] >>> df = pd.DataFrame(l, columns=[\" a \", \" b \", \" c \"]) >>> df.groupby(by=[\" b \"]).sum() a c b 1.0 2 3 2.0 2 5 >>> df.groupby(by=[\" b \"], dropna=False).sum() a c b 1.0 2 3 2.0 2 5 NaN 1 4 >>> l = [[\" a \", 12, 12], [None, 12.3, 33.], [\" b \", 12.3, 123], [\" a \", 1, 1]] >>> df = pd.DataFrame(l, columns=[\" a \", \" b \", \" c \"]) >>> df.groupby(by=\" a \").sum() b c a a 13.0 13.0 b 12.3 123.0 >>> df.groupby(by=\" a \", dropna=False).sum() b c a a 13.0 13.0 b 12.3 123.0 NaN 12.3 33.0 \"\"\" ) @ Appender ( _shared_docs [ \"groupby\" ] % _shared_doc_kwargs ) def groupby ( self , by = None , axis = 0 , level = None , as_index : bool = True , sort : bool = True , group_keys : bool = True , squeeze : bool = no_default , observed : bool = False , dropna : bool = True , ) -> \"DataFrameGroupBy\" : from pandas . core . groupby . generic import DataFrameGroupBy if squeeze is not no_default : warnings . warn ( ( \"The `squeeze` parameter is deprecated and \" \"will be removed in a future version.\" ), FutureWarning , stacklevel = 2 , ) else : squeeze = False if level is None and by is None : raise TypeError ( \"You have to supply one of 'by' and 'level'\" ) axis = self . _get_axis_number ( axis ) return DataFrameGroupBy ( obj = self , keys = by , axis = axis , level = level , as_index = as_index , sort = sort , group_keys = group_keys , squeeze = squeeze , observed = observed , dropna = dropna , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#gt","text":"def gt ( self , other , axis = 'columns' , level = None ) Get Greater than of dataframe and other, element-wise (binary operator gt ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , =! , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison.","title":"gt"},{"location":"reference/hielen2/datalink_prova_df/#parameters_58","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'}, default 'columns' Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_66","text":"DataFrame of bool Result of the comparison.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_65","text":"DataFrame.eq : Compare DataFrames for equality elementwise. DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_31","text":"Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ).","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_68","text":"df = pd.DataFrame({'cost': [250, 150, 100], ... 'revenue': [100, 250, 300]}, ... index=['A', 'B', 'C']) df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: df == 100 cost revenue A False True B False False C True False df.eq(100) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: df != pd.Series([100, 250], index=[\"cost\", \"revenue\"]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index') cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : df == [250, 100] cost revenue A True True B False False C False False Use the method to control the axis: df.eq([250, 250, 100], axis='index') cost revenue A True False B False True C True False Compare to a DataFrame of different shape. other = pd.DataFrame({'revenue': [300, 250, 100, 150]}, ... index=['A', 'B', 'C', 'D']) other revenue A 300 B 250 C 100 D 150 df.gt(other) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220], ... 'revenue': [100, 250, 300, 200, 175, 225]}, ... index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'], ... ['A', 'B', 'C', 'A', 'B', 'C']]) df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 df.le(df_multindex, level=1) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None ) : axis = self . _get_axis_number ( axis ) if axis is not None else 1 self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) new_data = dispatch_to_series ( self , other , op , axis = axis ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#head","text":"def head ( self : ~ FrameOrSeries , n : int = 5 ) -> ~ FrameOrSeries Return the first n rows. This function returns the first n rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it. For negative values of n , this function returns all rows except the last n rows, equivalent to df[:-n] .","title":"head"},{"location":"reference/hielen2/datalink_prova_df/#parameters_59","text":"n : int, default 5 Number of rows to select.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_67","text":"same type as caller The first n rows of the caller object.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_66","text":"DataFrame.tail: Returns the last n rows.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_69","text":"df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion', ... 'monkey', 'parrot', 'shark', 'whale', 'zebra']}) df animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the first 5 lines df.head() animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey Viewing the first n lines (three in this case) df.head(3) animal 0 alligator 1 bee 2 falcon For negative values of n df.head(-3) animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot View Source def head ( self : FrameOrSeries , n : int = 5 ) -> FrameOrSeries : \"\"\" Return the first `n` rows. This function returns the first `n` rows for the object based on position. It is useful for quickly testing if your object has the right type of data in it. For negative values of `n`, this function returns all rows except the last `n` rows, equivalent to ``df[:-n]``. Parameters ---------- n : int, default 5 Number of rows to select. Returns ------- same type as caller The first `n` rows of the caller object. See Also -------- DataFrame.tail: Returns the last `n` rows. Examples -------- >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion', ... 'monkey', 'parrot', 'shark', 'whale', 'zebra']}) >>> df animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the first 5 lines >>> df.head() animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey Viewing the first `n` lines (three in this case) >>> df.head(3) animal 0 alligator 1 bee 2 falcon For negative values of `n` >>> df.head(-3) animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot \"\"\" return self . iloc [ :n ]","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#hist","text":"def hist ( data : 'DataFrame' , column : Union [ Hashable , NoneType , Sequence [ Union [ Hashable , NoneType ]]] = None , by = None , grid : bool = True , xlabelsize : Union [ int , NoneType ] = None , xrot : Union [ float , NoneType ] = None , ylabelsize : Union [ int , NoneType ] = None , yrot : Union [ float , NoneType ] = None , ax = None , sharex : bool = False , sharey : bool = False , figsize : Union [ Tuple [ int , int ], NoneType ] = None , layout : Union [ Tuple [ int , int ], NoneType ] = None , bins : Union [ int , Sequence [ int ]] = 10 , backend : Union [ str , NoneType ] = None , legend : bool = False , ** kwargs ) Make a histogram of the DataFrame's. A histogram _ is a representation of the distribution of data. This function calls :meth: matplotlib.pyplot.hist , on each series in the DataFrame, resulting in one histogram per column. .. _histogram: https://en.wikipedia.org/wiki/Histogram","title":"hist"},{"location":"reference/hielen2/datalink_prova_df/#parameters_60","text":"data : DataFrame The pandas object holding the data. column : str or sequence If passed, will be used to limit data to a subset of columns. by : object, optional If passed, then used to form histograms for separate groups. grid : bool, default True Whether to show axis grid lines. xlabelsize : int, default None If specified changes the x-axis label size. xrot : float, default None Rotation of x axis labels. For example, a value of 90 displays the x labels rotated 90 degrees clockwise. ylabelsize : int, default None If specified changes the y-axis label size. yrot : float, default None Rotation of y axis labels. For example, a value of 90 displays the y labels rotated 90 degrees clockwise. ax : Matplotlib axes object, default None The axes to plot the histogram on. sharex : bool, default True if ax is None else False In case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in. Note that passing in both an ax and sharex=True will alter all x axis labels for all subplots in a figure. sharey : bool, default False In case subplots=True, share y axis and set some y axis labels to invisible. figsize : tuple The size in inches of the figure to create. Uses the value in matplotlib.rcParams by default. layout : tuple, optional Tuple of (rows, columns) for the layout of the histograms. bins : int or sequence, default 10 Number of histogram bins to be used. If an integer is given, bins + 1 bin edges are calculated and returned. If bins is a sequence, gives bin edges, including left edge of first bin and right edge of last bin. In this case, bins is returned unmodified. backend : str, default None Backend to use instead of the backend specified in the option plotting.backend . For instance, 'matplotlib'. Alternatively, to specify the plotting.backend for the whole session, set pd.options.plotting.backend . .. versionadded :: 1.0.0 legend : bool, default False Whether to show the legend. .. versionadded :: 1.1.0 **kwargs All other plotting keyword arguments to be passed to :meth: matplotlib.pyplot.hist .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_68","text":"matplotlib.AxesSubplot or numpy.ndarray of them","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_67","text":"matplotlib.pyplot.hist : Plot a histogram using matplotlib.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_70","text":"This example draws a histogram based on the length and width of some animals, displayed in three bins .. plot:: :context: close-figs >>> df = pd.DataFrame({ ... 'length': [1.5, 0.5, 1.2, 0.9, 3], ... 'width': [0.7, 0.2, 0.15, 0.2, 1.1] ... }, index=['pig', 'rabbit', 'duck', 'chicken', 'horse']) >>> hist = df.hist(bins=3) View Source def hist_frame ( data : \"DataFrame\" , column : Union [ Label, Sequence[Label ] ] = None , by = None , grid : bool = True , xlabelsize : Optional [ int ] = None , xrot : Optional [ float ] = None , ylabelsize : Optional [ int ] = None , yrot : Optional [ float ] = None , ax = None , sharex : bool = False , sharey : bool = False , figsize : Optional [ Tuple[int, int ] ] = None , layout : Optional [ Tuple[int, int ] ] = None , bins : Union [ int, Sequence[int ] ] = 10 , backend : Optional [ str ] = None , legend : bool = False , ** kwargs , ) : \"\"\" Make a histogram of the DataFrame's. A `histogram`_ is a representation of the distribution of data. This function calls :meth:`matplotlib.pyplot.hist`, on each series in the DataFrame, resulting in one histogram per column. .. _histogram: https://en.wikipedia.org/wiki/Histogram Parameters ---------- data : DataFrame The pandas object holding the data. column : str or sequence If passed, will be used to limit data to a subset of columns. by : object, optional If passed, then used to form histograms for separate groups. grid : bool, default True Whether to show axis grid lines. xlabelsize : int, default None If specified changes the x-axis label size. xrot : float, default None Rotation of x axis labels. For example, a value of 90 displays the x labels rotated 90 degrees clockwise. ylabelsize : int, default None If specified changes the y-axis label size. yrot : float, default None Rotation of y axis labels. For example, a value of 90 displays the y labels rotated 90 degrees clockwise. ax : Matplotlib axes object, default None The axes to plot the histogram on. sharex : bool, default True if ax is None else False In case subplots=True, share x axis and set some x axis labels to invisible; defaults to True if ax is None otherwise False if an ax is passed in. Note that passing in both an ax and sharex=True will alter all x axis labels for all subplots in a figure. sharey : bool, default False In case subplots=True, share y axis and set some y axis labels to invisible. figsize : tuple The size in inches of the figure to create. Uses the value in `matplotlib.rcParams` by default. layout : tuple, optional Tuple of (rows, columns) for the layout of the histograms. bins : int or sequence, default 10 Number of histogram bins to be used. If an integer is given, bins + 1 bin edges are calculated and returned. If bins is a sequence, gives bin edges, including left edge of first bin and right edge of last bin. In this case, bins is returned unmodified. backend : str, default None Backend to use instead of the backend specified in the option ``plotting.backend``. For instance, 'matplotlib'. Alternatively, to specify the ``plotting.backend`` for the whole session, set ``pd.options.plotting.backend``. .. versionadded:: 1.0.0 legend : bool, default False Whether to show the legend. .. versionadded:: 1.1.0 **kwargs All other plotting keyword arguments to be passed to :meth:`matplotlib.pyplot.hist`. Returns ------- matplotlib.AxesSubplot or numpy.ndarray of them See Also -------- matplotlib.pyplot.hist : Plot a histogram using matplotlib. Examples -------- This example draws a histogram based on the length and width of some animals, displayed in three bins .. plot:: :context: close-figs >>> df = pd.DataFrame({ ... 'length': [1.5, 0.5, 1.2, 0.9, 3], ... 'width': [0.7, 0.2, 0.15, 0.2, 1.1] ... }, index=['pig', 'rabbit', 'duck', 'chicken', 'horse']) >>> hist = df.hist(bins=3) \"\"\" plot_backend = _get_plot_backend ( backend ) return plot_backend . hist_frame ( data , column = column , by = by , grid = grid , xlabelsize = xlabelsize , xrot = xrot , ylabelsize = ylabelsize , yrot = yrot , ax = ax , sharex = sharex , sharey = sharey , figsize = figsize , layout = layout , legend = legend , bins = bins , ** kwargs , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#idxmax","text":"def idxmax ( self , axis = 0 , skipna = True ) -> pandas . core . series . Series Return index of first occurrence of maximum over requested axis. NA/null values are excluded.","title":"idxmax"},{"location":"reference/hielen2/datalink_prova_df/#parameters_61","text":"axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_69","text":"Series Indexes of maxima along the specified axis.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_8","text":"ValueError * If the row/column is empty","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_68","text":"Series.idxmax : Return index of the maximum element.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_32","text":"This method is the DataFrame version of ndarray.argmax .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_71","text":"Consider a dataset containing food consumption in Argentina. df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48], ... 'co2_emissions': [37.2, 19.66, 1712]}, ... index=['Pork', 'Wheat Products', 'Beef']) df consumption co2_emissions Pork 10.51 37.20 Wheat Products 103.11 19.66 Beef 55.48 1712.00 By default, it returns the index for the maximum value in each column. df.idxmax() consumption Wheat Products co2_emissions Beef dtype: object To return the index for the maximum value in each row, use axis=\"columns\" . df.idxmax(axis=\"columns\") Pork co2_emissions Wheat Products consumption Beef co2_emissions dtype: object View Source def idxmax ( self , axis = 0 , skipna = True ) -> Series : \"\"\" Return index of first occurrence of maximum over requested axis. NA/null values are excluded. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. Returns ------- Series Indexes of maxima along the specified axis. Raises ------ ValueError * If the row/column is empty See Also -------- Series.idxmax : Return index of the maximum element. Notes ----- This method is the DataFrame version of ``ndarray.argmax``. Examples -------- Consider a dataset containing food consumption in Argentina. >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48], ... 'co2_emissions': [37.2, 19.66, 1712]}, ... index=['Pork', 'Wheat Products', 'Beef']) >>> df consumption co2_emissions Pork 10.51 37.20 Wheat Products 103.11 19.66 Beef 55.48 1712.00 By default, it returns the index for the maximum value in each column. >>> df.idxmax() consumption Wheat Products co2_emissions Beef dtype: object To return the index for the maximum value in each row, use ``axis=\" columns \"``. >>> df.idxmax(axis=\" columns \") Pork co2_emissions Wheat Products consumption Beef co2_emissions dtype: object \"\"\" axis = self . _get_axis_number ( axis ) indices = nanops . nanargmax ( self . values , axis = axis , skipna = skipna ) # indices will always be np . ndarray since axis is not None and # values is a 2 d array for DataFrame # error : Item \"int\" of \"Union[int, Any]\" has no attribute \"__iter__\" assert isinstance ( indices , np . ndarray ) # for mypy index = self . _get_axis ( axis ) result = [ index[i ] if i >= 0 else np . nan for i in indices ] return self . _constructor_sliced ( result , index = self . _get_agg_axis ( axis ))","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#idxmin","text":"def idxmin ( self , axis = 0 , skipna = True ) -> pandas . core . series . Series Return index of first occurrence of minimum over requested axis. NA/null values are excluded.","title":"idxmin"},{"location":"reference/hielen2/datalink_prova_df/#parameters_62","text":"axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_70","text":"Series Indexes of minima along the specified axis.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_9","text":"ValueError * If the row/column is empty","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_69","text":"Series.idxmin : Return index of the minimum element.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_33","text":"This method is the DataFrame version of ndarray.argmin .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_72","text":"Consider a dataset containing food consumption in Argentina. df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48], ... 'co2_emissions': [37.2, 19.66, 1712]}, ... index=['Pork', 'Wheat Products', 'Beef']) df consumption co2_emissions Pork 10.51 37.20 Wheat Products 103.11 19.66 Beef 55.48 1712.00 By default, it returns the index for the minimum value in each column. df.idxmin() consumption Pork co2_emissions Wheat Products dtype: object To return the index for the minimum value in each row, use axis=\"columns\" . df.idxmin(axis=\"columns\") Pork consumption Wheat Products co2_emissions Beef consumption dtype: object View Source def idxmin ( self , axis = 0 , skipna = True ) -> Series : \"\"\" Return index of first occurrence of minimum over requested axis. NA/null values are excluded. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. Returns ------- Series Indexes of minima along the specified axis. Raises ------ ValueError * If the row/column is empty See Also -------- Series.idxmin : Return index of the minimum element. Notes ----- This method is the DataFrame version of ``ndarray.argmin``. Examples -------- Consider a dataset containing food consumption in Argentina. >>> df = pd.DataFrame({'consumption': [10.51, 103.11, 55.48], ... 'co2_emissions': [37.2, 19.66, 1712]}, ... index=['Pork', 'Wheat Products', 'Beef']) >>> df consumption co2_emissions Pork 10.51 37.20 Wheat Products 103.11 19.66 Beef 55.48 1712.00 By default, it returns the index for the minimum value in each column. >>> df.idxmin() consumption Pork co2_emissions Wheat Products dtype: object To return the index for the minimum value in each row, use ``axis=\" columns \"``. >>> df.idxmin(axis=\" columns \") Pork consumption Wheat Products co2_emissions Beef consumption dtype: object \"\"\" axis = self . _get_axis_number ( axis ) indices = nanops . nanargmin ( self . values , axis = axis , skipna = skipna ) # indices will always be np . ndarray since axis is not None and # values is a 2 d array for DataFrame # error : Item \"int\" of \"Union[int, Any]\" has no attribute \"__iter__\" assert isinstance ( indices , np . ndarray ) # for mypy index = self . _get_axis ( axis ) result = [ index[i ] if i >= 0 else np . nan for i in indices ] return self . _constructor_sliced ( result , index = self . _get_agg_axis ( axis ))","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#infer_objects","text":"def infer_objects ( self : ~ FrameOrSeries ) -> ~ FrameOrSeries Attempt to infer better dtypes for object columns. Attempts soft conversion of object-dtyped columns, leaving non-object and unconvertible columns unchanged. The inference rules are the same as during normal Series/DataFrame construction.","title":"infer_objects"},{"location":"reference/hielen2/datalink_prova_df/#returns_71","text":"converted : same type as input object","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_70","text":"to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to numeric type. convert_dtypes : Convert argument to best possible dtype.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_73","text":"df = pd.DataFrame({\"A\": [\"a\", 1, 2, 3]}) df = df.iloc[1:] df A 1 1 2 2 3 3 df.dtypes A object dtype: object df.infer_objects().dtypes A int64 dtype: object View Source def infer_objects ( self : FrameOrSeries ) -> FrameOrSeries : \"\"\" Attempt to infer better dtypes for object columns. Attempts soft conversion of object-dtyped columns, leaving non-object and unconvertible columns unchanged. The inference rules are the same as during normal Series/DataFrame construction. Returns ------- converted : same type as input object See Also -------- to_datetime : Convert argument to datetime. to_timedelta : Convert argument to timedelta. to_numeric : Convert argument to numeric type. convert_dtypes : Convert argument to best possible dtype. Examples -------- >>> df = pd.DataFrame({\" A \": [\" a \", 1, 2, 3]}) >>> df = df.iloc[1:] >>> df A 1 1 2 2 3 3 >>> df.dtypes A object dtype: object >>> df.infer_objects().dtypes A int64 dtype: object \"\"\" # numeric = False necessary to only soft convert ; # python objects will still be converted to # native numpy numeric types return self . _constructor ( self . _mgr . convert ( datetime = True , numeric = False , timedelta = True , coerce = False , copy = True ) ). __finalize__ ( self , method = \"infer_objects\" )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#info","text":"def info ( self , verbose : Union [ bool , NoneType ] = None , buf : Union [ IO [ str ], NoneType ] = None , max_cols : Union [ int , NoneType ] = None , memory_usage : Union [ bool , str , NoneType ] = None , null_counts : Union [ bool , NoneType ] = None ) -> None Print a concise summary of a DataFrame. This method prints information about a DataFrame including the index dtype and columns, non-null values and memory usage.","title":"info"},{"location":"reference/hielen2/datalink_prova_df/#parameters_63","text":"data : DataFrame DataFrame to print information about. verbose : bool, optional Whether to print the full summary. By default, the setting in pandas.options.display.max_info_columns is followed. buf : writable buffer, defaults to sys.stdout Where to send the output. By default, the output is printed to sys.stdout. Pass a writable buffer if you need to further process the output. max_cols : int, optional When to switch from the verbose to the truncated output. If the DataFrame has more than max_cols columns, the truncated output is used. By default, the setting in pandas.options.display.max_info_columns is used. memory_usage : bool, str, optional Specifies whether total memory usage of the DataFrame elements (including the index) should be displayed. By default, this follows the pandas.options.display.memory_usage setting. True always show memory usage. False never shows memory usage. A value of 'deep' is equivalent to \"True with deep introspection\". Memory usage is shown in human-readable units (base-2 representation). Without deep introspection a memory estimation is made based in column dtype and number of rows assuming values consume the same memory amount for corresponding dtypes. With deep memory introspection, a real memory usage calculation is performed at the cost of computational resources. null_counts : bool, optional Whether to show the non-null counts. By default, this is shown only if the DataFrame is smaller than pandas.options.display.max_info_rows and pandas.options.display.max_info_columns . A value of True always shows the counts, and False never shows the counts.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_72","text":"None This method prints a summary of a DataFrame and returns None.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_71","text":"DataFrame.describe: Generate descriptive statistics of DataFrame columns. DataFrame.memory_usage: Memory usage of DataFrame columns.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_74","text":">>> int_values = [ 1 , 2 , 3 , 4 , 5 ] >>> text_values = [ 'alpha' , 'beta' , 'gamma' , 'delta' , 'epsilon' ] >>> float_values = [ 0 . 0 , 0 . 25 , 0 . 5 , 0 . 75 , 1 . 0 ] >>> df = pd . DataFrame ( { \"int_col\" : int_values , \"text_col\" : text_values , ... \"float_col\" : float_values } ) >>> df int_col text_col float_col 0 1 alpha 0 . 00 1 2 beta 0 . 25 2 3 gamma 0 . 50 3 4 delta 0 . 75 4 5 epsilon 1 . 00 Prints information of all columns : >>> df . info ( verbose = True ) < class 'pandas.core.frame.DataFrame' > RangeIndex : 5 entries , 0 to 4 Data columns ( total 3 columns ): # Column Non-Null Count Dtype - -- ------ -------------- ----- 0 int_col 5 non - null int64 1 text_col 5 non - null object 2 float_col 5 non - null float64 dtypes : float64 ( 1 ), int64 ( 1 ), object ( 1 ) memory usage : 248 . 0 + bytes Prints a summary of columns count and its dtypes but not per column information : >>> df . info ( verbose = False ) < class 'pandas.core.frame.DataFrame' > RangeIndex : 5 entries , 0 to 4 Columns : 3 entries , int_col to float_col dtypes : float64 ( 1 ), int64 ( 1 ), object ( 1 ) memory usage : 248 . 0 + bytes Pipe output of DataFrame . info to buffer instead of sys . stdout , get buffer content and writes to a text file : >>> import io >>> buffer = io . StringIO () >>> df . info ( buf = buffer ) >>> s = buffer . getvalue () >>> with open ( \"df_info.txt\" , \"w\" , ... encoding = \"utf-8\" ) as f : # doctest: +SKIP ... f . write ( s ) 260 The `memory_usage` parameter allows deep introspection mode , specially useful for big DataFrames and fine - tune memory optimization : >>> random_strings_array = np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ) >>> df = pd . DataFrame ( { ... 'column_1' : np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ), ... 'column_2' : np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ), ... 'column_3' : np . random . choice ([ 'a' , 'b' , 'c' ], 10 ** 6 ) ... } ) >>> df . info () < class 'pandas.core.frame.DataFrame' > RangeIndex : 1000000 entries , 0 to 999999 Data columns ( total 3 columns ): # Column Non-Null Count Dtype - -- ------ -------------- ----- 0 column_1 1000000 non - null object 1 column_2 1000000 non - null object 2 column_3 1000000 non - null object dtypes : object ( 3 ) memory usage : 22 . 9 + MB >>> df . info ( memory_usage = 'deep' ) < class 'pandas.core.frame.DataFrame' > RangeIndex : 1000000 entries , 0 to 999999 Data columns ( total 3 columns ): # Column Non-Null Count Dtype - -- ------ -------------- ----- 0 column_1 1000000 non - null object 1 column_2 1000000 non - null object 2 column_3 1000000 non - null object dtypes : object ( 3 ) memory usage : 188 . 8 MB View Source @Substitution ( klass = \"DataFrame\" , type_sub = \" and columns\" , max_cols_sub = ( \"\"\"max_cols : int, optional When to switch from the verbose to the truncated output. If the DataFrame has more than `max_cols` columns, the truncated output is used. By default, the setting in ``pandas.options.display.max_info_columns`` is used. \"\"\" ), examples_sub = ( \"\"\" >>> int_values = [1, 2, 3, 4, 5] >>> text_values = ['alpha', 'beta', 'gamma', 'delta', 'epsilon'] >>> float_values = [0.0, 0.25, 0.5, 0.75, 1.0] >>> df = pd.DataFrame({\" int_col \": int_values, \" text_col \": text_values, ... \" float_col \": float_values}) >>> df int_col text_col float_col 0 1 alpha 0.00 1 2 beta 0.25 2 3 gamma 0.50 3 4 delta 0.75 4 5 epsilon 1.00 Prints information of all columns: >>> df.info(verbose=True) <class 'pandas.core.frame.DataFrame'> RangeIndex: 5 entries, 0 to 4 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 int_col 5 non-null int64 1 text_col 5 non-null object 2 float_col 5 non-null float64 dtypes: float64(1), int64(1), object(1) memory usage: 248.0+ bytes Prints a summary of columns count and its dtypes but not per column information: >>> df.info(verbose=False) <class 'pandas.core.frame.DataFrame'> RangeIndex: 5 entries, 0 to 4 Columns: 3 entries, int_col to float_col dtypes: float64(1), int64(1), object(1) memory usage: 248.0+ bytes Pipe output of DataFrame.info to buffer instead of sys.stdout, get buffer content and writes to a text file: >>> import io >>> buffer = io.StringIO() >>> df.info(buf=buffer) >>> s = buffer.getvalue() >>> with open(\" df_info . txt \", \" w \", ... encoding=\" utf - 8 \") as f: # doctest: +SKIP ... f.write(s) 260 The `memory_usage` parameter allows deep introspection mode, specially useful for big DataFrames and fine-tune memory optimization: >>> random_strings_array = np.random.choice(['a', 'b', 'c'], 10 ** 6) >>> df = pd.DataFrame({ ... 'column_1': np.random.choice(['a', 'b', 'c'], 10 ** 6), ... 'column_2': np.random.choice(['a', 'b', 'c'], 10 ** 6), ... 'column_3': np.random.choice(['a', 'b', 'c'], 10 ** 6) ... }) >>> df.info() <class 'pandas.core.frame.DataFrame'> RangeIndex: 1000000 entries, 0 to 999999 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 column_1 1000000 non-null object 1 column_2 1000000 non-null object 2 column_3 1000000 non-null object dtypes: object(3) memory usage: 22.9+ MB >>> df.info(memory_usage='deep') <class 'pandas.core.frame.DataFrame'> RangeIndex: 1000000 entries, 0 to 999999 Data columns (total 3 columns): # Column Non-Null Count Dtype --- ------ -------------- ----- 0 column_1 1000000 non-null object 1 column_2 1000000 non-null object 2 column_3 1000000 non-null object dtypes: object(3) memory usage: 188.8 MB\"\"\" ), see_also_sub = ( \"\"\" DataFrame.describe: Generate descriptive statistics of DataFrame columns. DataFrame.memory_usage: Memory usage of DataFrame columns.\"\"\" ), ) @doc ( DataFrameInfo . info ) def info ( self , verbose : Optional [ bool ] = None , buf : Optional [ IO[str ] ] = None , max_cols : Optional [ int ] = None , memory_usage : Optional [ Union[bool, str ] ] = None , null_counts : Optional [ bool ] = None , ) -> None : return DataFrameInfo ( self , verbose , buf , max_cols , memory_usage , null_counts ). info ()","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#insert","text":"def insert ( self , loc , column , value , allow_duplicates = False ) -> None Insert column into DataFrame at specified location. Raises a ValueError if column is already contained in the DataFrame, unless allow_duplicates is set to True.","title":"insert"},{"location":"reference/hielen2/datalink_prova_df/#parameters_64","text":"loc : int Insertion index. Must verify 0 <= loc <= len(columns). column : str, number, or hashable object Label of the inserted column. value : int, Series, or array-like allow_duplicates : bool, optional View Source def insert ( self , loc , column , value , allow_duplicates = False ) -> None : \"\"\" Insert column into DataFrame at specified location. Raises a ValueError if `column` is already contained in the DataFrame, unless `allow_duplicates` is set to True. Parameters ---------- loc : int Insertion index. Must verify 0 <= loc <= len(columns). column : str, number, or hashable object Label of the inserted column. value : int, Series, or array-like allow_duplicates : bool, optional \"\"\" self . _ensure_valid_index ( value ) value = self . _sanitize_column ( column , value , broadcast = False ) self . _mgr . insert ( loc , column , value , allow_duplicates = allow_duplicates )","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#interpolate","text":"def interpolate ( self : ~ FrameOrSeries , method : str = 'linear' , axis : Union [ str , int ] = 0 , limit : Union [ int , NoneType ] = None , inplace : bool = False , limit_direction : Union [ str , NoneType ] = None , limit_area : Union [ str , NoneType ] = None , downcast : Union [ str , NoneType ] = None , ** kwargs ) -> Union [ ~ FrameOrSeries , NoneType ] Please note that only method='linear' is supported for DataFrame/Series with a MultiIndex.","title":"interpolate"},{"location":"reference/hielen2/datalink_prova_df/#parameters_65","text":"method : str, default 'linear' Interpolation technique to use. One of: * 'linear' : Ignore the index and treat the values as equally spaced . This is the only method supported on MultiIndexes . * 'time' : Works on daily and higher resolution data to interpolate given length of interval . * 'index' , 'values' : use the actual numerical values of the index . * 'pad' : Fill in NaNs using existing values . * 'nearest' , 'zero' , 'slinear' , 'quadratic' , 'cubic' , 'spline' , 'barycentric' , 'polynomial' : Passed to `scipy.interpolate.interp1d` . These methods use the numerical values of the index . Both 'polynomial' and 'spline' require that you also specify an `order` ( int ), e . g . `` df . interpolate ( method = 'polynomial' , order = 5 ) `` . * 'krogh' , 'piecewise_polynomial' , 'spline' , 'pchip' , 'akima' , 'cubicspline' : Wrappers around the SciPy interpolation methods of similar names . See `Notes` . * 'from_derivatives' : Refers to `scipy.interpolate.BPoly.from_derivatives` which replaces 'piecewise_polynomial' interpolation method in scipy 0 . 18 . axis : {{0 or 'index', 1 or 'columns', None}}, default None Axis to interpolate along. limit : int, optional Maximum number of consecutive NaNs to fill. Must be greater than 0. inplace : bool, default False Update the data in place if possible. limit_direction : {{'forward', 'backward', 'both'}}, Optional Consecutive NaNs will be filled in this direction. If limit is specified : * If 'method' is 'pad' or 'ffill' , 'limit_direction' must be 'forward' . * If 'method' is 'backfill' or 'bfill' , 'limit_direction' must be 'backwards' . If 'limit' is not specified : * If 'method' is 'backfill' or 'bfill' , the default is 'backward' * else the default is 'forward' .. versionchanged :: 1 . 1 . 0 raises ValueError if `limit_direction` is 'forward' or 'both' and method is 'backfill' or 'bfill' . raises ValueError if `limit_direction` is 'backward' or 'both' and method is 'pad' or 'ffill' . limit_area : {{ None , 'inside', 'outside'}}, default None If limit is specified, consecutive NaNs will be filled with this restriction. * `` None `` : No fill restriction . * 'inside' : Only fill NaNs surrounded by valid values ( interpolate ). * 'outside' : Only fill NaNs outside valid values ( extrapolate ). .. versionadded :: 0 . 23 . 0 downcast : optional, 'infer' or None, defaults to None Downcast dtypes if possible. **kwargs Keyword arguments to pass on to the interpolating function.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_73","text":"Series or DataFrame Returns the same object type as the caller, interpolated at some or all NaN values.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_72","text":"fillna : Fill missing values using different methods. scipy.interpolate.Akima1DInterpolator : Piecewise cubic polynomials (Akima interpolator). scipy.interpolate.BPoly.from_derivatives : Piecewise polynomial in the Bernstein basis. scipy.interpolate.interp1d : Interpolate a 1-D function. scipy.interpolate.KroghInterpolator : Interpolate polynomial (Krogh interpolator). scipy.interpolate.PchipInterpolator : PCHIP 1-d monotonic cubic interpolation. scipy.interpolate.CubicSpline : Cubic spline data interpolator.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_34","text":"The 'krogh', 'piecewise_polynomial', 'spline', 'pchip' and 'akima' methods are wrappers around the respective SciPy implementations of similar names. These use the actual numerical values of the index. For more information on their behavior, see the SciPy documentation <https://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation> and SciPy tutorial <https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html> .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_75","text":"Filling in NaN in a :class: ~pandas.Series via linear interpolation. s = pd.Series([0, 1, np.nan, 3]) s 0 0.0 1 1.0 2 NaN 3 3.0 dtype: float64 s.interpolate() 0 0.0 1 1.0 2 2.0 3 3.0 dtype: float64 Filling in NaN in a Series by padding, but filling at most two consecutive NaN at a time. s = pd.Series([np.nan, \"single_one\", np.nan, ... \"fill_two_more\", np.nan, np.nan, np.nan, ... 4.71, np.nan]) s 0 NaN 1 single_one 2 NaN 3 fill_two_more 4 NaN 5 NaN 6 NaN 7 4.71 8 NaN dtype: object s.interpolate(method='pad', limit=2) 0 NaN 1 single_one 2 single_one 3 fill_two_more 4 fill_two_more 5 fill_two_more 6 NaN 7 4.71 8 4.71 dtype: object Filling in NaN in a Series via polynomial interpolation or splines: Both 'polynomial' and 'spline' methods require that you also specify an order (int). s = pd.Series([0, 2, np.nan, 8]) s.interpolate(method='polynomial', order=2) 0 0.000000 1 2.000000 2 4.666667 3 8.000000 dtype: float64 Fill the DataFrame forward (that is, going down) along each column using linear interpolation. Note how the last entry in column 'a' is interpolated differently, because there is no entry after it to use for interpolation. Note how the first entry in column 'b' remains NaN , because there is no entry before it to use for interpolation. df = pd.DataFrame([(0.0, np.nan, -1.0, 1.0), ... (np.nan, 2.0, np.nan, np.nan), ... (2.0, 3.0, np.nan, 9.0), ... (np.nan, 4.0, -4.0, 16.0)], ... columns=list('abcd')) df a b c d 0 0.0 NaN -1.0 1.0 1 NaN 2.0 NaN NaN 2 2.0 3.0 NaN 9.0 3 NaN 4.0 -4.0 16.0 df.interpolate(method='linear', limit_direction='forward', axis=0) a b c d 0 0.0 NaN -1.0 1.0 1 1.0 2.0 -2.0 5.0 2 2.0 3.0 -3.0 9.0 3 2.0 4.0 -4.0 16.0 Using polynomial interpolation. df['d'].interpolate(method='polynomial', order=2) 0 1.0 1 4.0 2 9.0 3 16.0 Name: d, dtype: float64 View Source def interpolate ( self : FrameOrSeries , method : str = \"linear\" , axis : Axis = 0 , limit : Optional [ int ] = None , inplace : bool_t = False , limit_direction : Optional [ str ] = None , limit_area : Optional [ str ] = None , downcast : Optional [ str ] = None , ** kwargs , ) -> Optional [ FrameOrSeries ]: \"\"\" Please note that only ``method='linear'`` is supported for DataFrame/Series with a MultiIndex. Parameters ---------- method : str, default 'linear' Interpolation technique to use. One of: * 'linear': Ignore the index and treat the values as equally spaced. This is the only method supported on MultiIndexes. * 'time': Works on daily and higher resolution data to interpolate given length of interval. * 'index', 'values': use the actual numerical values of the index. * 'pad': Fill in NaNs using existing values. * 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'spline', 'barycentric', 'polynomial': Passed to `scipy.interpolate.interp1d`. These methods use the numerical values of the index. Both 'polynomial' and 'spline' require that you also specify an `order` (int), e.g. ``df.interpolate(method='polynomial', order=5)``. * 'krogh', 'piecewise_polynomial', 'spline', 'pchip', 'akima', 'cubicspline': Wrappers around the SciPy interpolation methods of similar names. See `Notes`. * 'from_derivatives': Refers to `scipy.interpolate.BPoly.from_derivatives` which replaces 'piecewise_polynomial' interpolation method in scipy 0.18. axis : {{0 or 'index', 1 or 'columns', None}}, default None Axis to interpolate along. limit : int, optional Maximum number of consecutive NaNs to fill. Must be greater than 0. inplace : bool, default False Update the data in place if possible. limit_direction : {{'forward', 'backward', 'both'}}, Optional Consecutive NaNs will be filled in this direction. If limit is specified: * If 'method' is 'pad' or 'ffill', 'limit_direction' must be 'forward'. * If 'method' is 'backfill' or 'bfill', 'limit_direction' must be 'backwards'. If 'limit' is not specified: * If 'method' is 'backfill' or 'bfill', the default is 'backward' * else the default is 'forward' .. versionchanged:: 1.1.0 raises ValueError if `limit_direction` is 'forward' or 'both' and method is 'backfill' or 'bfill'. raises ValueError if `limit_direction` is 'backward' or 'both' and method is 'pad' or 'ffill'. limit_area : {{`None`, 'inside', 'outside'}}, default None If limit is specified, consecutive NaNs will be filled with this restriction. * ``None``: No fill restriction. * 'inside': Only fill NaNs surrounded by valid values (interpolate). * 'outside': Only fill NaNs outside valid values (extrapolate). .. versionadded:: 0.23.0 downcast : optional, 'infer' or None, defaults to None Downcast dtypes if possible. **kwargs Keyword arguments to pass on to the interpolating function. Returns ------- Series or DataFrame Returns the same object type as the caller, interpolated at some or all ``NaN`` values. See Also -------- fillna : Fill missing values using different methods. scipy.interpolate.Akima1DInterpolator : Piecewise cubic polynomials (Akima interpolator). scipy.interpolate.BPoly.from_derivatives : Piecewise polynomial in the Bernstein basis. scipy.interpolate.interp1d : Interpolate a 1-D function. scipy.interpolate.KroghInterpolator : Interpolate polynomial (Krogh interpolator). scipy.interpolate.PchipInterpolator : PCHIP 1-d monotonic cubic interpolation. scipy.interpolate.CubicSpline : Cubic spline data interpolator. Notes ----- The 'krogh', 'piecewise_polynomial', 'spline', 'pchip' and 'akima' methods are wrappers around the respective SciPy implementations of similar names. These use the actual numerical values of the index. For more information on their behavior, see the `SciPy documentation <https://docs.scipy.org/doc/scipy/reference/interpolate.html#univariate-interpolation>`__ and `SciPy tutorial <https://docs.scipy.org/doc/scipy/reference/tutorial/interpolate.html>`__. Examples -------- Filling in ``NaN`` in a :class:`~pandas.Series` via linear interpolation. >>> s = pd.Series([0, 1, np.nan, 3]) >>> s 0 0.0 1 1.0 2 NaN 3 3.0 dtype: float64 >>> s.interpolate() 0 0.0 1 1.0 2 2.0 3 3.0 dtype: float64 Filling in ``NaN`` in a Series by padding, but filling at most two consecutive ``NaN`` at a time. >>> s = pd.Series([np.nan, \" single_one \", np.nan, ... \" fill_two_more \", np.nan, np.nan, np.nan, ... 4.71, np.nan]) >>> s 0 NaN 1 single_one 2 NaN 3 fill_two_more 4 NaN 5 NaN 6 NaN 7 4.71 8 NaN dtype: object >>> s.interpolate(method='pad', limit=2) 0 NaN 1 single_one 2 single_one 3 fill_two_more 4 fill_two_more 5 fill_two_more 6 NaN 7 4.71 8 4.71 dtype: object Filling in ``NaN`` in a Series via polynomial interpolation or splines: Both 'polynomial' and 'spline' methods require that you also specify an ``order`` (int). >>> s = pd.Series([0, 2, np.nan, 8]) >>> s.interpolate(method='polynomial', order=2) 0 0.000000 1 2.000000 2 4.666667 3 8.000000 dtype: float64 Fill the DataFrame forward (that is, going down) along each column using linear interpolation. Note how the last entry in column 'a' is interpolated differently, because there is no entry after it to use for interpolation. Note how the first entry in column 'b' remains ``NaN``, because there is no entry before it to use for interpolation. >>> df = pd.DataFrame([(0.0, np.nan, -1.0, 1.0), ... (np.nan, 2.0, np.nan, np.nan), ... (2.0, 3.0, np.nan, 9.0), ... (np.nan, 4.0, -4.0, 16.0)], ... columns=list('abcd')) >>> df a b c d 0 0.0 NaN -1.0 1.0 1 NaN 2.0 NaN NaN 2 2.0 3.0 NaN 9.0 3 NaN 4.0 -4.0 16.0 >>> df.interpolate(method='linear', limit_direction='forward', axis=0) a b c d 0 0.0 NaN -1.0 1.0 1 1.0 2.0 -2.0 5.0 2 2.0 3.0 -3.0 9.0 3 2.0 4.0 -4.0 16.0 Using polynomial interpolation. >>> df['d'].interpolate(method='polynomial', order=2) 0 1.0 1 4.0 2 9.0 3 16.0 Name: d, dtype: float64 \"\"\" inplace = validate_bool_kwarg ( inplace , \"inplace\" ) axis = self . _get_axis_number ( axis ) fillna_methods = [ \"ffill\" , \"bfill\" , \"pad\" , \"backfill\" ] should_transpose = axis == 1 and method not in fillna_methods obj = self . T if should_transpose else self if obj . empty : return self . copy () if method not in fillna_methods : axis = self . _info_axis_number if isinstance ( obj . index , MultiIndex ) and method != \"linear\" : raise ValueError ( \"Only `method=linear` interpolation is supported on MultiIndexes.\" ) # Set `limit_direction` depending on `method` if limit_direction is None : limit_direction = ( \"backward\" if method in ( \"backfill\" , \"bfill\" ) else \"forward\" ) else : if method in ( \"pad\" , \"ffill\" ) and limit_direction != \"forward\" : raise ValueError ( f \"`limit_direction` must be 'forward' for method `{method}`\" ) if method in ( \"backfill\" , \"bfill\" ) and limit_direction != \"backward\" : raise ValueError ( f \"`limit_direction` must be 'backward' for method `{method}`\" ) if obj . ndim == 2 and np . all ( obj . dtypes == np . dtype ( object )): raise TypeError ( \"Cannot interpolate with all object-dtype columns \" \"in the DataFrame. Try setting at least one \" \"column to a numeric dtype.\" ) # create/use the index if method == \"linear\" : # prior default index = np . arange ( len ( obj . index )) else : index = obj . index methods = { \"index\" , \"values\" , \"nearest\" , \"time\" } is_numeric_or_datetime = ( is_numeric_dtype ( index . dtype ) or is_datetime64_any_dtype ( index . dtype ) or is_timedelta64_dtype ( index . dtype ) ) if method not in methods and not is_numeric_or_datetime : raise ValueError ( \"Index column must be numeric or datetime type when \" f \"using {method} method other than linear. \" \"Try setting a numeric or datetime index column before \" \"interpolating.\" ) if isna ( index ). any (): raise NotImplementedError ( \"Interpolation with NaNs in the index \" \"has not been implemented. Try filling \" \"those NaNs before interpolating.\" ) new_data = obj . _mgr . interpolate ( method = method , axis = axis , index = index , limit = limit , limit_direction = limit_direction , limit_area = limit_area , inplace = inplace , downcast = downcast , ** kwargs , ) result = self . _constructor ( new_data ) if should_transpose : result = result . T if inplace : return self . _update_inplace ( result ) else : return result . __finalize__ ( self , method = \"interpolate\" )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#isin","text":"def isin ( self , values ) -> 'DataFrame' Whether each element in the DataFrame is contained in values.","title":"isin"},{"location":"reference/hielen2/datalink_prova_df/#parameters_66","text":"values : iterable, Series, DataFrame or dict The result will only be true at a location if all the labels match. If values is a Series, that's the index. If values is a dict, the keys must be the column names, which must match. If values is a DataFrame, then both the index and column labels must match.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_74","text":"DataFrame DataFrame of booleans showing whether each element in the DataFrame is contained in values.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_73","text":"DataFrame.eq: Equality test for DataFrame. Series.isin: Equivalent method on Series. Series.str.contains: Test if pattern or regex is contained within a string of a Series or Index.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_76","text":"df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]}, ... index=['falcon', 'dog']) df num_legs num_wings falcon 2 2 dog 4 0 When values is a list check whether every value in the DataFrame is present in the list (which animals have 0 or 2 legs or wings) df.isin([0, 2]) num_legs num_wings falcon True True dog False True When values is a dict, we can pass values to check for each column separately: df.isin({'num_wings': [0, 3]}) num_legs num_wings falcon False False dog False True When values is a Series or DataFrame the index and column must match. Note that 'falcon' does not match based on the number of legs in df2. other = pd.DataFrame({'num_legs': [8, 2], 'num_wings': [0, 2]}, ... index=['spider', 'falcon']) df.isin(other) num_legs num_wings falcon True True dog False False View Source def isin ( self , values ) -> \"DataFrame\" : \"\"\" Whether each element in the DataFrame is contained in values. Parameters ---------- values : iterable, Series, DataFrame or dict The result will only be true at a location if all the labels match. If `values` is a Series, that's the index. If `values` is a dict, the keys must be the column names, which must match. If `values` is a DataFrame, then both the index and column labels must match. Returns ------- DataFrame DataFrame of booleans showing whether each element in the DataFrame is contained in values. See Also -------- DataFrame.eq: Equality test for DataFrame. Series.isin: Equivalent method on Series. Series.str.contains: Test if pattern or regex is contained within a string of a Series or Index. Examples -------- >>> df = pd.DataFrame({'num_legs': [2, 4], 'num_wings': [2, 0]}, ... index=['falcon', 'dog']) >>> df num_legs num_wings falcon 2 2 dog 4 0 When ``values`` is a list check whether every value in the DataFrame is present in the list (which animals have 0 or 2 legs or wings) >>> df.isin([0, 2]) num_legs num_wings falcon True True dog False True When ``values`` is a dict, we can pass values to check for each column separately: >>> df.isin({'num_wings': [0, 3]}) num_legs num_wings falcon False False dog False True When ``values`` is a Series or DataFrame the index and column must match. Note that 'falcon' does not match based on the number of legs in df2. >>> other = pd.DataFrame({'num_legs': [8, 2], 'num_wings': [0, 2]}, ... index=['spider', 'falcon']) >>> df.isin(other) num_legs num_wings falcon True True dog False False \"\"\" if isinstance ( values , dict ): from pandas . core . reshape . concat import concat values = collections . defaultdict ( list , values ) return concat ( ( self . iloc [:, [ i ]]. isin ( values [ col ]) for i , col in enumerate ( self . columns ) ), axis = 1 , ) elif isinstance ( values , Series ): if not values . index . is_unique : raise ValueError ( \"cannot compute isin with a duplicate axis.\" ) return self . eq ( values . reindex_like ( self ), axis = \"index\" ) elif isinstance ( values , DataFrame ): if not ( values . columns . is_unique and values . index . is_unique ): raise ValueError ( \"cannot compute isin with a duplicate axis.\" ) return self . eq ( values . reindex_like ( self )) else : if not is_list_like ( values ): raise TypeError ( \"only list-like or dict-like objects are allowed \" \"to be passed to DataFrame.isin(), \" f \"you passed a '{type(values).__name__}'\" ) return self . _constructor ( algorithms . isin ( self . values . ravel (), values ). reshape ( self . shape ), self . index , self . columns , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#isna","text":"def isna ( self ) -> 'DataFrame' Detect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None or :attr: numpy.NaN , gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ).","title":"isna"},{"location":"reference/hielen2/datalink_prova_df/#returns_75","text":"DataFrame Mask of bool values for each element in DataFrame that indicates whether an element is not an NA value.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_74","text":"DataFrame.isnull : Alias of isna. DataFrame.notna : Boolean inverse of isna. DataFrame.dropna : Omit axes labels with missing values. isna : Top-level isna.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_77","text":"Show which entries in a DataFrame are NA. df = pd.DataFrame({'age': [5, 6, np.NaN], ... 'born': [pd.NaT, pd.Timestamp('1939-05-27'), ... pd.Timestamp('1940-04-25')], ... 'name': ['Alfred', 'Batman', ''], ... 'toy': [None, 'Batmobile', 'Joker']}) df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939-05-27 Batman Batmobile 2 NaN 1940-04-25 Joker df.isna() age born name toy 0 False True False True 1 False False False False 2 True False False False Show which entries in a Series are NA. ser = pd.Series([5, 6, np.NaN]) ser 0 5.0 1 6.0 2 NaN dtype: float64 ser.isna() 0 False 1 False 2 True dtype: bool View Source @doc ( NDFrame . isna , klass = _shared_doc_kwargs [ \"klass\" ] ) def isna ( self ) -> \"DataFrame\" : result = self . _constructor ( self . _data . isna ( func = isna )) return result . __finalize__ ( self , method = \"isna\" )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#isnull","text":"def isnull ( self ) -> 'DataFrame' Detect missing values. Return a boolean same-sized object indicating if the values are NA. NA values, such as None or :attr: numpy.NaN , gets mapped to True values. Everything else gets mapped to False values. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ).","title":"isnull"},{"location":"reference/hielen2/datalink_prova_df/#returns_76","text":"DataFrame Mask of bool values for each element in DataFrame that indicates whether an element is not an NA value.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_75","text":"DataFrame.isnull : Alias of isna. DataFrame.notna : Boolean inverse of isna. DataFrame.dropna : Omit axes labels with missing values. isna : Top-level isna.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_78","text":"Show which entries in a DataFrame are NA. df = pd.DataFrame({'age': [5, 6, np.NaN], ... 'born': [pd.NaT, pd.Timestamp('1939-05-27'), ... pd.Timestamp('1940-04-25')], ... 'name': ['Alfred', 'Batman', ''], ... 'toy': [None, 'Batmobile', 'Joker']}) df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939-05-27 Batman Batmobile 2 NaN 1940-04-25 Joker df.isna() age born name toy 0 False True False True 1 False False False False 2 True False False False Show which entries in a Series are NA. ser = pd.Series([5, 6, np.NaN]) ser 0 5.0 1 6.0 2 NaN dtype: float64 ser.isna() 0 False 1 False 2 True dtype: bool View Source @doc ( NDFrame . isna , klass = _shared_doc_kwargs [ \"klass\" ] ) def isnull ( self ) -> \"DataFrame\" : return self . isna ()","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#items","text":"def items ( self ) -> Iterable [ Tuple [ Union [ Hashable , NoneType ], pandas . core . series . Series ]] Iterate over (column name, Series) pairs. Iterates over the DataFrame columns, returning a tuple with the column name and the content as a Series.","title":"items"},{"location":"reference/hielen2/datalink_prova_df/#yields","text":"label : object The column names for the DataFrame being iterated over. content : Series The column entries belonging to each label, as a Series.","title":"Yields"},{"location":"reference/hielen2/datalink_prova_df/#see-also_76","text":"DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs. DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_79","text":"df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'], ... 'population': [1864, 22000, 80000]}, ... index=['panda', 'polar', 'koala']) df species population panda bear 1864 polar bear 22000 koala marsupial 80000 for label, content in df.items(): ... print(f'label: {label}') ... print(f'content: {content}', sep='\\n') ... label: species content: panda bear polar bear koala marsupial Name: species, dtype: object label: population content: panda 1864 polar 22000 koala 80000 Name: population, dtype: int64 View Source @Appender ( _shared_docs [ \"items\" ] ) def items ( self ) -> Iterable [ Tuple[Label, Series ] ]: if self . columns . is_unique and hasattr ( self , \"_item_cache\" ) : for k in self . columns : yield k , self . _get_item_cache ( k ) else : for i , k in enumerate ( self . columns ) : yield k , self . _ixs ( i , axis = 1 )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#iteritems","text":"def iteritems ( self ) -> Iterable [ Tuple [ Union [ Hashable , NoneType ], pandas . core . series . Series ]] Iterate over (column name, Series) pairs. Iterates over the DataFrame columns, returning a tuple with the column name and the content as a Series.","title":"iteritems"},{"location":"reference/hielen2/datalink_prova_df/#yields_1","text":"label : object The column names for the DataFrame being iterated over. content : Series The column entries belonging to each label, as a Series.","title":"Yields"},{"location":"reference/hielen2/datalink_prova_df/#see-also_77","text":"DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs. DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_80","text":"df = pd.DataFrame({'species': ['bear', 'bear', 'marsupial'], ... 'population': [1864, 22000, 80000]}, ... index=['panda', 'polar', 'koala']) df species population panda bear 1864 polar bear 22000 koala marsupial 80000 for label, content in df.items(): ... print(f'label: {label}') ... print(f'content: {content}', sep='\\n') ... label: species content: panda bear polar bear koala marsupial Name: species, dtype: object label: population content: panda 1864 polar 22000 koala 80000 Name: population, dtype: int64 View Source @Appender ( _shared_docs [ \"items\" ] ) def iteritems ( self ) -> Iterable [ Tuple[Label, Series ] ]: yield from self . items ()","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#iterrows","text":"def iterrows ( self ) -> Iterable [ Tuple [ Union [ Hashable , NoneType ], pandas . core . series . Series ]] Iterate over DataFrame rows as (index, Series) pairs.","title":"iterrows"},{"location":"reference/hielen2/datalink_prova_df/#yields_2","text":"index : label or tuple of label The index of the row. A tuple for a MultiIndex . data : Series The data of the row as a Series. it : generator A generator that iterates over the rows of the frame.","title":"Yields"},{"location":"reference/hielen2/datalink_prova_df/#see-also_78","text":"DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values. DataFrame.items : Iterate over (column name, Series) pairs.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_35","text":"Because iterrows returns a Series for each row, it does not preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). For example, df = pd.DataFrame([[1, 1.5]], columns=['int', 'float']) row = next(df.iterrows())[1] row int 1.0 float 1.5 Name: 0, dtype: float64 print(row['int'].dtype) float64 print(df['int'].dtype) int64 To preserve dtypes while iterating over the rows, it is better to use :meth: itertuples which returns namedtuples of the values and which is generally faster than iterrows . You should never modify something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect. View Source def iterrows ( self ) -> Iterable [ Tuple [ Label , Series ]]: \"\"\" Iterate over DataFrame rows as (index, Series) pairs. Yields ------ index : label or tuple of label The index of the row. A tuple for a `MultiIndex`. data : Series The data of the row as a Series. it : generator A generator that iterates over the rows of the frame. See Also -------- DataFrame.itertuples : Iterate over DataFrame rows as namedtuples of the values. DataFrame.items : Iterate over (column name, Series) pairs. Notes ----- 1. Because ``iterrows`` returns a Series for each row, it does **not** preserve dtypes across the rows (dtypes are preserved across columns for DataFrames). For example, >>> df = pd.DataFrame([[1, 1.5]], columns=['int', 'float']) >>> row = next(df.iterrows())[1] >>> row int 1.0 float 1.5 Name: 0, dtype: float64 >>> print(row['int'].dtype) float64 >>> print(df['int'].dtype) int64 To preserve dtypes while iterating over the rows, it is better to use :meth:`itertuples` which returns namedtuples of the values and which is generally faster than ``iterrows``. 2. You should **never modify** something you are iterating over. This is not guaranteed to work in all cases. Depending on the data types, the iterator returns a copy and not a view, and writing to it will have no effect. \"\"\" columns = self . columns klass = self . _constructor_sliced for k , v in zip ( self . index , self . values ): s = klass ( v , index = columns , name = k ) yield k , s","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#itertuples","text":"def itertuples ( self , index = True , name = 'Pandas' ) Iterate over DataFrame rows as namedtuples.","title":"itertuples"},{"location":"reference/hielen2/datalink_prova_df/#parameters_67","text":"index : bool, default True If True, return the index as the first element of the tuple. name : str or None, default \"Pandas\" The name of the returned namedtuples or None to return regular tuples.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_77","text":"iterator An object to iterate over namedtuples for each row in the DataFrame with the first field possibly being the index and following fields being the column values.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_79","text":"DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs. DataFrame.items : Iterate over (column name, Series) pairs.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_36","text":"The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. On python versions < 3.7 regular tuples are returned for DataFrames with a large number of columns (>254).","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_81","text":"df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]}, ... index=['dog', 'hawk']) df num_legs num_wings dog 4 0 hawk 2 2 for row in df.itertuples(): ... print(row) ... Pandas(Index='dog', num_legs=4, num_wings=0) Pandas(Index='hawk', num_legs=2, num_wings=2) By setting the index parameter to False we can remove the index as the first element of the tuple: for row in df.itertuples(index=False): ... print(row) ... Pandas(num_legs=4, num_wings=0) Pandas(num_legs=2, num_wings=2) With the name parameter set we set a custom name for the yielded namedtuples: for row in df.itertuples(name='Animal'): ... print(row) ... Animal(Index='dog', num_legs=4, num_wings=0) Animal(Index='hawk', num_legs=2, num_wings=2) View Source def itertuples ( self , index = True , name = \"Pandas\" ): \"\"\" Iterate over DataFrame rows as namedtuples. Parameters ---------- index : bool, default True If True, return the index as the first element of the tuple. name : str or None, default \" Pandas \" The name of the returned namedtuples or None to return regular tuples. Returns ------- iterator An object to iterate over namedtuples for each row in the DataFrame with the first field possibly being the index and following fields being the column values. See Also -------- DataFrame.iterrows : Iterate over DataFrame rows as (index, Series) pairs. DataFrame.items : Iterate over (column name, Series) pairs. Notes ----- The column names will be renamed to positional names if they are invalid Python identifiers, repeated, or start with an underscore. On python versions < 3.7 regular tuples are returned for DataFrames with a large number of columns (>254). Examples -------- >>> df = pd.DataFrame({'num_legs': [4, 2], 'num_wings': [0, 2]}, ... index=['dog', 'hawk']) >>> df num_legs num_wings dog 4 0 hawk 2 2 >>> for row in df.itertuples(): ... print(row) ... Pandas(Index='dog', num_legs=4, num_wings=0) Pandas(Index='hawk', num_legs=2, num_wings=2) By setting the `index` parameter to False we can remove the index as the first element of the tuple: >>> for row in df.itertuples(index=False): ... print(row) ... Pandas(num_legs=4, num_wings=0) Pandas(num_legs=2, num_wings=2) With the `name` parameter set we set a custom name for the yielded namedtuples: >>> for row in df.itertuples(name='Animal'): ... print(row) ... Animal(Index='dog', num_legs=4, num_wings=0) Animal(Index='hawk', num_legs=2, num_wings=2) \"\"\" arrays = [] fields = list ( self . columns ) if index : arrays . append ( self . index ) fields . insert ( 0 , \"Index\" ) # use integer indexing because of possible duplicate column names arrays . extend ( self . iloc [:, k ] for k in range ( len ( self . columns ))) # Python versions before 3.7 support at most 255 arguments to constructors can_return_named_tuples = PY37 or len ( self . columns ) + index < 255 if name is not None and can_return_named_tuples : itertuple = collections . namedtuple ( name , fields , rename = True ) return map ( itertuple . _make , zip ( * arrays )) # fallback to regular tuples return zip ( * arrays )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#join","text":"def join ( self , other , on = None , how = 'left' , lsuffix = '' , rsuffix = '' , sort = False ) -> 'DataFrame' Join columns of another DataFrame. Join columns with other DataFrame either on index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list.","title":"join"},{"location":"reference/hielen2/datalink_prova_df/#parameters_68","text":"other : DataFrame, Series, or list of DataFrame Index should be similar to one of the columns in this one. If a Series is passed, its name attribute must be set, and that will be used as the column name in the resulting joined DataFrame. on : str, list of str, or array-like, optional Column or index level name(s) in the caller to join on the index in other , otherwise joins index-on-index. If multiple values given, the other DataFrame must have a MultiIndex. Can pass an array as the join key if it is not already contained in the calling DataFrame. Like an Excel VLOOKUP operation. how : {'left', 'right', 'outer', 'inner'}, default 'left' How to handle the operation of the two objects. * left : use calling frame 's index (or column if on is specified) * right: use `other`' s index . * outer : form union of calling frame 's index (or column if on is specified) with `other`' s index , and sort it . lexicographically . * inner : form intersection of calling frame 's index (or column if on is specified) with `other`' s index , preserving the order of the calling ' s one . lsuffix : str, default '' Suffix to use from left frame's overlapping columns. rsuffix : str, default '' Suffix to use from right frame's overlapping columns. sort : bool, default False Order result DataFrame lexicographically by the join key. If False, the order of the join key depends on the join type (how keyword).","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_78","text":"DataFrame A dataframe containing columns from both the caller and other .","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_80","text":"DataFrame.merge : For column(s)-on-columns(s) operations.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_37","text":"Parameters on , lsuffix , and rsuffix are not supported when passing a list of DataFrame objects. Support for specifying index levels as the on parameter was added in version 0.23.0.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_82","text":"df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'], ... 'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']}) df key A 0 K0 A0 1 K1 A1 2 K2 A2 3 K3 A3 4 K4 A4 5 K5 A5 other = pd.DataFrame({'key': ['K0', 'K1', 'K2'], ... 'B': ['B0', 'B1', 'B2']}) other key B 0 K0 B0 1 K1 B1 2 K2 B2 Join DataFrames using their indexes. df.join(other, lsuffix='_caller', rsuffix='_other') key_caller A key_other B 0 K0 A0 K0 B0 1 K1 A1 K1 B1 2 K2 A2 K2 B2 3 K3 A3 NaN NaN 4 K4 A4 NaN NaN 5 K5 A5 NaN NaN If we want to join using the key columns, we need to set key to be the index in both df and other . The joined DataFrame will have key as its index. df.set_index('key').join(other.set_index('key')) A B key K0 A0 B0 K1 A1 B1 K2 A2 B2 K3 A3 NaN K4 A4 NaN K5 A5 NaN Another option to join using the key columns is to use the on parameter. DataFrame.join always uses other 's index but we can use any column in df . This method preserves the original DataFrame's index in the result. df.join(other.set_index('key'), on='key') key A B 0 K0 A0 B0 1 K1 A1 B1 2 K2 A2 B2 3 K3 A3 NaN 4 K4 A4 NaN 5 K5 A5 NaN View Source def join ( self , other , on = None , how = \"left\" , lsuffix = \"\" , rsuffix = \"\" , sort = False ) -> \"DataFrame\" : \"\"\" Join columns of another DataFrame. Join columns with `other` DataFrame either on index or on a key column. Efficiently join multiple DataFrame objects by index at once by passing a list. Parameters ---------- other : DataFrame, Series, or list of DataFrame Index should be similar to one of the columns in this one. If a Series is passed, its name attribute must be set, and that will be used as the column name in the resulting joined DataFrame. on : str, list of str, or array-like, optional Column or index level name(s) in the caller to join on the index in `other`, otherwise joins index-on-index. If multiple values given, the `other` DataFrame must have a MultiIndex. Can pass an array as the join key if it is not already contained in the calling DataFrame. Like an Excel VLOOKUP operation. how : {'left', 'right', 'outer', 'inner'}, default 'left' How to handle the operation of the two objects. * left: use calling frame's index (or column if on is specified) * right: use `other`'s index. * outer: form union of calling frame's index (or column if on is specified) with `other`'s index, and sort it. lexicographically. * inner: form intersection of calling frame's index (or column if on is specified) with `other`'s index, preserving the order of the calling's one. lsuffix : str, default '' Suffix to use from left frame's overlapping columns. rsuffix : str, default '' Suffix to use from right frame's overlapping columns. sort : bool, default False Order result DataFrame lexicographically by the join key. If False, the order of the join key depends on the join type (how keyword). Returns ------- DataFrame A dataframe containing columns from both the caller and `other`. See Also -------- DataFrame.merge : For column(s)-on-columns(s) operations. Notes ----- Parameters `on`, `lsuffix`, and `rsuffix` are not supported when passing a list of `DataFrame` objects. Support for specifying index levels as the `on` parameter was added in version 0.23.0. Examples -------- >>> df = pd.DataFrame({'key': ['K0', 'K1', 'K2', 'K3', 'K4', 'K5'], ... 'A': ['A0', 'A1', 'A2', 'A3', 'A4', 'A5']}) >>> df key A 0 K0 A0 1 K1 A1 2 K2 A2 3 K3 A3 4 K4 A4 5 K5 A5 >>> other = pd.DataFrame({'key': ['K0', 'K1', 'K2'], ... 'B': ['B0', 'B1', 'B2']}) >>> other key B 0 K0 B0 1 K1 B1 2 K2 B2 Join DataFrames using their indexes. >>> df.join(other, lsuffix='_caller', rsuffix='_other') key_caller A key_other B 0 K0 A0 K0 B0 1 K1 A1 K1 B1 2 K2 A2 K2 B2 3 K3 A3 NaN NaN 4 K4 A4 NaN NaN 5 K5 A5 NaN NaN If we want to join using the key columns, we need to set key to be the index in both `df` and `other`. The joined DataFrame will have key as its index. >>> df.set_index('key').join(other.set_index('key')) A B key K0 A0 B0 K1 A1 B1 K2 A2 B2 K3 A3 NaN K4 A4 NaN K5 A5 NaN Another option to join using the key columns is to use the `on` parameter. DataFrame.join always uses `other`'s index but we can use any column in `df`. This method preserves the original DataFrame's index in the result. >>> df.join(other.set_index('key'), on='key') key A B 0 K0 A0 B0 1 K1 A1 B1 2 K2 A2 B2 3 K3 A3 NaN 4 K4 A4 NaN 5 K5 A5 NaN \"\"\" return self . _join_compat ( other , on = on , how = how , lsuffix = lsuffix , rsuffix = rsuffix , sort = sort )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#keys","text":"def keys ( self ) Get the 'info axis' (see Indexing for more). This is index for Series, columns for DataFrame.","title":"keys"},{"location":"reference/hielen2/datalink_prova_df/#returns_79","text":"Index Info axis. View Source def keys ( self ): \"\"\" Get the 'info axis' (see Indexing for more). This is index for Series, columns for DataFrame. Returns ------- Index Info axis. \"\"\" return self . _info_axis","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#kurt","text":"def kurt ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return unbiased kurtosis over requested axis. Kurtosis obtained using Fisher's definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1.","title":"kurt"},{"location":"reference/hielen2/datalink_prova_df/#parameters_69","text":"axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_80","text":"Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#kurtosis","text":"def kurtosis ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return unbiased kurtosis over requested axis. Kurtosis obtained using Fisher's definition of kurtosis (kurtosis of normal == 0.0). Normalized by N-1.","title":"kurtosis"},{"location":"reference/hielen2/datalink_prova_df/#parameters_70","text":"axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_81","text":"Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#last","text":"def last ( self : ~ FrameOrSeries , offset ) -> ~ FrameOrSeries Select final periods of time series data based on a date offset. When having a DataFrame with dates as index, this function can select the last few rows based on a date offset.","title":"last"},{"location":"reference/hielen2/datalink_prova_df/#parameters_71","text":"offset : str, DateOffset, dateutil.relativedelta The offset length of the data that will be selected. For instance, '3D' will display all the rows having their index within the last 3 days.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_82","text":"Series or DataFrame A subset of the caller.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_10","text":"TypeError If the index is not a :class: DatetimeIndex","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_81","text":"first : Select initial periods of time series based on a date offset. at_time : Select values at a particular time of the day. between_time : Select values between particular times of the day.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_83","text":"i = pd.date_range('2018-04-09', periods=4, freq='2D') ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) ts A 2018-04-09 1 2018-04-11 2 2018-04-13 3 2018-04-15 4 Get the rows for the last 3 days: ts.last('3D') A 2018-04-13 3 2018-04-15 4 Notice the data for 3 last calendar days were returned, not the last 3 observed days in the dataset, and therefore data for 2018-04-11 was not returned. View Source def last ( self : FrameOrSeries , offset ) -> FrameOrSeries : \"\"\" Select final periods of time series data based on a date offset. When having a DataFrame with dates as index, this function can select the last few rows based on a date offset. Parameters ---------- offset : str, DateOffset, dateutil.relativedelta The offset length of the data that will be selected. For instance, '3D' will display all the rows having their index within the last 3 days. Returns ------- Series or DataFrame A subset of the caller. Raises ------ TypeError If the index is not a :class:`DatetimeIndex` See Also -------- first : Select initial periods of time series based on a date offset. at_time : Select values at a particular time of the day. between_time : Select values between particular times of the day. Examples -------- >>> i = pd.date_range('2018-04-09', periods=4, freq='2D') >>> ts = pd.DataFrame({'A': [1, 2, 3, 4]}, index=i) >>> ts A 2018-04-09 1 2018-04-11 2 2018-04-13 3 2018-04-15 4 Get the rows for the last 3 days: >>> ts.last('3D') A 2018-04-13 3 2018-04-15 4 Notice the data for 3 last calendar days were returned, not the last 3 observed days in the dataset, and therefore data for 2018-04-11 was not returned. \"\"\" if not isinstance ( self . index , DatetimeIndex ): raise TypeError ( \"'last' only supports a DatetimeIndex index\" ) if len ( self . index ) == 0 : return self offset = to_offset ( offset ) start_date = self . index [ - 1 ] - offset start = self . index . searchsorted ( start_date , side = \"right\" ) return self . iloc [ start :]","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#last_valid_index","text":"def last_valid_index ( self ) Return index for last non-NA/null value.","title":"last_valid_index"},{"location":"reference/hielen2/datalink_prova_df/#returns_83","text":"scalar : type of index","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#notes_38","text":"If all elements are non-NA/null, returns None. Also returns None for empty Series/DataFrame. View Source @doc ( first_valid_index , position = \"last\" , klass = _shared_doc_kwargs [ \"klass\" ] ) def last_valid_index ( self ) : return self . _find_valid_index ( \"last\" )","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#le","text":"def le ( self , other , axis = 'columns' , level = None ) Get Less than or equal to of dataframe and other, element-wise (binary operator le ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , =! , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison.","title":"le"},{"location":"reference/hielen2/datalink_prova_df/#parameters_72","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'}, default 'columns' Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_84","text":"DataFrame of bool Result of the comparison.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_82","text":"DataFrame.eq : Compare DataFrames for equality elementwise. DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_39","text":"Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ).","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_84","text":"df = pd.DataFrame({'cost': [250, 150, 100], ... 'revenue': [100, 250, 300]}, ... index=['A', 'B', 'C']) df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: df == 100 cost revenue A False True B False False C True False df.eq(100) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: df != pd.Series([100, 250], index=[\"cost\", \"revenue\"]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index') cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : df == [250, 100] cost revenue A True True B False False C False False Use the method to control the axis: df.eq([250, 250, 100], axis='index') cost revenue A True False B False True C True False Compare to a DataFrame of different shape. other = pd.DataFrame({'revenue': [300, 250, 100, 150]}, ... index=['A', 'B', 'C', 'D']) other revenue A 300 B 250 C 100 D 150 df.gt(other) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220], ... 'revenue': [100, 250, 300, 200, 175, 225]}, ... index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'], ... ['A', 'B', 'C', 'A', 'B', 'C']]) df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 df.le(df_multindex, level=1) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None ) : axis = self . _get_axis_number ( axis ) if axis is not None else 1 self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) new_data = dispatch_to_series ( self , other , op , axis = axis ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#lookup","text":"def lookup ( self , row_labels , col_labels ) -> numpy . ndarray Label-based \"fancy indexing\" function for DataFrame. Given equal-length arrays of row and column labels, return an array of the values corresponding to each (row, col) pair.","title":"lookup"},{"location":"reference/hielen2/datalink_prova_df/#parameters_73","text":"row_labels : sequence The row labels to use for lookup. col_labels : sequence The column labels to use for lookup.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_85","text":"numpy.ndarray The found values. View Source def lookup ( self , row_labels , col_labels ) -> np . ndarray : \"\"\" Label-based \" fancy indexing \" function for DataFrame. Given equal-length arrays of row and column labels, return an array of the values corresponding to each (row, col) pair. Parameters ---------- row_labels : sequence The row labels to use for lookup. col_labels : sequence The column labels to use for lookup. Returns ------- numpy.ndarray The found values. \"\"\" n = len ( row_labels ) if n != len ( col_labels ) : raise ValueError ( \"Row labels must have same size as column labels\" ) if not ( self . index . is_unique and self . columns . is_unique ) : # GH#33041 raise ValueError ( \"DataFrame.lookup requires unique index and columns\" ) thresh = 1000 if not self . _is_mixed_type or n > thresh : values = self . values ridx = self . index . get_indexer ( row_labels ) cidx = self . columns . get_indexer ( col_labels ) if ( ridx == - 1 ). any () : raise KeyError ( \"One or more row labels was not found\" ) if ( cidx == - 1 ). any () : raise KeyError ( \"One or more column labels was not found\" ) flat_index = ridx * len ( self . columns ) + cidx result = values . flat [ flat_index ] else : result = np . empty ( n , dtype = \"O\" ) for i , ( r , c ) in enumerate ( zip ( row_labels , col_labels )) : result [ i ] = self . _get_value ( r , c ) if is_object_dtype ( result ) : result = lib . maybe_convert_objects ( result ) return result","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#lt","text":"def lt ( self , other , axis = 'columns' , level = None ) Get Less than of dataframe and other, element-wise (binary operator lt ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , =! , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison.","title":"lt"},{"location":"reference/hielen2/datalink_prova_df/#parameters_74","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'}, default 'columns' Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_86","text":"DataFrame of bool Result of the comparison.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_83","text":"DataFrame.eq : Compare DataFrames for equality elementwise. DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_40","text":"Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ).","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_85","text":"df = pd.DataFrame({'cost': [250, 150, 100], ... 'revenue': [100, 250, 300]}, ... index=['A', 'B', 'C']) df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: df == 100 cost revenue A False True B False False C True False df.eq(100) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: df != pd.Series([100, 250], index=[\"cost\", \"revenue\"]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index') cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : df == [250, 100] cost revenue A True True B False False C False False Use the method to control the axis: df.eq([250, 250, 100], axis='index') cost revenue A True False B False True C True False Compare to a DataFrame of different shape. other = pd.DataFrame({'revenue': [300, 250, 100, 150]}, ... index=['A', 'B', 'C', 'D']) other revenue A 300 B 250 C 100 D 150 df.gt(other) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220], ... 'revenue': [100, 250, 300, 200, 175, 225]}, ... index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'], ... ['A', 'B', 'C', 'A', 'B', 'C']]) df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 df.le(df_multindex, level=1) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None ) : axis = self . _get_axis_number ( axis ) if axis is not None else 1 self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) new_data = dispatch_to_series ( self , other , op , axis = axis ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#mad","text":"def mad ( self , axis = None , skipna = None , level = None ) Return the mean absolute deviation of the values for the requested axis.","title":"mad"},{"location":"reference/hielen2/datalink_prova_df/#parameters_75","text":"axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default None Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_87","text":"Series or DataFrame (if level specified) View Source @doc ( desc = \"Return the mean absolute deviation of the values \" \"for the requested axis.\" , name1 = name1 , name2 = name2 , axis_descr = axis_descr , see_also = \"\" , examples = \"\" , ) def mad ( self , axis = None , skipna = None , level = None ) : \"\"\" {desc} Parameters ---------- axis : {axis_descr} Axis for the function to be applied on. skipna : bool, default None Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a {name1}. Returns ------- {name1} or {name2} (if level specified)\\ {see_also}\\ {examples} \"\"\" if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( \"mad\" , axis = axis , level = level , skipna = skipna ) data = self . _get_numeric_data () if axis == 0 : demeaned = data - data . mean ( axis = 0 ) else : demeaned = data . sub ( data . mean ( axis = 1 ), axis = 0 ) return np . abs ( demeaned ). mean ( axis = axis , skipna = skipna )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#mask","text":"def mask ( self , cond , other = nan , inplace = False , axis = None , level = None , errors = 'raise' , try_cast = False ) Replace values where the condition is True.","title":"mask"},{"location":"reference/hielen2/datalink_prova_df/#parameters_76","text":"cond : bool Series/DataFrame, array-like, or callable Where cond is False, keep the original value. Where True, replace with corresponding value from other . If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn't check it). other : scalar, Series/DataFrame, or callable Entries where cond is True are replaced with corresponding value from other . If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn't check it). inplace : bool, default False Whether to perform the operation in place on the data. axis : int, default None Alignment axis if needed. level : int, default None Alignment level if needed. errors : str, {'raise', 'ignore'}, default 'raise' Note that currently this parameter won't affect the results and will always coerce to a suitable dtype. - 'raise' : allow exceptions to be raised. - 'ignore' : suppress exceptions. On error return original object. try_cast : bool, default False Try to cast the result back to the input type (if possible).","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_88","text":"Same type as caller","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_84","text":":func: DataFrame.where : Return an object of same shape as self.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_41","text":"The mask method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is False the element is used; otherwise the corresponding element from the DataFrame other is used. The signature for :func: DataFrame.where differs from :func: numpy.where . Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2) . For further details and examples see the mask documentation in :ref: indexing <indexing.where_mask> .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_86","text":"s = pd.Series(range(5)) s.where(s > 0) 0 NaN 1 1.0 2 2.0 3 3.0 4 4.0 dtype: float64 s.mask(s > 0) 0 0.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 s.where(s > 1, 10) 0 10 1 10 2 2 3 3 4 4 dtype: int64 df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B']) df A B 0 0 1 1 2 3 2 4 5 3 6 7 4 8 9 m = df % 3 == 0 df.where(m, -df) A B 0 0 -1 1 -2 3 2 -4 -5 3 6 -7 4 -8 9 df.where(m, -df) == np.where(m, df, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True df.where(m, -df) == df.mask(~m, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True View Source @doc ( where , klass = _shared_doc_kwargs [ \"klass\" ] , cond = \"False\" , cond_rev = \"True\" , name = \"mask\" , name_other = \"where\" , ) def mask ( self , cond , other = np . nan , inplace = False , axis = None , level = None , errors = \"raise\" , try_cast = False , ) : inplace = validate_bool_kwarg ( inplace , \"inplace\" ) cond = com . apply_if_callable ( cond , self ) # see gh - 21891 if not hasattr ( cond , \"__invert__\" ) : cond = np . array ( cond ) return self . where ( ~ cond , other = other , inplace = inplace , axis = axis , level = level , try_cast = try_cast , errors = errors , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#max","text":"def max ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return the maximum of the values for the requested axis. If you want the index of the maximum, use idxmax . This isthe equivalent of the numpy.ndarray method argmax .","title":"max"},{"location":"reference/hielen2/datalink_prova_df/#parameters_77","text":"axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_89","text":"Series or DataFrame (if level specified)","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_85","text":"Series.sum : Return the sum. Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_87","text":"idx = pd.MultiIndex.from_arrays([ ... ['warm', 'warm', 'cold', 'cold'], ... ['dog', 'falcon', 'fish', 'spider']], ... names=['blooded', 'animal']) s = pd.Series([4, 2, 0, 8], name='legs', index=idx) s blooded animal warm dog 4 falcon 2 cold fish 0 spider 8 Name: legs, dtype: int64 s.max() 8 Max using level names, as well as indices. s.max(level='blooded') blooded warm 4 cold 8 Name: legs, dtype: int64 s.max(level=0) blooded warm 4 cold 8 Name: legs, dtype: int64 View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#mean","text":"def mean ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return the mean of the values for the requested axis.","title":"mean"},{"location":"reference/hielen2/datalink_prova_df/#parameters_78","text":"axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_90","text":"Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#median","text":"def median ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return the median of the values for the requested axis.","title":"median"},{"location":"reference/hielen2/datalink_prova_df/#parameters_79","text":"axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_91","text":"Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#melt","text":"def melt ( self , id_vars = None , value_vars = None , var_name = None , value_name = 'value' , col_level = None , ignore_index = True ) -> 'DataFrame' Unpivot a DataFrame from wide to long format, optionally leaving identifiers set. This function is useful to massage a DataFrame into a format where one or more columns are identifier variables ( id_vars ), while all other columns, considered measured variables ( value_vars ), are \"unpivoted\" to the row axis, leaving just two non-identifier columns, 'variable' and 'value'. .. versionadded:: 0.20.0","title":"melt"},{"location":"reference/hielen2/datalink_prova_df/#parameters_80","text":"id_vars : tuple, list, or ndarray, optional Column(s) to use as identifier variables. value_vars : tuple, list, or ndarray, optional Column(s) to unpivot. If not specified, uses all columns that are not set as id_vars . var_name : scalar Name to use for the 'variable' column. If None it uses frame.columns.name or 'variable'. value_name : scalar, default 'value' Name to use for the 'value' column. col_level : int or str, optional If columns are a MultiIndex then use this level to melt. ignore_index : bool, default True If True, original index is ignored. If False, the original index is retained. Index labels will be repeated as necessary. .. versionadded :: 1.1.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_92","text":"DataFrame Unpivoted DataFrame.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_86","text":"melt : Identical method. pivot_table : Create a spreadsheet-style pivot table as a DataFrame. DataFrame.pivot : Return reshaped DataFrame organized by given index / column values. DataFrame.explode : Explode a DataFrame from list-like columns to long format.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_88","text":"df = pd.DataFrame({'A': {0: 'a', 1: 'b', 2: 'c'}, ... 'B': {0: 1, 1: 3, 2: 5}, ... 'C': {0: 2, 1: 4, 2: 6}}) df A B C 0 a 1 2 1 b 3 4 2 c 5 6 df.melt(id_vars=['A'], value_vars=['B']) A variable value 0 a B 1 1 b B 3 2 c B 5 df.melt(id_vars=['A'], value_vars=['B', 'C']) A variable value 0 a B 1 1 b B 3 2 c B 5 3 a C 2 4 b C 4 5 c C 6 The names of 'variable' and 'value' columns can be customized: df.melt(id_vars=['A'], value_vars=['B'], ... var_name='myVarname', value_name='myValname') A myVarname myValname 0 a B 1 1 b B 3 2 c B 5 Original index values can be kept around: df.melt(id_vars=['A'], value_vars=['B', 'C'], ignore_index=False) A variable value 0 a B 1 1 b B 3 2 c B 5 0 a C 2 1 b C 4 2 c C 6 If you have multi-index columns: df.columns = [list('ABC'), list('DEF')] df A B C D E F 0 a 1 2 1 b 3 4 2 c 5 6 df.melt(col_level=0, id_vars=['A'], value_vars=['B']) A variable value 0 a B 1 1 b B 3 2 c B 5 df.melt(id_vars=[('A', 'D')], value_vars=[('B', 'E')]) (A, D) variable_0 variable_1 value 0 a B E 1 1 b B E 3 2 c B E 5 View Source @ Appender ( _shared_docs [ \"melt\" ] % dict( caller = \"df.melt(\" , versionadded = \"\\n .. versionadded:: 0.20.0\\n\" , other = \"melt\" , ) ) def melt( self , id_vars = None , value_vars = None , var_name = None , value_name = \"value\" , col_level = None , ignore_index = True , ) -> \"DataFrame\" : return melt( self , id_vars = id_vars , value_vars = value_vars , var_name = var_name , value_name = value_name , col_level = col_level , ignore_index = ignore_index , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#memory_usage","text":"def memory_usage ( self , index = True , deep = False ) -> pandas . core . series . Series Return the memory usage of each column in bytes. The memory usage can optionally include the contribution of the index and elements of object dtype. This value is displayed in DataFrame.info by default. This can be suppressed by setting pandas.options.display.memory_usage to False.","title":"memory_usage"},{"location":"reference/hielen2/datalink_prova_df/#parameters_81","text":"index : bool, default True Specifies whether to include the memory usage of the DataFrame's index in returned Series. If index=True , the memory usage of the index is the first item in the output. deep : bool, default False If True, introspect the data deeply by interrogating object dtypes for system-level memory consumption, and include it in the returned values.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_93","text":"Series A Series whose index is the original column names and whose values is the memory usage of each column in bytes.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_87","text":"numpy.ndarray.nbytes : Total bytes consumed by the elements of an ndarray. Series.memory_usage : Bytes consumed by a Series. Categorical : Memory-efficient array for string values with many repeated values. DataFrame.info : Concise summary of a DataFrame.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_89","text":"dtypes = ['int64', 'float64', 'complex128', 'object', 'bool'] data = dict([(t, np.ones(shape=5000).astype(t)) ... for t in dtypes]) df = pd.DataFrame(data) df.head() int64 float64 complex128 object bool 0 1 1.0 1.000000+0.000000j 1 True 1 1 1.0 1.000000+0.000000j 1 True 2 1 1.0 1.000000+0.000000j 1 True 3 1 1.0 1.000000+0.000000j 1 True 4 1 1.0 1.000000+0.000000j 1 True df.memory_usage() Index 128 int64 40000 float64 40000 complex128 80000 object 40000 bool 5000 dtype: int64 df.memory_usage(index=False) int64 40000 float64 40000 complex128 80000 object 40000 bool 5000 dtype: int64 The memory footprint of object dtype columns is ignored by default: df.memory_usage(deep=True) Index 128 int64 40000 float64 40000 complex128 80000 object 160000 bool 5000 dtype: int64 Use a Categorical for efficient storage of an object-dtype column with many repeated values. df['object'].astype('category').memory_usage(deep=True) 5216 View Source def memory_usage ( self , index = True , deep = False ) -> Series : \"\"\" Return the memory usage of each column in bytes. The memory usage can optionally include the contribution of the index and elements of `object` dtype. This value is displayed in `DataFrame.info` by default. This can be suppressed by setting ``pandas.options.display.memory_usage`` to False. Parameters ---------- index : bool, default True Specifies whether to include the memory usage of the DataFrame's index in returned Series. If ``index=True``, the memory usage of the index is the first item in the output. deep : bool, default False If True, introspect the data deeply by interrogating `object` dtypes for system-level memory consumption, and include it in the returned values. Returns ------- Series A Series whose index is the original column names and whose values is the memory usage of each column in bytes. See Also -------- numpy.ndarray.nbytes : Total bytes consumed by the elements of an ndarray. Series.memory_usage : Bytes consumed by a Series. Categorical : Memory-efficient array for string values with many repeated values. DataFrame.info : Concise summary of a DataFrame. Examples -------- >>> dtypes = ['int64', 'float64', 'complex128', 'object', 'bool'] >>> data = dict([(t, np.ones(shape=5000).astype(t)) ... for t in dtypes]) >>> df = pd.DataFrame(data) >>> df.head() int64 float64 complex128 object bool 0 1 1.0 1.000000+0.000000j 1 True 1 1 1.0 1.000000+0.000000j 1 True 2 1 1.0 1.000000+0.000000j 1 True 3 1 1.0 1.000000+0.000000j 1 True 4 1 1.0 1.000000+0.000000j 1 True >>> df.memory_usage() Index 128 int64 40000 float64 40000 complex128 80000 object 40000 bool 5000 dtype: int64 >>> df.memory_usage(index=False) int64 40000 float64 40000 complex128 80000 object 40000 bool 5000 dtype: int64 The memory footprint of `object` dtype columns is ignored by default: >>> df.memory_usage(deep=True) Index 128 int64 40000 float64 40000 complex128 80000 object 160000 bool 5000 dtype: int64 Use a Categorical for efficient storage of an object-dtype column with many repeated values. >>> df['object'].astype('category').memory_usage(deep=True) 5216 \"\"\" result = self . _constructor_sliced ( [ c . memory_usage ( index = False , deep = deep ) for col , c in self . items ()], index = self . columns , ) if index : result = self . _constructor_sliced ( self . index . memory_usage ( deep = deep ), index = [ \"Index\" ] ). append ( result ) return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#merge","text":"def merge ( self , right , how = 'inner' , on = None , left_on = None , right_on = None , left_index = False , right_index = False , sort = False , suffixes = ( '_x' , '_y' ), copy = True , indicator = False , validate = None ) -> 'DataFrame' Merge DataFrame or named Series objects with a database-style join. The join is done on columns or indexes. If joining columns on columns, the DataFrame indexes will be ignored . Otherwise if joining indexes on indexes or indexes on a column or columns, the index will be passed on.","title":"merge"},{"location":"reference/hielen2/datalink_prova_df/#parameters_82","text":"right : DataFrame or named Series Object to merge with. how : {'left', 'right', 'outer', 'inner'}, default 'inner' Type of merge to be performed. * left: use only keys from left frame, similar to a SQL left outer join; preserve key order. * right: use only keys from right frame, similar to a SQL right outer join; preserve key order. * outer: use union of keys from both frames, similar to a SQL full outer join; sort keys lexicographically. * inner: use intersection of keys from both frames, similar to a SQL inner join; preserve the order of the left keys. on : label or list Column or index level names to join on. These must be found in both DataFrames. If on is None and not merging on indexes then this defaults to the intersection of the columns in both DataFrames. left_on : label or list, or array-like Column or index level names to join on in the left DataFrame. Can also be an array or list of arrays of the length of the left DataFrame. These arrays are treated as if they are columns. right_on : label or list, or array-like Column or index level names to join on in the right DataFrame. Can also be an array or list of arrays of the length of the right DataFrame. These arrays are treated as if they are columns. left_index : bool, default False Use the index from the left DataFrame as the join key(s). If it is a MultiIndex, the number of keys in the other DataFrame (either the index or a number of columns) must match the number of levels. right_index : bool, default False Use the index from the right DataFrame as the join key. Same caveats as left_index. sort : bool, default False Sort the join keys lexicographically in the result DataFrame. If False, the order of the join keys depends on the join type (how keyword). suffixes : list-like, default is (\"_x\", \"_y\") A length-2 sequence where each element is optionally a string indicating the suffix to add to overlapping column names in left and right respectively. Pass a value of None instead of a string to indicate that the column name from left or right should be left as-is, with no suffix. At least one of the values must not be None. copy : bool, default True If False, avoid copy if possible. indicator : bool or str, default False If True, adds a column to the output DataFrame called \"_merge\" with information on the source of each row. The column can be given a different name by providing a string argument. The column will have a Categorical type with the value of \"left_only\" for observations whose merge key only appears in the left DataFrame, \"right_only\" for observations whose merge key only appears in the right DataFrame, and \"both\" if the observation's merge key is found in both DataFrames. validate : str, optional If specified, checks if merge is of specified type. * \"one_to_one\" or \"1:1\": check if merge keys are unique in both left and right datasets. * \"one_to_many\" or \"1:m\": check if merge keys are unique in left dataset. * \"many_to_one\" or \"m:1\": check if merge keys are unique in right dataset. * \"many_to_many\" or \"m:m\": allowed, but does not result in checks.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_94","text":"DataFrame A DataFrame of the two merged objects.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_88","text":"merge_ordered : Merge with optional filling/interpolation. merge_asof : Merge on nearest keys. DataFrame.join : Similar method using indices.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_42","text":"Support for specifying index levels as the on , left_on , and right_on parameters was added in version 0.23.0 Support for merging named Series objects was added in version 0.24.0","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_90","text":"df1 = pd.DataFrame({'lkey': ['foo', 'bar', 'baz', 'foo'], ... 'value': [1, 2, 3, 5]}) df2 = pd.DataFrame({'rkey': ['foo', 'bar', 'baz', 'foo'], ... 'value': [5, 6, 7, 8]}) df1 lkey value 0 foo 1 1 bar 2 2 baz 3 3 foo 5 df2 rkey value 0 foo 5 1 bar 6 2 baz 7 3 foo 8 Merge df1 and df2 on the lkey and rkey columns. The value columns have the default suffixes, _x and _y, appended. df1.merge(df2, left_on='lkey', right_on='rkey') lkey value_x rkey value_y 0 foo 1 foo 5 1 foo 1 foo 8 2 foo 5 foo 5 3 foo 5 foo 8 4 bar 2 bar 6 5 baz 3 baz 7 Merge DataFrames df1 and df2 with specified left and right suffixes appended to any overlapping columns. df1.merge(df2, left_on='lkey', right_on='rkey', ... suffixes=('_left', '_right')) lkey value_left rkey value_right 0 foo 1 foo 5 1 foo 1 foo 8 2 foo 5 foo 5 3 foo 5 foo 8 4 bar 2 bar 6 5 baz 3 baz 7 Merge DataFrames df1 and df2, but raise an exception if the DataFrames have any overlapping columns. df1.merge(df2, left_on='lkey', right_on='rkey', suffixes=(False, False)) Traceback (most recent call last): ... ValueError: columns overlap but no suffix specified: Index(['value'], dtype='object') View Source @Substitution ( \"\" ) @Appender ( _merge_doc , indents = 2 ) def merge ( self , right , how = \"inner\" , on = None , left_on = None , right_on = None , left_index = False , right_index = False , sort = False , suffixes = ( \"_x\" , \"_y\" ), copy = True , indicator = False , validate = None , ) -> \"DataFrame\" : from pandas.core.reshape.merge import merge return merge ( self , right , how = how , on = on , left_on = left_on , right_on = right_on , left_index = left_index , right_index = right_index , sort = sort , suffixes = suffixes , copy = copy , indicator = indicator , validate = validate , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#min","text":"def min ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return the minimum of the values for the requested axis. If you want the index of the minimum, use idxmin . This isthe equivalent of the numpy.ndarray method argmin .","title":"min"},{"location":"reference/hielen2/datalink_prova_df/#parameters_83","text":"axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_95","text":"Series or DataFrame (if level specified)","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_89","text":"Series.sum : Return the sum. Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_91","text":"idx = pd.MultiIndex.from_arrays([ ... ['warm', 'warm', 'cold', 'cold'], ... ['dog', 'falcon', 'fish', 'spider']], ... names=['blooded', 'animal']) s = pd.Series([4, 2, 0, 8], name='legs', index=idx) s blooded animal warm dog 4 falcon 2 cold fish 0 spider 8 Name: legs, dtype: int64 s.min() 0 Min using level names, as well as indices. s.min(level='blooded') blooded warm 2 cold 0 Name: legs, dtype: int64 s.min(level=0) blooded warm 2 cold 0 Name: legs, dtype: int64 View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#mod","text":"def mod ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Modulo of dataframe and other, element-wise (binary operator mod ). Equivalent to dataframe % other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmod . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"mod"},{"location":"reference/hielen2/datalink_prova_df/#parameters_84","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_96","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_90","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_43","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_92","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#mode","text":"def mode ( self , axis = 0 , numeric_only = False , dropna = True ) -> 'DataFrame' Get the mode(s) of each element along the selected axis. The mode of a set of values is the value that appears most often. It can be multiple values.","title":"mode"},{"location":"reference/hielen2/datalink_prova_df/#parameters_85","text":"axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to iterate over while searching for the mode: * 0 or 'index' : get mode of each column * 1 or 'columns' : get mode of each row. numeric_only : bool, default False If True, only apply to numeric columns. dropna : bool, default True Don't consider counts of NaN/NaT. .. versionadded :: 0.24.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_97","text":"DataFrame The modes of each column or row.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_91","text":"Series.mode : Return the highest frequency value in a Series. Series.value_counts : Return the counts of values in a Series.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_93","text":"df = pd.DataFrame([('bird', 2, 2), ... ('mammal', 4, np.nan), ... ('arthropod', 8, 0), ... ('bird', 2, np.nan)], ... index=('falcon', 'horse', 'spider', 'ostrich'), ... columns=('species', 'legs', 'wings')) df species legs wings falcon bird 2 2.0 horse mammal 4 NaN spider arthropod 8 0.0 ostrich bird 2 NaN By default, missing values are not considered, and the mode of wings are both 0 and 2. The second row of species and legs contains NaN , because they have only one mode, but the DataFrame has two rows. df.mode() species legs wings 0 bird 2.0 0.0 1 NaN NaN 2.0 Setting dropna=False NaN values are considered and they can be the mode (like for wings). df.mode(dropna=False) species legs wings 0 bird 2 NaN Setting numeric_only=True , only the mode of numeric columns is computed, and columns of other types are ignored. df.mode(numeric_only=True) legs wings 0 2.0 0.0 1 NaN 2.0 To compute the mode over columns and not rows, use the axis parameter: df.mode(axis='columns', numeric_only=True) 0 1 falcon 2.0 NaN horse 4.0 NaN spider 0.0 8.0 ostrich 2.0 NaN View Source def mode ( self , axis = 0 , numeric_only = False , dropna = True ) -> \"DataFrame\" : \"\"\" Get the mode(s) of each element along the selected axis. The mode of a set of values is the value that appears most often. It can be multiple values. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to iterate over while searching for the mode: * 0 or 'index' : get mode of each column * 1 or 'columns' : get mode of each row. numeric_only : bool, default False If True, only apply to numeric columns. dropna : bool, default True Don't consider counts of NaN/NaT. .. versionadded:: 0.24.0 Returns ------- DataFrame The modes of each column or row. See Also -------- Series.mode : Return the highest frequency value in a Series. Series.value_counts : Return the counts of values in a Series. Examples -------- >>> df = pd.DataFrame([('bird', 2, 2), ... ('mammal', 4, np.nan), ... ('arthropod', 8, 0), ... ('bird', 2, np.nan)], ... index=('falcon', 'horse', 'spider', 'ostrich'), ... columns=('species', 'legs', 'wings')) >>> df species legs wings falcon bird 2 2.0 horse mammal 4 NaN spider arthropod 8 0.0 ostrich bird 2 NaN By default, missing values are not considered, and the mode of wings are both 0 and 2. The second row of species and legs contains ``NaN``, because they have only one mode, but the DataFrame has two rows. >>> df.mode() species legs wings 0 bird 2.0 0.0 1 NaN NaN 2.0 Setting ``dropna=False`` ``NaN`` values are considered and they can be the mode (like for wings). >>> df.mode(dropna=False) species legs wings 0 bird 2 NaN Setting ``numeric_only=True``, only the mode of numeric columns is computed, and columns of other types are ignored. >>> df.mode(numeric_only=True) legs wings 0 2.0 0.0 1 NaN 2.0 To compute the mode over columns and not rows, use the axis parameter: >>> df.mode(axis='columns', numeric_only=True) 0 1 falcon 2.0 NaN horse 4.0 NaN spider 0.0 8.0 ostrich 2.0 NaN \"\"\" data = self if not numeric_only else self . _get_numeric_data () def f ( s ): return s . mode ( dropna = dropna ) return data . apply ( f , axis = axis )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#mul","text":"def mul ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Multiplication of dataframe and other, element-wise (binary operator mul ). Equivalent to dataframe * other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmul . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"mul"},{"location":"reference/hielen2/datalink_prova_df/#parameters_86","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_98","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_92","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_44","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_94","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#multiply","text":"def multiply ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Multiplication of dataframe and other, element-wise (binary operator mul ). Equivalent to dataframe * other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rmul . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"multiply"},{"location":"reference/hielen2/datalink_prova_df/#parameters_87","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_99","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_93","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_45","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_95","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#ne","text":"def ne ( self , other , axis = 'columns' , level = None ) Get Not equal to of dataframe and other, element-wise (binary operator ne ). Among flexible wrappers ( eq , ne , le , lt , ge , gt ) to comparison operators. Equivalent to == , =! , <= , < , >= , > with support to choose axis (rows or columns) and level for comparison.","title":"ne"},{"location":"reference/hielen2/datalink_prova_df/#parameters_88","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'}, default 'columns' Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_100","text":"DataFrame of bool Result of the comparison.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_94","text":"DataFrame.eq : Compare DataFrames for equality elementwise. DataFrame.ne : Compare DataFrames for inequality elementwise. DataFrame.le : Compare DataFrames for less than inequality or equality elementwise. DataFrame.lt : Compare DataFrames for strictly less than inequality elementwise. DataFrame.ge : Compare DataFrames for greater than inequality or equality elementwise. DataFrame.gt : Compare DataFrames for strictly greater than inequality elementwise.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_46","text":"Mismatched indices will be unioned together. NaN values are considered different (i.e. NaN != NaN ).","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_96","text":"df = pd.DataFrame({'cost': [250, 150, 100], ... 'revenue': [100, 250, 300]}, ... index=['A', 'B', 'C']) df cost revenue A 250 100 B 150 250 C 100 300 Comparison with a scalar, using either the operator or method: df == 100 cost revenue A False True B False False C True False df.eq(100) cost revenue A False True B False False C True False When other is a :class: Series , the columns of a DataFrame are aligned with the index of other and broadcast: df != pd.Series([100, 250], index=[\"cost\", \"revenue\"]) cost revenue A True True B True False C False True Use the method to control the broadcast axis: df.ne(pd.Series([100, 300], index=[\"A\", \"D\"]), axis='index') cost revenue A True False B True True C True True D True True When comparing to an arbitrary sequence, the number of columns must match the number elements in other : df == [250, 100] cost revenue A True True B False False C False False Use the method to control the axis: df.eq([250, 250, 100], axis='index') cost revenue A True False B False True C True False Compare to a DataFrame of different shape. other = pd.DataFrame({'revenue': [300, 250, 100, 150]}, ... index=['A', 'B', 'C', 'D']) other revenue A 300 B 250 C 100 D 150 df.gt(other) cost revenue A False False B False False C False True D False False Compare to a MultiIndex by level. df_multindex = pd.DataFrame({'cost': [250, 150, 100, 150, 300, 220], ... 'revenue': [100, 250, 300, 200, 175, 225]}, ... index=[['Q1', 'Q1', 'Q1', 'Q2', 'Q2', 'Q2'], ... ['A', 'B', 'C', 'A', 'B', 'C']]) df_multindex cost revenue Q1 A 250 100 B 150 250 C 100 300 Q2 A 150 200 B 300 175 C 220 225 df.le(df_multindex, level=1) cost revenue Q1 A True True B True True C True True Q2 A False True B True False C True False View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None ) : axis = self . _get_axis_number ( axis ) if axis is not None else 1 self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) new_data = dispatch_to_series ( self , other , op , axis = axis ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#nlargest","text":"def nlargest ( self , n , columns , keep = 'first' ) -> 'DataFrame' Return the first n rows ordered by columns in descending order. Return the first n rows with the largest values in columns , in descending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to df.sort_values(columns, ascending=False).head(n) , but more performant.","title":"nlargest"},{"location":"reference/hielen2/datalink_prova_df/#parameters_89","text":"n : int Number of rows to return. columns : label or list of labels Column label(s) to order by. keep : {'first', 'last', 'all'}, default 'first' Where there are duplicate values: - `first` : prioritize the first occurrence ( s ) - `last` : prioritize the last occurrence ( s ) - `` all `` : do not drop any duplicates , even it means selecting more than `n` items . .. versionadded :: 0 . 24 . 0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_101","text":"DataFrame The first n rows ordered by the given columns in descending order.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_95","text":"DataFrame.nsmallest : Return the first n rows ordered by columns in ascending order. DataFrame.sort_values : Sort DataFrame by the values. DataFrame.head : Return the first n rows without re-ordering.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_47","text":"This function cannot be used with all column types. For example, when specifying columns with object or category dtypes, TypeError is raised.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_97","text":"df = pd.DataFrame({'population': [59000000, 65000000, 434000, ... 434000, 434000, 337000, 11300, ... 11300, 11300], ... 'GDP': [1937894, 2583560 , 12011, 4520, 12128, ... 17036, 182, 38, 311], ... 'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\", ... \"IS\", \"NR\", \"TV\", \"AI\"]}, ... index=[\"Italy\", \"France\", \"Malta\", ... \"Maldives\", \"Brunei\", \"Iceland\", ... \"Nauru\", \"Tuvalu\", \"Anguilla\"]) df population GDP alpha-2 Italy 59000000 1937894 IT France 65000000 2583560 FR Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN Iceland 337000 17036 IS Nauru 11300 182 NR Tuvalu 11300 38 TV Anguilla 11300 311 AI In the following example, we will use nlargest to select the three rows having the largest values in column \"population\". df.nlargest(3, 'population') population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT When using keep='last' , ties are resolved in reverse order: df.nlargest(3, 'population', keep='last') population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Brunei 434000 12128 BN When using keep='all' , all duplicate items are maintained: df.nlargest(3, 'population', keep='all') population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN To order by the largest values in column \"population\" and then \"GDP\", we can specify multiple columns like in the next example. df.nlargest(3, ['population', 'GDP']) population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Brunei 434000 12128 BN View Source def nlargest ( self , n , columns , keep = \"first\" ) -> \"DataFrame\" : \"\"\" Return the first `n` rows ordered by `columns` in descending order. Return the first `n` rows with the largest values in `columns`, in descending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to ``df.sort_values(columns, ascending=False).head(n)``, but more performant. Parameters ---------- n : int Number of rows to return. columns : label or list of labels Column label(s) to order by. keep : {'first', 'last', 'all'}, default 'first' Where there are duplicate values: - `first` : prioritize the first occurrence(s) - `last` : prioritize the last occurrence(s) - ``all`` : do not drop any duplicates, even it means selecting more than `n` items. .. versionadded:: 0.24.0 Returns ------- DataFrame The first `n` rows ordered by the given columns in descending order. See Also -------- DataFrame.nsmallest : Return the first `n` rows ordered by `columns` in ascending order. DataFrame.sort_values : Sort DataFrame by the values. DataFrame.head : Return the first `n` rows without re-ordering. Notes ----- This function cannot be used with all column types. For example, when specifying columns with `object` or `category` dtypes, ``TypeError`` is raised. Examples -------- >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000, ... 434000, 434000, 337000, 11300, ... 11300, 11300], ... 'GDP': [1937894, 2583560 , 12011, 4520, 12128, ... 17036, 182, 38, 311], ... 'alpha-2': [\" IT \", \" FR \", \" MT \", \" MV \", \" BN \", ... \" IS \", \" NR \", \" TV \", \" AI \"]}, ... index=[\" Italy \", \" France \", \" Malta \", ... \" Maldives \", \" Brunei \", \" Iceland \", ... \" Nauru \", \" Tuvalu \", \" Anguilla \"]) >>> df population GDP alpha-2 Italy 59000000 1937894 IT France 65000000 2583560 FR Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN Iceland 337000 17036 IS Nauru 11300 182 NR Tuvalu 11300 38 TV Anguilla 11300 311 AI In the following example, we will use ``nlargest`` to select the three rows having the largest values in column \" population \". >>> df.nlargest(3, 'population') population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT When using ``keep='last'``, ties are resolved in reverse order: >>> df.nlargest(3, 'population', keep='last') population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Brunei 434000 12128 BN When using ``keep='all'``, all duplicate items are maintained: >>> df.nlargest(3, 'population', keep='all') population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN To order by the largest values in column \" population \" and then \" GDP \", we can specify multiple columns like in the next example. >>> df.nlargest(3, ['population', 'GDP']) population GDP alpha-2 France 65000000 2583560 FR Italy 59000000 1937894 IT Brunei 434000 12128 BN \"\"\" return algorithms . SelectNFrame ( self , n = n , keep = keep , columns = columns ). nlargest ()","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#notna","text":"def notna ( self ) -> 'DataFrame' Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). NA values, such as None or :attr: numpy.NaN , get mapped to False values.","title":"notna"},{"location":"reference/hielen2/datalink_prova_df/#returns_102","text":"DataFrame Mask of bool values for each element in DataFrame that indicates whether an element is not an NA value.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_96","text":"DataFrame.notnull : Alias of notna. DataFrame.isna : Boolean inverse of notna. DataFrame.dropna : Omit axes labels with missing values. notna : Top-level notna.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_98","text":"Show which entries in a DataFrame are not NA. df = pd.DataFrame({'age': [5, 6, np.NaN], ... 'born': [pd.NaT, pd.Timestamp('1939-05-27'), ... pd.Timestamp('1940-04-25')], ... 'name': ['Alfred', 'Batman', ''], ... 'toy': [None, 'Batmobile', 'Joker']}) df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939-05-27 Batman Batmobile 2 NaN 1940-04-25 Joker df.notna() age born name toy 0 True False True False 1 True True True True 2 False True True True Show which entries in a Series are not NA. ser = pd.Series([5, 6, np.NaN]) ser 0 5.0 1 6.0 2 NaN dtype: float64 ser.notna() 0 True 1 True 2 False dtype: bool View Source @doc ( NDFrame . notna , klass = _shared_doc_kwargs [ \"klass\" ] ) def notna ( self ) -> \"DataFrame\" : return ~ self . isna ()","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#notnull","text":"def notnull ( self ) -> 'DataFrame' Detect existing (non-missing) values. Return a boolean same-sized object indicating if the values are not NA. Non-missing values get mapped to True. Characters such as empty strings '' or :attr: numpy.inf are not considered NA values (unless you set pandas.options.mode.use_inf_as_na = True ). NA values, such as None or :attr: numpy.NaN , get mapped to False values.","title":"notnull"},{"location":"reference/hielen2/datalink_prova_df/#returns_103","text":"DataFrame Mask of bool values for each element in DataFrame that indicates whether an element is not an NA value.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_97","text":"DataFrame.notnull : Alias of notna. DataFrame.isna : Boolean inverse of notna. DataFrame.dropna : Omit axes labels with missing values. notna : Top-level notna.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_99","text":"Show which entries in a DataFrame are not NA. df = pd.DataFrame({'age': [5, 6, np.NaN], ... 'born': [pd.NaT, pd.Timestamp('1939-05-27'), ... pd.Timestamp('1940-04-25')], ... 'name': ['Alfred', 'Batman', ''], ... 'toy': [None, 'Batmobile', 'Joker']}) df age born name toy 0 5.0 NaT Alfred None 1 6.0 1939-05-27 Batman Batmobile 2 NaN 1940-04-25 Joker df.notna() age born name toy 0 True False True False 1 True True True True 2 False True True True Show which entries in a Series are not NA. ser = pd.Series([5, 6, np.NaN]) ser 0 5.0 1 6.0 2 NaN dtype: float64 ser.notna() 0 True 1 True 2 False dtype: bool View Source @doc ( NDFrame . notna , klass = _shared_doc_kwargs [ \"klass\" ] ) def notnull ( self ) -> \"DataFrame\" : return ~ self . isna ()","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#nsmallest","text":"def nsmallest ( self , n , columns , keep = 'first' ) -> 'DataFrame' Return the first n rows ordered by columns in ascending order. Return the first n rows with the smallest values in columns , in ascending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to df.sort_values(columns, ascending=True).head(n) , but more performant.","title":"nsmallest"},{"location":"reference/hielen2/datalink_prova_df/#parameters_90","text":"n : int Number of items to retrieve. columns : list or str Column name or names to order by. keep : {'first', 'last', 'all'}, default 'first' Where there are duplicate values: - `` first `` : take the first occurrence . - `` last `` : take the last occurrence . - `` all `` : do not drop any duplicates , even it means selecting more than `n` items . .. versionadded :: 0 . 24 . 0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_104","text":"DataFrame","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_98","text":"DataFrame.nlargest : Return the first n rows ordered by columns in descending order. DataFrame.sort_values : Sort DataFrame by the values. DataFrame.head : Return the first n rows without re-ordering.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_100","text":"df = pd.DataFrame({'population': [59000000, 65000000, 434000, ... 434000, 434000, 337000, 337000, ... 11300, 11300], ... 'GDP': [1937894, 2583560 , 12011, 4520, 12128, ... 17036, 182, 38, 311], ... 'alpha-2': [\"IT\", \"FR\", \"MT\", \"MV\", \"BN\", ... \"IS\", \"NR\", \"TV\", \"AI\"]}, ... index=[\"Italy\", \"France\", \"Malta\", ... \"Maldives\", \"Brunei\", \"Iceland\", ... \"Nauru\", \"Tuvalu\", \"Anguilla\"]) df population GDP alpha-2 Italy 59000000 1937894 IT France 65000000 2583560 FR Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN Iceland 337000 17036 IS Nauru 337000 182 NR Tuvalu 11300 38 TV Anguilla 11300 311 AI In the following example, we will use nsmallest to select the three rows having the smallest values in column \"population\". df.nsmallest(3, 'population') population GDP alpha-2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS When using keep='last' , ties are resolved in reverse order: df.nsmallest(3, 'population', keep='last') population GDP alpha-2 Anguilla 11300 311 AI Tuvalu 11300 38 TV Nauru 337000 182 NR When using keep='all' , all duplicate items are maintained: df.nsmallest(3, 'population', keep='all') population GDP alpha-2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS Nauru 337000 182 NR To order by the smallest values in column \"population\" and then \"GDP\", we can specify multiple columns like in the next example. df.nsmallest(3, ['population', 'GDP']) population GDP alpha-2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Nauru 337000 182 NR View Source def nsmallest ( self , n , columns , keep = \"first\" ) -> \"DataFrame\" : \"\"\" Return the first `n` rows ordered by `columns` in ascending order. Return the first `n` rows with the smallest values in `columns`, in ascending order. The columns that are not specified are returned as well, but not used for ordering. This method is equivalent to ``df.sort_values(columns, ascending=True).head(n)``, but more performant. Parameters ---------- n : int Number of items to retrieve. columns : list or str Column name or names to order by. keep : {'first', 'last', 'all'}, default 'first' Where there are duplicate values: - ``first`` : take the first occurrence. - ``last`` : take the last occurrence. - ``all`` : do not drop any duplicates, even it means selecting more than `n` items. .. versionadded:: 0.24.0 Returns ------- DataFrame See Also -------- DataFrame.nlargest : Return the first `n` rows ordered by `columns` in descending order. DataFrame.sort_values : Sort DataFrame by the values. DataFrame.head : Return the first `n` rows without re-ordering. Examples -------- >>> df = pd.DataFrame({'population': [59000000, 65000000, 434000, ... 434000, 434000, 337000, 337000, ... 11300, 11300], ... 'GDP': [1937894, 2583560 , 12011, 4520, 12128, ... 17036, 182, 38, 311], ... 'alpha-2': [\" IT \", \" FR \", \" MT \", \" MV \", \" BN \", ... \" IS \", \" NR \", \" TV \", \" AI \"]}, ... index=[\" Italy \", \" France \", \" Malta \", ... \" Maldives \", \" Brunei \", \" Iceland \", ... \" Nauru \", \" Tuvalu \", \" Anguilla \"]) >>> df population GDP alpha-2 Italy 59000000 1937894 IT France 65000000 2583560 FR Malta 434000 12011 MT Maldives 434000 4520 MV Brunei 434000 12128 BN Iceland 337000 17036 IS Nauru 337000 182 NR Tuvalu 11300 38 TV Anguilla 11300 311 AI In the following example, we will use ``nsmallest`` to select the three rows having the smallest values in column \" population \". >>> df.nsmallest(3, 'population') population GDP alpha-2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS When using ``keep='last'``, ties are resolved in reverse order: >>> df.nsmallest(3, 'population', keep='last') population GDP alpha-2 Anguilla 11300 311 AI Tuvalu 11300 38 TV Nauru 337000 182 NR When using ``keep='all'``, all duplicate items are maintained: >>> df.nsmallest(3, 'population', keep='all') population GDP alpha-2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Iceland 337000 17036 IS Nauru 337000 182 NR To order by the smallest values in column \" population \" and then \" GDP \", we can specify multiple columns like in the next example. >>> df.nsmallest(3, ['population', 'GDP']) population GDP alpha-2 Tuvalu 11300 38 TV Anguilla 11300 311 AI Nauru 337000 182 NR \"\"\" return algorithms . SelectNFrame ( self , n = n , keep = keep , columns = columns ). nsmallest ()","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#nunique","text":"def nunique ( self , axis = 0 , dropna = True ) -> pandas . core . series . Series Count distinct observations over requested axis. Return Series with number of distinct observations. Can ignore NaN values.","title":"nunique"},{"location":"reference/hielen2/datalink_prova_df/#parameters_91","text":"axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. dropna : bool, default True Don't include NaN in the counts.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_105","text":"Series","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_99","text":"Series.nunique: Method nunique for Series. DataFrame.count: Count non-NA cells for each column or row.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_101","text":"df = pd.DataFrame({'A': [1, 2, 3], 'B': [1, 1, 1]}) df.nunique() A 3 B 1 dtype: int64 df.nunique(axis=1) 0 1 1 2 2 2 dtype: int64 View Source def nunique ( self , axis = 0 , dropna = True ) -> Series : \"\"\" Count distinct observations over requested axis. Return Series with number of distinct observations. Can ignore NaN values. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to use. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. dropna : bool, default True Don't include NaN in the counts. Returns ------- Series See Also -------- Series.nunique: Method nunique for Series. DataFrame.count: Count non-NA cells for each column or row. Examples -------- >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [1, 1, 1]}) >>> df.nunique() A 3 B 1 dtype: int64 >>> df.nunique(axis=1) 0 1 1 2 2 2 dtype: int64 \"\"\" return self . apply ( Series . nunique , axis = axis , dropna = dropna )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#pad","text":"def pad ( self : ~ FrameOrSeries , axis = None , inplace : bool = False , limit = None , downcast = None ) -> Union [ ~ FrameOrSeries , NoneType ] Synonym for :meth: DataFrame.fillna with method='ffill' .","title":"pad"},{"location":"reference/hielen2/datalink_prova_df/#returns_106","text":"{klass} or None Object with missing values filled or None if inplace=True . View Source def ffill ( self : FrameOrSeries , axis = None , inplace : bool_t = False , limit = None , downcast = None , ) -> Optional [ FrameOrSeries ] : \"\"\" Synonym for :meth:`DataFrame.fillna` with ``method='ffill'``. Returns ------- {klass} or None Object with missing values filled or None if ``inplace=True``. \"\"\" return self . fillna ( method = \"ffill\" , axis = axis , inplace = inplace , limit = limit , downcast = downcast )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#pct_change","text":"def pct_change ( self : ~ FrameOrSeries , periods = 1 , fill_method = 'pad' , limit = None , freq = None , ** kwargs ) -> ~ FrameOrSeries Percentage change between the current and a prior element. Computes the percentage change from the immediately previous row by default. This is useful in comparing the percentage of change in a time series of elements.","title":"pct_change"},{"location":"reference/hielen2/datalink_prova_df/#parameters_92","text":"periods : int, default 1 Periods to shift for forming percent change. fill_method : str, default 'pad' How to handle NAs before computing percent changes. limit : int, default None The number of consecutive NAs to fill before stopping. freq : DateOffset, timedelta, or str, optional Increment to use from time series API (e.g. 'M' or BDay()). **kwargs Additional keyword arguments are passed into DataFrame.shift or Series.shift .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_107","text":"chg : Series or DataFrame The same type as the calling object.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_100","text":"Series.diff : Compute the difference of two elements in a Series. DataFrame.diff : Compute the difference of two elements in a DataFrame. Series.shift : Shift the index by some number of periods. DataFrame.shift : Shift the index by some number of periods.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_102","text":"Series s = pd.Series([90, 91, 85]) s 0 90 1 91 2 85 dtype: int64 s.pct_change() 0 NaN 1 0.011111 2 -0.065934 dtype: float64 s.pct_change(periods=2) 0 NaN 1 NaN 2 -0.055556 dtype: float64 See the percentage change in a Series where filling NAs with last valid observation forward to next valid. s = pd.Series([90, 91, None, 85]) s 0 90.0 1 91.0 2 NaN 3 85.0 dtype: float64 s.pct_change(fill_method='ffill') 0 NaN 1 0.011111 2 0.000000 3 -0.065934 dtype: float64 DataFrame Percentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01. df = pd.DataFrame({ ... 'FR': [4.0405, 4.0963, 4.3149], ... 'GR': [1.7246, 1.7482, 1.8519], ... 'IT': [804.74, 810.01, 860.13]}, ... index=['1980-01-01', '1980-02-01', '1980-03-01']) df FR GR IT 1980-01-01 4.0405 1.7246 804.74 1980-02-01 4.0963 1.7482 810.01 1980-03-01 4.3149 1.8519 860.13 df.pct_change() FR GR IT 1980-01-01 NaN NaN NaN 1980-02-01 0.013810 0.013684 0.006549 1980-03-01 0.053365 0.059318 0.061876 Percentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns. df = pd.DataFrame({ ... '2016': [1769950, 30586265], ... '2015': [1500923, 40912316], ... '2014': [1371819, 41403351]}, ... index=['GOOG', 'APPL']) df 2016 2015 2014 GOOG 1769950 1500923 1371819 APPL 30586265 40912316 41403351 df.pct_change(axis='columns') 2016 2015 2014 GOOG NaN -0.151997 -0.086016 APPL NaN 0.337604 0.012002 View Source def pct_change ( self : FrameOrSeries , periods = 1 , fill_method = \"pad\" , limit = None , freq = None , ** kwargs , ) -> FrameOrSeries : \"\"\" Percentage change between the current and a prior element. Computes the percentage change from the immediately previous row by default. This is useful in comparing the percentage of change in a time series of elements. Parameters ---------- periods : int, default 1 Periods to shift for forming percent change. fill_method : str, default 'pad' How to handle NAs before computing percent changes. limit : int, default None The number of consecutive NAs to fill before stopping. freq : DateOffset, timedelta, or str, optional Increment to use from time series API (e.g. 'M' or BDay()). **kwargs Additional keyword arguments are passed into `DataFrame.shift` or `Series.shift`. Returns ------- chg : Series or DataFrame The same type as the calling object. See Also -------- Series.diff : Compute the difference of two elements in a Series. DataFrame.diff : Compute the difference of two elements in a DataFrame. Series.shift : Shift the index by some number of periods. DataFrame.shift : Shift the index by some number of periods. Examples -------- **Series** >>> s = pd.Series([90, 91, 85]) >>> s 0 90 1 91 2 85 dtype: int64 >>> s.pct_change() 0 NaN 1 0.011111 2 -0.065934 dtype: float64 >>> s.pct_change(periods=2) 0 NaN 1 NaN 2 -0.055556 dtype: float64 See the percentage change in a Series where filling NAs with last valid observation forward to next valid. >>> s = pd.Series([90, 91, None, 85]) >>> s 0 90.0 1 91.0 2 NaN 3 85.0 dtype: float64 >>> s.pct_change(fill_method='ffill') 0 NaN 1 0.011111 2 0.000000 3 -0.065934 dtype: float64 **DataFrame** Percentage change in French franc, Deutsche Mark, and Italian lira from 1980-01-01 to 1980-03-01. >>> df = pd.DataFrame({ ... 'FR': [4.0405, 4.0963, 4.3149], ... 'GR': [1.7246, 1.7482, 1.8519], ... 'IT': [804.74, 810.01, 860.13]}, ... index=['1980-01-01', '1980-02-01', '1980-03-01']) >>> df FR GR IT 1980-01-01 4.0405 1.7246 804.74 1980-02-01 4.0963 1.7482 810.01 1980-03-01 4.3149 1.8519 860.13 >>> df.pct_change() FR GR IT 1980-01-01 NaN NaN NaN 1980-02-01 0.013810 0.013684 0.006549 1980-03-01 0.053365 0.059318 0.061876 Percentage of change in GOOG and APPL stock volume. Shows computing the percentage change between columns. >>> df = pd.DataFrame({ ... '2016': [1769950, 30586265], ... '2015': [1500923, 40912316], ... '2014': [1371819, 41403351]}, ... index=['GOOG', 'APPL']) >>> df 2016 2015 2014 GOOG 1769950 1500923 1371819 APPL 30586265 40912316 41403351 >>> df.pct_change(axis='columns') 2016 2015 2014 GOOG NaN -0.151997 -0.086016 APPL NaN 0.337604 0.012002 \"\"\" axis = self . _get_axis_number ( kwargs . pop ( \"axis\" , self . _stat_axis_name )) if fill_method is None : data = self else : _data = self . fillna ( method = fill_method , axis = axis , limit = limit ) assert _data is not None # needed for mypy data = _data rs = data . div ( data . shift ( periods = periods , freq = freq , axis = axis , ** kwargs )) - 1 if freq is not None : # Shift method is implemented differently when freq is not None # We want to restore the original index rs = rs . loc [ ~ rs . index . duplicated ()] rs = rs . reindex_like ( data ) return rs","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#pipe","text":"def pipe ( self , func , * args , ** kwargs ) Apply func(self, *args, **kwargs).","title":"pipe"},{"location":"reference/hielen2/datalink_prova_df/#parameters_93","text":"func : function Function to apply to the Series/DataFrame. args , and kwargs are passed into func . Alternatively a (callable, data_keyword) tuple where data_keyword is a string indicating the keyword of callable that expects the Series/DataFrame. args : iterable, optional Positional arguments passed into func . kwargs : mapping, optional A dictionary of keyword arguments passed into func .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_108","text":"object : the return type of func .","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_101","text":"DataFrame.apply : Apply a function along input axis of DataFrame. DataFrame.applymap : Apply a function elementwise on a whole DataFrame. Series.map : Apply a mapping correspondence on a :class: ~pandas.Series .","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_48","text":"Use .pipe when chaining together functions that expect Series, DataFrames or GroupBy objects. Instead of writing func(g(h(df), arg1=a), arg2=b, arg3=c) # doctest: +SKIP You can write (df.pipe(h) ... .pipe(g, arg1=a) ... .pipe(func, arg2=b, arg3=c) ... ) # doctest: +SKIP If you have a function that takes the data as (say) the second argument, pass a tuple indicating which keyword expects the data. For example, suppose f takes its data as arg2 : (df.pipe(h) ... .pipe(g, arg1=a) ... .pipe((func, 'arg2'), arg1=a, arg3=c) ... ) # doctest: +SKIP View Source @ doc ( klass = _shared_doc_kwargs [ \"klass\" ]) def pipe ( self , func , * args , ** kwargs ): r \"\"\" Apply func(self, \\*args, \\*\\*kwargs). Parameters ---------- func : function Function to apply to the {klass}. ``args``, and ``kwargs`` are passed into ``func``. Alternatively a ``(callable, data_keyword)`` tuple where ``data_keyword`` is a string indicating the keyword of ``callable`` that expects the {klass}. args : iterable, optional Positional arguments passed into ``func``. kwargs : mapping, optional A dictionary of keyword arguments passed into ``func``. Returns ------- object : the return type of ``func``. See Also -------- DataFrame.apply : Apply a function along input axis of DataFrame. DataFrame.applymap : Apply a function elementwise on a whole DataFrame. Series.map : Apply a mapping correspondence on a :class:`~pandas.Series`. Notes ----- Use ``.pipe`` when chaining together functions that expect Series, DataFrames or GroupBy objects. Instead of writing >>> func(g(h(df), arg1=a), arg2=b, arg3=c) # doctest: +SKIP You can write >>> (df.pipe(h) ... .pipe(g, arg1=a) ... .pipe(func, arg2=b, arg3=c) ... ) # doctest: +SKIP If you have a function that takes the data as (say) the second argument, pass a tuple indicating which keyword expects the data. For example, suppose ``f`` takes its data as ``arg2``: >>> (df.pipe(h) ... .pipe(g, arg1=a) ... .pipe((func, 'arg2'), arg1=a, arg3=c) ... ) # doctest: +SKIP \"\"\" return com . pipe ( self , func , * args , ** kwargs )","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#pivot","text":"def pivot ( self , index = None , columns = None , values = None ) -> 'DataFrame' Return reshaped DataFrame organized by given index / column values. Reshape data (produce a \"pivot\" table) based on column values. Uses unique values from specified index / columns to form axes of the resulting DataFrame. This function does not support data aggregation, multiple values will result in a MultiIndex in the columns. See the :ref: User Guide <reshaping> for more on reshaping.","title":"pivot"},{"location":"reference/hielen2/datalink_prova_df/#parameters_94","text":"index : str or object or a list of str, optional Column to use to make new frame's index. If None, uses existing index. .. versionchanged :: 1.1.0 Also accept list of index names. columns : str or object or a list of str Column to use to make new frame's columns. .. versionchanged :: 1.1.0 Also accept list of columns names. values : str, object or a list of the previous, optional Column(s) to use for populating new frame's values. If not specified, all remaining columns will be used and the result will have hierarchically indexed columns. .. versionchanged :: 0.23.0 Also accept list of column names.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_109","text":"DataFrame Returns reshaped DataFrame.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_11","text":"ValueError: When there are any index , columns combinations with multiple values. DataFrame.pivot_table when you need to aggregate.","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_102","text":"DataFrame.pivot_table : Generalization of pivot that can handle duplicate values for one index/column pair. DataFrame.unstack : Pivot based on the index values instead of a column.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_49","text":"For finer-tuned control, see hierarchical indexing documentation along with the related stack/unstack methods.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_103","text":"df = pd.DataFrame({'foo': ['one', 'one', 'one', 'two', 'two', ... 'two'], ... 'bar': ['A', 'B', 'C', 'A', 'B', 'C'], ... 'baz': [1, 2, 3, 4, 5, 6], ... 'zoo': ['x', 'y', 'z', 'q', 'w', 't']}) df foo bar baz zoo 0 one A 1 x 1 one B 2 y 2 one C 3 z 3 two A 4 q 4 two B 5 w 5 two C 6 t df.pivot(index='foo', columns='bar', values='baz') bar A B C foo one 1 2 3 two 4 5 6 df.pivot(index='foo', columns='bar')['baz'] bar A B C foo one 1 2 3 two 4 5 6 df.pivot(index='foo', columns='bar', values=['baz', 'zoo']) baz zoo bar A B C A B C foo one 1 2 3 x y z two 4 5 6 q w t You could also assign a list of column names or a list of index names. df = pd.DataFrame({ ... \"lev1\": [1, 1, 1, 2, 2, 2], ... \"lev2\": [1, 1, 2, 1, 1, 2], ... \"lev3\": [1, 2, 1, 2, 1, 2], ... \"lev4\": [1, 2, 3, 4, 5, 6], ... \"values\": [0, 1, 2, 3, 4, 5]}) df lev1 lev2 lev3 lev4 values 0 1 1 1 1 0 1 1 1 2 2 1 2 1 2 1 3 2 3 2 1 2 4 3 4 2 1 1 5 4 5 2 2 2 6 5 df.pivot(index=\"lev1\", columns=[\"lev2\", \"lev3\"],values=\"values\") lev2 1 2 lev3 1 2 1 2 lev1 1 0.0 1.0 2.0 NaN 2 4.0 3.0 NaN 5.0 df.pivot(index=[\"lev1\", \"lev2\"], columns=[\"lev3\"],values=\"values\") lev3 1 2 lev1 lev2 1 1 0.0 1.0 2 2.0 NaN 2 1 4.0 3.0 2 NaN 5.0 A ValueError is raised if there are any duplicates. df = pd.DataFrame({\"foo\": ['one', 'one', 'two', 'two'], ... \"bar\": ['A', 'A', 'B', 'C'], ... \"baz\": [1, 2, 3, 4]}) df foo bar baz 0 one A 1 1 one A 2 2 two B 3 3 two C 4 Notice that the first two rows are the same for our index and columns arguments. df.pivot(index='foo', columns='bar', values='baz') Traceback (most recent call last): ... ValueError: Index contains duplicate entries, cannot reshape View Source @Substitution ( \"\" ) @Appender ( _shared_docs [ \"pivot\" ]) def pivot ( self , index = None , columns = None , values = None ) -> \"DataFrame\" : from pandas.core.reshape.pivot import pivot return pivot ( self , index = index , columns = columns , values = values )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#pivot_table","text":"def pivot_table ( self , values = None , index = None , columns = None , aggfunc = 'mean' , fill_value = None , margins = False , dropna = True , margins_name = 'All' , observed = False ) -> 'DataFrame' Create a spreadsheet-style pivot table as a DataFrame. The levels in the pivot table will be stored in MultiIndex objects (hierarchical indexes) on the index and columns of the result DataFrame.","title":"pivot_table"},{"location":"reference/hielen2/datalink_prova_df/#parameters_95","text":"values : column to aggregate, optional index : column, Grouper, array, or list of the previous If an array is passed, it must be the same length as the data. The list can contain any of the other types (except list). Keys to group by on the pivot table index. If an array is passed, it is being used as the same manner as column values. columns : column, Grouper, array, or list of the previous If an array is passed, it must be the same length as the data. The list can contain any of the other types (except list). Keys to group by on the pivot table column. If an array is passed, it is being used as the same manner as column values. aggfunc : function, list of functions, dict, default numpy.mean If list of functions passed, the resulting pivot table will have hierarchical columns whose top level are the function names (inferred from the function objects themselves) If dict is passed, the key is column to aggregate and value is function or list of functions. fill_value : scalar, default None Value to replace missing values with (in the resulting pivot table, after aggregation). margins : bool, default False Add all row / columns (e.g. for subtotal / grand totals). dropna : bool, default True Do not include columns whose entries are all NaN. margins_name : str, default 'All' Name of the row / column that will contain the totals when margins is True. observed : bool, default False This only applies if any of the groupers are Categoricals. If True: only show observed values for categorical groupers. If False: show all values for categorical groupers. .. versionchanged :: 0.25.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_110","text":"DataFrame An Excel style pivot table.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_103","text":"DataFrame.pivot : Pivot without aggregation that can handle non-numeric data.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_104","text":"df = pd.DataFrame({\"A\": [\"foo\", \"foo\", \"foo\", \"foo\", \"foo\", ... \"bar\", \"bar\", \"bar\", \"bar\"], ... \"B\": [\"one\", \"one\", \"one\", \"two\", \"two\", ... \"one\", \"one\", \"two\", \"two\"], ... \"C\": [\"small\", \"large\", \"large\", \"small\", ... \"small\", \"large\", \"small\", \"small\", ... \"large\"], ... \"D\": [1, 2, 2, 3, 3, 4, 5, 6, 7], ... \"E\": [2, 4, 5, 5, 6, 6, 8, 9, 9]}) df A B C D E 0 foo one small 1 2 1 foo one large 2 4 2 foo one large 2 5 3 foo two small 3 5 4 foo two small 3 6 5 bar one large 4 6 6 bar one small 5 8 7 bar two small 6 9 8 bar two large 7 9 This first example aggregates values by taking the sum. table = pd.pivot_table(df, values='D', index=['A', 'B'], ... columns=['C'], aggfunc=np.sum) table C large small A B bar one 4.0 5.0 two 7.0 6.0 foo one 4.0 1.0 two NaN 6.0 We can also fill missing values using the fill_value parameter. table = pd.pivot_table(df, values='D', index=['A', 'B'], ... columns=['C'], aggfunc=np.sum, fill_value=0) table C large small A B bar one 4 5 two 7 6 foo one 4 1 two 0 6 The next example aggregates by taking the mean across multiple columns. table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'], ... aggfunc={'D': np.mean, ... 'E': np.mean}) table D E A C bar large 5.500000 7.500000 small 5.500000 8.500000 foo large 2.000000 4.500000 small 2.333333 4.333333 We can also calculate multiple types of aggregations for any given value column. table = pd.pivot_table(df, values=['D', 'E'], index=['A', 'C'], ... aggfunc={'D': np.mean, ... 'E': [min, max, np.mean]}) table D E mean max mean min A C bar large 5.500000 9.0 7.500000 6.0 small 5.500000 9.0 8.500000 8.0 foo large 2.000000 5.0 4.500000 4.0 small 2.333333 6.0 4.333333 2.0 View Source @Substitution ( \"\" ) @Appender ( _shared_docs [ \"pivot_table\" ]) def pivot_table ( self , values = None , index = None , columns = None , aggfunc = \"mean\" , fill_value = None , margins = False , dropna = True , margins_name = \"All\" , observed = False , ) -> \"DataFrame\" : from pandas.core.reshape.pivot import pivot_table return pivot_table ( self , values = values , index = index , columns = columns , aggfunc = aggfunc , fill_value = fill_value , margins = margins , dropna = dropna , margins_name = margins_name , observed = observed , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#pop","text":"def pop ( self , item : Union [ Hashable , NoneType ] ) -> pandas . core . series . Series Return item and drop from frame. Raise KeyError if not found.","title":"pop"},{"location":"reference/hielen2/datalink_prova_df/#parameters_96","text":"item : label Label of column to be popped.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_111","text":"Series","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#examples_105","text":"df = pd.DataFrame([('falcon', 'bird', 389.0), ... ('parrot', 'bird', 24.0), ... ('lion', 'mammal', 80.5), ... ('monkey', 'mammal', np.nan)], ... columns=('name', 'class', 'max_speed')) df name class max_speed 0 falcon bird 389.0 1 parrot bird 24.0 2 lion mammal 80.5 3 monkey mammal NaN df.pop('class') 0 bird 1 bird 2 mammal 3 mammal Name: class, dtype: object df name max_speed 0 falcon 389.0 1 parrot 24.0 2 lion 80.5 3 monkey NaN View Source def pop ( self , item : Label ) -> Series : \"\"\" Return item and drop from frame. Raise KeyError if not found. Parameters ---------- item : label Label of column to be popped. Returns ------- Series Examples -------- >>> df = pd.DataFrame([('falcon', 'bird', 389.0), ... ('parrot', 'bird', 24.0), ... ('lion', 'mammal', 80.5), ... ('monkey', 'mammal', np.nan)], ... columns=('name', 'class', 'max_speed')) >>> df name class max_speed 0 falcon bird 389.0 1 parrot bird 24.0 2 lion mammal 80.5 3 monkey mammal NaN >>> df.pop('class') 0 bird 1 bird 2 mammal 3 mammal Name: class, dtype: object >>> df name max_speed 0 falcon 389.0 1 parrot 24.0 2 lion 80.5 3 monkey NaN \"\"\" return super (). pop ( item = item )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#pow","text":"def pow ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Exponential power of dataframe and other, element-wise (binary operator pow ). Equivalent to dataframe ** other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rpow . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"pow"},{"location":"reference/hielen2/datalink_prova_df/#parameters_97","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_112","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_104","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_50","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_106","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#prod","text":"def prod ( self , axis = None , skipna = None , level = None , numeric_only = None , min_count = 0 , ** kwargs ) Return the product of the values for the requested axis.","title":"prod"},{"location":"reference/hielen2/datalink_prova_df/#parameters_98","text":"axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. min_count : int, default 0 The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. .. versionadded :: 0.22.0 Added with the default being 0. This means the sum of an all-NA or empty Series is 0, and the product of an all-NA or empty Series is 1. **kwargs Additional keyword arguments to be passed to the function.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_113","text":"Series or DataFrame (if level specified)","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#examples_107","text":"By default, the product of an empty or all-NA Series is 1 pd.Series([]).prod() 1.0 This can be controlled with the min_count parameter pd.Series([]).prod(min_count=1) nan Thanks to the skipna parameter, min_count handles all-NA and empty series identically. pd.Series([np.nan]).prod() 1.0 pd.Series([np.nan]).prod(min_count=1) nan View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = _min_count_stub , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , min_count = 0 , ** kwargs , ) : if name == \"sum\" : nv . validate_sum ( tuple (), kwargs ) elif name == \"prod\" : nv . validate_prod ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna , min_count = min_count ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only , min_count = min_count , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#product","text":"def product ( self , axis = None , skipna = None , level = None , numeric_only = None , min_count = 0 , ** kwargs ) Return the product of the values for the requested axis.","title":"product"},{"location":"reference/hielen2/datalink_prova_df/#parameters_99","text":"axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. min_count : int, default 0 The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. .. versionadded :: 0.22.0 Added with the default being 0. This means the sum of an all-NA or empty Series is 0, and the product of an all-NA or empty Series is 1. **kwargs Additional keyword arguments to be passed to the function.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_114","text":"Series or DataFrame (if level specified)","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#examples_108","text":"By default, the product of an empty or all-NA Series is 1 pd.Series([]).prod() 1.0 This can be controlled with the min_count parameter pd.Series([]).prod(min_count=1) nan Thanks to the skipna parameter, min_count handles all-NA and empty series identically. pd.Series([np.nan]).prod() 1.0 pd.Series([np.nan]).prod(min_count=1) nan View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = _min_count_stub , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , min_count = 0 , ** kwargs , ) : if name == \"sum\" : nv . validate_sum ( tuple (), kwargs ) elif name == \"prod\" : nv . validate_prod ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna , min_count = min_count ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only , min_count = min_count , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#quantile","text":"def quantile ( self , q = 0.5 , axis = 0 , numeric_only = True , interpolation = 'linear' ) Return values at the given quantile over requested axis.","title":"quantile"},{"location":"reference/hielen2/datalink_prova_df/#parameters_100","text":"q : float or array-like, default 0.5 (50% quantile) Value between 0 <= q <= 1, the quantile(s) to compute. axis : {0, 1, 'index', 'columns'}, default 0 Equals 0 or 'index' for row-wise, 1 or 'columns' for column-wise. numeric_only : bool, default True If False, the quantile of datetime and timedelta data will be computed as well. interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'} This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points i and j : * linear : `i + (j - i) * fraction` , where `fraction` is the fractional part of the index surrounded by `i` and `j` . * lower : `i` . * higher : `j` . * nearest : `i` or `j` whichever is nearest . * midpoint : ( `i` + `j` ) / 2 .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_115","text":"Series or DataFrame If `` q `` is an array , a DataFrame will be returned where the index is `` q `` , the columns are the columns of self , and the values are the quantiles . If `` q `` is a float , a Series will be returned where the index is the columns of self and the values are the quantiles .","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_105","text":"core.window.Rolling.quantile: Rolling quantile. numpy.percentile: Numpy function to compute the percentile.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_109","text":"df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]), ... columns=['a', 'b']) df.quantile(.1) a 1.3 b 3.7 Name: 0.1, dtype: float64 df.quantile([.1, .5]) a b 0.1 1.3 3.7 0.5 2.5 55.0 Specifying numeric_only=False will also compute the quantile of datetime and timedelta data. df = pd.DataFrame({'A': [1, 2], ... 'B': [pd.Timestamp('2010'), ... pd.Timestamp('2011')], ... 'C': [pd.Timedelta('1 days'), ... pd.Timedelta('2 days')]}) df.quantile(0.5, numeric_only=False) A 1.5 B 2010-07-02 12:00:00 C 1 days 12:00:00 Name: 0.5, dtype: object View Source def quantile ( self , q = 0 . 5 , axis = 0 , numeric_only = True , interpolation = \"linear\" ): \"\"\" Return values at the given quantile over requested axis. Parameters ---------- q : float or array-like, default 0.5 (50% quantile) Value between 0 <= q <= 1, the quantile(s) to compute. axis : {0, 1, 'index', 'columns'}, default 0 Equals 0 or 'index' for row-wise, 1 or 'columns' for column-wise. numeric_only : bool, default True If False, the quantile of datetime and timedelta data will be computed as well. interpolation : {'linear', 'lower', 'higher', 'midpoint', 'nearest'} This optional parameter specifies the interpolation method to use, when the desired quantile lies between two data points `i` and `j`: * linear: `i + (j - i) * fraction`, where `fraction` is the fractional part of the index surrounded by `i` and `j`. * lower: `i`. * higher: `j`. * nearest: `i` or `j` whichever is nearest. * midpoint: (`i` + `j`) / 2. Returns ------- Series or DataFrame If ``q`` is an array, a DataFrame will be returned where the index is ``q``, the columns are the columns of self, and the values are the quantiles. If ``q`` is a float, a Series will be returned where the index is the columns of self and the values are the quantiles. See Also -------- core.window.Rolling.quantile: Rolling quantile. numpy.percentile: Numpy function to compute the percentile. Examples -------- >>> df = pd.DataFrame(np.array([[1, 1], [2, 10], [3, 100], [4, 100]]), ... columns=['a', 'b']) >>> df.quantile(.1) a 1.3 b 3.7 Name: 0.1, dtype: float64 >>> df.quantile([.1, .5]) a b 0.1 1.3 3.7 0.5 2.5 55.0 Specifying `numeric_only=False` will also compute the quantile of datetime and timedelta data. >>> df = pd.DataFrame({'A': [1, 2], ... 'B': [pd.Timestamp('2010'), ... pd.Timestamp('2011')], ... 'C': [pd.Timedelta('1 days'), ... pd.Timedelta('2 days')]}) >>> df.quantile(0.5, numeric_only=False) A 1.5 B 2010-07-02 12:00:00 C 1 days 12:00:00 Name: 0.5, dtype: object \"\"\" validate_percentile ( q ) data = self . _get_numeric_data () if numeric_only else self axis = self . _get_axis_number ( axis ) is_transposed = axis == 1 if is_transposed : data = data . T if len ( data . columns ) == 0 : # GH#23925 _get_numeric_data may have dropped all columns cols = Index ([], name = self . columns . name ) if is_list_like ( q ): return self . _constructor ([], index = q , columns = cols ) return self . _constructor_sliced ([], index = cols , name = q , dtype = np . float64 ) result = data . _mgr . quantile ( qs = q , axis = 1 , interpolation = interpolation , transposed = is_transposed ) if result . ndim == 2 : result = self . _constructor ( result ) else : result = self . _constructor_sliced ( result , name = q ) if is_transposed : result = result . T return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#query","text":"def query ( self , expr , inplace = False , ** kwargs ) Query the columns of a DataFrame with a boolean expression.","title":"query"},{"location":"reference/hielen2/datalink_prova_df/#parameters_101","text":"expr : str The query string to evaluate. You can refer to variables in the environment by prefixing them with an '@' character like `` @ a + b `` . You can refer to column names that contain spaces or operators by surrounding them in backticks . This way you can also escape names that start with a digit , or those that are a Python keyword . Basically when it is not valid Python identifier . See notes down for more details . For example , if one of your columns is called `` a a `` and you want to sum it with `` b `` , your query should be ```a a` + b `` . .. versionadded :: 0 . 25 . 0 Backtick quoting introduced . .. versionadded :: 1 . 0 . 0 Expanding functionality of backtick quoting for more than only spaces . inplace : bool Whether the query should modify the data in place or return a modified copy. **kwargs See the documentation for :func: eval for complete details on the keyword arguments accepted by :meth: DataFrame.query .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_116","text":"DataFrame DataFrame resulting from the provided query expression.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_106","text":"eval : Evaluate a string describing operations on DataFrame columns. DataFrame.eval : Evaluate a string describing operations on DataFrame columns.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_51","text":"The result of the evaluation of this expression is first passed to :attr: DataFrame.loc and if that fails because of a multidimensional key (e.g., a DataFrame) then the result will be passed to :meth: DataFrame.__getitem__ . This method uses the top-level :func: eval function to evaluate the passed query. The :meth: ~pandas.DataFrame.query method uses a slightly modified Python syntax by default. For example, the & and | (bitwise) operators have the precedence of their boolean cousins, :keyword: and and :keyword: or . This is syntactically valid Python, however the semantics are different. You can change the semantics of the expression by passing the keyword argument parser='python' . This enforces the same semantics as evaluation in Python space. Likewise, you can pass engine='python' to evaluate an expression using Python itself as a backend. This is not recommended as it is inefficient compared to using numexpr as the engine. The :attr: DataFrame.index and :attr: DataFrame.columns attributes of the :class: ~pandas.DataFrame instance are placed in the query namespace by default, which allows you to treat both the index and columns of the frame as a column in the frame. The identifier index is used for the frame index; you can also use the name of the index to identify it in a query. Please note that Python keywords may not be used as identifiers. For further details and examples see the query documentation in :ref: indexing <indexing.query> . Backtick quoted variables Backtick quoted variables are parsed as literal Python code and are converted internally to a Python valid identifier. This can lead to the following problems. During parsing a number of disallowed characters inside the backtick quoted string are replaced by strings that are allowed as a Python identifier. These characters include all operators in Python, the space character, the question mark, the exclamation mark, the dollar sign, and the euro sign. For other characters that fall outside the ASCII range (U+0001..U+007F) and those that are not further specified in PEP 3131, the query parser will raise an error. This excludes whitespace different than the space character, but also the hashtag (as it is used for comments) and the backtick itself (backtick can also not be escaped). In a special case, quotes that make a pair around a backtick can confuse the parser. For example, it's` > `that's will raise an error, as it forms a quoted string ( 's > `that' ) with a backtick inside. See also the Python documentation about lexical analysis (https://docs.python.org/3/reference/lexical_analysis.html) in combination with the source code in :mod: pandas.core.computation.parsing .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_110","text":"df = pd.DataFrame({'A': range(1, 6), ... 'B': range(10, 0, -2), ... 'C C': range(10, 5, -1)}) df A B C C 0 1 10 10 1 2 8 9 2 3 6 8 3 4 4 7 4 5 2 6 df.query('A > B') A B C C 4 5 2 6 The previous expression is equivalent to df[df.A > df.B] A B C C 4 5 2 6 For columns with spaces in their name, you can use backtick quoting. df.query('B == C C ') A B C C 0 1 10 10 The previous expression is equivalent to df[df.B == df['C C']] A B C C 0 1 10 10 View Source def query ( self , expr , inplace = False , ** kwargs ): \"\"\" Query the columns of a DataFrame with a boolean expression. Parameters ---------- expr : str The query string to evaluate. You can refer to variables in the environment by prefixing them with an '@' character like ``@a + b``. You can refer to column names that contain spaces or operators by surrounding them in backticks. This way you can also escape names that start with a digit, or those that are a Python keyword. Basically when it is not valid Python identifier. See notes down for more details. For example, if one of your columns is called ``a a`` and you want to sum it with ``b``, your query should be ```a a` + b``. .. versionadded:: 0.25.0 Backtick quoting introduced. .. versionadded:: 1.0.0 Expanding functionality of backtick quoting for more than only spaces. inplace : bool Whether the query should modify the data in place or return a modified copy. **kwargs See the documentation for :func:`eval` for complete details on the keyword arguments accepted by :meth:`DataFrame.query`. Returns ------- DataFrame DataFrame resulting from the provided query expression. See Also -------- eval : Evaluate a string describing operations on DataFrame columns. DataFrame.eval : Evaluate a string describing operations on DataFrame columns. Notes ----- The result of the evaluation of this expression is first passed to :attr:`DataFrame.loc` and if that fails because of a multidimensional key (e.g., a DataFrame) then the result will be passed to :meth:`DataFrame.__getitem__`. This method uses the top-level :func:`eval` function to evaluate the passed query. The :meth:`~pandas.DataFrame.query` method uses a slightly modified Python syntax by default. For example, the ``&`` and ``|`` (bitwise) operators have the precedence of their boolean cousins, :keyword:`and` and :keyword:`or`. This *is* syntactically valid Python, however the semantics are different. You can change the semantics of the expression by passing the keyword argument ``parser='python'``. This enforces the same semantics as evaluation in Python space. Likewise, you can pass ``engine='python'`` to evaluate an expression using Python itself as a backend. This is not recommended as it is inefficient compared to using ``numexpr`` as the engine. The :attr:`DataFrame.index` and :attr:`DataFrame.columns` attributes of the :class:`~pandas.DataFrame` instance are placed in the query namespace by default, which allows you to treat both the index and columns of the frame as a column in the frame. The identifier ``index`` is used for the frame index; you can also use the name of the index to identify it in a query. Please note that Python keywords may not be used as identifiers. For further details and examples see the ``query`` documentation in :ref:`indexing <indexing.query>`. *Backtick quoted variables* Backtick quoted variables are parsed as literal Python code and are converted internally to a Python valid identifier. This can lead to the following problems. During parsing a number of disallowed characters inside the backtick quoted string are replaced by strings that are allowed as a Python identifier. These characters include all operators in Python, the space character, the question mark, the exclamation mark, the dollar sign, and the euro sign. For other characters that fall outside the ASCII range (U+0001..U+007F) and those that are not further specified in PEP 3131, the query parser will raise an error. This excludes whitespace different than the space character, but also the hashtag (as it is used for comments) and the backtick itself (backtick can also not be escaped). In a special case, quotes that make a pair around a backtick can confuse the parser. For example, ```it's` > `that's``` will raise an error, as it forms a quoted string (``'s > `that'``) with a backtick inside. See also the Python documentation about lexical analysis (https://docs.python.org/3/reference/lexical_analysis.html) in combination with the source code in :mod:`pandas.core.computation.parsing`. Examples -------- >>> df = pd.DataFrame({'A': range(1, 6), ... 'B': range(10, 0, -2), ... 'C C': range(10, 5, -1)}) >>> df A B C C 0 1 10 10 1 2 8 9 2 3 6 8 3 4 4 7 4 5 2 6 >>> df.query('A > B') A B C C 4 5 2 6 The previous expression is equivalent to >>> df[df.A > df.B] A B C C 4 5 2 6 For columns with spaces in their name, you can use backtick quoting. >>> df.query('B == `C C`') A B C C 0 1 10 10 The previous expression is equivalent to >>> df[df.B == df['C C']] A B C C 0 1 10 10 \"\"\" inplace = validate_bool_kwarg ( inplace , \"inplace\" ) if not isinstance ( expr , str ): msg = f \"expr must be a string to be evaluated, {type(expr)} given\" raise ValueError ( msg ) kwargs [ \"level\" ] = kwargs . pop ( \"level\" , 0 ) + 1 kwargs [ \"target\" ] = None res = self . eval ( expr , ** kwargs ) try : result = self . loc [ res ] except ValueError : # when res is multi-dimensional loc raises, but this is sometimes a # valid query result = self [ res ] if inplace : self . _update_inplace ( result ) else : return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#radd","text":"def radd ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Addition of dataframe and other, element-wise (binary operator radd ). Equivalent to other + dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, add . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"radd"},{"location":"reference/hielen2/datalink_prova_df/#parameters_102","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_117","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_107","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_52","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_111","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#rank","text":"def rank ( self : ~ FrameOrSeries , axis = 0 , method : str = 'average' , numeric_only : Union [ bool , NoneType ] = None , na_option : str = 'keep' , ascending : bool = True , pct : bool = False ) -> ~ FrameOrSeries Compute numerical data ranks (1 through n) along axis. By default, equal values are assigned a rank that is the average of the ranks of those values.","title":"rank"},{"location":"reference/hielen2/datalink_prova_df/#parameters_103","text":"axis : {0 or 'index', 1 or 'columns'}, default 0 Index to direct ranking. method : {'average', 'min', 'max', 'first', 'dense'}, default 'average' How to rank the group of records that have the same value (i.e. ties): * average: average rank of the group * min: lowest rank in the group * max: highest rank in the group * first: ranks assigned in order they appear in the array * dense: like 'min', but rank always increases by 1 between groups. numeric_only : bool, optional For DataFrame objects, rank only numeric columns if set to True. na_option : {'keep', 'top', 'bottom'}, default 'keep' How to rank NaN values: * keep: assign NaN rank to NaN values * top: assign smallest rank to NaN values if ascending * bottom: assign highest rank to NaN values if ascending. ascending : bool, default True Whether or not the elements should be ranked in ascending order. pct : bool, default False Whether or not to display the returned rankings in percentile form.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_118","text":"same type as caller Return a Series or DataFrame with data ranks as values.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_108","text":"core.groupby.GroupBy.rank : Rank of values within each group.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_112","text":"df = pd.DataFrame(data={'Animal': ['cat', 'penguin', 'dog', ... 'spider', 'snake'], ... 'Number_legs': [4, 2, 4, 8, np.nan]}) df Animal Number_legs 0 cat 4.0 1 penguin 2.0 2 dog 4.0 3 spider 8.0 4 snake NaN The following example shows how the method behaves with the above parameters: default_rank: this is the default behaviour obtained without using any parameter. max_rank: setting method = 'max' the records that have the same values are ranked using the highest rank (e.g.: since 'cat' and 'dog' are both in the 2nd and 3rd position, rank 3 is assigned.) NA_bottom: choosing na_option = 'bottom' , if there are records with NaN values they are placed at the bottom of the ranking. pct_rank: when setting pct = True , the ranking is expressed as percentile rank. df['default_rank'] = df['Number_legs'].rank() df['max_rank'] = df['Number_legs'].rank(method='max') df['NA_bottom'] = df['Number_legs'].rank(na_option='bottom') df['pct_rank'] = df['Number_legs'].rank(pct=True) df Animal Number_legs default_rank max_rank NA_bottom pct_rank 0 cat 4.0 2.5 3.0 2.5 0.625 1 penguin 2.0 1.0 1.0 1.0 0.250 2 dog 4.0 2.5 3.0 2.5 0.625 3 spider 8.0 4.0 4.0 4.0 1.000 4 snake NaN NaN NaN 5.0 NaN View Source def rank ( self : FrameOrSeries , axis = 0 , method : str = \"average\" , numeric_only : Optional [ bool_t ] = None , na_option : str = \"keep\" , ascending : bool_t = True , pct : bool_t = False , ) -> FrameOrSeries : \"\"\" Compute numerical data ranks (1 through n) along axis. By default, equal values are assigned a rank that is the average of the ranks of those values. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 Index to direct ranking. method : {'average', 'min', 'max', 'first', 'dense'}, default 'average' How to rank the group of records that have the same value (i.e. ties): * average: average rank of the group * min: lowest rank in the group * max: highest rank in the group * first: ranks assigned in order they appear in the array * dense: like 'min', but rank always increases by 1 between groups. numeric_only : bool, optional For DataFrame objects, rank only numeric columns if set to True. na_option : {'keep', 'top', 'bottom'}, default 'keep' How to rank NaN values: * keep: assign NaN rank to NaN values * top: assign smallest rank to NaN values if ascending * bottom: assign highest rank to NaN values if ascending. ascending : bool, default True Whether or not the elements should be ranked in ascending order. pct : bool, default False Whether or not to display the returned rankings in percentile form. Returns ------- same type as caller Return a Series or DataFrame with data ranks as values. See Also -------- core.groupby.GroupBy.rank : Rank of values within each group. Examples -------- >>> df = pd.DataFrame(data={'Animal': ['cat', 'penguin', 'dog', ... 'spider', 'snake'], ... 'Number_legs': [4, 2, 4, 8, np.nan]}) >>> df Animal Number_legs 0 cat 4.0 1 penguin 2.0 2 dog 4.0 3 spider 8.0 4 snake NaN The following example shows how the method behaves with the above parameters: * default_rank: this is the default behaviour obtained without using any parameter. * max_rank: setting ``method = 'max'`` the records that have the same values are ranked using the highest rank (e.g.: since 'cat' and 'dog' are both in the 2nd and 3rd position, rank 3 is assigned.) * NA_bottom: choosing ``na_option = 'bottom'``, if there are records with NaN values they are placed at the bottom of the ranking. * pct_rank: when setting ``pct = True``, the ranking is expressed as percentile rank. >>> df['default_rank'] = df['Number_legs'].rank() >>> df['max_rank'] = df['Number_legs'].rank(method='max') >>> df['NA_bottom'] = df['Number_legs'].rank(na_option='bottom') >>> df['pct_rank'] = df['Number_legs'].rank(pct=True) >>> df Animal Number_legs default_rank max_rank NA_bottom pct_rank 0 cat 4.0 2.5 3.0 2.5 0.625 1 penguin 2.0 1.0 1.0 1.0 0.250 2 dog 4.0 2.5 3.0 2.5 0.625 3 spider 8.0 4.0 4.0 4.0 1.000 4 snake NaN NaN NaN 5.0 NaN \"\"\" axis = self . _get_axis_number ( axis ) if na_option not in { \"keep\" , \"top\" , \"bottom\" }: msg = \"na_option must be one of 'keep', 'top', or 'bottom'\" raise ValueError ( msg ) def ranker ( data ) : ranks = algos . rank ( data . values , axis = axis , method = method , ascending = ascending , na_option = na_option , pct = pct , ) ranks = self . _constructor ( ranks , ** data . _construct_axes_dict ()) return ranks . __finalize__ ( self , method = \"rank\" ) # if numeric_only is None , and we can ' t get anything , we try with # numeric_only = True if numeric_only is None : try : return ranker ( self ) except TypeError : numeric_only = True if numeric_only : data = self . _get_numeric_data () else : data = self return ranker ( data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#rdiv","text":"def rdiv ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Floating division of dataframe and other, element-wise (binary operator rtruediv ). Equivalent to other / dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, truediv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"rdiv"},{"location":"reference/hielen2/datalink_prova_df/#parameters_104","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_119","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_109","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_53","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_113","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#reindex","text":"def reindex ( self , labels = None , index = None , columns = None , axis = None , method = None , copy = True , level = None , fill_value = nan , limit = None , tolerance = None ) Conform Series/DataFrame to new index with optional filling logic. Places NA/NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False .","title":"reindex"},{"location":"reference/hielen2/datalink_prova_df/#parameters_105","text":"keywords for axes : array-like, optional New labels / index to conform to, should be specified using keywords. Preferably an Index object to avoid duplicating data. method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'} Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index. * None (default): don't fill gaps * pad / ffill: Propagate last valid observation forward to next valid. * backfill / bfill: Use next valid observation to fill gap. * nearest: Use nearest valid observations to fill gap. copy : bool, default True Return a new object, even if the passed indexes are the same. level : int or name Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : scalar, default np.NaN Value to use for missing values. Defaults to NaN, but can be any \"compatible\" value. limit : int, default None Maximum number of consecutive elements to forward or backward fill. tolerance : optional Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation abs(index[indexer] - target) <= tolerance . Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index's type.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_120","text":"Series/DataFrame with changed index.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_110","text":"DataFrame.set_index : Set row labels. DataFrame.reset_index : Remove row labels or move them to new columns. DataFrame.reindex_like : Change to same indices as other DataFrame.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_114","text":"DataFrame.reindex supports two calling conventions (index=index_labels, columns=column_labels, ...) (labels, axis={'index', 'columns'}, ...) We highly recommend using keyword arguments to clarify your intent. Create a dataframe with some fictional data. index = ['Firefox', 'Chrome', 'Safari', 'IE10', 'Konqueror'] df = pd.DataFrame({'http_status': [200, 200, 404, 404, 301], ... 'response_time': [0.04, 0.02, 0.07, 0.08, 1.0]}, ... index=index) df http_status response_time Firefox 200 0.04 Chrome 200 0.02 Safari 404 0.07 IE10 404 0.08 Konqueror 301 1.00 Create a new index and reindex the dataframe. By default values in the new index that do not have corresponding records in the dataframe are assigned NaN . new_index = ['Safari', 'Iceweasel', 'Comodo Dragon', 'IE10', ... 'Chrome'] df.reindex(new_index) http_status response_time Safari 404.0 0.07 Iceweasel NaN NaN Comodo Dragon NaN NaN IE10 404.0 0.08 Chrome 200.0 0.02 We can fill in the missing values by passing a value to the keyword fill_value . Because the index is not monotonically increasing or decreasing, we cannot use arguments to the keyword method to fill the NaN values. df.reindex(new_index, fill_value=0) http_status response_time Safari 404 0.07 Iceweasel 0 0.00 Comodo Dragon 0 0.00 IE10 404 0.08 Chrome 200 0.02 df.reindex(new_index, fill_value='missing') http_status response_time Safari 404 0.07 Iceweasel missing missing Comodo Dragon missing missing IE10 404 0.08 Chrome 200 0.02 We can also reindex the columns. df.reindex(columns=['http_status', 'user_agent']) http_status user_agent Firefox 200 NaN Chrome 200 NaN Safari 404 NaN IE10 404 NaN Konqueror 301 NaN Or we can use \"axis-style\" keyword arguments df.reindex(['http_status', 'user_agent'], axis=\"columns\") http_status user_agent Firefox 200 NaN Chrome 200 NaN Safari 404 NaN IE10 404 NaN Konqueror 301 NaN To further illustrate the filling functionality in reindex , we will create a dataframe with a monotonically increasing index (for example, a sequence of dates). date_index = pd.date_range('1/1/2010', periods=6, freq='D') df2 = pd.DataFrame({\"prices\": [100, 101, np.nan, 100, 89, 88]}, ... index=date_index) df2 prices 2010-01-01 100.0 2010-01-02 101.0 2010-01-03 NaN 2010-01-04 100.0 2010-01-05 89.0 2010-01-06 88.0 Suppose we decide to expand the dataframe to cover a wider date range. date_index2 = pd.date_range('12/29/2009', periods=10, freq='D') df2.reindex(date_index2) prices 2009-12-29 NaN 2009-12-30 NaN 2009-12-31 NaN 2010-01-01 100.0 2010-01-02 101.0 2010-01-03 NaN 2010-01-04 100.0 2010-01-05 89.0 2010-01-06 88.0 2010-01-07 NaN The index entries that did not have a value in the original data frame (for example, '2009-12-29') are by default filled with NaN . If desired, we can fill in the missing values using one of several options. For example, to back-propagate the last valid value to fill the NaN values, pass bfill as an argument to the method keyword. df2.reindex(date_index2, method='bfill') prices 2009-12-29 100.0 2009-12-30 100.0 2009-12-31 100.0 2010-01-01 100.0 2010-01-02 101.0 2010-01-03 NaN 2010-01-04 100.0 2010-01-05 89.0 2010-01-06 88.0 2010-01-07 NaN Please note that the NaN value present in the original dataframe (at index value 2010-01-03) will not be filled by any of the value propagation schemes. This is because filling while reindexing does not look at dataframe values, but only compares the original and desired indexes. If you do want to fill in the NaN values present in the original dataframe, use the fillna() method. See the :ref: user guide <basics.reindexing> for more. View Source @ Substitution ( ** _shared_doc_kwargs ) @ Appender ( NDFrame . reindex . __doc__ ) @ rewrite_axis_style_signature ( \"labels\" , [ ( \"method\" , None ), ( \"copy\" , True ), ( \"level\" , None ), ( \"fill_value\" , np . nan ), ( \"limit\" , None ), ( \"tolerance\" , None ), ], ) def reindex ( self , * args , ** kwargs ) -> \"DataFrame\" : axes = validate_axis_style_args ( self , args , kwargs , \"labels\" , \"reindex\" ) kwargs . update ( axes ) # Pop these, since the values are in `kwargs` under different names kwargs . pop ( \"axis\" , None ) kwargs . pop ( \"labels\" , None ) return super (). reindex ( ** kwargs )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#reindex_like","text":"def reindex_like ( self : ~ FrameOrSeries , other , method : Union [ str , NoneType ] = None , copy : bool = True , limit = None , tolerance = None ) -> ~ FrameOrSeries Return an object with matching indices as other object. Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False.","title":"reindex_like"},{"location":"reference/hielen2/datalink_prova_df/#parameters_106","text":"other : Object of the same data type Its row and column indices are used to define the new indices of this object. method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'} Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index. * None (default): don't fill gaps * pad / ffill: propagate last valid observation forward to next valid * backfill / bfill: use next valid observation to fill gap * nearest: use nearest valid observations to fill gap. copy : bool, default True Return a new object, even if the passed indexes are the same. limit : int, default None Maximum number of consecutive labels to fill for inexact matches. tolerance : optional Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation abs(index[indexer] - target) <= tolerance . Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index's type.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_121","text":"Series or DataFrame Same type as caller, but with changed indices on each axis.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_111","text":"DataFrame.set_index : Set row labels. DataFrame.reset_index : Remove row labels or move them to new columns. DataFrame.reindex : Change to new indices or expand indices.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_54","text":"Same as calling .reindex(index=other.index, columns=other.columns,...) .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_115","text":"df1 = pd.DataFrame([[24.3, 75.7, 'high'], ... [31, 87.8, 'high'], ... [22, 71.6, 'medium'], ... [35, 95, 'medium']], ... columns=['temp_celsius', 'temp_fahrenheit', ... 'windspeed'], ... index=pd.date_range(start='2014-02-12', ... end='2014-02-15', freq='D')) df1 temp_celsius temp_fahrenheit windspeed 2014-02-12 24.3 75.7 high 2014-02-13 31.0 87.8 high 2014-02-14 22.0 71.6 medium 2014-02-15 35.0 95.0 medium df2 = pd.DataFrame([[28, 'low'], ... [30, 'low'], ... [35.1, 'medium']], ... columns=['temp_celsius', 'windspeed'], ... index=pd.DatetimeIndex(['2014-02-12', '2014-02-13', ... '2014-02-15'])) df2 temp_celsius windspeed 2014-02-12 28.0 low 2014-02-13 30.0 low 2014-02-15 35.1 medium df2.reindex_like(df1) temp_celsius temp_fahrenheit windspeed 2014-02-12 28.0 NaN low 2014-02-13 30.0 NaN low 2014-02-14 NaN NaN NaN 2014-02-15 35.1 NaN medium View Source def reindex_like ( self : FrameOrSeries , other , method : Optional [ str ] = None , copy : bool_t = True , limit = None , tolerance = None , ) -> FrameOrSeries : \"\"\" Return an object with matching indices as other object. Conform the object to the same index on all axes. Optional filling logic, placing NaN in locations having no value in the previous index. A new object is produced unless the new index is equivalent to the current one and copy=False. Parameters ---------- other : Object of the same data type Its row and column indices are used to define the new indices of this object. method : {None, 'backfill'/'bfill', 'pad'/'ffill', 'nearest'} Method to use for filling holes in reindexed DataFrame. Please note: this is only applicable to DataFrames/Series with a monotonically increasing/decreasing index. * None (default): don't fill gaps * pad / ffill: propagate last valid observation forward to next valid * backfill / bfill: use next valid observation to fill gap * nearest: use nearest valid observations to fill gap. copy : bool, default True Return a new object, even if the passed indexes are the same. limit : int, default None Maximum number of consecutive labels to fill for inexact matches. tolerance : optional Maximum distance between original and new labels for inexact matches. The values of the index at the matching locations most satisfy the equation ``abs(index[indexer] - target) <= tolerance``. Tolerance may be a scalar value, which applies the same tolerance to all values, or list-like, which applies variable tolerance per element. List-like includes list, tuple, array, Series, and must be the same size as the index and its dtype must exactly match the index's type. Returns ------- Series or DataFrame Same type as caller, but with changed indices on each axis. See Also -------- DataFrame.set_index : Set row labels. DataFrame.reset_index : Remove row labels or move them to new columns. DataFrame.reindex : Change to new indices or expand indices. Notes ----- Same as calling ``.reindex(index=other.index, columns=other.columns,...)``. Examples -------- >>> df1 = pd.DataFrame([[24.3, 75.7, 'high'], ... [31, 87.8, 'high'], ... [22, 71.6, 'medium'], ... [35, 95, 'medium']], ... columns=['temp_celsius', 'temp_fahrenheit', ... 'windspeed'], ... index=pd.date_range(start='2014-02-12', ... end='2014-02-15', freq='D')) >>> df1 temp_celsius temp_fahrenheit windspeed 2014-02-12 24.3 75.7 high 2014-02-13 31.0 87.8 high 2014-02-14 22.0 71.6 medium 2014-02-15 35.0 95.0 medium >>> df2 = pd.DataFrame([[28, 'low'], ... [30, 'low'], ... [35.1, 'medium']], ... columns=['temp_celsius', 'windspeed'], ... index=pd.DatetimeIndex(['2014-02-12', '2014-02-13', ... '2014-02-15'])) >>> df2 temp_celsius windspeed 2014-02-12 28.0 low 2014-02-13 30.0 low 2014-02-15 35.1 medium >>> df2.reindex_like(df1) temp_celsius temp_fahrenheit windspeed 2014-02-12 28.0 NaN low 2014-02-13 30.0 NaN low 2014-02-14 NaN NaN NaN 2014-02-15 35.1 NaN medium \"\"\" d = other . _construct_axes_dict ( axes = self . _AXIS_ORDERS , method = method , copy = copy , limit = limit , tolerance = tolerance , ) return self . reindex ( ** d )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#rename","text":"def rename ( self , mapper = None , index = None , columns = None , axis = None , copy = True , inplace = False , level = None , errors = 'ignore' ) Alter axes labels. Function / dict values must be unique (1-to-1). Labels not contained in a dict / Series will be left as-is. Extra labels listed don't throw an error. See the :ref: user guide <basics.rename> for more.","title":"rename"},{"location":"reference/hielen2/datalink_prova_df/#parameters_107","text":"mapper : dict-like or function Dict-like or functions transformations to apply to that axis' values. Use either mapper and axis to specify the axis to target with mapper , or index and columns . index : dict-like or function Alternative to specifying axis ( mapper, axis=0 is equivalent to index=mapper ). columns : dict-like or function Alternative to specifying axis ( mapper, axis=1 is equivalent to columns=mapper ). axis : {0 or 'index', 1 or 'columns'}, default 0 Axis to target with mapper . Can be either the axis name ('index', 'columns') or number (0, 1). The default is 'index'. copy : bool, default True Also copy underlying data. inplace : bool, default False Whether to return a new DataFrame. If True then value of copy is ignored. level : int or level name, default None In case of a MultiIndex, only rename labels in the specified level. errors : {'ignore', 'raise'}, default 'ignore' If 'raise', raise a KeyError when a dict-like mapper , index , or columns contains labels that are not present in the Index being transformed. If 'ignore', existing keys will be renamed and extra keys will be ignored.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_122","text":"DataFrame DataFrame with the renamed axis labels.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_12","text":"KeyError If any of the labels is not found in the selected axis and \"errors='raise'\".","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_112","text":"DataFrame.rename_axis : Set the name of the axis.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_116","text":"DataFrame.rename supports two calling conventions (index=index_mapper, columns=columns_mapper, ...) (mapper, axis={'index', 'columns'}, ...) We highly recommend using keyword arguments to clarify your intent. Rename columns using a mapping: df = pd.DataFrame({\"A\": [1, 2, 3], \"B\": [4, 5, 6]}) df.rename(columns={\"A\": \"a\", \"B\": \"c\"}) a c 0 1 4 1 2 5 2 3 6 Rename index using a mapping: df.rename(index={0: \"x\", 1: \"y\", 2: \"z\"}) A B x 1 4 y 2 5 z 3 6 Cast index labels to a different type: df.index RangeIndex(start=0, stop=3, step=1) df.rename(index=str).index Index(['0', '1', '2'], dtype='object') df.rename(columns={\"A\": \"a\", \"B\": \"b\", \"C\": \"c\"}, errors=\"raise\") Traceback (most recent call last): KeyError: ['C'] not found in axis Using axis-style parameters df.rename(str.lower, axis='columns') a b 0 1 4 1 2 5 2 3 6 df.rename({1: 2, 2: 4}, axis='index') A B 0 1 4 2 2 5 4 3 6 View Source @ rewrite_axis_style_signature ( \"mapper\" , [( \"copy\" , True ), ( \"inplace\" , False ), ( \"level\" , None ), ( \"errors\" , \"ignore\" )], ) def rename ( self , mapper : Optional [ Renamer ] = None , * , index : Optional [ Renamer ] = None , columns : Optional [ Renamer ] = None , axis : Optional [ Axis ] = None , copy : bool = True , inplace : bool = False , level : Optional [ Level ] = None , errors : str = \"ignore\" , ) -> Optional [ \"DataFrame\" ]: \"\"\" Alter axes labels. Function / dict values must be unique (1-to-1). Labels not contained in a dict / Series will be left as-is. Extra labels listed don't throw an error. See the :ref:`user guide <basics.rename>` for more. Parameters ---------- mapper : dict-like or function Dict-like or functions transformations to apply to that axis' values. Use either ``mapper`` and ``axis`` to specify the axis to target with ``mapper``, or ``index`` and ``columns``. index : dict-like or function Alternative to specifying axis (``mapper, axis=0`` is equivalent to ``index=mapper``). columns : dict-like or function Alternative to specifying axis (``mapper, axis=1`` is equivalent to ``columns=mapper``). axis : {0 or 'index', 1 or 'columns'}, default 0 Axis to target with ``mapper``. Can be either the axis name ('index', 'columns') or number (0, 1). The default is 'index'. copy : bool, default True Also copy underlying data. inplace : bool, default False Whether to return a new DataFrame. If True then value of copy is ignored. level : int or level name, default None In case of a MultiIndex, only rename labels in the specified level. errors : {'ignore', 'raise'}, default 'ignore' If 'raise', raise a `KeyError` when a dict-like `mapper`, `index`, or `columns` contains labels that are not present in the Index being transformed. If 'ignore', existing keys will be renamed and extra keys will be ignored. Returns ------- DataFrame DataFrame with the renamed axis labels. Raises ------ KeyError If any of the labels is not found in the selected axis and \" errors = 'raise' \". See Also -------- DataFrame.rename_axis : Set the name of the axis. Examples -------- ``DataFrame.rename`` supports two calling conventions * ``(index=index_mapper, columns=columns_mapper, ...)`` * ``(mapper, axis={'index', 'columns'}, ...)`` We *highly* recommend using keyword arguments to clarify your intent. Rename columns using a mapping: >>> df = pd.DataFrame({\" A \": [1, 2, 3], \" B \": [4, 5, 6]}) >>> df.rename(columns={\" A \": \" a \", \" B \": \" c \"}) a c 0 1 4 1 2 5 2 3 6 Rename index using a mapping: >>> df.rename(index={0: \" x \", 1: \" y \", 2: \" z \"}) A B x 1 4 y 2 5 z 3 6 Cast index labels to a different type: >>> df.index RangeIndex(start=0, stop=3, step=1) >>> df.rename(index=str).index Index(['0', '1', '2'], dtype='object') >>> df.rename(columns={\" A \": \" a \", \" B \": \" b \", \" C \": \" c \"}, errors=\" raise \") Traceback (most recent call last): KeyError: ['C'] not found in axis Using axis-style parameters >>> df.rename(str.lower, axis='columns') a b 0 1 4 1 2 5 2 3 6 >>> df.rename({1: 2, 2: 4}, axis='index') A B 0 1 4 2 2 5 4 3 6 \"\"\" return super (). rename ( mapper = mapper , index = index , columns = columns , axis = axis , copy = copy , inplace = inplace , level = level , errors = errors , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#rename_axis","text":"def rename_axis ( self , mapper = None , index = None , columns = None , axis = None , copy = True , inplace = False ) Set the name of the axis for the index or columns.","title":"rename_axis"},{"location":"reference/hielen2/datalink_prova_df/#parameters_108","text":"mapper : scalar, list-like, optional Value to set the axis name attribute. index, columns : scalar, list-like, dict-like or function, optional A scalar, list-like, dict-like or functions transformations to apply to that axis' values. Note that the columns parameter is not allowed if the object is a Series. This parameter only apply for DataFrame type objects. Use either `` mapper `` and `` axis `` to specify the axis to target with `` mapper `` , or `` index `` and / or `` columns `` . .. versionchanged :: 0 . 24 . 0 axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to rename. copy : bool, default True Also copy underlying data. inplace : bool, default False Modifies the object directly, instead of creating a new Series or DataFrame.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_123","text":"Series, DataFrame, or None The same type as the caller or None if inplace is True.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_113","text":"Series.rename : Alter Series index labels or name. DataFrame.rename : Alter DataFrame index labels or name. Index.rename : Set new names on index.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_55","text":"DataFrame.rename_axis supports two calling conventions (index=index_mapper, columns=columns_mapper, ...) (mapper, axis={'index', 'columns'}, ...) The first calling convention will only modify the names of the index and/or the names of the Index object that is the columns. In this case, the parameter copy is ignored. The second calling convention will modify the names of the the corresponding index if mapper is a list or a scalar. However, if mapper is dict-like or a function, it will use the deprecated behavior of modifying the axis labels . We highly recommend using keyword arguments to clarify your intent.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_117","text":"Series s = pd.Series([\"dog\", \"cat\", \"monkey\"]) s 0 dog 1 cat 2 monkey dtype: object s.rename_axis(\"animal\") animal 0 dog 1 cat 2 monkey dtype: object DataFrame df = pd.DataFrame({\"num_legs\": [4, 4, 2], ... \"num_arms\": [0, 0, 2]}, ... [\"dog\", \"cat\", \"monkey\"]) df num_legs num_arms dog 4 0 cat 4 0 monkey 2 2 df = df.rename_axis(\"animal\") df num_legs num_arms animal dog 4 0 cat 4 0 monkey 2 2 df = df.rename_axis(\"limbs\", axis=\"columns\") df limbs num_legs num_arms animal dog 4 0 cat 4 0 monkey 2 2 MultiIndex df.index = pd.MultiIndex.from_product([['mammal'], ... ['dog', 'cat', 'monkey']], ... names=['type', 'name']) df limbs num_legs num_arms type name mammal dog 4 0 cat 4 0 monkey 2 2 df.rename_axis(index={'type': 'class'}) limbs num_legs num_arms class name mammal dog 4 0 cat 4 0 monkey 2 2 df.rename_axis(columns=str.upper) LIMBS num_legs num_arms type name mammal dog 4 0 cat 4 0 monkey 2 2 View Source @ rewrite_axis_style_signature ( \"mapper\" , [( \"copy\" , True ), ( \"inplace\" , False )]) def rename_axis ( self , mapper = lib . no_default , ** kwargs ): \"\"\" Set the name of the axis for the index or columns. Parameters ---------- mapper : scalar, list-like, optional Value to set the axis name attribute. index, columns : scalar, list-like, dict-like or function, optional A scalar, list-like, dict-like or functions transformations to apply to that axis' values. Note that the ``columns`` parameter is not allowed if the object is a Series. This parameter only apply for DataFrame type objects. Use either ``mapper`` and ``axis`` to specify the axis to target with ``mapper``, or ``index`` and/or ``columns``. .. versionchanged:: 0.24.0 axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to rename. copy : bool, default True Also copy underlying data. inplace : bool, default False Modifies the object directly, instead of creating a new Series or DataFrame. Returns ------- Series, DataFrame, or None The same type as the caller or None if `inplace` is True. See Also -------- Series.rename : Alter Series index labels or name. DataFrame.rename : Alter DataFrame index labels or name. Index.rename : Set new names on index. Notes ----- ``DataFrame.rename_axis`` supports two calling conventions * ``(index=index_mapper, columns=columns_mapper, ...)`` * ``(mapper, axis={'index', 'columns'}, ...)`` The first calling convention will only modify the names of the index and/or the names of the Index object that is the columns. In this case, the parameter ``copy`` is ignored. The second calling convention will modify the names of the the corresponding index if mapper is a list or a scalar. However, if mapper is dict-like or a function, it will use the deprecated behavior of modifying the axis *labels*. We *highly* recommend using keyword arguments to clarify your intent. Examples -------- **Series** >>> s = pd.Series([\" dog \", \" cat \", \" monkey \"]) >>> s 0 dog 1 cat 2 monkey dtype: object >>> s.rename_axis(\" animal \") animal 0 dog 1 cat 2 monkey dtype: object **DataFrame** >>> df = pd.DataFrame({\" num_legs \": [4, 4, 2], ... \" num_arms \": [0, 0, 2]}, ... [\" dog \", \" cat \", \" monkey \"]) >>> df num_legs num_arms dog 4 0 cat 4 0 monkey 2 2 >>> df = df.rename_axis(\" animal \") >>> df num_legs num_arms animal dog 4 0 cat 4 0 monkey 2 2 >>> df = df.rename_axis(\" limbs \", axis=\" columns \") >>> df limbs num_legs num_arms animal dog 4 0 cat 4 0 monkey 2 2 **MultiIndex** >>> df.index = pd.MultiIndex.from_product([['mammal'], ... ['dog', 'cat', 'monkey']], ... names=['type', 'name']) >>> df limbs num_legs num_arms type name mammal dog 4 0 cat 4 0 monkey 2 2 >>> df.rename_axis(index={'type': 'class'}) limbs num_legs num_arms class name mammal dog 4 0 cat 4 0 monkey 2 2 >>> df.rename_axis(columns=str.upper) LIMBS num_legs num_arms type name mammal dog 4 0 cat 4 0 monkey 2 2 \"\"\" axes , kwargs = self . _construct_axes_from_arguments ( (), kwargs , sentinel = lib . no_default ) copy = kwargs . pop ( \"copy\" , True ) inplace = kwargs . pop ( \"inplace\" , False ) axis = kwargs . pop ( \"axis\" , 0 ) if axis is not None : axis = self . _get_axis_number ( axis ) if kwargs : raise TypeError ( \"rename_axis() got an unexpected keyword \" f 'argument \"{list(kwargs.keys())[0]}\"' ) inplace = validate_bool_kwarg ( inplace , \"inplace\" ) if mapper is not lib . no_default : # Use v0.23 behavior if a scalar or list non_mapper = is_scalar ( mapper ) or ( is_list_like ( mapper ) and not is_dict_like ( mapper ) ) if non_mapper : return self . _set_axis_name ( mapper , axis = axis , inplace = inplace ) else : raise ValueError ( \"Use `.rename` to alter labels with a mapper.\" ) else : # Use new behavior. Means that index and/or columns # is specified result = self if inplace else self . copy ( deep = copy ) for axis in range ( self . _AXIS_LEN ): v = axes . get ( self . _get_axis_name ( axis )) if v is lib . no_default : continue non_mapper = is_scalar ( v ) or ( is_list_like ( v ) and not is_dict_like ( v )) if non_mapper : newnames = v else : f = com . get_rename_function ( v ) curnames = self . _get_axis ( axis ). names newnames = [ f ( name ) for name in curnames ] result . _set_axis_name ( newnames , axis = axis , inplace = True ) if not inplace : return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#reorder_levels","text":"def reorder_levels ( self , order , axis = 0 ) -> 'DataFrame' Rearrange index levels using input order. May not drop or duplicate levels.","title":"reorder_levels"},{"location":"reference/hielen2/datalink_prova_df/#parameters_109","text":"order : list of int or list of str List representing new level order. Reference level by number (position) or by key (label). axis : {0 or 'index', 1 or 'columns'}, default 0 Where to reorder levels.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_124","text":"DataFrame View Source def reorder_levels ( self , order , axis = 0 ) -> \"DataFrame\" : \"\"\" Rearrange index levels using input order. May not drop or duplicate levels. Parameters ---------- order : list of int or list of str List representing new level order. Reference level by number (position) or by key (label). axis : {0 or 'index', 1 or 'columns'}, default 0 Where to reorder levels. Returns ------- DataFrame \"\"\" axis = self . _get_axis_number ( axis ) if not isinstance ( self . _get_axis ( axis ), MultiIndex ): # pragma : no cover raise TypeError ( \"Can only reorder levels on a hierarchical axis.\" ) result = self . copy () if axis == 0 : assert isinstance ( result . index , MultiIndex ) result . index = result . index . reorder_levels ( order ) else : assert isinstance ( result . columns , MultiIndex ) result . columns = result . columns . reorder_levels ( order ) return result","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#replace","text":"def replace ( self , to_replace = None , value = None , inplace = False , limit = None , regex = False , method = 'pad' ) Replace values given in to_replace with value . Values of the DataFrame are replaced with other values dynamically. This differs from updating with .loc or .iloc , which require you to specify a location to update with some value.","title":"replace"},{"location":"reference/hielen2/datalink_prova_df/#parameters_110","text":"to_replace : str, regex, list, dict, Series, int, float, or None How to find the values that will be replaced. * numeric , str or regex : - numeric : numeric values equal to `to_replace` will be replaced with `value` - str : string exactly matching `to_replace` will be replaced with `value` - regex : regexs matching `to_replace` will be replaced with `value` * list of str , regex , or numeric : - First , if `to_replace` and `value` are both lists , they ** must ** be the same length . - Second , if `` regex = True `` then all of the strings in ** both ** lists will be interpreted as regexs otherwise they will match directly . This doesn 't matter much for `value` since there are only a few possible substitution regexes you can use. - str, regex and numeric rules apply as above. * dict: - Dicts can be used to specify different replacement values for different existing values. For example, ``{' a ': ' b ', ' y ': ' z '}`` replaces the value ' a ' with ' b ' and ' y ' with ' z '. To use a dict in this way the `value` parameter should be `None`. - For a DataFrame a dict can specify that different values should be replaced in different columns. For example, ``{' a ': 1, ' b ': ' z '}`` looks for the value 1 in column ' a ' and the value ' z ' in column ' b ' and replaces these values with whatever is specified in `value`. The `value` parameter should not be ``None`` in this case. You can treat this as a special case of passing two lists except that you are specifying the column to search in. - For a DataFrame nested dictionaries, e.g., ``{' a ': {' b ': np.nan}}``, are read as follows: look in column ' a ' for the value ' b ' and replace it with NaN . The `value` parameter should be `` None `` to use a nested dict in this way . You can nest regular expressions as well . Note that column names ( the top - level dictionary keys in a nested dictionary ) ** cannot ** be regular expressions . * None : - This means that the `regex` argument must be a string , compiled regular expression , or list , dict , ndarray or Series of such elements . If `value` is also `` None `` then this ** must ** be a nested dictionary or Series . See the examples section for examples of each of these . value : scalar, dict, list, str, regex, default None Value to replace any values matching to_replace with. For a DataFrame a dict of values can be used to specify which value to use for each column (columns not in the dict will not be filled). Regular expressions, strings and lists or dicts of such objects are also allowed. inplace : bool, default False If True, in place. Note: this will modify any other views on this object (e.g. a column from a DataFrame). Returns the caller if this is True. limit : int, default None Maximum size gap to forward or backward fill. regex : bool or same types as to_replace , default False Whether to interpret to_replace and/or value as regular expressions. If this is True then to_replace must be a string. Alternatively, this could be a regular expression or a list, dict, or array of regular expressions in which case to_replace must be None . method : {'pad', 'ffill', 'bfill', None } The method to use when for replacement, when to_replace is a scalar, list or tuple and value is None . .. versionchanged :: 0.23.0 Added to DataFrame.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_125","text":"DataFrame Object after replacement.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_13","text":"AssertionError * If regex is not a bool and to_replace is not None . TypeError * If to_replace is not a scalar, array-like, dict , or None * If to_replace is a dict and value is not a list , dict , ndarray , or Series * If to_replace is None and regex is not compilable into a regular expression or is a list, dict, ndarray, or Series. * When replacing multiple bool or datetime64 objects and the arguments to to_replace does not match the type of the value being replaced ValueError * If a list or an ndarray is passed to to_replace and value but they are not the same length.","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_114","text":"DataFrame.fillna : Fill NA values. DataFrame.where : Replace values based on boolean condition. Series.str.replace : Simple string replacement.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_56","text":"Regex substitution is performed under the hood with re.sub . The rules for substitution for re.sub are the same. Regular expressions will only substitute on strings, meaning you cannot provide, for example, a regular expression matching floating point numbers and expect the columns in your frame that have a numeric dtype to be matched. However, if those floating point numbers are strings, then you can do this. This method has a lot of options. You are encouraged to experiment and play with this method to gain intuition about how it works. When dict is used as the to_replace value, it is like key(s) in the dict are the to_replace part and value(s) in the dict are the value parameter.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_118","text":"Scalar to_replace and value s = pd.Series([0, 1, 2, 3, 4]) s.replace(0, 5) 0 5 1 1 2 2 3 3 4 4 dtype: int64 df = pd.DataFrame({'A': [0, 1, 2, 3, 4], ... 'B': [5, 6, 7, 8, 9], ... 'C': ['a', 'b', 'c', 'd', 'e']}) df.replace(0, 5) A B C 0 5 5 a 1 1 6 b 2 2 7 c 3 3 8 d 4 4 9 e List-like to_replace df.replace([0, 1, 2, 3], 4) A B C 0 4 5 a 1 4 6 b 2 4 7 c 3 4 8 d 4 4 9 e df.replace([0, 1, 2, 3], [4, 3, 2, 1]) A B C 0 4 5 a 1 3 6 b 2 2 7 c 3 1 8 d 4 4 9 e s.replace([1, 2], method='bfill') 0 0 1 3 2 3 3 3 4 4 dtype: int64 dict-like to_replace df.replace({0: 10, 1: 100}) A B C 0 10 5 a 1 100 6 b 2 2 7 c 3 3 8 d 4 4 9 e df.replace({'A': 0, 'B': 5}, 100) A B C 0 100 100 a 1 1 6 b 2 2 7 c 3 3 8 d 4 4 9 e df.replace({'A': {0: 100, 4: 400}}) A B C 0 100 5 a 1 1 6 b 2 2 7 c 3 3 8 d 4 400 9 e Regular expression to_replace df = pd.DataFrame({'A': ['bat', 'foo', 'bait'], ... 'B': ['abc', 'bar', 'xyz']}) df.replace(to_replace=r'^ba.$', value='new', regex=True) A B 0 new abc 1 foo new 2 bait xyz df.replace({'A': r'^ba.$'}, {'A': 'new'}, regex=True) A B 0 new abc 1 foo bar 2 bait xyz df.replace(regex=r'^ba.$', value='new') A B 0 new abc 1 foo new 2 bait xyz df.replace(regex={r'^ba.$': 'new', 'foo': 'xyz'}) A B 0 new abc 1 xyz new 2 bait xyz df.replace(regex=[r'^ba.$', 'foo'], value='new') A B 0 new abc 1 new new 2 bait xyz Note that when replacing multiple bool or datetime64 objects, the data types in the to_replace parameter must match the data type of the value being replaced: df = pd.DataFrame({'A': [True, False, True], ... 'B': [False, True, False]}) df.replace({'a string': 'new value', True: False}) # raises Traceback (most recent call last): ... TypeError: Cannot compare types 'ndarray(dtype=bool)' and 'str' This raises a TypeError because one of the dict keys is not of the correct type for replacement. Compare the behavior of s.replace({'a': None}) and s.replace('a', None) to understand the peculiarities of the to_replace parameter: s = pd.Series([10, 'a', 'a', 'b', 'a']) When one uses a dict as the to_replace value, it is like the value(s) in the dict are equal to the value parameter. s.replace({'a': None}) is equivalent to s.replace(to_replace={'a': None}, value=None, method=None) : s.replace({'a': None}) 0 10 1 None 2 None 3 b 4 None dtype: object When value=None and to_replace is a scalar, list or tuple, replace uses the method parameter (default 'pad') to do the replacement. So this is why the 'a' values are being replaced by 10 in rows 1 and 2 and 'b' in row 4 in this case. The command s.replace('a', None) is actually equivalent to s.replace(to_replace='a', value=None, method='pad') : s.replace('a', None) 0 10 1 10 2 10 3 b 4 b dtype: object View Source @doc ( NDFrame . replace , ** _shared_doc_kwargs ) def replace ( self , to_replace = None , value = None , inplace = False , limit = None , regex = False , method = \"pad\" , ) : return super (). replace ( to_replace = to_replace , value = value , inplace = inplace , limit = limit , regex = regex , method = method , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#resample","text":"def resample ( self , rule , axis = 0 , closed : Union [ str , NoneType ] = None , label : Union [ str , NoneType ] = None , convention : str = 'start' , kind : Union [ str , NoneType ] = None , loffset = None , base : Union [ int , NoneType ] = None , on = None , level = None , origin : Union [ str , ForwardRef ( 'Timestamp' ), datetime . datetime , numpy . datetime64 , int , numpy . int64 , float ] = 'start_day' , offset : Union [ ForwardRef ( 'Timedelta' ), datetime . timedelta , numpy . timedelta64 , int , numpy . int64 , float , str , NoneType ] = None ) -> 'Resampler' Resample time-series data. Convenience method for frequency conversion and resampling of time series. Object must have a datetime-like index ( DatetimeIndex , PeriodIndex , or TimedeltaIndex ), or pass datetime-like values to the on or level keyword.","title":"resample"},{"location":"reference/hielen2/datalink_prova_df/#parameters_111","text":"rule : DateOffset, Timedelta or str The offset string or object representing target conversion. axis : {0 or 'index', 1 or 'columns'}, default 0 Which axis to use for up- or down-sampling. For Series this will default to 0, i.e. along the rows. Must be DatetimeIndex , TimedeltaIndex or PeriodIndex . closed : {'right', 'left'}, default None Which side of bin interval is closed. The default is 'left' for all frequency offsets except for 'M', 'A', 'Q', 'BM', 'BA', 'BQ', and 'W' which all have a default of 'right'. label : {'right', 'left'}, default None Which bin edge label to label bucket with. The default is 'left' for all frequency offsets except for 'M', 'A', 'Q', 'BM', 'BA', 'BQ', and 'W' which all have a default of 'right'. convention : {'start', 'end', 's', 'e'}, default 'start' For PeriodIndex only, controls whether to use the start or end of rule . kind : {'timestamp', 'period'}, optional, default None Pass 'timestamp' to convert the resulting index to a DateTimeIndex or 'period' to convert it to a PeriodIndex . By default the input representation is retained. loffset : timedelta, default None Adjust the resampled time labels. .. deprecated :: 1.1.0 You should add the loffset to the `df.index` after the resample. See below. base : int, default 0 For frequencies that evenly subdivide 1 day, the \"origin\" of the aggregated intervals. For example, for '5min' frequency, base could range from 0 through 4. Defaults to 0. .. deprecated :: 1.1.0 The new arguments that you should use are 'offset' or 'origin'. on : str, optional For a DataFrame, column to use instead of index for resampling. Column must be datetime-like. level : str or int, optional For a MultiIndex, level (name or number) to use for resampling. level must be datetime-like. origin : {'epoch', 'start', 'start_day'}, Timestamp or str, default 'start_day' The timestamp on which to adjust the grouping. The timezone of origin must match the timezone of the index. If a timestamp is not used, these values are also supported: - 'epoch' : `origin` is 1970 - 01 - 01 - 'start' : `origin` is the first value of the timeseries - 'start_day' : `origin` is the first day at midnight of the timeseries .. versionadded :: 1 . 1 . 0 offset : Timedelta or str, default is None An offset timedelta added to the origin. .. versionadded :: 1.1.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_126","text":"Resampler object","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_115","text":"groupby : Group by mapping, function, label, or list of labels. Series.resample : Resample a Series. DataFrame.resample: Resample a DataFrame.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_57","text":"See the user guide <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling> _ for more. To learn more about the offset strings, please see this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects> __.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_119","text":"Start by creating a series with 9 one minute timestamps. index = pd.date_range('1/1/2000', periods=9, freq='T') series = pd.Series(range(9), index=index) series 2000-01-01 00:00:00 0 2000-01-01 00:01:00 1 2000-01-01 00:02:00 2 2000-01-01 00:03:00 3 2000-01-01 00:04:00 4 2000-01-01 00:05:00 5 2000-01-01 00:06:00 6 2000-01-01 00:07:00 7 2000-01-01 00:08:00 8 Freq: T, dtype: int64 Downsample the series into 3 minute bins and sum the values of the timestamps falling into a bin. series.resample('3T').sum() 2000-01-01 00:00:00 3 2000-01-01 00:03:00 12 2000-01-01 00:06:00 21 Freq: 3T, dtype: int64 Downsample the series into 3 minute bins as above, but label each bin using the right edge instead of the left. Please note that the value in the bucket used as the label is not included in the bucket, which it labels. For example, in the original series the bucket 2000-01-01 00:03:00 contains the value 3, but the summed value in the resampled bucket with the label 2000-01-01 00:03:00 does not include 3 (if it did, the summed value would be 6, not 3). To include this value close the right side of the bin interval as illustrated in the example below this one. series.resample('3T', label='right').sum() 2000-01-01 00:03:00 3 2000-01-01 00:06:00 12 2000-01-01 00:09:00 21 Freq: 3T, dtype: int64 Downsample the series into 3 minute bins as above, but close the right side of the bin interval. series.resample('3T', label='right', closed='right').sum() 2000-01-01 00:00:00 0 2000-01-01 00:03:00 6 2000-01-01 00:06:00 15 2000-01-01 00:09:00 15 Freq: 3T, dtype: int64 Upsample the series into 30 second bins. series.resample('30S').asfreq()[0:5] # Select first 5 rows 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 NaN 2000-01-01 00:01:00 1.0 2000-01-01 00:01:30 NaN 2000-01-01 00:02:00 2.0 Freq: 30S, dtype: float64 Upsample the series into 30 second bins and fill the NaN values using the pad method. series.resample('30S').pad()[0:5] 2000-01-01 00:00:00 0 2000-01-01 00:00:30 0 2000-01-01 00:01:00 1 2000-01-01 00:01:30 1 2000-01-01 00:02:00 2 Freq: 30S, dtype: int64 Upsample the series into 30 second bins and fill the NaN values using the bfill method. series.resample('30S').bfill()[0:5] 2000-01-01 00:00:00 0 2000-01-01 00:00:30 1 2000-01-01 00:01:00 1 2000-01-01 00:01:30 2 2000-01-01 00:02:00 2 Freq: 30S, dtype: int64 Pass a custom function via apply def custom_resampler(array_like): ... return np.sum(array_like) + 5 ... series.resample('3T').apply(custom_resampler) 2000-01-01 00:00:00 8 2000-01-01 00:03:00 17 2000-01-01 00:06:00 26 Freq: 3T, dtype: int64 For a Series with a PeriodIndex, the keyword convention can be used to control whether to use the start or end of rule . Resample a year by quarter using 'start' convention . Values are assigned to the first quarter of the period. s = pd.Series([1, 2], index=pd.period_range('2012-01-01', ... freq='A', ... periods=2)) s 2012 1 2013 2 Freq: A-DEC, dtype: int64 s.resample('Q', convention='start').asfreq() 2012Q1 1.0 2012Q2 NaN 2012Q3 NaN 2012Q4 NaN 2013Q1 2.0 2013Q2 NaN 2013Q3 NaN 2013Q4 NaN Freq: Q-DEC, dtype: float64 Resample quarters by month using 'end' convention . Values are assigned to the last month of the period. q = pd.Series([1, 2, 3, 4], index=pd.period_range('2018-01-01', ... freq='Q', ... periods=4)) q 2018Q1 1 2018Q2 2 2018Q3 3 2018Q4 4 Freq: Q-DEC, dtype: int64 q.resample('M', convention='end').asfreq() 2018-03 1.0 2018-04 NaN 2018-05 NaN 2018-06 2.0 2018-07 NaN 2018-08 NaN 2018-09 3.0 2018-10 NaN 2018-11 NaN 2018-12 4.0 Freq: M, dtype: float64 For DataFrame objects, the keyword on can be used to specify the column instead of the index for resampling. d = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19], ... 'volume': [50, 60, 40, 100, 50, 100, 40, 50]}) df = pd.DataFrame(d) df['week_starting'] = pd.date_range('01/01/2018', ... periods=8, ... freq='W') df price volume week_starting 0 10 50 2018-01-07 1 11 60 2018-01-14 2 9 40 2018-01-21 3 13 100 2018-01-28 4 14 50 2018-02-04 5 18 100 2018-02-11 6 17 40 2018-02-18 7 19 50 2018-02-25 df.resample('M', on='week_starting').mean() price volume week_starting 2018-01-31 10.75 62.5 2018-02-28 17.00 60.0 For a DataFrame with MultiIndex, the keyword level can be used to specify on which level the resampling needs to take place. days = pd.date_range('1/1/2000', periods=4, freq='D') d2 = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19], ... 'volume': [50, 60, 40, 100, 50, 100, 40, 50]}) df2 = pd.DataFrame(d2, ... index=pd.MultiIndex.from_product([days, ... ['morning', ... 'afternoon']] ... )) df2 price volume 2000-01-01 morning 10 50 afternoon 11 60 2000-01-02 morning 9 40 afternoon 13 100 2000-01-03 morning 14 50 afternoon 18 100 2000-01-04 morning 17 40 afternoon 19 50 df2.resample('D', level=0).sum() price volume 2000-01-01 21 110 2000-01-02 22 140 2000-01-03 32 150 2000-01-04 36 90 If you want to adjust the start of the bins based on a fixed timestamp: start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00' rng = pd.date_range(start, end, freq='7min') ts = pd.Series(np.arange(len(rng)) * 3, index=rng) ts 2000-10-01 23:30:00 0 2000-10-01 23:37:00 3 2000-10-01 23:44:00 6 2000-10-01 23:51:00 9 2000-10-01 23:58:00 12 2000-10-02 00:05:00 15 2000-10-02 00:12:00 18 2000-10-02 00:19:00 21 2000-10-02 00:26:00 24 Freq: 7T, dtype: int64 ts.resample('17min').sum() 2000-10-01 23:14:00 0 2000-10-01 23:31:00 9 2000-10-01 23:48:00 21 2000-10-02 00:05:00 54 2000-10-02 00:22:00 24 Freq: 17T, dtype: int64 ts.resample('17min', origin='epoch').sum() 2000-10-01 23:18:00 0 2000-10-01 23:35:00 18 2000-10-01 23:52:00 27 2000-10-02 00:09:00 39 2000-10-02 00:26:00 24 Freq: 17T, dtype: int64 ts.resample('17min', origin='2000-01-01').sum() 2000-10-01 23:24:00 3 2000-10-01 23:41:00 15 2000-10-01 23:58:00 45 2000-10-02 00:15:00 45 Freq: 17T, dtype: int64 If you want to adjust the start of the bins with an offset Timedelta, the two following lines are equivalent: ts.resample('17min', origin='start').sum() 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17T, dtype: int64 ts.resample('17min', offset='23h30min').sum() 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17T, dtype: int64 To replace the use of the deprecated base argument, you can now use offset , in this example it is equivalent to have base=2 : ts.resample('17min', offset='2min').sum() 2000-10-01 23:16:00 0 2000-10-01 23:33:00 9 2000-10-01 23:50:00 36 2000-10-02 00:07:00 39 2000-10-02 00:24:00 24 Freq: 17T, dtype: int64 To replace the use of the deprecated loffset argument: from pandas.tseries.frequencies import to_offset loffset = '19min' ts_out = ts.resample('17min').sum() ts_out.index = ts_out.index + to_offset(loffset) ts_out 2000-10-01 23:33:00 0 2000-10-01 23:50:00 9 2000-10-02 00:07:00 21 2000-10-02 00:24:00 54 2000-10-02 00:41:00 24 Freq: 17T, dtype: int64 View Source def resample ( self , rule , axis = 0 , closed : Optional [ str ] = None , label : Optional [ str ] = None , convention : str = \"start\" , kind : Optional [ str ] = None , loffset = None , base : Optional [ int ] = None , on = None , level = None , origin : Union [ str , TimestampConvertibleTypes ] = \"start_day\" , offset : Optional [ TimedeltaConvertibleTypes ] = None , ) -> \"Resampler\" : \"\"\" Resample time-series data. Convenience method for frequency conversion and resampling of time series. Object must have a datetime-like index (`DatetimeIndex`, `PeriodIndex`, or `TimedeltaIndex`), or pass datetime-like values to the `on` or `level` keyword. Parameters ---------- rule : DateOffset, Timedelta or str The offset string or object representing target conversion. axis : {0 or 'index', 1 or 'columns'}, default 0 Which axis to use for up- or down-sampling. For `Series` this will default to 0, i.e. along the rows. Must be `DatetimeIndex`, `TimedeltaIndex` or `PeriodIndex`. closed : {'right', 'left'}, default None Which side of bin interval is closed. The default is 'left' for all frequency offsets except for 'M', 'A', 'Q', 'BM', 'BA', 'BQ', and 'W' which all have a default of 'right'. label : {'right', 'left'}, default None Which bin edge label to label bucket with. The default is 'left' for all frequency offsets except for 'M', 'A', 'Q', 'BM', 'BA', 'BQ', and 'W' which all have a default of 'right'. convention : {'start', 'end', 's', 'e'}, default 'start' For `PeriodIndex` only, controls whether to use the start or end of `rule`. kind : {'timestamp', 'period'}, optional, default None Pass 'timestamp' to convert the resulting index to a `DateTimeIndex` or 'period' to convert it to a `PeriodIndex`. By default the input representation is retained. loffset : timedelta, default None Adjust the resampled time labels. .. deprecated:: 1.1.0 You should add the loffset to the `df.index` after the resample. See below. base : int, default 0 For frequencies that evenly subdivide 1 day, the \" origin \" of the aggregated intervals. For example, for '5min' frequency, base could range from 0 through 4. Defaults to 0. .. deprecated:: 1.1.0 The new arguments that you should use are 'offset' or 'origin'. on : str, optional For a DataFrame, column to use instead of index for resampling. Column must be datetime-like. level : str or int, optional For a MultiIndex, level (name or number) to use for resampling. `level` must be datetime-like. origin : {'epoch', 'start', 'start_day'}, Timestamp or str, default 'start_day' The timestamp on which to adjust the grouping. The timezone of origin must match the timezone of the index. If a timestamp is not used, these values are also supported: - 'epoch': `origin` is 1970-01-01 - 'start': `origin` is the first value of the timeseries - 'start_day': `origin` is the first day at midnight of the timeseries .. versionadded:: 1.1.0 offset : Timedelta or str, default is None An offset timedelta added to the origin. .. versionadded:: 1.1.0 Returns ------- Resampler object See Also -------- groupby : Group by mapping, function, label, or list of labels. Series.resample : Resample a Series. DataFrame.resample: Resample a DataFrame. Notes ----- See the `user guide <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#resampling>`_ for more. To learn more about the offset strings, please see `this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#dateoffset-objects>`__. Examples -------- Start by creating a series with 9 one minute timestamps. >>> index = pd.date_range('1/1/2000', periods=9, freq='T') >>> series = pd.Series(range(9), index=index) >>> series 2000-01-01 00:00:00 0 2000-01-01 00:01:00 1 2000-01-01 00:02:00 2 2000-01-01 00:03:00 3 2000-01-01 00:04:00 4 2000-01-01 00:05:00 5 2000-01-01 00:06:00 6 2000-01-01 00:07:00 7 2000-01-01 00:08:00 8 Freq: T, dtype: int64 Downsample the series into 3 minute bins and sum the values of the timestamps falling into a bin. >>> series.resample('3T').sum() 2000-01-01 00:00:00 3 2000-01-01 00:03:00 12 2000-01-01 00:06:00 21 Freq: 3T, dtype: int64 Downsample the series into 3 minute bins as above, but label each bin using the right edge instead of the left. Please note that the value in the bucket used as the label is not included in the bucket, which it labels. For example, in the original series the bucket ``2000-01-01 00:03:00`` contains the value 3, but the summed value in the resampled bucket with the label ``2000-01-01 00:03:00`` does not include 3 (if it did, the summed value would be 6, not 3). To include this value close the right side of the bin interval as illustrated in the example below this one. >>> series.resample('3T', label='right').sum() 2000-01-01 00:03:00 3 2000-01-01 00:06:00 12 2000-01-01 00:09:00 21 Freq: 3T, dtype: int64 Downsample the series into 3 minute bins as above, but close the right side of the bin interval. >>> series.resample('3T', label='right', closed='right').sum() 2000-01-01 00:00:00 0 2000-01-01 00:03:00 6 2000-01-01 00:06:00 15 2000-01-01 00:09:00 15 Freq: 3T, dtype: int64 Upsample the series into 30 second bins. >>> series.resample('30S').asfreq()[0:5] # Select first 5 rows 2000-01-01 00:00:00 0.0 2000-01-01 00:00:30 NaN 2000-01-01 00:01:00 1.0 2000-01-01 00:01:30 NaN 2000-01-01 00:02:00 2.0 Freq: 30S, dtype: float64 Upsample the series into 30 second bins and fill the ``NaN`` values using the ``pad`` method. >>> series.resample('30S').pad()[0:5] 2000-01-01 00:00:00 0 2000-01-01 00:00:30 0 2000-01-01 00:01:00 1 2000-01-01 00:01:30 1 2000-01-01 00:02:00 2 Freq: 30S, dtype: int64 Upsample the series into 30 second bins and fill the ``NaN`` values using the ``bfill`` method. >>> series.resample('30S').bfill()[0:5] 2000-01-01 00:00:00 0 2000-01-01 00:00:30 1 2000-01-01 00:01:00 1 2000-01-01 00:01:30 2 2000-01-01 00:02:00 2 Freq: 30S, dtype: int64 Pass a custom function via ``apply`` >>> def custom_resampler(array_like): ... return np.sum(array_like) + 5 ... >>> series.resample('3T').apply(custom_resampler) 2000-01-01 00:00:00 8 2000-01-01 00:03:00 17 2000-01-01 00:06:00 26 Freq: 3T, dtype: int64 For a Series with a PeriodIndex, the keyword `convention` can be used to control whether to use the start or end of `rule`. Resample a year by quarter using 'start' `convention`. Values are assigned to the first quarter of the period. >>> s = pd.Series([1, 2], index=pd.period_range('2012-01-01', ... freq='A', ... periods=2)) >>> s 2012 1 2013 2 Freq: A-DEC, dtype: int64 >>> s.resample('Q', convention='start').asfreq() 2012Q1 1.0 2012Q2 NaN 2012Q3 NaN 2012Q4 NaN 2013Q1 2.0 2013Q2 NaN 2013Q3 NaN 2013Q4 NaN Freq: Q-DEC, dtype: float64 Resample quarters by month using 'end' `convention`. Values are assigned to the last month of the period. >>> q = pd.Series([1, 2, 3, 4], index=pd.period_range('2018-01-01', ... freq='Q', ... periods=4)) >>> q 2018Q1 1 2018Q2 2 2018Q3 3 2018Q4 4 Freq: Q-DEC, dtype: int64 >>> q.resample('M', convention='end').asfreq() 2018-03 1.0 2018-04 NaN 2018-05 NaN 2018-06 2.0 2018-07 NaN 2018-08 NaN 2018-09 3.0 2018-10 NaN 2018-11 NaN 2018-12 4.0 Freq: M, dtype: float64 For DataFrame objects, the keyword `on` can be used to specify the column instead of the index for resampling. >>> d = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19], ... 'volume': [50, 60, 40, 100, 50, 100, 40, 50]}) >>> df = pd.DataFrame(d) >>> df['week_starting'] = pd.date_range('01/01/2018', ... periods=8, ... freq='W') >>> df price volume week_starting 0 10 50 2018-01-07 1 11 60 2018-01-14 2 9 40 2018-01-21 3 13 100 2018-01-28 4 14 50 2018-02-04 5 18 100 2018-02-11 6 17 40 2018-02-18 7 19 50 2018-02-25 >>> df.resample('M', on='week_starting').mean() price volume week_starting 2018-01-31 10.75 62.5 2018-02-28 17.00 60.0 For a DataFrame with MultiIndex, the keyword `level` can be used to specify on which level the resampling needs to take place. >>> days = pd.date_range('1/1/2000', periods=4, freq='D') >>> d2 = dict({'price': [10, 11, 9, 13, 14, 18, 17, 19], ... 'volume': [50, 60, 40, 100, 50, 100, 40, 50]}) >>> df2 = pd.DataFrame(d2, ... index=pd.MultiIndex.from_product([days, ... ['morning', ... 'afternoon']] ... )) >>> df2 price volume 2000-01-01 morning 10 50 afternoon 11 60 2000-01-02 morning 9 40 afternoon 13 100 2000-01-03 morning 14 50 afternoon 18 100 2000-01-04 morning 17 40 afternoon 19 50 >>> df2.resample('D', level=0).sum() price volume 2000-01-01 21 110 2000-01-02 22 140 2000-01-03 32 150 2000-01-04 36 90 If you want to adjust the start of the bins based on a fixed timestamp: >>> start, end = '2000-10-01 23:30:00', '2000-10-02 00:30:00' >>> rng = pd.date_range(start, end, freq='7min') >>> ts = pd.Series(np.arange(len(rng)) * 3, index=rng) >>> ts 2000-10-01 23:30:00 0 2000-10-01 23:37:00 3 2000-10-01 23:44:00 6 2000-10-01 23:51:00 9 2000-10-01 23:58:00 12 2000-10-02 00:05:00 15 2000-10-02 00:12:00 18 2000-10-02 00:19:00 21 2000-10-02 00:26:00 24 Freq: 7T, dtype: int64 >>> ts.resample('17min').sum() 2000-10-01 23:14:00 0 2000-10-01 23:31:00 9 2000-10-01 23:48:00 21 2000-10-02 00:05:00 54 2000-10-02 00:22:00 24 Freq: 17T, dtype: int64 >>> ts.resample('17min', origin='epoch').sum() 2000-10-01 23:18:00 0 2000-10-01 23:35:00 18 2000-10-01 23:52:00 27 2000-10-02 00:09:00 39 2000-10-02 00:26:00 24 Freq: 17T, dtype: int64 >>> ts.resample('17min', origin='2000-01-01').sum() 2000-10-01 23:24:00 3 2000-10-01 23:41:00 15 2000-10-01 23:58:00 45 2000-10-02 00:15:00 45 Freq: 17T, dtype: int64 If you want to adjust the start of the bins with an `offset` Timedelta, the two following lines are equivalent: >>> ts.resample('17min', origin='start').sum() 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17T, dtype: int64 >>> ts.resample('17min', offset='23h30min').sum() 2000-10-01 23:30:00 9 2000-10-01 23:47:00 21 2000-10-02 00:04:00 54 2000-10-02 00:21:00 24 Freq: 17T, dtype: int64 To replace the use of the deprecated `base` argument, you can now use `offset`, in this example it is equivalent to have `base=2`: >>> ts.resample('17min', offset='2min').sum() 2000-10-01 23:16:00 0 2000-10-01 23:33:00 9 2000-10-01 23:50:00 36 2000-10-02 00:07:00 39 2000-10-02 00:24:00 24 Freq: 17T, dtype: int64 To replace the use of the deprecated `loffset` argument: >>> from pandas.tseries.frequencies import to_offset >>> loffset = '19min' >>> ts_out = ts.resample('17min').sum() >>> ts_out.index = ts_out.index + to_offset(loffset) >>> ts_out 2000-10-01 23:33:00 0 2000-10-01 23:50:00 9 2000-10-02 00:07:00 21 2000-10-02 00:24:00 54 2000-10-02 00:41:00 24 Freq: 17T, dtype: int64 \"\"\" from pandas . core . resample import get_resampler axis = self . _get_axis_number ( axis ) return get_resampler ( self , freq = rule , label = label , closed = closed , axis = axis , kind = kind , loffset = loffset , convention = convention , base = base , key = on , level = level , origin = origin , offset = offset , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#reset_index","text":"def reset_index ( self , level : Union [ Hashable , Sequence [ Hashable ], NoneType ] = None , drop : bool = False , inplace : bool = False , col_level : Hashable = 0 , col_fill : Union [ Hashable , NoneType ] = '' ) -> Union [ ForwardRef ( 'DataFrame' ), NoneType ] Reset the index, or a level of it. Reset the index of the DataFrame, and use the default one instead. If the DataFrame has a MultiIndex, this method can remove one or more levels.","title":"reset_index"},{"location":"reference/hielen2/datalink_prova_df/#parameters_112","text":"level : int, str, tuple, or list, default None Only remove the given levels from the index. Removes all levels by default. drop : bool, default False Do not try to insert index into dataframe columns. This resets the index to the default integer index. inplace : bool, default False Modify the DataFrame in place (do not create a new object). col_level : int or str, default 0 If the columns have multiple levels, determines which level the labels are inserted into. By default it is inserted into the first level. col_fill : object, default '' If the columns have multiple levels, determines how the other levels are named. If None then the index name is repeated.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_127","text":"DataFrame or None DataFrame with the new index or None if inplace=True .","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_116","text":"DataFrame.set_index : Opposite of reset_index. DataFrame.reindex : Change to new indices or expand indices. DataFrame.reindex_like : Change to same indices as other DataFrame.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_120","text":"df = pd.DataFrame([('bird', 389.0), ... ('bird', 24.0), ... ('mammal', 80.5), ... ('mammal', np.nan)], ... index=['falcon', 'parrot', 'lion', 'monkey'], ... columns=('class', 'max_speed')) df class max_speed falcon bird 389.0 parrot bird 24.0 lion mammal 80.5 monkey mammal NaN When we reset the index, the old index is added as a column, and a new sequential index is used: df.reset_index() index class max_speed 0 falcon bird 389.0 1 parrot bird 24.0 2 lion mammal 80.5 3 monkey mammal NaN We can use the drop parameter to avoid the old index being added as a column: df.reset_index(drop=True) class max_speed 0 bird 389.0 1 bird 24.0 2 mammal 80.5 3 mammal NaN You can also use reset_index with MultiIndex . index = pd.MultiIndex.from_tuples([('bird', 'falcon'), ... ('bird', 'parrot'), ... ('mammal', 'lion'), ... ('mammal', 'monkey')], ... names=['class', 'name']) columns = pd.MultiIndex.from_tuples([('speed', 'max'), ... ('species', 'type')]) df = pd.DataFrame([(389.0, 'fly'), ... ( 24.0, 'fly'), ... ( 80.5, 'run'), ... (np.nan, 'jump')], ... index=index, ... columns=columns) df speed species max type class name bird falcon 389.0 fly parrot 24.0 fly mammal lion 80.5 run monkey NaN jump If the index has multiple levels, we can reset a subset of them: df.reset_index(level='class') class speed species max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump If we are not dropping the index, by default, it is placed in the top level. We can place it in another level: df.reset_index(level='class', col_level=1) speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump When the index is inserted under another level, we can specify under which one with the parameter col_fill : df.reset_index(level='class', col_level=1, col_fill='species') species speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump If we specify a nonexistent level for col_fill , it is created: df.reset_index(level='class', col_level=1, col_fill='genus') genus speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump View Source def reset_index ( self , level : Optional [ Union[Hashable, Sequence[Hashable ] ]] = None , drop : bool = False , inplace : bool = False , col_level : Hashable = 0 , col_fill : Label = \"\" , ) -> Optional [ \"DataFrame\" ] : \"\"\" Reset the index, or a level of it. Reset the index of the DataFrame, and use the default one instead. If the DataFrame has a MultiIndex, this method can remove one or more levels. Parameters ---------- level : int, str, tuple, or list, default None Only remove the given levels from the index. Removes all levels by default. drop : bool, default False Do not try to insert index into dataframe columns. This resets the index to the default integer index. inplace : bool, default False Modify the DataFrame in place (do not create a new object). col_level : int or str, default 0 If the columns have multiple levels, determines which level the labels are inserted into. By default it is inserted into the first level. col_fill : object, default '' If the columns have multiple levels, determines how the other levels are named. If None then the index name is repeated. Returns ------- DataFrame or None DataFrame with the new index or None if ``inplace=True``. See Also -------- DataFrame.set_index : Opposite of reset_index. DataFrame.reindex : Change to new indices or expand indices. DataFrame.reindex_like : Change to same indices as other DataFrame. Examples -------- >>> df = pd.DataFrame([('bird', 389.0), ... ('bird', 24.0), ... ('mammal', 80.5), ... ('mammal', np.nan)], ... index=['falcon', 'parrot', 'lion', 'monkey'], ... columns=('class', 'max_speed')) >>> df class max_speed falcon bird 389.0 parrot bird 24.0 lion mammal 80.5 monkey mammal NaN When we reset the index, the old index is added as a column, and a new sequential index is used: >>> df.reset_index() index class max_speed 0 falcon bird 389.0 1 parrot bird 24.0 2 lion mammal 80.5 3 monkey mammal NaN We can use the `drop` parameter to avoid the old index being added as a column: >>> df.reset_index(drop=True) class max_speed 0 bird 389.0 1 bird 24.0 2 mammal 80.5 3 mammal NaN You can also use `reset_index` with `MultiIndex`. >>> index = pd.MultiIndex.from_tuples([('bird', 'falcon'), ... ('bird', 'parrot'), ... ('mammal', 'lion'), ... ('mammal', 'monkey')], ... names=['class', 'name']) >>> columns = pd.MultiIndex.from_tuples([('speed', 'max'), ... ('species', 'type')]) >>> df = pd.DataFrame([(389.0, 'fly'), ... ( 24.0, 'fly'), ... ( 80.5, 'run'), ... (np.nan, 'jump')], ... index=index, ... columns=columns) >>> df speed species max type class name bird falcon 389.0 fly parrot 24.0 fly mammal lion 80.5 run monkey NaN jump If the index has multiple levels, we can reset a subset of them: >>> df.reset_index(level='class') class speed species max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump If we are not dropping the index, by default, it is placed in the top level. We can place it in another level: >>> df.reset_index(level='class', col_level=1) speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump When the index is inserted under another level, we can specify under which one with the parameter `col_fill`: >>> df.reset_index(level='class', col_level=1, col_fill='species') species speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump If we specify a nonexistent level for `col_fill`, it is created: >>> df.reset_index(level='class', col_level=1, col_fill='genus') genus speed species class max type name falcon bird 389.0 fly parrot bird 24.0 fly lion mammal 80.5 run monkey mammal NaN jump \"\"\" inplace = validate_bool_kwarg ( inplace , \"inplace\" ) if inplace : new_obj = self else : new_obj = self . copy () def _maybe_casted_values ( index , labels = None ) : values = index . _values if not isinstance ( index , ( PeriodIndex , DatetimeIndex )) : if values . dtype == np . object_ : values = lib . maybe_convert_objects ( values ) # if we have the labels , extract the values with a mask if labels is not None : mask = labels == - 1 # we can have situations where the whole mask is - 1 , # meaning there is nothing found in labels , so make all nan 's if mask.size > 0 and mask.all(): dtype = index.dtype fill_value = na_value_for_dtype(dtype) values = construct_1d_arraylike_from_scalar( fill_value, len(mask), dtype ) else: values = values.take(labels) # TODO(https://github.com/pandas-dev/pandas/issues/24206) # Push this into maybe_upcast_putmask? # We can' t pass EAs there right now . Looks a bit # complicated . # So we unbox the ndarray_values , op , re - box . values_type = type ( values ) values_dtype = values . dtype if issubclass ( values_type , DatetimeLikeArray ) : values = values . _data # TODO : can we de - kludge yet ? if mask . any () : values , _ = maybe_upcast_putmask ( values , mask , np . nan ) if issubclass ( values_type , DatetimeLikeArray ) : values = values_type ( values , dtype = values_dtype ) return values new_index = ibase . default_index ( len ( new_obj )) if level is not None : if not isinstance ( level , ( tuple , list )) : level = [ level ] level = [ self.index._get_level_number(lev) for lev in level ] if len ( level ) < self . index . nlevels : new_index = self . index . droplevel ( level ) if not drop : to_insert : Iterable [ Tuple[Any, Optional[Any ] ]] if isinstance ( self . index , MultiIndex ) : names = [ (n if n is not None else f\"level_{i}\") for i, n in enumerate(self.index.names) ] to_insert = zip ( self . index . levels , self . index . codes ) else : default = \"index\" if \"index\" not in self else \"level_0\" names = [ default ] if self . index . name is None else [ self.index.name ] to_insert = (( self . index , None ),) multi_col = isinstance ( self . columns , MultiIndex ) for i , ( lev , lab ) in reversed ( list ( enumerate ( to_insert ))) : if not ( level is None or i in level ) : continue name = names [ i ] if multi_col : col_name = list ( name ) if isinstance ( name , tuple ) else [ name ] if col_fill is None : if len ( col_name ) not in ( 1 , self . columns . nlevels ) : raise ValueError ( \"col_fill=None is incompatible \" f \"with incomplete column name {name}\" ) col_fill = col_name [ 0 ] lev_num = self . columns . _get_level_number ( col_level ) name_lst = [ col_fill ] * lev_num + col_name missing = self . columns . nlevels - len ( name_lst ) name_lst += [ col_fill ] * missing name = tuple ( name_lst ) # to ndarray and maybe infer different dtype level_values = _maybe_casted_values ( lev , lab ) new_obj . insert ( 0 , name , level_values ) new_obj . index = new_index if not inplace : return new_obj return None","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#rfloordiv","text":"def rfloordiv ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Integer division of dataframe and other, element-wise (binary operator rfloordiv ). Equivalent to other // dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, floordiv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"rfloordiv"},{"location":"reference/hielen2/datalink_prova_df/#parameters_113","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_128","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_117","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_58","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_121","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#rmod","text":"def rmod ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Modulo of dataframe and other, element-wise (binary operator rmod ). Equivalent to other % dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, mod . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"rmod"},{"location":"reference/hielen2/datalink_prova_df/#parameters_114","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_129","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_118","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_59","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_122","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#rmul","text":"def rmul ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Multiplication of dataframe and other, element-wise (binary operator rmul ). Equivalent to other * dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, mul . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"rmul"},{"location":"reference/hielen2/datalink_prova_df/#parameters_115","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_130","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_119","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_60","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_123","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#rolling","text":"def rolling ( self , window , min_periods = None , center = False , win_type = None , on = None , axis = 0 , closed = None ) Provide rolling window calculations.","title":"rolling"},{"location":"reference/hielen2/datalink_prova_df/#parameters_116","text":"window : int, offset, or BaseIndexer subclass Size of the moving window. This is the number of observations used for calculating the statistic. Each window will be a fixed size. If its an offset then this will be the time period of each window . Each window will be a variable sized based on the observations included in the time - period . This is only valid for datetimelike indexes . If a BaseIndexer subclass is passed , calculates the window boundaries based on the defined `` get_window_bounds `` method . Additional rolling keyword arguments , namely `min_periods` , `center` , and `closed` will be passed to `get_window_bounds` . min_periods : int, default None Minimum number of observations in window required to have a value (otherwise result is NA). For a window that is specified by an offset, min_periods will default to 1. Otherwise, min_periods will default to the size of the window. center : bool, default False Set the labels at the center of the window. win_type : str, default None Provide a window type. If None , all points are evenly weighted. See the notes below for further information. on : str, optional For a DataFrame, a datetime-like column or MultiIndex level on which to calculate the rolling window, rather than the DataFrame's index. Provided integer column is ignored and excluded from result since an integer index is not used to calculate the rolling window. axis : int or str, default 0 closed : str, default None Make the interval closed on the 'right', 'left', 'both' or 'neither' endpoints. For offset-based windows, it defaults to 'right'. For fixed windows, defaults to 'both'. Remaining cases not implemented for fixed windows.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_131","text":"a Window or Rolling sub-classed for the particular operation","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_120","text":"expanding : Provides expanding transformations. ewm : Provides exponential weighted functions.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_61","text":"By default, the result is set to the right edge of the window. This can be changed to the center of the window by setting center=True . To learn more about the offsets & frequency strings, please see this link <https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases> __. The recognized win_types are: boxcar triang blackman hamming bartlett parzen bohman blackmanharris nuttall barthann kaiser (needs parameter: beta) gaussian (needs parameter: std) general_gaussian (needs parameters: power, width) slepian (needs parameter: width) exponential (needs parameter: tau), center is set to None. If win_type=None all points are evenly weighted. To learn more about different window types see scipy.signal window functions <https://docs.scipy.org/doc/scipy/reference/signal.html#window-functions> __. Certain window types require additional parameters to be passed. Please see the third example below on how to add the additional parameters.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_124","text":"df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]}) df B 0 0.0 1 1.0 2 2.0 3 NaN 4 4.0 Rolling sum with a window length of 2, using the 'triang' window type. df.rolling(2, win_type='triang').sum() B 0 NaN 1 0.5 2 1.5 3 NaN 4 NaN Rolling sum with a window length of 2, using the 'gaussian' window type (note how we need to specify std). df.rolling(2, win_type='gaussian').sum(std=3) B 0 NaN 1 0.986207 2 2.958621 3 NaN 4 NaN Rolling sum with a window length of 2, min_periods defaults to the window length. df.rolling(2).sum() B 0 NaN 1 1.0 2 3.0 3 NaN 4 NaN Same as above, but explicitly set the min_periods df.rolling(2, min_periods=1).sum() B 0 0.0 1 1.0 2 3.0 3 2.0 4 4.0 Same as above, but with forward-looking windows indexer = pd.api.indexers.FixedForwardWindowIndexer(window_size=2) df.rolling(window=indexer, min_periods=1).sum() B 0 1.0 1 3.0 2 2.0 3 4.0 4 4.0 A ragged (meaning not-a-regular frequency), time-indexed DataFrame df = pd.DataFrame({'B': [0, 1, 2, np.nan, 4]}, ... index = [pd.Timestamp('20130101 09:00:00'), ... pd.Timestamp('20130101 09:00:02'), ... pd.Timestamp('20130101 09:00:03'), ... pd.Timestamp('20130101 09:00:05'), ... pd.Timestamp('20130101 09:00:06')]) df B 2013-01-01 09:00:00 0.0 2013-01-01 09:00:02 1.0 2013-01-01 09:00:03 2.0 2013-01-01 09:00:05 NaN 2013-01-01 09:00:06 4.0 Contrasting to an integer rolling window, this will roll a variable length window corresponding to the time period. The default for min_periods is 1. df.rolling('2s').sum() B 2013-01-01 09:00:00 0.0 2013-01-01 09:00:02 1.0 2013-01-01 09:00:03 3.0 2013-01-01 09:00:05 NaN 2013-01-01 09:00:06 4.0 View Source @doc ( Rolling ) def rolling ( self , window , min_periods = None , center = False , win_type = None , on = None , axis = 0 , closed = None , ) : axis = self . _get_axis_number ( axis ) if win_type is not None : return Window ( self , window = window , min_periods = min_periods , center = center , win_type = win_type , on = on , axis = axis , closed = closed , ) return Rolling ( self , window = window , min_periods = min_periods , center = center , win_type = win_type , on = on , axis = axis , closed = closed , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#round","text":"def round ( self , decimals = 0 , * args , ** kwargs ) -> 'DataFrame' Round a DataFrame to a variable number of decimal places.","title":"round"},{"location":"reference/hielen2/datalink_prova_df/#parameters_117","text":"decimals : int, dict, Series Number of decimal places to round each column to. If an int is given, round each column to the same number of places. Otherwise dict and Series round to variable numbers of places. Column names should be in the keys if decimals is a dict-like, or in the index if decimals is a Series. Any columns not included in decimals will be left as is. Elements of decimals which are not columns of the input will be ignored. args Additional keywords have no effect but might be accepted for compatibility with numpy. *kwargs Additional keywords have no effect but might be accepted for compatibility with numpy.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_132","text":"DataFrame A DataFrame with the affected columns rounded to the specified number of decimal places.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_121","text":"numpy.around : Round a numpy array to the given number of decimals. Series.round : Round a Series to the given number of decimals.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_125","text":"df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)], ... columns=['dogs', 'cats']) df dogs cats 0 0.21 0.32 1 0.01 0.67 2 0.66 0.03 3 0.21 0.18 By providing an integer each column is rounded to the same number of decimal places df.round(1) dogs cats 0 0.2 0.3 1 0.0 0.7 2 0.7 0.0 3 0.2 0.2 With a dict, the number of places for specific columns can be specified with the column names as key and the number of decimal places as value df.round({'dogs': 1, 'cats': 0}) dogs cats 0 0.2 0.0 1 0.0 1.0 2 0.7 0.0 3 0.2 0.0 Using a Series, the number of places for specific columns can be specified with the column names as index and the number of decimal places as value decimals = pd.Series([0, 1], index=['cats', 'dogs']) df.round(decimals) dogs cats 0 0.2 0.0 1 0.0 1.0 2 0.7 0.0 3 0.2 0.0 View Source def round ( self , decimals = 0 , * args , ** kwargs ) -> \"DataFrame\" : \"\"\" Round a DataFrame to a variable number of decimal places. Parameters ---------- decimals : int, dict, Series Number of decimal places to round each column to. If an int is given, round each column to the same number of places. Otherwise dict and Series round to variable numbers of places. Column names should be in the keys if `decimals` is a dict-like, or in the index if `decimals` is a Series. Any columns not included in `decimals` will be left as is. Elements of `decimals` which are not columns of the input will be ignored. *args Additional keywords have no effect but might be accepted for compatibility with numpy. **kwargs Additional keywords have no effect but might be accepted for compatibility with numpy. Returns ------- DataFrame A DataFrame with the affected columns rounded to the specified number of decimal places. See Also -------- numpy.around : Round a numpy array to the given number of decimals. Series.round : Round a Series to the given number of decimals. Examples -------- >>> df = pd.DataFrame([(.21, .32), (.01, .67), (.66, .03), (.21, .18)], ... columns=['dogs', 'cats']) >>> df dogs cats 0 0.21 0.32 1 0.01 0.67 2 0.66 0.03 3 0.21 0.18 By providing an integer each column is rounded to the same number of decimal places >>> df.round(1) dogs cats 0 0.2 0.3 1 0.0 0.7 2 0.7 0.0 3 0.2 0.2 With a dict, the number of places for specific columns can be specified with the column names as key and the number of decimal places as value >>> df.round({'dogs': 1, 'cats': 0}) dogs cats 0 0.2 0.0 1 0.0 1.0 2 0.7 0.0 3 0.2 0.0 Using a Series, the number of places for specific columns can be specified with the column names as index and the number of decimal places as value >>> decimals = pd.Series([0, 1], index=['cats', 'dogs']) >>> df.round(decimals) dogs cats 0 0.2 0.0 1 0.0 1.0 2 0.7 0.0 3 0.2 0.0 \"\"\" from pandas . core . reshape . concat import concat def _dict_round ( df , decimals ): for col , vals in df . items (): try : yield _series_round ( vals , decimals [ col ]) except KeyError : yield vals def _series_round ( s , decimals ): if is_integer_dtype ( s ) or is_float_dtype ( s ): return s . round ( decimals ) return s nv . validate_round ( args , kwargs ) if isinstance ( decimals , ( dict , Series )): if isinstance ( decimals , Series ): if not decimals . index . is_unique : raise ValueError ( \"Index of decimals must be unique\" ) new_cols = list ( _dict_round ( self , decimals )) elif is_integer ( decimals ): # Dispatch to Series.round new_cols = [ _series_round ( v , decimals ) for _ , v in self . items ()] else : raise TypeError ( \"decimals must be an integer, a dict-like or a Series\" ) if len ( new_cols ) > 0 : return self . _constructor ( concat ( new_cols , axis = 1 ), index = self . index , columns = self . columns ) else : return self","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#rpow","text":"def rpow ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Exponential power of dataframe and other, element-wise (binary operator rpow ). Equivalent to other ** dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, pow . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"rpow"},{"location":"reference/hielen2/datalink_prova_df/#parameters_118","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_133","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_122","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_62","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_126","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#rsub","text":"def rsub ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Subtraction of dataframe and other, element-wise (binary operator rsub ). Equivalent to other - dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, sub . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"rsub"},{"location":"reference/hielen2/datalink_prova_df/#parameters_119","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_134","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_123","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_63","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_127","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#rtruediv","text":"def rtruediv ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Floating division of dataframe and other, element-wise (binary operator rtruediv ). Equivalent to other / dataframe , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, truediv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"rtruediv"},{"location":"reference/hielen2/datalink_prova_df/#parameters_120","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_135","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_124","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_64","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_128","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#sample","text":"def sample ( self : ~ FrameOrSeries , n = None , frac = None , replace = False , weights = None , random_state = None , axis = None ) -> ~ FrameOrSeries Return a random sample of items from an axis of object. You can use random_state for reproducibility.","title":"sample"},{"location":"reference/hielen2/datalink_prova_df/#parameters_121","text":"n : int, optional Number of items from axis to return. Cannot be used with frac . Default = 1 if frac = None. frac : float, optional Fraction of axis items to return. Cannot be used with n . replace : bool, default False Allow or disallow sampling of the same row more than once. weights : str or ndarray-like, optional Default 'None' results in equal probability weighting. If passed a Series, will align with target object on index. Index values in weights not found in sampled object will be ignored and index values in sampled object not in weights will be assigned weights of zero. If called on a DataFrame, will accept the name of a column when axis = 0. Unless weights are a Series, weights must be same length as axis being sampled. If weights do not sum to 1, they will be normalized to sum to 1. Missing values in the weights column will be treated as zero. Infinite values not allowed. random_state : int, array-like, BitGenerator, np.random.RandomState, optional If int, array-like, or BitGenerator (NumPy>=1.17), seed for random number generator If np.random.RandomState, use as numpy RandomState object. .. versionchanged :: 1.1.0 array-like and BitGenerator (for NumPy>=1.17) object now passed to np.random.RandomState() as seed axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None Axis to sample. Accepts axis number or name. Default is stat axis for given data type (0 for Series and DataFrames).","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_136","text":"Series or DataFrame A new object of same type as caller containing n items randomly sampled from the caller object.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_125","text":"DataFrameGroupBy.sample: Generates random samples from each group of a DataFrame object. SeriesGroupBy.sample: Generates random samples from each group of a Series object. numpy.random.choice: Generates a random sample from a given 1-D numpy array.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_65","text":"If frac > 1, replacement should be set to True .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_129","text":"df = pd.DataFrame({'num_legs': [2, 4, 8, 0], ... 'num_wings': [2, 0, 0, 0], ... 'num_specimen_seen': [10, 2, 1, 8]}, ... index=['falcon', 'dog', 'spider', 'fish']) df num_legs num_wings num_specimen_seen falcon 2 2 10 dog 4 0 2 spider 8 0 1 fish 0 0 8 Extract 3 random elements from the Series df['num_legs'] : Note that we use random_state to ensure the reproducibility of the examples. df['num_legs'].sample(n=3, random_state=1) fish 0 spider 8 falcon 2 Name: num_legs, dtype: int64 A random 50% sample of the DataFrame with replacement: df.sample(frac=0.5, replace=True, random_state=1) num_legs num_wings num_specimen_seen dog 4 0 2 fish 0 0 8 An upsample sample of the DataFrame with replacement: Note that replace parameter has to be True for frac parameter > 1. df.sample(frac=2, replace=True, random_state=1) num_legs num_wings num_specimen_seen dog 4 0 2 fish 0 0 8 falcon 2 2 10 falcon 2 2 10 fish 0 0 8 dog 4 0 2 fish 0 0 8 dog 4 0 2 Using a DataFrame column as weights. Rows with larger value in the num_specimen_seen column are more likely to be sampled. df.sample(n=2, weights='num_specimen_seen', random_state=1) num_legs num_wings num_specimen_seen falcon 2 2 10 fish 0 0 8 View Source def sample ( self : FrameOrSeries , n = None , frac = None , replace = False , weights = None , random_state = None , axis = None , ) -> FrameOrSeries : \"\"\" Return a random sample of items from an axis of object. You can use `random_state` for reproducibility. Parameters ---------- n : int, optional Number of items from axis to return. Cannot be used with `frac`. Default = 1 if `frac` = None. frac : float, optional Fraction of axis items to return. Cannot be used with `n`. replace : bool, default False Allow or disallow sampling of the same row more than once. weights : str or ndarray-like, optional Default 'None' results in equal probability weighting. If passed a Series, will align with target object on index. Index values in weights not found in sampled object will be ignored and index values in sampled object not in weights will be assigned weights of zero. If called on a DataFrame, will accept the name of a column when axis = 0. Unless weights are a Series, weights must be same length as axis being sampled. If weights do not sum to 1, they will be normalized to sum to 1. Missing values in the weights column will be treated as zero. Infinite values not allowed. random_state : int, array-like, BitGenerator, np.random.RandomState, optional If int, array-like, or BitGenerator (NumPy>=1.17), seed for random number generator If np.random.RandomState, use as numpy RandomState object. .. versionchanged:: 1.1.0 array-like and BitGenerator (for NumPy>=1.17) object now passed to np.random.RandomState() as seed axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default None Axis to sample. Accepts axis number or name. Default is stat axis for given data type (0 for Series and DataFrames). Returns ------- Series or DataFrame A new object of same type as caller containing `n` items randomly sampled from the caller object. See Also -------- DataFrameGroupBy.sample: Generates random samples from each group of a DataFrame object. SeriesGroupBy.sample: Generates random samples from each group of a Series object. numpy.random.choice: Generates a random sample from a given 1-D numpy array. Notes ----- If `frac` > 1, `replacement` should be set to `True`. Examples -------- >>> df = pd.DataFrame({'num_legs': [2, 4, 8, 0], ... 'num_wings': [2, 0, 0, 0], ... 'num_specimen_seen': [10, 2, 1, 8]}, ... index=['falcon', 'dog', 'spider', 'fish']) >>> df num_legs num_wings num_specimen_seen falcon 2 2 10 dog 4 0 2 spider 8 0 1 fish 0 0 8 Extract 3 random elements from the ``Series`` ``df['num_legs']``: Note that we use `random_state` to ensure the reproducibility of the examples. >>> df['num_legs'].sample(n=3, random_state=1) fish 0 spider 8 falcon 2 Name: num_legs, dtype: int64 A random 50% sample of the ``DataFrame`` with replacement: >>> df.sample(frac=0.5, replace=True, random_state=1) num_legs num_wings num_specimen_seen dog 4 0 2 fish 0 0 8 An upsample sample of the ``DataFrame`` with replacement: Note that `replace` parameter has to be `True` for `frac` parameter > 1. >>> df.sample(frac=2, replace=True, random_state=1) num_legs num_wings num_specimen_seen dog 4 0 2 fish 0 0 8 falcon 2 2 10 falcon 2 2 10 fish 0 0 8 dog 4 0 2 fish 0 0 8 dog 4 0 2 Using a DataFrame column as weights. Rows with larger value in the `num_specimen_seen` column are more likely to be sampled. >>> df.sample(n=2, weights='num_specimen_seen', random_state=1) num_legs num_wings num_specimen_seen falcon 2 2 10 fish 0 0 8 \"\"\" if axis is None : axis = self . _stat_axis_number axis = self . _get_axis_number ( axis ) axis_length = self . shape [ axis ] # Process random_state argument rs = com . random_state ( random_state ) # Check weights for compliance if weights is not None : # If a series, align with frame if isinstance ( weights , ABCSeries ): weights = weights . reindex ( self . axes [ axis ]) # Strings acceptable if a dataframe and axis = 0 if isinstance ( weights , str ): if isinstance ( self , ABCDataFrame ): if axis == 0 : try : weights = self [ weights ] except KeyError as err : raise KeyError ( \"String passed to weights not a valid column\" ) from err else : raise ValueError ( \"Strings can only be passed to \" \"weights when sampling from rows on \" \"a DataFrame\" ) else : raise ValueError ( \"Strings cannot be passed as weights \" \"when sampling from a Series.\" ) weights = pd . Series ( weights , dtype = \"float64\" ) if len ( weights ) != axis_length : raise ValueError ( \"Weights and axis to be sampled must be of same length\" ) if ( weights == np . inf ). any () or ( weights == - np . inf ). any (): raise ValueError ( \"weight vector may not include `inf` values\" ) if ( weights < 0 ). any (): raise ValueError ( \"weight vector many not include negative values\" ) # If has nan, set to zero. weights = weights . fillna ( 0 ) # Renormalize if don't sum to 1 if weights . sum () != 1 : if weights . sum () != 0 : weights = weights / weights . sum () else : raise ValueError ( \"Invalid weights: weights sum to zero\" ) weights = weights . _values # If no frac or n, default to n=1. if n is None and frac is None : n = 1 elif frac is not None and frac > 1 and not replace : raise ValueError ( \"Replace has to be set to `True` when \" \"upsampling the population `frac` > 1.\" ) elif n is not None and frac is None and n % 1 != 0 : raise ValueError ( \"Only integers accepted as `n` values\" ) elif n is None and frac is not None : n = int ( round ( frac * axis_length )) elif n is not None and frac is not None : raise ValueError ( \"Please enter a value for `frac` OR `n`, not both\" ) # Check for negative sizes if n < 0 : raise ValueError ( \"A negative number of rows requested. Please provide positive value.\" ) locs = rs . choice ( axis_length , size = n , replace = replace , p = weights ) return self . take ( locs , axis = axis )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#save","text":"def save ( self ) View Source def save ( self ): try : self . lock . acquire () try : self . to_csv ( self . csv , header = None , sep = \";\" ) self . md5 = hashfile ( self . csv ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e","title":"save"},{"location":"reference/hielen2/datalink_prova_df/#select_dtypes","text":"def select_dtypes ( self , include = None , exclude = None ) -> 'DataFrame' Return a subset of the DataFrame's columns based on the column dtypes.","title":"select_dtypes"},{"location":"reference/hielen2/datalink_prova_df/#parameters_122","text":"include, exclude : scalar or list-like A selection of dtypes or strings to be included/excluded. At least one of these parameters must be supplied.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_137","text":"DataFrame The subset of the frame including the dtypes in include and excluding the dtypes in exclude .","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_14","text":"ValueError * If both of include and exclude are empty * If include and exclude have overlapping elements * If any kind of string dtype is passed in.","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_126","text":"DataFrame.dtypes: Return Series with the data type of each column.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_66","text":"To select all numeric types, use np.number or 'number' To select strings you must use the object dtype, but note that this will return all object dtype columns See the numpy dtype hierarchy <https://numpy.org/doc/stable/reference/arrays.scalars.html> __ To select datetimes, use np.datetime64 , 'datetime' or 'datetime64' To select timedeltas, use np.timedelta64 , 'timedelta' or 'timedelta64' To select Pandas categorical dtypes, use 'category' To select Pandas datetimetz dtypes, use 'datetimetz' (new in 0.20.0) or 'datetime64[ns, tz]'","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_130","text":"df = pd.DataFrame({'a': [1, 2] * 3, ... 'b': [True, False] * 3, ... 'c': [1.0, 2.0] * 3}) df a b c 0 1 True 1.0 1 2 False 2.0 2 1 True 1.0 3 2 False 2.0 4 1 True 1.0 5 2 False 2.0 df.select_dtypes(include='bool') b 0 True 1 False 2 True 3 False 4 True 5 False df.select_dtypes(include=['float64']) c 0 1.0 1 2.0 2 1.0 3 2.0 4 1.0 5 2.0 df.select_dtypes(exclude=['int64']) b c 0 True 1.0 1 False 2.0 2 True 1.0 3 False 2.0 4 True 1.0 5 False 2.0 View Source def select_dtypes ( self , include = None , exclude = None ) -> \"DataFrame\" : \"\"\" Return a subset of the DataFrame's columns based on the column dtypes. Parameters ---------- include, exclude : scalar or list-like A selection of dtypes or strings to be included/excluded. At least one of these parameters must be supplied. Returns ------- DataFrame The subset of the frame including the dtypes in ``include`` and excluding the dtypes in ``exclude``. Raises ------ ValueError * If both of ``include`` and ``exclude`` are empty * If ``include`` and ``exclude`` have overlapping elements * If any kind of string dtype is passed in. See Also -------- DataFrame.dtypes: Return Series with the data type of each column. Notes ----- * To select all *numeric* types, use ``np.number`` or ``'number'`` * To select strings you must use the ``object`` dtype, but note that this will return *all* object dtype columns * See the `numpy dtype hierarchy <https://numpy.org/doc/stable/reference/arrays.scalars.html>`__ * To select datetimes, use ``np.datetime64``, ``'datetime'`` or ``'datetime64'`` * To select timedeltas, use ``np.timedelta64``, ``'timedelta'`` or ``'timedelta64'`` * To select Pandas categorical dtypes, use ``'category'`` * To select Pandas datetimetz dtypes, use ``'datetimetz'`` (new in 0.20.0) or ``'datetime64[ns, tz]'`` Examples -------- >>> df = pd.DataFrame({'a': [1, 2] * 3, ... 'b': [True, False] * 3, ... 'c': [1.0, 2.0] * 3}) >>> df a b c 0 1 True 1.0 1 2 False 2.0 2 1 True 1.0 3 2 False 2.0 4 1 True 1.0 5 2 False 2.0 >>> df.select_dtypes(include='bool') b 0 True 1 False 2 True 3 False 4 True 5 False >>> df.select_dtypes(include=['float64']) c 0 1.0 1 2.0 2 1.0 3 2.0 4 1.0 5 2.0 >>> df.select_dtypes(exclude=['int64']) b c 0 True 1.0 1 False 2.0 2 True 1.0 3 False 2.0 4 True 1.0 5 False 2.0 \"\"\" if not is_list_like ( include ): include = ( include ,) if include is not None else () if not is_list_like ( exclude ): exclude = ( exclude ,) if exclude is not None else () selection = ( frozenset ( include ), frozenset ( exclude )) if not any ( selection ): raise ValueError ( \"at least one of include or exclude must be nonempty\" ) # convert the myriad valid dtypes object to a single representation include = frozenset ( infer_dtype_from_object ( x ) for x in include ) exclude = frozenset ( infer_dtype_from_object ( x ) for x in exclude ) for dtypes in ( include , exclude ): invalidate_string_dtypes ( dtypes ) # can't both include AND exclude! if not include . isdisjoint ( exclude ): raise ValueError ( f \"include and exclude overlap on {(include & exclude)}\" ) # We raise when both include and exclude are empty # Hence, we can just shrink the columns we want to keep keep_these = np . full ( self . shape [ 1 ], True ) def extract_unique_dtypes_from_dtypes_set ( dtypes_set : FrozenSet [ Dtype ], unique_dtypes : np . ndarray ) -> List [ Dtype ]: extracted_dtypes = [ unique_dtype for unique_dtype in unique_dtypes if issubclass ( unique_dtype . type , tuple ( dtypes_set )) # type: ignore ] return extracted_dtypes unique_dtypes = self . dtypes . unique () if include : included_dtypes = extract_unique_dtypes_from_dtypes_set ( include , unique_dtypes ) keep_these &= self . dtypes . isin ( included_dtypes ) if exclude : excluded_dtypes = extract_unique_dtypes_from_dtypes_set ( exclude , unique_dtypes ) keep_these &= ~ self . dtypes . isin ( excluded_dtypes ) return self . iloc [:, keep_these . values ]","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#sem","text":"def sem ( self , axis = None , skipna = None , level = None , ddof = 1 , numeric_only = None , ** kwargs ) Return unbiased standard error of the mean over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument","title":"sem"},{"location":"reference/hielen2/datalink_prova_df/#parameters_123","text":"axis : {index (0), columns (1)} skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. ddof : int, default 1 Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_138","text":"Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr ) @Appender ( _num_ddof_doc ) def stat_func ( self , axis = None , skipna = None , level = None , ddof = 1 , numeric_only = None , ** kwargs ) : nv . validate_stat_ddof_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna , ddof = ddof ) return self . _reduce ( func , name , axis = axis , numeric_only = numeric_only , skipna = skipna , ddof = ddof )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#set_axis","text":"def set_axis ( self , labels , axis : Union [ str , int ] = 0 , inplace : bool = False ) Assign desired index to given axis. Indexes for column or row labels can be changed by assigning a list-like or Index.","title":"set_axis"},{"location":"reference/hielen2/datalink_prova_df/#parameters_124","text":"labels : list-like, Index The values for the new index. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to update. The value 0 identifies the rows, and 1 identifies the columns. inplace : bool, default False Whether to return a new DataFrame instance.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_139","text":"renamed : DataFrame or None An object of type DataFrame if inplace=False, None otherwise.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_127","text":"DataFrame.rename_axis : Alter the name of the index or columns. Examples -------- >>> df = pd . DataFrame ( { \"A\" : [ 1 , 2 , 3 ], \"B\" : [ 4 , 5 , 6 ] } ) Change the row labels . >>> df . set_axis ([ 'a' , 'b' , 'c' ], axis = 'index' ) A B a 1 4 b 2 5 c 3 6 Change the column labels . >>> df . set_axis ([ 'I' , 'II' ], axis = 'columns' ) I II 0 1 4 1 2 5 2 3 6 Now , update the labels inplace . >>> df . set_axis ([ 'i' , 'ii' ], axis = 'columns' , inplace = True ) >>> df i ii 0 1 4 1 2 5 2 3 6 View Source @Appender ( \"\"\" Examples -------- >>> df = pd.DataFrame({\" A \": [1, 2, 3], \" B \": [4, 5, 6]}) Change the row labels. >>> df.set_axis(['a', 'b', 'c'], axis='index') A B a 1 4 b 2 5 c 3 6 Change the column labels. >>> df.set_axis(['I', 'II'], axis='columns') I II 0 1 4 1 2 5 2 3 6 Now, update the labels inplace. >>> df.set_axis(['i', 'ii'], axis='columns', inplace=True) >>> df i ii 0 1 4 1 2 5 2 3 6 \"\"\" ) @Substitution ( ** _shared_doc_kwargs , extended_summary_sub = \" column or\" , axis_description_sub = \", and 1 identifies the columns\" , see_also_sub = \" or columns\" , ) @Appender ( NDFrame . set_axis . __doc__ ) def set_axis ( self , labels , axis : Axis = 0 , inplace : bool = False ) : return super (). set_axis ( labels , axis = axis , inplace = inplace )","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#set_index","text":"def set_index ( self , keys , drop = True , append = False , inplace = False , verify_integrity = False ) Set the DataFrame index using existing columns. Set the DataFrame index (row labels) using one or more existing columns or arrays (of the correct length). The index can replace the existing index or expand on it.","title":"set_index"},{"location":"reference/hielen2/datalink_prova_df/#parameters_125","text":"keys : label or array-like or list of labels/arrays This parameter can be either a single column key, a single array of the same length as the calling DataFrame, or a list containing an arbitrary combination of column keys and arrays. Here, \"array\" encompasses :class: Series , :class: Index , np.ndarray , and instances of :class: ~collections.abc.Iterator . drop : bool, default True Delete columns to be used as the new index. append : bool, default False Whether to append columns to existing index. inplace : bool, default False Modify the DataFrame in place (do not create a new object). verify_integrity : bool, default False Check the new index for duplicates. Otherwise defer the check until necessary. Setting to False will improve the performance of this method.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_140","text":"DataFrame Changed row labels.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_128","text":"DataFrame.reset_index : Opposite of set_index. DataFrame.reindex : Change to new indices or expand indices. DataFrame.reindex_like : Change to same indices as other DataFrame.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_131","text":"df = pd.DataFrame({'month': [1, 4, 7, 10], ... 'year': [2012, 2014, 2013, 2014], ... 'sale': [55, 40, 84, 31]}) df month year sale 0 1 2012 55 1 4 2014 40 2 7 2013 84 3 10 2014 31 Set the index to become the 'month' column: df.set_index('month') year sale month 1 2012 55 4 2014 40 7 2013 84 10 2014 31 Create a MultiIndex using columns 'year' and 'month': df.set_index(['year', 'month']) sale year month 2012 1 55 2014 4 40 2013 7 84 2014 10 31 Create a MultiIndex using an Index and a column: df.set_index([pd.Index([1, 2, 3, 4]), 'year']) month sale year 1 2012 1 55 2 2014 4 40 3 2013 7 84 4 2014 10 31 Create a MultiIndex using two Series: s = pd.Series([1, 2, 3, 4]) df.set_index([s, s**2]) month year sale 1 1 1 2012 55 2 4 4 2014 40 3 9 7 2013 84 4 16 10 2014 31 View Source def set_index ( self , keys , drop = True , append = False , inplace = False , verify_integrity = False ) : \"\"\" Set the DataFrame index using existing columns. Set the DataFrame index (row labels) using one or more existing columns or arrays (of the correct length). The index can replace the existing index or expand on it. Parameters ---------- keys : label or array-like or list of labels/arrays This parameter can be either a single column key, a single array of the same length as the calling DataFrame, or a list containing an arbitrary combination of column keys and arrays. Here, \" array \" encompasses :class:`Series`, :class:`Index`, ``np.ndarray``, and instances of :class:`~collections.abc.Iterator`. drop : bool, default True Delete columns to be used as the new index. append : bool, default False Whether to append columns to existing index. inplace : bool, default False Modify the DataFrame in place (do not create a new object). verify_integrity : bool, default False Check the new index for duplicates. Otherwise defer the check until necessary. Setting to False will improve the performance of this method. Returns ------- DataFrame Changed row labels. See Also -------- DataFrame.reset_index : Opposite of set_index. DataFrame.reindex : Change to new indices or expand indices. DataFrame.reindex_like : Change to same indices as other DataFrame. Examples -------- >>> df = pd.DataFrame({'month': [1, 4, 7, 10], ... 'year': [2012, 2014, 2013, 2014], ... 'sale': [55, 40, 84, 31]}) >>> df month year sale 0 1 2012 55 1 4 2014 40 2 7 2013 84 3 10 2014 31 Set the index to become the 'month' column: >>> df.set_index('month') year sale month 1 2012 55 4 2014 40 7 2013 84 10 2014 31 Create a MultiIndex using columns 'year' and 'month': >>> df.set_index(['year', 'month']) sale year month 2012 1 55 2014 4 40 2013 7 84 2014 10 31 Create a MultiIndex using an Index and a column: >>> df.set_index([pd.Index([1, 2, 3, 4]), 'year']) month sale year 1 2012 1 55 2 2014 4 40 3 2013 7 84 4 2014 10 31 Create a MultiIndex using two Series: >>> s = pd.Series([1, 2, 3, 4]) >>> df.set_index([s, s**2]) month year sale 1 1 1 2012 55 2 4 4 2014 40 3 9 7 2013 84 4 16 10 2014 31 \"\"\" inplace = validate_bool_kwarg ( inplace , \"inplace\" ) if not isinstance ( keys , list ) : keys = [ keys ] err_msg = ( 'The parameter \"keys\" may be a column key, one-dimensional ' \"array, or a list containing only valid column keys and \" \"one-dimensional arrays.\" ) missing : List [ Label ] = [] for col in keys : if isinstance ( col , ( Index , Series , np . ndarray , list , abc . Iterator )) : # arrays are fine as long as they are one - dimensional # iterators get converted to list below if getattr ( col , \"ndim\" , 1 ) != 1 : raise ValueError ( err_msg ) else : # everything else gets tried as a key ; see GH 24969 try : found = col in self . columns except TypeError as err : raise TypeError ( f \"{err_msg}. Received column of type {type(col)}\" ) from err else : if not found : missing . append ( col ) if missing : raise KeyError ( f \"None of {missing} are in the columns\" ) if inplace : frame = self else : frame = self . copy () arrays = [] names = [] if append : names = list ( self . index . names ) if isinstance ( self . index , MultiIndex ) : for i in range ( self . index . nlevels ) : arrays . append ( self . index . _get_level_values ( i )) else : arrays . append ( self . index ) to_remove : List [ Label ] = [] for col in keys : if isinstance ( col , MultiIndex ) : for n in range ( col . nlevels ) : arrays . append ( col . _get_level_values ( n )) names . extend ( col . names ) elif isinstance ( col , ( Index , Series )) : # if Index then not MultiIndex ( treated above ) arrays . append ( col ) names . append ( col . name ) elif isinstance ( col , ( list , np . ndarray )) : arrays . append ( col ) names . append ( None ) elif isinstance ( col , abc . Iterator ) : arrays . append ( list ( col )) names . append ( None ) # from here , col can only be a column label else : arrays . append ( frame [ col ] . _values ) names . append ( col ) if drop : to_remove . append ( col ) if len ( arrays [ -1 ] ) != len ( self ) : # check newest element against length of calling frame , since # ensure_index_from_sequences would not raise for append = False . raise ValueError ( f \"Length mismatch: Expected {len(self)} rows, \" f \"received array of length {len(arrays[-1])}\" ) index = ensure_index_from_sequences ( arrays , names ) if verify_integrity and not index . is_unique : duplicates = index [ index.duplicated() ] . unique () raise ValueError ( f \"Index has duplicate keys: {duplicates}\" ) # use set to handle duplicate column names gracefully in case of drop for c in set ( to_remove ) : del frame [ c ] # clear up memory usage index . _cleanup () frame . index = index if not inplace : return frame","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#shift","text":"def shift ( self , periods = 1 , freq = None , axis = 0 , fill_value = None ) -> 'DataFrame' Shift index by desired number of periods with an optional time freq . When freq is not passed, shift the index without realigning the data. If freq is passed (in this case, the index must be date or datetime, or it will raise a NotImplementedError ), the index will be increased using the periods and the freq . freq can be inferred when specified as \"infer\" as long as either freq or inferred_freq attribute is set in the index.","title":"shift"},{"location":"reference/hielen2/datalink_prova_df/#parameters_126","text":"periods : int Number of periods to shift. Can be positive or negative. freq : DateOffset, tseries.offsets, timedelta, or str, optional Offset to use from the tseries module or time rule (e.g. 'EOM'). If freq is specified then the index values are shifted but the data is not realigned. That is, use freq if you would like to extend the index when shifting and preserve the original data. If freq is specified as \"infer\" then it will be inferred from the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown axis : {0 or 'index', 1 or 'columns', None}, default None Shift direction. fill_value : object, optional The scalar value to use for newly introduced missing values. the default depends on the dtype of self . For numeric data, np.nan is used. For datetime, timedelta, or period data, etc. :attr: NaT is used. For extension dtypes, self.dtype.na_value is used. .. versionchanged :: 1.1.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_141","text":"DataFrame Copy of input object, shifted.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_129","text":"Index.shift : Shift values of Index. DatetimeIndex.shift : Shift values of DatetimeIndex. PeriodIndex.shift : Shift values of PeriodIndex. tshift : Shift the time index, using the index's frequency if available.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_132","text":"df = pd.DataFrame({\"Col1\": [10, 20, 15, 30, 45], ... \"Col2\": [13, 23, 18, 33, 48], ... \"Col3\": [17, 27, 22, 37, 52]}, ... index=pd.date_range(\"2020-01-01\", \"2020-01-05\")) df Col1 Col2 Col3 2020-01-01 10 13 17 2020-01-02 20 23 27 2020-01-03 15 18 22 2020-01-04 30 33 37 2020-01-05 45 48 52 df.shift(periods=3) Col1 Col2 Col3 2020-01-01 NaN NaN NaN 2020-01-02 NaN NaN NaN 2020-01-03 NaN NaN NaN 2020-01-04 10.0 13.0 17.0 2020-01-05 20.0 23.0 27.0 df.shift(periods=1, axis=\"columns\") Col1 Col2 Col3 2020-01-01 NaN 10.0 13.0 2020-01-02 NaN 20.0 23.0 2020-01-03 NaN 15.0 18.0 2020-01-04 NaN 30.0 33.0 2020-01-05 NaN 45.0 48.0 df.shift(periods=3, fill_value=0) Col1 Col2 Col3 2020-01-01 0 0 0 2020-01-02 0 0 0 2020-01-03 0 0 0 2020-01-04 10 13 17 2020-01-05 20 23 27 df.shift(periods=3, freq=\"D\") Col1 Col2 Col3 2020-01-04 10 13 17 2020-01-05 20 23 27 2020-01-06 15 18 22 2020-01-07 30 33 37 2020-01-08 45 48 52 df.shift(periods=3, freq=\"infer\") Col1 Col2 Col3 2020-01-04 10 13 17 2020-01-05 20 23 27 2020-01-06 15 18 22 2020-01-07 30 33 37 2020-01-08 45 48 52 View Source @doc ( NDFrame . shift , klass = _shared_doc_kwargs [ \"klass\" ] ) def shift ( self , periods = 1 , freq = None , axis = 0 , fill_value = None ) -> \"DataFrame\" : return super (). shift ( periods = periods , freq = freq , axis = axis , fill_value = fill_value )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#skew","text":"def skew ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) Return unbiased skew over requested axis. Normalized by N-1.","title":"skew"},{"location":"reference/hielen2/datalink_prova_df/#parameters_127","text":"axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. **kwargs Additional keyword arguments to be passed to the function.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_142","text":"Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = \"\" , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , ** kwargs ) : if name == \"median\" : nv . validate_median ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#slice_shift","text":"def slice_shift ( self : ~ FrameOrSeries , periods : int = 1 , axis = 0 ) -> ~ FrameOrSeries Equivalent to shift without copying data. The shifted data will not include the dropped periods and the shifted axis will be smaller than the original.","title":"slice_shift"},{"location":"reference/hielen2/datalink_prova_df/#parameters_128","text":"periods : int Number of periods to move, can be positive or negative.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_143","text":"shifted : same type as caller","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#notes_67","text":"While the slice_shift is faster than shift , you may pay for it later during alignment. View Source def slice_shift ( self : FrameOrSeries , periods : int = 1 , axis = 0 ) -> FrameOrSeries : \"\"\" Equivalent to `shift` without copying data. The shifted data will not include the dropped periods and the shifted axis will be smaller than the original. Parameters ---------- periods : int Number of periods to move, can be positive or negative. Returns ------- shifted : same type as caller Notes ----- While the `slice_shift` is faster than `shift`, you may pay for it later during alignment. \"\"\" if periods == 0 : return self if periods > 0 : vslicer = slice ( None , - periods ) islicer = slice ( periods , None ) else : vslicer = slice ( - periods , None ) islicer = slice ( None , periods ) new_obj = self . _slice ( vslicer , axis = axis ) shifted_axis = self . _get_axis ( axis )[ islicer ] new_obj . set_axis ( shifted_axis , axis = axis , inplace = True ) return new_obj . __finalize__ ( self , method = \"slice_shift\" )","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#sort_index","text":"def sort_index ( self , axis = 0 , level = None , ascending : bool = True , inplace : bool = False , kind : str = 'quicksort' , na_position : str = 'last' , sort_remaining : bool = True , ignore_index : bool = False , key : Union [ Callable [[ ForwardRef ( 'Index' )], Union [ ForwardRef ( 'Index' ), ~ AnyArrayLike ]], NoneType ] = None ) Sort object by labels (along an axis). Returns a new DataFrame sorted by label if inplace argument is False , otherwise updates the original DataFrame and returns None.","title":"sort_index"},{"location":"reference/hielen2/datalink_prova_df/#parameters_129","text":"axis : {0 or 'index', 1 or 'columns'}, default 0 The axis along which to sort. The value 0 identifies the rows, and 1 identifies the columns. level : int or level name or list of ints or list of level names If not None, sort on values in specified index level(s). ascending : bool or list of bools, default True Sort ascending vs. descending. When the index is a MultiIndex the sort direction can be controlled for each level individually. inplace : bool, default False If True, perform operation in-place. kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort' Choice of sorting algorithm. See also ndarray.np.sort for more information. mergesort is the only stable algorithm. For DataFrames, this option is only applied when sorting on a single column or label. na_position : {'first', 'last'}, default 'last' Puts NaNs at the beginning if first ; last puts NaNs at the end. Not implemented for MultiIndex. sort_remaining : bool, default True If True and sorting by level and index is multilevel, sort by other levels too (in order) after sorting by specified level. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. .. versionadded :: 1.0.0 key : callable, optional If not None, apply the key function to the index values before sorting. This is similar to the key argument in the builtin :meth: sorted function, with the notable difference that this key function should be vectorized . It should expect an Index and return an Index of the same shape. For MultiIndex inputs, the key is applied per level . .. versionadded :: 1.1.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_144","text":"DataFrame The original DataFrame sorted by the labels.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_130","text":"Series.sort_index : Sort Series by the index. DataFrame.sort_values : Sort DataFrame by the value. Series.sort_values : Sort Series by the value.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_133","text":"df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150], ... columns=['A']) df.sort_index() A 1 4 29 2 100 1 150 5 234 3 By default, it sorts in ascending order, to sort in descending order, use ascending=False df.sort_index(ascending=False) A 234 3 150 5 100 1 29 2 1 4 A key function can be specified which is applied to the index before sorting. For a MultiIndex this is applied to each level separately. df = pd.DataFrame({\"a\": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd']) df.sort_index(key=lambda x: x.str.lower()) a A 1 b 2 C 3 d 4 View Source def sort_index ( self , axis = 0 , level = None , ascending : bool = True , inplace : bool = False , kind : str = \"quicksort\" , na_position : str = \"last\" , sort_remaining : bool = True , ignore_index : bool = False , key : IndexKeyFunc = None , ): \"\"\" Sort object by labels (along an axis). Returns a new DataFrame sorted by label if `inplace` argument is ``False``, otherwise updates the original DataFrame and returns None. Parameters ---------- axis : {0 or 'index', 1 or 'columns'}, default 0 The axis along which to sort. The value 0 identifies the rows, and 1 identifies the columns. level : int or level name or list of ints or list of level names If not None, sort on values in specified index level(s). ascending : bool or list of bools, default True Sort ascending vs. descending. When the index is a MultiIndex the sort direction can be controlled for each level individually. inplace : bool, default False If True, perform operation in-place. kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort' Choice of sorting algorithm. See also ndarray.np.sort for more information. `mergesort` is the only stable algorithm. For DataFrames, this option is only applied when sorting on a single column or label. na_position : {'first', 'last'}, default 'last' Puts NaNs at the beginning if `first`; `last` puts NaNs at the end. Not implemented for MultiIndex. sort_remaining : bool, default True If True and sorting by level and index is multilevel, sort by other levels too (in order) after sorting by specified level. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. .. versionadded:: 1.0.0 key : callable, optional If not None, apply the key function to the index values before sorting. This is similar to the `key` argument in the builtin :meth:`sorted` function, with the notable difference that this `key` function should be *vectorized*. It should expect an ``Index`` and return an ``Index`` of the same shape. For MultiIndex inputs, the key is applied *per level*. .. versionadded:: 1.1.0 Returns ------- DataFrame The original DataFrame sorted by the labels. See Also -------- Series.sort_index : Sort Series by the index. DataFrame.sort_values : Sort DataFrame by the value. Series.sort_values : Sort Series by the value. Examples -------- >>> df = pd.DataFrame([1, 2, 3, 4, 5], index=[100, 29, 234, 1, 150], ... columns=['A']) >>> df.sort_index() A 1 4 29 2 100 1 150 5 234 3 By default, it sorts in ascending order, to sort in descending order, use ``ascending=False`` >>> df.sort_index(ascending=False) A 234 3 150 5 100 1 29 2 1 4 A key function can be specified which is applied to the index before sorting. For a ``MultiIndex`` this is applied to each level separately. >>> df = pd.DataFrame({\" a \": [1, 2, 3, 4]}, index=['A', 'b', 'C', 'd']) >>> df.sort_index(key=lambda x: x.str.lower()) a A 1 b 2 C 3 d 4 \"\"\" # TODO: this can be combined with Series.sort_index impl as # almost identical inplace = validate_bool_kwarg ( inplace , \"inplace\" ) axis = self . _get_axis_number ( axis ) labels = self . _get_axis ( axis ) labels = ensure_key_mapped ( labels , key , levels = level ) # make sure that the axis is lexsorted to start # if not we need to reconstruct to get the correct indexer labels = labels . _sort_levels_monotonic () if level is not None : new_axis , indexer = labels . sortlevel ( level , ascending = ascending , sort_remaining = sort_remaining ) elif isinstance ( labels , MultiIndex ): from pandas . core . sorting import lexsort_indexer indexer = lexsort_indexer ( labels . _get_codes_for_sorting (), orders = ascending , na_position = na_position , ) else : from pandas . core . sorting import nargsort # Check monotonic-ness before sort an index # GH11080 if ( ascending and labels . is_monotonic_increasing ) or ( not ascending and labels . is_monotonic_decreasing ): if inplace : return else : return self . copy () indexer = nargsort ( labels , kind = kind , ascending = ascending , na_position = na_position ) baxis = self . _get_block_manager_axis ( axis ) new_data = self . _mgr . take ( indexer , axis = baxis , verify = False ) # reconstruct axis if needed new_data . axes [ baxis ] = new_data . axes [ baxis ]. _sort_levels_monotonic () if ignore_index : new_data . axes [ 1 ] = ibase . default_index ( len ( indexer )) result = self . _constructor ( new_data ) if inplace : return self . _update_inplace ( result ) else : return result . __finalize__ ( self , method = \"sort_index\" )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#sort_values","text":"def sort_values ( self , by , axis = 0 , ascending = True , inplace = False , kind = 'quicksort' , na_position = 'last' , ignore_index = False , key : Union [ Callable [[ ForwardRef ( 'Series' )], Union [ ForwardRef ( 'Series' ), ~ AnyArrayLike ]], NoneType ] = None ) Sort by the values along either axis.","title":"sort_values"},{"location":"reference/hielen2/datalink_prova_df/#parameters_130","text":"by : str or list of str Name or list of names to sort by . - if `axis` is 0 or `'index'` then `by` may contain index levels and / or column labels . - if `axis` is 1 or `'columns'` then `by` may contain column levels and / or index labels . .. versionchanged :: 0 . 23 . 0 Allow specifying index or column level names . axis : {0 or 'index', 1 or 'columns'}, default 0 Axis to be sorted. ascending : bool or list of bool, default True Sort ascending vs. descending. Specify list for multiple sort orders. If this is a list of bools, must match the length of the by. inplace : bool, default False If True, perform operation in-place. kind : {'quicksort', 'mergesort', 'heapsort'}, default 'quicksort' Choice of sorting algorithm. See also ndarray.np.sort for more information. mergesort is the only stable algorithm. For DataFrames, this option is only applied when sorting on a single column or label. na_position : {'first', 'last'}, default 'last' Puts NaNs at the beginning if first ; last puts NaNs at the end. ignore_index : bool, default False If True, the resulting axis will be labeled 0, 1, \u2026, n - 1. .. versionadded:: 1.0.0 key : callable, optional Apply the key function to the values before sorting. This is similar to the key argument in the builtin :meth: sorted function, with the notable difference that this key function should be vectorized . It should expect a Series and return a Series with the same shape as the input. It will be applied to each column in by independently. .. versionadded :: 1.1.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_145","text":"DataFrame or None DataFrame with sorted values if inplace=False, None otherwise.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_131","text":"DataFrame.sort_index : Sort a DataFrame by the index. Series.sort_values : Similar method for a Series.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_134","text":"df = pd.DataFrame({ ... 'col1': ['A', 'A', 'B', np.nan, 'D', 'C'], ... 'col2': [2, 1, 9, 8, 7, 4], ... 'col3': [0, 1, 9, 4, 2, 3], ... 'col4': ['a', 'B', 'c', 'D', 'e', 'F'] ... }) df col1 col2 col3 col4 0 A 2 0 a 1 A 1 1 B 2 B 9 9 c 3 NaN 8 4 D 4 D 7 2 e 5 C 4 3 F Sort by col1 df.sort_values(by=['col1']) col1 col2 col3 col4 0 A 2 0 a 1 A 1 1 B 2 B 9 9 c 5 C 4 3 F 4 D 7 2 e 3 NaN 8 4 D Sort by multiple columns df.sort_values(by=['col1', 'col2']) col1 col2 col3 col4 1 A 1 1 B 0 A 2 0 a 2 B 9 9 c 5 C 4 3 F 4 D 7 2 e 3 NaN 8 4 D Sort Descending df.sort_values(by='col1', ascending=False) col1 col2 col3 col4 4 D 7 2 e 5 C 4 3 F 2 B 9 9 c 0 A 2 0 a 1 A 1 1 B 3 NaN 8 4 D Putting NAs first df.sort_values(by='col1', ascending=False, na_position='first') col1 col2 col3 col4 3 NaN 8 4 D 4 D 7 2 e 5 C 4 3 F 2 B 9 9 c 0 A 2 0 a 1 A 1 1 B Sorting with a key function df.sort_values(by='col4', key=lambda col: col.str.lower()) col1 col2 col3 col4 0 A 2 0 a 1 A 1 1 B 2 B 9 9 c 3 NaN 8 4 D 4 D 7 2 e 5 C 4 3 F View Source @Substitution ( ** _shared_doc_kwargs ) @Appender ( NDFrame . sort_values . __doc__ ) def sort_values ( # type: ignore[override] # NOQA # issue 27237 self , by , axis = 0 , ascending = True , inplace = False , kind = \"quicksort\" , na_position = \"last\" , ignore_index = False , key : ValueKeyFunc = None , ): inplace = validate_bool_kwarg ( inplace , \"inplace\" ) axis = self . _get_axis_number ( axis ) if not isinstance ( by , list ): by = [ by ] if is_sequence ( ascending ) and len ( by ) != len ( ascending ): raise ValueError ( f \"Length of ascending ({len(ascending)}) != length of by ({len(by)})\" ) if len ( by ) > 1 : from pandas.core.sorting import lexsort_indexer keys = [ self . _get_label_or_level_values ( x , axis = axis ) for x in by ] # need to rewrap columns in Series to apply key function if key is not None : keys = [ Series ( k , name = name ) for ( k , name ) in zip ( keys , by )] indexer = lexsort_indexer ( keys , orders = ascending , na_position = na_position , key = key ) indexer = ensure_platform_int ( indexer ) else : from pandas.core.sorting import nargsort by = by [ 0 ] k = self . _get_label_or_level_values ( by , axis = axis ) # need to rewrap column in Series to apply key function if key is not None : k = Series ( k , name = by ) if isinstance ( ascending , ( tuple , list )): ascending = ascending [ 0 ] indexer = nargsort ( k , kind = kind , ascending = ascending , na_position = na_position , key = key ) new_data = self . _mgr . take ( indexer , axis = self . _get_block_manager_axis ( axis ), verify = False ) if ignore_index : new_data . axes [ 1 ] = ibase . default_index ( len ( indexer )) result = self . _constructor ( new_data ) if inplace : return self . _update_inplace ( result ) else : return result . __finalize__ ( self , method = \"sort_values\" )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#squeeze","text":"def squeeze ( self , axis = None ) Squeeze 1 dimensional axis objects into scalars. Series or DataFrames with a single element are squeezed to a scalar. DataFrames with a single column or a single row are squeezed to a Series. Otherwise the object is unchanged. This method is most useful when you don't know if your object is a Series or DataFrame, but you do know it has just a single column. In that case you can safely call squeeze to ensure you have a Series.","title":"squeeze"},{"location":"reference/hielen2/datalink_prova_df/#parameters_131","text":"axis : {0 or 'index', 1 or 'columns', None}, default None A specific axis to squeeze. By default, all length-1 axes are squeezed.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_146","text":"DataFrame, Series, or scalar The projection after squeezing axis or all the axes.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_132","text":"Series.iloc : Integer-location based indexing for selecting scalars. DataFrame.iloc : Integer-location based indexing for selecting Series. Series.to_frame : Inverse of DataFrame.squeeze for a single-column DataFrame.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_135","text":"primes = pd.Series([2, 3, 5, 7]) Slicing might produce a Series with a single value: even_primes = primes[primes % 2 == 0] even_primes 0 2 dtype: int64 even_primes.squeeze() 2 Squeezing objects with more than one value in every axis does nothing: odd_primes = primes[primes % 2 == 1] odd_primes 1 3 2 5 3 7 dtype: int64 odd_primes.squeeze() 1 3 2 5 3 7 dtype: int64 Squeezing is even more effective when used with DataFrames. df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b']) df a b 0 1 2 1 3 4 Slicing a single column will produce a DataFrame with the columns having only one value: df_a = df[['a']] df_a a 0 1 1 3 So the columns can be squeezed down, resulting in a Series: df_a.squeeze('columns') 0 1 1 3 Name: a, dtype: int64 Slicing a single row from a single column will produce a single scalar DataFrame: df_0a = df.loc[df.index < 1, ['a']] df_0a a 0 1 Squeezing the rows produces a single scalar Series: df_0a.squeeze('rows') a 1 Name: 0, dtype: int64 Squeezing all axes will project directly into a scalar: df_0a.squeeze() 1 View Source def squeeze ( self , axis = None ): \"\"\" Squeeze 1 dimensional axis objects into scalars. Series or DataFrames with a single element are squeezed to a scalar. DataFrames with a single column or a single row are squeezed to a Series. Otherwise the object is unchanged. This method is most useful when you don't know if your object is a Series or DataFrame, but you do know it has just a single column. In that case you can safely call `squeeze` to ensure you have a Series. Parameters ---------- axis : {0 or 'index', 1 or 'columns', None}, default None A specific axis to squeeze. By default, all length-1 axes are squeezed. Returns ------- DataFrame, Series, or scalar The projection after squeezing `axis` or all the axes. See Also -------- Series.iloc : Integer-location based indexing for selecting scalars. DataFrame.iloc : Integer-location based indexing for selecting Series. Series.to_frame : Inverse of DataFrame.squeeze for a single-column DataFrame. Examples -------- >>> primes = pd.Series([2, 3, 5, 7]) Slicing might produce a Series with a single value: >>> even_primes = primes[primes % 2 == 0] >>> even_primes 0 2 dtype: int64 >>> even_primes.squeeze() 2 Squeezing objects with more than one value in every axis does nothing: >>> odd_primes = primes[primes % 2 == 1] >>> odd_primes 1 3 2 5 3 7 dtype: int64 >>> odd_primes.squeeze() 1 3 2 5 3 7 dtype: int64 Squeezing is even more effective when used with DataFrames. >>> df = pd.DataFrame([[1, 2], [3, 4]], columns=['a', 'b']) >>> df a b 0 1 2 1 3 4 Slicing a single column will produce a DataFrame with the columns having only one value: >>> df_a = df[['a']] >>> df_a a 0 1 1 3 So the columns can be squeezed down, resulting in a Series: >>> df_a.squeeze('columns') 0 1 1 3 Name: a, dtype: int64 Slicing a single row from a single column will produce a single scalar DataFrame: >>> df_0a = df.loc[df.index < 1, ['a']] >>> df_0a a 0 1 Squeezing the rows produces a single scalar Series: >>> df_0a.squeeze('rows') a 1 Name: 0, dtype: int64 Squeezing all axes will project directly into a scalar: >>> df_0a.squeeze() 1 \"\"\" axis = range ( self . _AXIS_LEN ) if axis is None else ( self . _get_axis_number ( axis ),) return self . iloc [ tuple ( 0 if i in axis and len ( a ) == 1 else slice ( None ) for i , a in enumerate ( self . axes ) ) ]","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#stack","text":"def stack ( self , level =- 1 , dropna = True ) Stack the prescribed level(s) from columns to index. Return a reshaped DataFrame or Series having a multi-level index with one or more new inner-most levels compared to the current DataFrame. The new inner-most levels are created by pivoting the columns of the current dataframe: if the columns have a single level, the output is a Series; if the columns have multiple levels, the new index level(s) is (are) taken from the prescribed level(s) and the output is a DataFrame.","title":"stack"},{"location":"reference/hielen2/datalink_prova_df/#parameters_132","text":"level : int, str, list, default -1 Level(s) to stack from the column axis onto the index axis, defined as one index or label, or a list of indices or labels. dropna : bool, default True Whether to drop rows in the resulting Frame/Series with missing values. Stacking a column level onto the index axis can create combinations of index and column values that are missing from the original dataframe. See Examples section.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_147","text":"DataFrame or Series Stacked dataframe or series.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_133","text":"DataFrame.unstack : Unstack prescribed level(s) from index axis onto column axis. DataFrame.pivot : Reshape dataframe from long format to wide format. DataFrame.pivot_table : Create a spreadsheet-style pivot table as a DataFrame.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_68","text":"The function is named by analogy with a collection of books being reorganized from being side by side on a horizontal position (the columns of the dataframe) to being stacked vertically on top of each other (in the index of the dataframe).","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_136","text":"Single level columns df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]], ... index=['cat', 'dog'], ... columns=['weight', 'height']) Stacking a dataframe with a single level column axis returns a Series: df_single_level_cols weight height cat 0 1 dog 2 3 df_single_level_cols.stack() cat weight 0 height 1 dog weight 2 height 3 dtype: int64 Multi level columns: simple case multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'), ... ('weight', 'pounds')]) df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]], ... index=['cat', 'dog'], ... columns=multicol1) Stacking a dataframe with a multi-level column axis: df_multi_level_cols1 weight kg pounds cat 1 2 dog 2 4 df_multi_level_cols1.stack() weight cat kg 1 pounds 2 dog kg 2 pounds 4 Missing values multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'), ... ('height', 'm')]) df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]], ... index=['cat', 'dog'], ... columns=multicol2) It is common to have missing values when stacking a dataframe with multi-level columns, as the stacked dataframe typically has more values than the original dataframe. Missing values are filled with NaNs: df_multi_level_cols2 weight height kg m cat 1.0 2.0 dog 3.0 4.0 df_multi_level_cols2.stack() height weight cat kg NaN 1.0 m 2.0 NaN dog kg NaN 3.0 m 4.0 NaN Prescribing the level(s) to be stacked The first parameter controls which level or levels are stacked: df_multi_level_cols2.stack(0) kg m cat height NaN 2.0 weight 1.0 NaN dog height NaN 4.0 weight 3.0 NaN df_multi_level_cols2.stack([0, 1]) cat height m 2.0 weight kg 1.0 dog height m 4.0 weight kg 3.0 dtype: float64 Dropping missing values df_multi_level_cols3 = pd.DataFrame([[None, 1.0], [2.0, 3.0]], ... index=['cat', 'dog'], ... columns=multicol2) Note that rows where all values are missing are dropped by default but this behaviour can be controlled via the dropna keyword parameter: df_multi_level_cols3 weight height kg m cat NaN 1.0 dog 2.0 3.0 df_multi_level_cols3.stack(dropna=False) height weight cat kg NaN NaN m 1.0 NaN dog kg NaN 2.0 m 3.0 NaN df_multi_level_cols3.stack(dropna=True) height weight cat m 1.0 NaN dog kg NaN 2.0 m 3.0 NaN View Source def stack ( self , level =- 1 , dropna = True ): \"\"\" Stack the prescribed level(s) from columns to index. Return a reshaped DataFrame or Series having a multi-level index with one or more new inner-most levels compared to the current DataFrame. The new inner-most levels are created by pivoting the columns of the current dataframe: - if the columns have a single level, the output is a Series; - if the columns have multiple levels, the new index level(s) is (are) taken from the prescribed level(s) and the output is a DataFrame. Parameters ---------- level : int, str, list, default -1 Level(s) to stack from the column axis onto the index axis, defined as one index or label, or a list of indices or labels. dropna : bool, default True Whether to drop rows in the resulting Frame/Series with missing values. Stacking a column level onto the index axis can create combinations of index and column values that are missing from the original dataframe. See Examples section. Returns ------- DataFrame or Series Stacked dataframe or series. See Also -------- DataFrame.unstack : Unstack prescribed level(s) from index axis onto column axis. DataFrame.pivot : Reshape dataframe from long format to wide format. DataFrame.pivot_table : Create a spreadsheet-style pivot table as a DataFrame. Notes ----- The function is named by analogy with a collection of books being reorganized from being side by side on a horizontal position (the columns of the dataframe) to being stacked vertically on top of each other (in the index of the dataframe). Examples -------- **Single level columns** >>> df_single_level_cols = pd.DataFrame([[0, 1], [2, 3]], ... index=['cat', 'dog'], ... columns=['weight', 'height']) Stacking a dataframe with a single level column axis returns a Series: >>> df_single_level_cols weight height cat 0 1 dog 2 3 >>> df_single_level_cols.stack() cat weight 0 height 1 dog weight 2 height 3 dtype: int64 **Multi level columns: simple case** >>> multicol1 = pd.MultiIndex.from_tuples([('weight', 'kg'), ... ('weight', 'pounds')]) >>> df_multi_level_cols1 = pd.DataFrame([[1, 2], [2, 4]], ... index=['cat', 'dog'], ... columns=multicol1) Stacking a dataframe with a multi-level column axis: >>> df_multi_level_cols1 weight kg pounds cat 1 2 dog 2 4 >>> df_multi_level_cols1.stack() weight cat kg 1 pounds 2 dog kg 2 pounds 4 **Missing values** >>> multicol2 = pd.MultiIndex.from_tuples([('weight', 'kg'), ... ('height', 'm')]) >>> df_multi_level_cols2 = pd.DataFrame([[1.0, 2.0], [3.0, 4.0]], ... index=['cat', 'dog'], ... columns=multicol2) It is common to have missing values when stacking a dataframe with multi-level columns, as the stacked dataframe typically has more values than the original dataframe. Missing values are filled with NaNs: >>> df_multi_level_cols2 weight height kg m cat 1.0 2.0 dog 3.0 4.0 >>> df_multi_level_cols2.stack() height weight cat kg NaN 1.0 m 2.0 NaN dog kg NaN 3.0 m 4.0 NaN **Prescribing the level(s) to be stacked** The first parameter controls which level or levels are stacked: >>> df_multi_level_cols2.stack(0) kg m cat height NaN 2.0 weight 1.0 NaN dog height NaN 4.0 weight 3.0 NaN >>> df_multi_level_cols2.stack([0, 1]) cat height m 2.0 weight kg 1.0 dog height m 4.0 weight kg 3.0 dtype: float64 **Dropping missing values** >>> df_multi_level_cols3 = pd.DataFrame([[None, 1.0], [2.0, 3.0]], ... index=['cat', 'dog'], ... columns=multicol2) Note that rows where all values are missing are dropped by default but this behaviour can be controlled via the dropna keyword parameter: >>> df_multi_level_cols3 weight height kg m cat NaN 1.0 dog 2.0 3.0 >>> df_multi_level_cols3.stack(dropna=False) height weight cat kg NaN NaN m 1.0 NaN dog kg NaN 2.0 m 3.0 NaN >>> df_multi_level_cols3.stack(dropna=True) height weight cat m 1.0 NaN dog kg NaN 2.0 m 3.0 NaN \"\"\" from pandas . core . reshape . reshape import stack , stack_multiple if isinstance ( level , ( tuple , list )): return stack_multiple ( self , level , dropna = dropna ) else : return stack ( self , level , dropna = dropna )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#std","text":"def std ( self , axis = None , skipna = None , level = None , ddof = 1 , numeric_only = None , ** kwargs ) Return sample standard deviation over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument","title":"std"},{"location":"reference/hielen2/datalink_prova_df/#parameters_133","text":"axis : {index (0), columns (1)} skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. ddof : int, default 1 Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_148","text":"Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr ) @Appender ( _num_ddof_doc ) def stat_func ( self , axis = None , skipna = None , level = None , ddof = 1 , numeric_only = None , ** kwargs ) : nv . validate_stat_ddof_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna , ddof = ddof ) return self . _reduce ( func , name , axis = axis , numeric_only = numeric_only , skipna = skipna , ddof = ddof )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#sub","text":"def sub ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Subtraction of dataframe and other, element-wise (binary operator sub ). Equivalent to dataframe - other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rsub . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"sub"},{"location":"reference/hielen2/datalink_prova_df/#parameters_134","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_149","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_134","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_69","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_137","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#subtract","text":"def subtract ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Subtraction of dataframe and other, element-wise (binary operator sub ). Equivalent to dataframe - other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rsub . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"subtract"},{"location":"reference/hielen2/datalink_prova_df/#parameters_135","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_150","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_135","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_70","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_138","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#sum","text":"def sum ( self , axis = None , skipna = None , level = None , numeric_only = None , min_count = 0 , ** kwargs ) Return the sum of the values for the requested axis. This is equivalent to the method numpy.sum .","title":"sum"},{"location":"reference/hielen2/datalink_prova_df/#parameters_136","text":"axis : {index (0), columns (1)} Axis for the function to be applied on. skipna : bool, default True Exclude NA/null values when computing the result. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series. min_count : int, default 0 The required number of valid values to perform the operation. If fewer than min_count non-NA values are present the result will be NA. .. versionadded :: 0.22.0 Added with the default being 0. This means the sum of an all-NA or empty Series is 0, and the product of an all-NA or empty Series is 1. **kwargs Additional keyword arguments to be passed to the function.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_151","text":"Series or DataFrame (if level specified)","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_136","text":"Series.sum : Return the sum. Series.min : Return the minimum. Series.max : Return the maximum. Series.idxmin : Return the index of the minimum. Series.idxmax : Return the index of the maximum. DataFrame.sum : Return the sum over the requested axis. DataFrame.min : Return the minimum over the requested axis. DataFrame.max : Return the maximum over the requested axis. DataFrame.idxmin : Return the index of the minimum over the requested axis. DataFrame.idxmax : Return the index of the maximum over the requested axis.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_139","text":"idx = pd.MultiIndex.from_arrays([ ... ['warm', 'warm', 'cold', 'cold'], ... ['dog', 'falcon', 'fish', 'spider']], ... names=['blooded', 'animal']) s = pd.Series([4, 2, 0, 8], name='legs', index=idx) s blooded animal warm dog 4 falcon 2 cold fish 0 spider 8 Name: legs, dtype: int64 s.sum() 14 Sum using level names, as well as indices. s.sum(level='blooded') blooded warm 6 cold 8 Name: legs, dtype: int64 s.sum(level=0) blooded warm 6 cold 8 Name: legs, dtype: int64 By default, the sum of an empty or all-NA Series is 0 . pd.Series([]).sum() # min_count=0 is the default 0.0 This can be controlled with the min_count parameter. For example, if you'd like the sum of an empty series to be NaN, pass min_count=1 . pd.Series([]).sum(min_count=1) nan Thanks to the skipna parameter, min_count handles all-NA and empty series identically. pd.Series([np.nan]).sum() 0.0 pd.Series([np.nan]).sum(min_count=1) nan View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr , min_count = _min_count_stub , see_also = see_also , examples = examples , ) @Appender ( _num_doc ) def stat_func ( self , axis = None , skipna = None , level = None , numeric_only = None , min_count = 0 , ** kwargs , ) : if name == \"sum\" : nv . validate_sum ( tuple (), kwargs ) elif name == \"prod\" : nv . validate_prod ( tuple (), kwargs ) else : nv . validate_stat_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna , min_count = min_count ) return self . _reduce ( func , name = name , axis = axis , skipna = skipna , numeric_only = numeric_only , min_count = min_count , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#swapaxes","text":"def swapaxes ( self : ~ FrameOrSeries , axis1 , axis2 , copy = True ) -> ~ FrameOrSeries Interchange axes and swap values axes appropriately.","title":"swapaxes"},{"location":"reference/hielen2/datalink_prova_df/#returns_152","text":"y : same as input View Source def swapaxes ( self : FrameOrSeries , axis1 , axis2 , copy = True ) -> FrameOrSeries : \"\"\" Interchange axes and swap values axes appropriately. Returns ------- y : same as input \"\"\" i = self . _get_axis_number ( axis1 ) j = self . _get_axis_number ( axis2 ) if i == j : if copy : return self . copy () return self mapping = { i : j , j : i } new_axes = ( self . _get_axis ( mapping . get ( k , k )) for k in range ( self . _AXIS_LEN )) new_values = self . values . swapaxes ( i , j ) if copy : new_values = new_values . copy () # ignore needed because of NDFrame constructor is different than # DataFrame / Series constructors . return self . _constructor ( new_values , * new_axes ). __finalize__ ( # type : ignore self , method = \"swapaxes\" )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#swaplevel","text":"def swaplevel ( self , i =- 2 , j =- 1 , axis = 0 ) -> 'DataFrame' Swap levels i and j in a MultiIndex on a particular axis.","title":"swaplevel"},{"location":"reference/hielen2/datalink_prova_df/#parameters_137","text":"i, j : int or str Levels of the indices to be swapped. Can pass level name as string. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to swap levels on. 0 or 'index' for row-wise, 1 or 'columns' for column-wise.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_153","text":"DataFrame View Source def swaplevel ( self , i =- 2 , j =- 1 , axis = 0 ) -> \"DataFrame\" : \"\"\" Swap levels i and j in a MultiIndex on a particular axis. Parameters ---------- i, j : int or str Levels of the indices to be swapped. Can pass level name as string. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to swap levels on. 0 or 'index' for row-wise, 1 or 'columns' for column-wise. Returns ------- DataFrame \"\"\" result = self . copy () axis = self . _get_axis_number ( axis ) if not isinstance ( result . _get_axis ( axis ), MultiIndex ): # pragma : no cover raise TypeError ( \"Can only swap levels on a hierarchical axis.\" ) if axis == 0 : assert isinstance ( result . index , MultiIndex ) result . index = result . index . swaplevel ( i , j ) else : assert isinstance ( result . columns , MultiIndex ) result . columns = result . columns . swaplevel ( i , j ) return result","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#tail","text":"def tail ( self : ~ FrameOrSeries , n : int = 5 ) -> ~ FrameOrSeries Return the last n rows. This function returns last n rows from the object based on position. It is useful for quickly verifying data, for example, after sorting or appending rows. For negative values of n , this function returns all rows except the first n rows, equivalent to df[n:] .","title":"tail"},{"location":"reference/hielen2/datalink_prova_df/#parameters_138","text":"n : int, default 5 Number of rows to select.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_154","text":"type of caller The last n rows of the caller object.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_137","text":"DataFrame.head : The first n rows of the caller object.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_140","text":"df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion', ... 'monkey', 'parrot', 'shark', 'whale', 'zebra']}) df animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the last 5 lines df.tail() animal 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the last n lines (three in this case) df.tail(3) animal 6 shark 7 whale 8 zebra For negative values of n df.tail(-3) animal 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra View Source def tail ( self : FrameOrSeries , n : int = 5 ) -> FrameOrSeries : \"\"\" Return the last `n` rows. This function returns last `n` rows from the object based on position. It is useful for quickly verifying data, for example, after sorting or appending rows. For negative values of `n`, this function returns all rows except the first `n` rows, equivalent to ``df[n:]``. Parameters ---------- n : int, default 5 Number of rows to select. Returns ------- type of caller The last `n` rows of the caller object. See Also -------- DataFrame.head : The first `n` rows of the caller object. Examples -------- >>> df = pd.DataFrame({'animal': ['alligator', 'bee', 'falcon', 'lion', ... 'monkey', 'parrot', 'shark', 'whale', 'zebra']}) >>> df animal 0 alligator 1 bee 2 falcon 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the last 5 lines >>> df.tail() animal 4 monkey 5 parrot 6 shark 7 whale 8 zebra Viewing the last `n` lines (three in this case) >>> df.tail(3) animal 6 shark 7 whale 8 zebra For negative values of `n` >>> df.tail(-3) animal 3 lion 4 monkey 5 parrot 6 shark 7 whale 8 zebra \"\"\" if n == 0 : return self . iloc [ 0 : 0 ] return self . iloc [ - n :]","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#take","text":"def take ( self : ~ FrameOrSeries , indices , axis = 0 , is_copy : Union [ bool , NoneType ] = None , ** kwargs ) -> ~ FrameOrSeries Return the elements in the given positional indices along an axis. This means that we are not indexing according to actual values in the index attribute of the object. We are indexing according to the actual position of the element in the object.","title":"take"},{"location":"reference/hielen2/datalink_prova_df/#parameters_139","text":"indices : array-like An array of ints indicating which positions to take. axis : {0 or 'index', 1 or 'columns', None}, default 0 The axis on which to select elements. 0 means that we are selecting rows, 1 means that we are selecting columns. is_copy : bool Before pandas 1.0, is_copy=False can be specified to ensure that the return value is an actual copy. Starting with pandas 1.0, take always returns a copy, and the keyword is therefore deprecated. .. deprecated :: 1.0.0 **kwargs For compatibility with :meth: numpy.take . Has no effect on the output.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_155","text":"taken : same type as caller An array-like containing the elements taken from the object.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_138","text":"DataFrame.loc : Select a subset of a DataFrame by labels. DataFrame.iloc : Select a subset of a DataFrame by positions. numpy.take : Take elements from an array along an axis.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_141","text":"df = pd.DataFrame([('falcon', 'bird', 389.0), ... ('parrot', 'bird', 24.0), ... ('lion', 'mammal', 80.5), ... ('monkey', 'mammal', np.nan)], ... columns=['name', 'class', 'max_speed'], ... index=[0, 2, 3, 1]) df name class max_speed 0 falcon bird 389.0 2 parrot bird 24.0 3 lion mammal 80.5 1 monkey mammal NaN Take elements at positions 0 and 3 along the axis 0 (default). Note how the actual indices selected (0 and 1) do not correspond to our selected indices 0 and 3. That's because we are selecting the 0th and 3rd rows, not rows whose indices equal 0 and 3. df.take([0, 3]) name class max_speed 0 falcon bird 389.0 1 monkey mammal NaN Take elements at indices 1 and 2 along the axis 1 (column selection). df.take([1, 2], axis=1) class max_speed 0 bird 389.0 2 bird 24.0 3 mammal 80.5 1 mammal NaN We may take elements using negative integers for positive indices, starting from the end of the object, just like with Python lists. df.take([-1, -2]) name class max_speed 1 monkey mammal NaN 3 lion mammal 80.5 View Source def take ( self : FrameOrSeries , indices , axis = 0 , is_copy : Optional [ bool_t ] = None , ** kwargs ) -> FrameOrSeries : \"\"\" Return the elements in the given *positional* indices along an axis. This means that we are not indexing according to actual values in the index attribute of the object. We are indexing according to the actual position of the element in the object. Parameters ---------- indices : array-like An array of ints indicating which positions to take. axis : {0 or 'index', 1 or 'columns', None}, default 0 The axis on which to select elements. ``0`` means that we are selecting rows, ``1`` means that we are selecting columns. is_copy : bool Before pandas 1.0, ``is_copy=False`` can be specified to ensure that the return value is an actual copy. Starting with pandas 1.0, ``take`` always returns a copy, and the keyword is therefore deprecated. .. deprecated:: 1.0.0 **kwargs For compatibility with :meth:`numpy.take`. Has no effect on the output. Returns ------- taken : same type as caller An array-like containing the elements taken from the object. See Also -------- DataFrame.loc : Select a subset of a DataFrame by labels. DataFrame.iloc : Select a subset of a DataFrame by positions. numpy.take : Take elements from an array along an axis. Examples -------- >>> df = pd.DataFrame([('falcon', 'bird', 389.0), ... ('parrot', 'bird', 24.0), ... ('lion', 'mammal', 80.5), ... ('monkey', 'mammal', np.nan)], ... columns=['name', 'class', 'max_speed'], ... index=[0, 2, 3, 1]) >>> df name class max_speed 0 falcon bird 389.0 2 parrot bird 24.0 3 lion mammal 80.5 1 monkey mammal NaN Take elements at positions 0 and 3 along the axis 0 (default). Note how the actual indices selected (0 and 1) do not correspond to our selected indices 0 and 3. That's because we are selecting the 0th and 3rd rows, not rows whose indices equal 0 and 3. >>> df.take([0, 3]) name class max_speed 0 falcon bird 389.0 1 monkey mammal NaN Take elements at indices 1 and 2 along the axis 1 (column selection). >>> df.take([1, 2], axis=1) class max_speed 0 bird 389.0 2 bird 24.0 3 mammal 80.5 1 mammal NaN We may take elements using negative integers for positive indices, starting from the end of the object, just like with Python lists. >>> df.take([-1, -2]) name class max_speed 1 monkey mammal NaN 3 lion mammal 80.5 \"\"\" if is_copy is not None : warnings . warn ( \"is_copy is deprecated and will be removed in a future version. \" \"'take' always returns a copy, so there is no need to specify this.\" , FutureWarning , stacklevel = 2 , ) nv . validate_take ( tuple (), kwargs ) new_data = self . _mgr . take ( indices , axis = self . _get_block_manager_axis ( axis ), verify = True ) return self . _constructor ( new_data ). __finalize__ ( self , method = \"take\" )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_clipboard","text":"def to_clipboard ( self , excel : bool = True , sep : Union [ str , NoneType ] = None , ** kwargs ) -> None Copy object to the system clipboard. Write a text representation of object to the system clipboard. This can be pasted into Excel, for example.","title":"to_clipboard"},{"location":"reference/hielen2/datalink_prova_df/#parameters_140","text":"excel : bool, default True Produce output in a csv format for easy pasting into excel. - True, use the provided separator for csv pasting. - False, write a string representation of the object to the clipboard. sep : str, default '\\t' Field delimiter. **kwargs These parameters will be passed to DataFrame.to_csv.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#see-also_139","text":"DataFrame.to_csv : Write a DataFrame to a comma-separated values (csv) file. read_clipboard : Read text from clipboard and pass to read_table.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_71","text":"Requirements for your platform. Linux : xclip , or xsel (with PyQt4 modules) Windows : none OS X : none","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_142","text":"Copy the contents of a DataFrame to the clipboard. df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C']) df.to_clipboard(sep=',') # doctest: +SKIP ... # Wrote the following to the system clipboard: ... # ,A,B,C ... # 0,1,2,3 ... # 1,4,5,6 We can omit the index by passing the keyword index and setting it to false. df.to_clipboard(sep=',', index=False) # doctest: +SKIP ... # Wrote the following to the system clipboard: ... # A,B,C ... # 1,2,3 ... # 4,5,6 View Source def to_clipboard ( self , excel : bool_t = True , sep : Optional [ str ] = None , ** kwargs ) -> None : r \"\"\" Copy object to the system clipboard. Write a text representation of object to the system clipboard. This can be pasted into Excel, for example. Parameters ---------- excel : bool, default True Produce output in a csv format for easy pasting into excel. - True, use the provided separator for csv pasting. - False, write a string representation of the object to the clipboard. sep : str, default ``'\\t'`` Field delimiter. **kwargs These parameters will be passed to DataFrame.to_csv. See Also -------- DataFrame.to_csv : Write a DataFrame to a comma-separated values (csv) file. read_clipboard : Read text from clipboard and pass to read_table. Notes ----- Requirements for your platform. - Linux : `xclip`, or `xsel` (with `PyQt4` modules) - Windows : none - OS X : none Examples -------- Copy the contents of a DataFrame to the clipboard. >>> df = pd.DataFrame([[1, 2, 3], [4, 5, 6]], columns=['A', 'B', 'C']) >>> df.to_clipboard(sep=',') # doctest: +SKIP ... # Wrote the following to the system clipboard: ... # ,A,B,C ... # 0,1,2,3 ... # 1,4,5,6 We can omit the index by passing the keyword `index` and setting it to false. >>> df.to_clipboard(sep=',', index=False) # doctest: +SKIP ... # Wrote the following to the system clipboard: ... # A,B,C ... # 1,2,3 ... # 4,5,6 \"\"\" from pandas . io import clipboards clipboards . to_clipboard ( self , excel = excel , sep = sep , ** kwargs )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_csv","text":"def to_csv ( self , path_or_buf : Union [ str , pathlib . Path , IO [ ~ AnyStr ], NoneType ] = None , sep : str = ',' , na_rep : str = '' , float_format : Union [ str , NoneType ] = None , columns : Union [ Sequence [ Union [ Hashable , NoneType ]], NoneType ] = None , header : Union [ bool , List [ str ]] = True , index : bool = True , index_label : Union [ bool , str , Sequence [ Union [ Hashable , NoneType ]], NoneType ] = None , mode : str = 'w' , encoding : Union [ str , NoneType ] = None , compression : Union [ str , Mapping [ str , str ], NoneType ] = 'infer' , quoting : Union [ int , NoneType ] = None , quotechar : str = '\"' , line_terminator : Union [ str , NoneType ] = None , chunksize : Union [ int , NoneType ] = None , date_format : Union [ str , NoneType ] = None , doublequote : bool = True , escapechar : Union [ str , NoneType ] = None , decimal : Union [ str , NoneType ] = '.' , errors : str = 'strict' ) -> Union [ str , NoneType ] Write object to a comma-separated values (csv) file. .. versionchanged:: 0.24.0 The order of arguments for Series was changed.","title":"to_csv"},{"location":"reference/hielen2/datalink_prova_df/#parameters_141","text":"path_or_buf : str or file handle, default None File path or object, if None is provided the result is returned as a string. If a file object is passed it should be opened with newline='' , disabling universal newlines. .. versionchanged :: 0.24.0 Was previously named \"path\" for Series. sep : str, default ',' String of length 1. Field delimiter for the output file. na_rep : str, default '' Missing data representation. float_format : str, default None Format string for floating point numbers. columns : sequence, optional Columns to write. header : bool or list of str, default True Write out the column names. If a list of strings is given it is assumed to be aliases for the column names. .. versionchanged :: 0.24.0 Previously defaulted to False for Series. index : bool, default True Write row names (index). index_label : str or sequence, or False, default None Column label for index column(s) if desired. If None is given, and header and index are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R. mode : str Python write mode, default 'w'. encoding : str, optional A string representing the encoding to use in the output file, defaults to 'utf-8'. compression : str or dict, default 'infer' If str, represents compression mode. If dict, value at 'method' is the compression mode. Compression mode may be any of the following possible values: {'infer', 'gzip', 'bz2', 'zip', 'xz', None}. If compression mode is 'infer' and path_or_buf is path-like, then detect compression mode from the following extensions: '.gz', '.bz2', '.zip' or '.xz'. (otherwise no compression). If dict given and mode is one of {'zip', 'gzip', 'bz2'}, or inferred as one of the above, other entries passed as additional compression options. .. versionchanged :: 1.0.0 May now be a dict with key 'method' as compression mode and other entries as additional compression options if compression mode is 'zip'. .. versionchanged :: 1.1.0 Passing compression options as keys in dict is supported for compression modes 'gzip' and 'bz2' as well as 'zip'. quoting : optional constant from csv module Defaults to csv.QUOTE_MINIMAL. If you have set a float_format then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric. quotechar : str, default '\\\"' String of length 1. Character used to quote fields. line_terminator : str, optional The newline character or character sequence to use in the output file. Defaults to os.linesep , which depends on the OS in which this method is called ('\\n' for linux, '\\r\\n' for Windows, i.e.). .. versionchanged :: 0.24.0 chunksize : int or None Rows to write at a time. date_format : str, default None Format string for datetime objects. doublequote : bool, default True Control quoting of quotechar inside a field. escapechar : str, default None String of length 1. Character used to escape sep and quotechar when appropriate. decimal : str, default '.' Character recognized as decimal separator. E.g. use ',' for European data. errors : str, default 'strict' Specifies how encoding and decoding errors are to be handled. See the errors argument for :func: open for a full list of options. .. versionadded :: 1.1.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_156","text":"None or str If path_or_buf is None, returns the resulting csv format as a string. Otherwise returns None.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_140","text":"read_csv : Load a CSV file into a DataFrame. to_excel : Write DataFrame to an Excel file.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_143","text":"df = pd.DataFrame({'name': ['Raphael', 'Donatello'], ... 'mask': ['red', 'purple'], ... 'weapon': ['sai', 'bo staff']}) df.to_csv(index=False) 'name,mask,weapon\\nRaphael,red,sai\\nDonatello,purple,bo staff\\n' Create 'out.zip' containing 'out.csv' compression_opts = dict(method='zip', ... archive_name='out.csv') # doctest: +SKIP df.to_csv('out.zip', index=False, ... compression=compression_opts) # doctest: +SKIP View Source def to_csv ( self , path_or_buf : Optional [ FilePathOrBuffer ] = None , sep : str = \",\" , na_rep : str = \"\" , float_format : Optional [ str ] = None , columns : Optional [ Sequence[Label ] ] = None , header : Union [ bool_t, List[str ] ] = True , index : bool_t = True , index_label : Optional [ Union[bool_t, str, Sequence[Label ] ]] = None , mode : str = \"w\" , encoding : Optional [ str ] = None , compression : Optional [ Union[str, Mapping[str, str ] ]] = \"infer\" , quoting : Optional [ int ] = None , quotechar : str = '\"' , line_terminator : Optional [ str ] = None , chunksize : Optional [ int ] = None , date_format : Optional [ str ] = None , doublequote : bool_t = True , escapechar : Optional [ str ] = None , decimal : Optional [ str ] = \".\" , errors : str = \"strict\" , ) -> Optional [ str ] : r \"\"\" Write object to a comma-separated values (csv) file. .. versionchanged:: 0.24.0 The order of arguments for Series was changed. Parameters ---------- path_or_buf : str or file handle, default None File path or object, if None is provided the result is returned as a string. If a file object is passed it should be opened with `newline=''`, disabling universal newlines. .. versionchanged:: 0.24.0 Was previously named \" path \" for Series. sep : str, default ',' String of length 1. Field delimiter for the output file. na_rep : str, default '' Missing data representation. float_format : str, default None Format string for floating point numbers. columns : sequence, optional Columns to write. header : bool or list of str, default True Write out the column names. If a list of strings is given it is assumed to be aliases for the column names. .. versionchanged:: 0.24.0 Previously defaulted to False for Series. index : bool, default True Write row names (index). index_label : str or sequence, or False, default None Column label for index column(s) if desired. If None is given, and `header` and `index` are True, then the index names are used. A sequence should be given if the object uses MultiIndex. If False do not print fields for index names. Use index_label=False for easier importing in R. mode : str Python write mode, default 'w'. encoding : str, optional A string representing the encoding to use in the output file, defaults to 'utf-8'. compression : str or dict, default 'infer' If str, represents compression mode. If dict, value at 'method' is the compression mode. Compression mode may be any of the following possible values: {'infer', 'gzip', 'bz2', 'zip', 'xz', None}. If compression mode is 'infer' and `path_or_buf` is path-like, then detect compression mode from the following extensions: '.gz', '.bz2', '.zip' or '.xz'. (otherwise no compression). If dict given and mode is one of {'zip', 'gzip', 'bz2'}, or inferred as one of the above, other entries passed as additional compression options. .. versionchanged:: 1.0.0 May now be a dict with key 'method' as compression mode and other entries as additional compression options if compression mode is 'zip'. .. versionchanged:: 1.1.0 Passing compression options as keys in dict is supported for compression modes 'gzip' and 'bz2' as well as 'zip'. quoting : optional constant from csv module Defaults to csv.QUOTE_MINIMAL. If you have set a `float_format` then floats are converted to strings and thus csv.QUOTE_NONNUMERIC will treat them as non-numeric. quotechar : str, default '\\\" ' String of length 1. Character used to quote fields. line_terminator : str, optional The newline character or character sequence to use in the output file. Defaults to `os.linesep`, which depends on the OS in which this method is called (' \\ n ' for linux, ' \\ r \\ n ' for Windows, i.e.). .. versionchanged:: 0.24.0 chunksize : int or None Rows to write at a time. date_format : str, default None Format string for datetime objects. doublequote : bool, default True Control quoting of `quotechar` inside a field. escapechar : str, default None String of length 1. Character used to escape `sep` and `quotechar` when appropriate. decimal : str, default ' . ' Character recognized as decimal separator. E.g. use ' , ' for European data. errors : str, default ' strict ' Specifies how encoding and decoding errors are to be handled. See the errors argument for :func:`open` for a full list of options. .. versionadded:: 1.1.0 Returns ------- None or str If path_or_buf is None, returns the resulting csv format as a string. Otherwise returns None. See Also -------- read_csv : Load a CSV file into a DataFrame. to_excel : Write DataFrame to an Excel file. Examples -------- >>> df = pd.DataFrame({' name ': [' Raphael ', ' Donatello '], ... ' mask ': [' red ', ' purple '], ... ' weapon ': [' sai ', ' bo staff ']}) >>> df.to_csv(index=False) ' name , mask , weapon \\ nRaphael , red , sai \\ nDonatello , purple , bo staff \\ n ' Create ' out . zip ' containing ' out . csv ' >>> compression_opts = dict(method=' zip ', ... archive_name=' out . csv ') # doctest: +SKIP >>> df.to_csv(' out . zip ' , index = False , ... compression = compression_opts ) # doctest : + SKIP \"\" \" df = self if isinstance ( self , ABCDataFrame ) else self . to_frame () from pandas . io . formats . csvs import CSVFormatter formatter = CSVFormatter ( df , path_or_buf , line_terminator = line_terminator , sep = sep , encoding = encoding , errors = errors , compression = compression , quoting = quoting , na_rep = na_rep , float_format = float_format , cols = columns , header = header , index = index , index_label = index_label , mode = mode , chunksize = chunksize , quotechar = quotechar , date_format = date_format , doublequote = doublequote , escapechar = escapechar , decimal = decimal , ) formatter . save () if path_or_buf is None : return formatter . path_or_buf . getvalue () return None","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_dict","text":"def to_dict ( self , orient = 'dict' , into =< class ' dict '> ) Convert the DataFrame to a dictionary. The type of the key-value pairs can be customized with the parameters (see below).","title":"to_dict"},{"location":"reference/hielen2/datalink_prova_df/#parameters_142","text":"orient : str {'dict', 'list', 'series', 'split', 'records', 'index'} Determines the type of the values of the dictionary. - 'dict' ( default ) : dict like { column -> { index -> value }} - 'list' : dict like { column -> [ values ] } - 'series' : dict like { column -> Series ( values ) } - 'split' : dict like { 'index' -> [ index ], 'columns' -> [ columns ], 'data' -> [ values ] } - 'records' : list like [ { column -> value } , ... , { column -> value } ] - 'index' : dict like { index -> { column -> value }} Abbreviations are allowed . `s` indicates `series` and `sp` indicates `split` . into : class, default dict The collections.abc.Mapping subclass used for all Mappings in the return value. Can be the actual class or an empty instance of the mapping type you want. If you want a collections.defaultdict, you must pass it initialized.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_157","text":"dict, list or collections.abc.Mapping Return a collections.abc.Mapping object representing the DataFrame. The resulting transformation depends on the orient parameter.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_141","text":"DataFrame.from_dict: Create a DataFrame from a dictionary. DataFrame.to_json: Convert a DataFrame to JSON format.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_144","text":"df = pd.DataFrame({'col1': [1, 2], ... 'col2': [0.5, 0.75]}, ... index=['row1', 'row2']) df col1 col2 row1 1 0.50 row2 2 0.75 df.to_dict() {'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}} You can specify the return orientation. df.to_dict('series') {'col1': row1 1 row2 2 Name: col1, dtype: int64, 'col2': row1 0.50 row2 0.75 Name: col2, dtype: float64} df.to_dict('split') {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'], 'data': [[1, 0.5], [2, 0.75]]} df.to_dict('records') [{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}] df.to_dict('index') {'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}} You can also specify the mapping type. from collections import OrderedDict, defaultdict df.to_dict(into=OrderedDict) OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])), ('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))]) If you want a defaultdict , you need to initialize it: dd = defaultdict(list) df.to_dict('records', into=dd) [defaultdict( , {'col1': 1, 'col2': 0.5}), defaultdict( , {'col1': 2, 'col2': 0.75})] View Source def to_dict ( self , orient = \"dict\" , into = dict ): \"\"\" Convert the DataFrame to a dictionary. The type of the key-value pairs can be customized with the parameters (see below). Parameters ---------- orient : str {'dict', 'list', 'series', 'split', 'records', 'index'} Determines the type of the values of the dictionary. - 'dict' (default) : dict like {column -> {index -> value}} - 'list' : dict like {column -> [values]} - 'series' : dict like {column -> Series(values)} - 'split' : dict like {'index' -> [index], 'columns' -> [columns], 'data' -> [values]} - 'records' : list like [{column -> value}, ... , {column -> value}] - 'index' : dict like {index -> {column -> value}} Abbreviations are allowed. `s` indicates `series` and `sp` indicates `split`. into : class, default dict The collections.abc.Mapping subclass used for all Mappings in the return value. Can be the actual class or an empty instance of the mapping type you want. If you want a collections.defaultdict, you must pass it initialized. Returns ------- dict, list or collections.abc.Mapping Return a collections.abc.Mapping object representing the DataFrame. The resulting transformation depends on the `orient` parameter. See Also -------- DataFrame.from_dict: Create a DataFrame from a dictionary. DataFrame.to_json: Convert a DataFrame to JSON format. Examples -------- >>> df = pd.DataFrame({'col1': [1, 2], ... 'col2': [0.5, 0.75]}, ... index=['row1', 'row2']) >>> df col1 col2 row1 1 0.50 row2 2 0.75 >>> df.to_dict() {'col1': {'row1': 1, 'row2': 2}, 'col2': {'row1': 0.5, 'row2': 0.75}} You can specify the return orientation. >>> df.to_dict('series') {'col1': row1 1 row2 2 Name: col1, dtype: int64, 'col2': row1 0.50 row2 0.75 Name: col2, dtype: float64} >>> df.to_dict('split') {'index': ['row1', 'row2'], 'columns': ['col1', 'col2'], 'data': [[1, 0.5], [2, 0.75]]} >>> df.to_dict('records') [{'col1': 1, 'col2': 0.5}, {'col1': 2, 'col2': 0.75}] >>> df.to_dict('index') {'row1': {'col1': 1, 'col2': 0.5}, 'row2': {'col1': 2, 'col2': 0.75}} You can also specify the mapping type. >>> from collections import OrderedDict, defaultdict >>> df.to_dict(into=OrderedDict) OrderedDict([('col1', OrderedDict([('row1', 1), ('row2', 2)])), ('col2', OrderedDict([('row1', 0.5), ('row2', 0.75)]))]) If you want a `defaultdict`, you need to initialize it: >>> dd = defaultdict(list) >>> df.to_dict('records', into=dd) [defaultdict(<class 'list'>, {'col1': 1, 'col2': 0.5}), defaultdict(<class 'list'>, {'col1': 2, 'col2': 0.75})] \"\"\" if not self . columns . is_unique : warnings . warn ( \"DataFrame columns are not unique, some columns will be omitted.\" , UserWarning , stacklevel = 2 , ) # GH16122 into_c = com . standardize_mapping ( into ) orient = orient . lower () # GH32515 if orient . startswith (( \"d\" , \"l\" , \"s\" , \"r\" , \"i\" )) and orient not in { \"dict\" , \"list\" , \"series\" , \"split\" , \"records\" , \"index\" , } : warnings . warn ( \"Using short name for 'orient' is deprecated. Only the \" \"options: ('dict', list, 'series', 'split', 'records', 'index') \" \"will be used in a future version. Use one of the above \" \"to silence this warning.\" , FutureWarning , ) if orient . startswith ( \"d\" ): orient = \"dict\" elif orient . startswith ( \"l\" ): orient = \"list\" elif orient . startswith ( \"sp\" ): orient = \"split\" elif orient . startswith ( \"s\" ): orient = \"series\" elif orient . startswith ( \"r\" ): orient = \"records\" elif orient . startswith ( \"i\" ): orient = \"index\" if orient == \"dict\" : return into_c (( k , v . to_dict ( into )) for k , v in self . items ()) elif orient == \"list\" : return into_c (( k , v . tolist ()) for k , v in self . items ()) elif orient == \"split\" : return into_c ( ( ( \"index\" , self . index . tolist ()), ( \"columns\" , self . columns . tolist ()), ( \"data\" , [ list ( map ( com . maybe_box_datetimelike , t )) for t in self . itertuples ( index = False , name = None ) ], ), ) ) elif orient == \"series\" : return into_c (( k , com . maybe_box_datetimelike ( v )) for k , v in self . items ()) elif orient == \"records\" : columns = self . columns . tolist () rows = ( dict ( zip ( columns , row )) for row in self . itertuples ( index = False , name = None ) ) return [ into_c (( k , com . maybe_box_datetimelike ( v )) for k , v in row . items ()) for row in rows ] elif orient == \"index\" : if not self . index . is_unique : raise ValueError ( \"DataFrame index must be unique for orient='index'.\" ) return into_c ( ( t [ 0 ], dict ( zip ( self . columns , t [ 1 :]))) for t in self . itertuples ( name = None ) ) else : raise ValueError ( f \"orient '{orient}' not understood\" )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_excel","text":"def to_excel ( self , excel_writer , sheet_name = 'Sheet1' , na_rep = '' , float_format = None , columns = None , header = True , index = True , index_label = None , startrow = 0 , startcol = 0 , engine = None , merge_cells = True , encoding = None , inf_rep = 'inf' , verbose = True , freeze_panes = None ) -> None Write object to an Excel sheet. To write a single object to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an ExcelWriter object with a target file name, and specify a sheet in the file to write to. Multiple sheets may be written to by specifying unique sheet_name . With all data written to the file it is necessary to save the changes. Note that creating an ExcelWriter object with a file name that already exists will result in the contents of the existing file being erased.","title":"to_excel"},{"location":"reference/hielen2/datalink_prova_df/#parameters_143","text":"excel_writer : str or ExcelWriter object File path or existing ExcelWriter. sheet_name : str, default 'Sheet1' Name of sheet which will contain DataFrame. na_rep : str, default '' Missing data representation. float_format : str, optional Format string for floating point numbers. For example float_format=\"%.2f\" will format 0.1234 to 0.12. columns : sequence or list of str, optional Columns to write. header : bool or list of str, default True Write out the column names. If a list of string is given it is assumed to be aliases for the column names. index : bool, default True Write row names (index). index_label : str or sequence, optional Column label for index column(s) if desired. If not specified, and header and index are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. startrow : int, default 0 Upper left cell row to dump data frame. startcol : int, default 0 Upper left cell column to dump data frame. engine : str, optional Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this via the options io.excel.xlsx.writer , io.excel.xls.writer , and io.excel.xlsm.writer . merge_cells : bool, default True Write MultiIndex and Hierarchical Rows as merged cells. encoding : str, optional Encoding of the resulting excel file. Only necessary for xlwt, other writers support unicode natively. inf_rep : str, default 'inf' Representation for infinity (there is no native representation for infinity in Excel). verbose : bool, default True Display more information in the error logs. freeze_panes : tuple of int (length 2), optional Specifies the one-based bottommost row and rightmost column that is to be frozen.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#see-also_142","text":"to_csv : Write DataFrame to a comma-separated values (csv) file. ExcelWriter : Class for writing DataFrame objects into excel sheets. read_excel : Read an Excel file into a pandas DataFrame. read_csv : Read a comma-separated values (csv) file into DataFrame.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_72","text":"For compatibility with :meth: ~DataFrame.to_csv , to_excel serializes lists and dicts to strings before writing. Once a workbook has been saved it is not possible write further data without rewriting the whole workbook.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_145","text":"Create, write to and save a workbook: df1 = pd.DataFrame([['a', 'b'], ['c', 'd']], ... index=['row 1', 'row 2'], ... columns=['col 1', 'col 2']) df1.to_excel(\"output.xlsx\") # doctest: +SKIP To specify the sheet name: df1.to_excel(\"output.xlsx\", ... sheet_name='Sheet_name_1') # doctest: +SKIP If you wish to write to more than one sheet in the workbook, it is necessary to specify an ExcelWriter object: df2 = df1.copy() with pd.ExcelWriter('output.xlsx') as writer: # doctest: +SKIP ... df1.to_excel(writer, sheet_name='Sheet_name_1') ... df2.to_excel(writer, sheet_name='Sheet_name_2') ExcelWriter can also be used to append to an existing Excel file: with pd.ExcelWriter('output.xlsx', ... mode='a') as writer: # doctest: +SKIP ... df.to_excel(writer, sheet_name='Sheet_name_3') To set the library that is used to write the Excel file, you can pass the engine keyword (the default engine is automatically chosen depending on the file extension): df1.to_excel('output1.xlsx', engine='xlsxwriter') # doctest: +SKIP View Source @ doc ( klass = \"object\" ) def to_excel ( self , excel_writer , sheet_name = \"Sheet1\" , na_rep = \"\" , float_format = None , columns = None , header = True , index = True , index_label = None , startrow = 0 , startcol = 0 , engine = None , merge_cells = True , encoding = None , inf_rep = \"inf\" , verbose = True , freeze_panes = None , ) -> None : \"\"\" Write {klass} to an Excel sheet. To write a single {klass} to an Excel .xlsx file it is only necessary to specify a target file name. To write to multiple sheets it is necessary to create an `ExcelWriter` object with a target file name, and specify a sheet in the file to write to. Multiple sheets may be written to by specifying unique `sheet_name`. With all data written to the file it is necessary to save the changes. Note that creating an `ExcelWriter` object with a file name that already exists will result in the contents of the existing file being erased. Parameters ---------- excel_writer : str or ExcelWriter object File path or existing ExcelWriter. sheet_name : str, default 'Sheet1' Name of sheet which will contain DataFrame. na_rep : str, default '' Missing data representation. float_format : str, optional Format string for floating point numbers. For example ``float_format=\" % . 2 f \"`` will format 0.1234 to 0.12. columns : sequence or list of str, optional Columns to write. header : bool or list of str, default True Write out the column names. If a list of string is given it is assumed to be aliases for the column names. index : bool, default True Write row names (index). index_label : str or sequence, optional Column label for index column(s) if desired. If not specified, and `header` and `index` are True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. startrow : int, default 0 Upper left cell row to dump data frame. startcol : int, default 0 Upper left cell column to dump data frame. engine : str, optional Write engine to use, 'openpyxl' or 'xlsxwriter'. You can also set this via the options ``io.excel.xlsx.writer``, ``io.excel.xls.writer``, and ``io.excel.xlsm.writer``. merge_cells : bool, default True Write MultiIndex and Hierarchical Rows as merged cells. encoding : str, optional Encoding of the resulting excel file. Only necessary for xlwt, other writers support unicode natively. inf_rep : str, default 'inf' Representation for infinity (there is no native representation for infinity in Excel). verbose : bool, default True Display more information in the error logs. freeze_panes : tuple of int (length 2), optional Specifies the one-based bottommost row and rightmost column that is to be frozen. See Also -------- to_csv : Write DataFrame to a comma-separated values (csv) file. ExcelWriter : Class for writing DataFrame objects into excel sheets. read_excel : Read an Excel file into a pandas DataFrame. read_csv : Read a comma-separated values (csv) file into DataFrame. Notes ----- For compatibility with :meth:`~DataFrame.to_csv`, to_excel serializes lists and dicts to strings before writing. Once a workbook has been saved it is not possible write further data without rewriting the whole workbook. Examples -------- Create, write to and save a workbook: >>> df1 = pd.DataFrame([['a', 'b'], ['c', 'd']], ... index=['row 1', 'row 2'], ... columns=['col 1', 'col 2']) >>> df1.to_excel(\" output . xlsx \") # doctest: +SKIP To specify the sheet name: >>> df1.to_excel(\" output . xlsx \", ... sheet_name='Sheet_name_1') # doctest: +SKIP If you wish to write to more than one sheet in the workbook, it is necessary to specify an ExcelWriter object: >>> df2 = df1.copy() >>> with pd.ExcelWriter('output.xlsx') as writer: # doctest: +SKIP ... df1.to_excel(writer, sheet_name='Sheet_name_1') ... df2.to_excel(writer, sheet_name='Sheet_name_2') ExcelWriter can also be used to append to an existing Excel file: >>> with pd.ExcelWriter('output.xlsx', ... mode='a') as writer: # doctest: +SKIP ... df.to_excel(writer, sheet_name='Sheet_name_3') To set the library that is used to write the Excel file, you can pass the `engine` keyword (the default engine is automatically chosen depending on the file extension): >>> df1.to_excel('output1.xlsx', engine='xlsxwriter') # doctest: +SKIP \"\"\" df = self if isinstance ( self , ABCDataFrame ) else self . to_frame () from pandas . io . formats . excel import ExcelFormatter formatter = ExcelFormatter ( df , na_rep = na_rep , cols = columns , header = header , float_format = float_format , index = index , index_label = index_label , merge_cells = merge_cells , inf_rep = inf_rep , ) formatter . write ( excel_writer , sheet_name = sheet_name , startrow = startrow , startcol = startcol , freeze_panes = freeze_panes , engine = engine , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_feather","text":"def to_feather ( self , path , ** kwargs ) -> None Write a DataFrame to the binary Feather format.","title":"to_feather"},{"location":"reference/hielen2/datalink_prova_df/#parameters_144","text":"path : str String file path. **kwargs : Additional keywords passed to :func: pyarrow.feather.write_feather . Starting with pyarrow 0.17, this includes the compression , compression_level , chunksize and version keywords. .. versionadded :: 1.1.0 View Source @deprecate_kwarg ( old_arg_name = \"fname\" , new_arg_name = \"path\" ) def to_feather ( self , path , ** kwargs ) -> None : \"\"\" Write a DataFrame to the binary Feather format. Parameters ---------- path : str String file path. **kwargs : Additional keywords passed to :func:`pyarrow.feather.write_feather`. Starting with pyarrow 0.17, this includes the `compression`, `compression_level`, `chunksize` and `version` keywords. .. versionadded:: 1.1.0 \"\"\" from pandas.io.feather_format import to_feather to_feather ( self , path , ** kwargs )","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#to_gbq","text":"def to_gbq ( self , destination_table , project_id = None , chunksize = None , reauth = False , if_exists = 'fail' , auth_local_webserver = False , table_schema = None , location = None , progress_bar = True , credentials = None ) -> None Write a DataFrame to a Google BigQuery table. This function requires the pandas-gbq package <https://pandas-gbq.readthedocs.io> __. See the How to authenticate with Google BigQuery <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html> __ guide for authentication instructions.","title":"to_gbq"},{"location":"reference/hielen2/datalink_prova_df/#parameters_145","text":"destination_table : str Name of table to be written, in the form dataset.tablename . project_id : str, optional Google BigQuery Account project ID. Optional when available from the environment. chunksize : int, optional Number of rows to be inserted in each chunk from the dataframe. Set to None to load the whole dataframe at once. reauth : bool, default False Force Google BigQuery to re-authenticate the user. This is useful if multiple accounts are used. if_exists : str, default 'fail' Behavior when the destination table exists. Value can be one of: ``'fail'`` If table exists raise pandas_gbq.gbq.TableCreationError. ``'replace'`` If table exists, drop it, recreate it, and insert data. ``'append'`` If table exists, insert data. Create if does not exist. auth_local_webserver : bool, default False Use the local webserver flow instead of the console flow when getting user credentials. .. _local webserver flow: https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server .. _console flow: https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console *New in version 0.2.0 of pandas-gbq* . table_schema : list of dicts, optional List of BigQuery table fields to which according DataFrame columns conform to, e.g. [{'name': 'col1', 'type': 'STRING'},...] . If schema is not provided, it will be generated according to dtypes of DataFrame columns. See BigQuery API documentation on available names of a field. *New in version 0.3.1 of pandas-gbq*. location : str, optional Location where the load job should run. See the BigQuery locations documentation <https://cloud.google.com/bigquery/docs/dataset-locations> __ for a list of available locations. The location must match that of the target dataset. *New in version 0.5.0 of pandas-gbq*. progress_bar : bool, default True Use the library tqdm to show the progress bar for the upload, chunk by chunk. *New in version 0.5.0 of pandas-gbq*. credentials : google.auth.credentials.Credentials, optional Credentials for accessing Google APIs. Use this parameter to override default credentials, such as to use Compute Engine :class: google.auth.compute_engine.Credentials or Service Account :class: google.oauth2.service_account.Credentials directly. * New in version 0 . 8 . 0 of pandas - gbq * . .. versionadded :: 0 . 24 . 0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#see-also_143","text":"pandas_gbq.to_gbq : This function in the pandas-gbq library. read_gbq : Read a DataFrame from Google BigQuery. View Source def to_gbq ( self , destination_table , project_id = None , chunksize = None , reauth = False , if_exists = \"fail\" , auth_local_webserver = False , table_schema = None , location = None , progress_bar = True , credentials = None , ) -> None : \"\"\" Write a DataFrame to a Google BigQuery table. This function requires the `pandas-gbq package <https://pandas-gbq.readthedocs.io>`__. See the `How to authenticate with Google BigQuery <https://pandas-gbq.readthedocs.io/en/latest/howto/authentication.html>`__ guide for authentication instructions. Parameters ---------- destination_table : str Name of table to be written, in the form ``dataset.tablename``. project_id : str, optional Google BigQuery Account project ID. Optional when available from the environment. chunksize : int, optional Number of rows to be inserted in each chunk from the dataframe. Set to ``None`` to load the whole dataframe at once. reauth : bool, default False Force Google BigQuery to re-authenticate the user. This is useful if multiple accounts are used. if_exists : str, default 'fail' Behavior when the destination table exists. Value can be one of: ``'fail'`` If table exists raise pandas_gbq.gbq.TableCreationError. ``'replace'`` If table exists, drop it, recreate it, and insert data. ``'append'`` If table exists, insert data. Create if does not exist. auth_local_webserver : bool, default False Use the `local webserver flow`_ instead of the `console flow`_ when getting user credentials. .. _local webserver flow: https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_local_server .. _console flow: https://google-auth-oauthlib.readthedocs.io/en/latest/reference/google_auth_oauthlib.flow.html#google_auth_oauthlib.flow.InstalledAppFlow.run_console *New in version 0.2.0 of pandas-gbq*. table_schema : list of dicts, optional List of BigQuery table fields to which according DataFrame columns conform to, e.g. ``[{'name': 'col1', 'type': 'STRING'},...]``. If schema is not provided, it will be generated according to dtypes of DataFrame columns. See BigQuery API documentation on available names of a field. *New in version 0.3.1 of pandas-gbq*. location : str, optional Location where the load job should run. See the `BigQuery locations documentation <https://cloud.google.com/bigquery/docs/dataset-locations>`__ for a list of available locations. The location must match that of the target dataset. *New in version 0.5.0 of pandas-gbq*. progress_bar : bool, default True Use the library `tqdm` to show the progress bar for the upload, chunk by chunk. *New in version 0.5.0 of pandas-gbq*. credentials : google.auth.credentials.Credentials, optional Credentials for accessing Google APIs. Use this parameter to override default credentials, such as to use Compute Engine :class:`google.auth.compute_engine.Credentials` or Service Account :class:`google.oauth2.service_account.Credentials` directly. *New in version 0.8.0 of pandas-gbq*. .. versionadded:: 0.24.0 See Also -------- pandas_gbq.to_gbq : This function in the pandas-gbq library. read_gbq : Read a DataFrame from Google BigQuery. \"\"\" from pandas . io import gbq gbq . to_gbq ( self , destination_table , project_id = project_id , chunksize = chunksize , reauth = reauth , if_exists = if_exists , auth_local_webserver = auth_local_webserver , table_schema = table_schema , location = location , progress_bar = progress_bar , credentials = credentials , )","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#to_hdf","text":"def to_hdf ( self , path_or_buf , key : str , mode : str = 'a' , complevel : Union [ int , NoneType ] = None , complib : Union [ str , NoneType ] = None , append : bool = False , format : Union [ str , NoneType ] = None , index : bool = True , min_itemsize : Union [ int , Dict [ str , int ], NoneType ] = None , nan_rep = None , dropna : Union [ bool , NoneType ] = None , data_columns : Union [ bool , List [ str ], NoneType ] = None , errors : str = 'strict' , encoding : str = 'UTF-8' ) -> None Write the contained data to an HDF5 file using HDFStore. Hierarchical Data Format (HDF) is self-describing, allowing an application to interpret the structure and contents of a file with no outside information. One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects. In order to add another DataFrame or Series to an existing HDF file please use append mode and a different a key. For more information see the :ref: user guide <io.hdf5> .","title":"to_hdf"},{"location":"reference/hielen2/datalink_prova_df/#parameters_146","text":"path_or_buf : str or pandas.HDFStore File path or HDFStore object. key : str Identifier for the group in the store. mode : {'a', 'w', 'r+'}, default 'a' Mode to open file: - 'w': write, a new file is created (an existing file with the same name would be deleted). - 'a': append, an existing file is opened for reading and writing, and if the file does not exist it is created. - 'r+': similar to 'a', but the file must already exist. complevel : {0-9}, optional Specifies a compression level for data. A value of 0 disables compression. complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib' Specifies the compression library to be used. As of v0.20.2 these additional compressors for Blosc are supported (default if no compressor specified: 'blosc:blosclz'): {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy', 'blosc:zlib', 'blosc:zstd'}. Specifying a compression library which is not available issues a ValueError. append : bool, default False For Table formats, append the input data to the existing. format : {'fixed', 'table', None}, default 'fixed' Possible values: - 'fixed': Fixed format. Fast writing/reading. Not-appendable, nor searchable. - 'table': Table format. Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data. - If None, pd.get_option('io.hdf.default_format') is checked, followed by fallback to \"fixed\" errors : str, default 'strict' Specifies how encoding and decoding errors are to be handled. See the errors argument for :func: open for a full list of options. encoding : str, default \"UTF-8\" min_itemsize : dict or int, optional Map column names to minimum string sizes for columns. nan_rep : Any, optional How to represent null values as str. Not allowed with append=True. data_columns : list of columns or True, optional List of columns to create as indexed data columns for on-disk queries, or True to use all columns. By default only the axes of the object are indexed. See :ref: io.hdf5-query-data-columns . Applicable only to format='table'.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#see-also_144","text":"DataFrame.read_hdf : Read from HDF file. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. DataFrame.to_sql : Write to a sql table. DataFrame.to_feather : Write out feather-format for DataFrames. DataFrame.to_csv : Write out to a csv file.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_146","text":"df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, ... index=['a', 'b', 'c']) df.to_hdf('data.h5', key='df', mode='w') We can add another object to the same file: s = pd.Series([1, 2, 3, 4]) s.to_hdf('data.h5', key='s') Reading from HDF file: pd.read_hdf('data.h5', 'df') A B a 1 4 b 2 5 c 3 6 pd.read_hdf('data.h5', 's') 0 1 1 2 2 3 3 4 dtype: int64 Deleting file with data: import os os.remove('data.h5') View Source def to_hdf ( self , path_or_buf , key : str , mode : str = \"a\" , complevel : Optional [ int ] = None , complib : Optional [ str ] = None , append : bool_t = False , format : Optional [ str ] = None , index : bool_t = True , min_itemsize : Optional [ Union[int, Dict[str, int ] ]] = None , nan_rep = None , dropna : Optional [ bool_t ] = None , data_columns : Optional [ Union[bool_t, List[str ] ]] = None , errors : str = \"strict\" , encoding : str = \"UTF-8\" , ) -> None : \"\"\" Write the contained data to an HDF5 file using HDFStore. Hierarchical Data Format (HDF) is self-describing, allowing an application to interpret the structure and contents of a file with no outside information. One HDF file can hold a mix of related objects which can be accessed as a group or as individual objects. In order to add another DataFrame or Series to an existing HDF file please use append mode and a different a key. For more information see the :ref:`user guide <io.hdf5>`. Parameters ---------- path_or_buf : str or pandas.HDFStore File path or HDFStore object. key : str Identifier for the group in the store. mode : {'a', 'w', 'r+'}, default 'a' Mode to open file: - 'w': write, a new file is created (an existing file with the same name would be deleted). - 'a': append, an existing file is opened for reading and writing, and if the file does not exist it is created. - 'r+': similar to 'a', but the file must already exist. complevel : {0-9}, optional Specifies a compression level for data. A value of 0 disables compression. complib : {'zlib', 'lzo', 'bzip2', 'blosc'}, default 'zlib' Specifies the compression library to be used. As of v0.20.2 these additional compressors for Blosc are supported (default if no compressor specified: 'blosc:blosclz'): {'blosc:blosclz', 'blosc:lz4', 'blosc:lz4hc', 'blosc:snappy', 'blosc:zlib', 'blosc:zstd'}. Specifying a compression library which is not available issues a ValueError. append : bool, default False For Table formats, append the input data to the existing. format : {'fixed', 'table', None}, default 'fixed' Possible values: - 'fixed': Fixed format. Fast writing/reading. Not-appendable, nor searchable. - 'table': Table format. Write as a PyTables Table structure which may perform worse but allow more flexible operations like searching / selecting subsets of the data. - If None, pd.get_option('io.hdf.default_format') is checked, followed by fallback to \" fixed \" errors : str, default 'strict' Specifies how encoding and decoding errors are to be handled. See the errors argument for :func:`open` for a full list of options. encoding : str, default \" UTF - 8 \" min_itemsize : dict or int, optional Map column names to minimum string sizes for columns. nan_rep : Any, optional How to represent null values as str. Not allowed with append=True. data_columns : list of columns or True, optional List of columns to create as indexed data columns for on-disk queries, or True to use all columns. By default only the axes of the object are indexed. See :ref:`io.hdf5-query-data-columns`. Applicable only to format='table'. See Also -------- DataFrame.read_hdf : Read from HDF file. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. DataFrame.to_sql : Write to a sql table. DataFrame.to_feather : Write out feather-format for DataFrames. DataFrame.to_csv : Write out to a csv file. Examples -------- >>> df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]}, ... index=['a', 'b', 'c']) >>> df.to_hdf('data.h5', key='df', mode='w') We can add another object to the same file: >>> s = pd.Series([1, 2, 3, 4]) >>> s.to_hdf('data.h5', key='s') Reading from HDF file: >>> pd.read_hdf('data.h5', 'df') A B a 1 4 b 2 5 c 3 6 >>> pd.read_hdf('data.h5', 's') 0 1 1 2 2 3 3 4 dtype: int64 Deleting file with data: >>> import os >>> os.remove('data.h5') \"\"\" from pandas . io import pytables pytables . to_hdf ( path_or_buf , key , self , mode = mode , complevel = complevel , complib = complib , append = append , format = format , index = index , min_itemsize = min_itemsize , nan_rep = nan_rep , dropna = dropna , data_columns = data_columns , errors = errors , encoding = encoding , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_html","text":"def to_html ( self , buf = None , columns = None , col_space = None , header = True , index = True , na_rep = 'NaN' , formatters = None , float_format = None , sparsify = None , index_names = True , justify = None , max_rows = None , max_cols = None , show_dimensions = False , decimal = '.' , bold_rows = True , classes = None , escape = True , notebook = False , border = None , table_id = None , render_links = False , encoding = None ) Render a DataFrame as an HTML table.","title":"to_html"},{"location":"reference/hielen2/datalink_prova_df/#parameters_147","text":"buf : str, Path or StringIO-like, optional, default None Buffer to write to. If None, the output is returned as a string. columns : sequence, optional, default None The subset of columns to write. Writes all columns by default. col_space : str or int, list or dict of int or str, optional The minimum width of each column in CSS length units. An int is assumed to be px units. .. versionadded :: 0.25.0 Ability to use str. header : bool, optional Whether to print column labels, default True. index : bool, optional, default True Whether to print index (row) labels. na_rep : str, optional, default 'NaN' String representation of NAN to use. formatters : list, tuple or dict of one-param. functions, optional Formatter functions to apply to columns' elements by position or name. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns. float_format : one-parameter function, optional, default None Formatter function to apply to columns' elements if they are floats. The result of this function must be a unicode string. sparsify : bool, optional, default True Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row. index_names : bool, optional, default True Prints the names of the indexes. justify : str, default None How to justify the column labels. If None uses the option from the print configuration (controlled by set_option), 'right' out of the box. Valid values are * left * right * center * justify * justify-all * start * end * inherit * match-parent * initial * unset. max_rows : int, optional Maximum number of rows to display in the console. min_rows : int, optional The number of rows to display in the console in a truncated repr (when number of rows is above max_rows ). max_cols : int, optional Maximum number of columns to display in the console. show_dimensions : bool, default False Display DataFrame dimensions (number of rows by number of columns). decimal : str, default '.' Character recognized as decimal separator, e.g. ',' in Europe. bold_rows : bool, default True Make the row labels bold in the output. classes : str or list or tuple, default None CSS class(es) to apply to the resulting html table. escape : bool, default True Convert the characters <, >, and & to HTML-safe sequences. notebook : {True, False}, default False Whether the generated HTML is for IPython Notebook. border : int A border=border attribute is included in the opening <table> tag. Default pd.options.display.html.border . encoding : str, default \"utf-8\" Set character encoding. .. versionadded :: 1.0 table_id : str, optional A css id is included in the opening <table> tag if specified. .. versionadded :: 0.23.0 render_links : bool, default False Convert URLs to HTML links. .. versionadded :: 0.24.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_158","text":"str or None If buf is None, returns the result as a string. Otherwise returns None.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_145","text":"to_string : Convert DataFrame to a string. View Source @ Substitution ( header_type = \"bool\" , header = \"Whether to print column labels, default True\" , col_space_type = \"str or int, list or dict of int or str\" , col_space = \"The minimum width of each column in CSS length \" \"units. An int is assumed to be px units.\\n\\n\" \" .. versionadded:: 0.25.0\\n\" \" Ability to use str\" , ) @ Substitution ( shared_params = fmt . common_docstring , returns = fmt . return_docstring ) def to_html( self , buf = None , columns = None , col_space = None , header = True , index = True , na_rep = \"NaN\" , formatters = None , float_format = None , sparsify = None , index_names = True , justify = None , max_rows = None , max_cols = None , show_dimensions = False , decimal = \".\" , bold_rows = True , classes = None , escape = True , notebook = False , border = None , table_id = None , render_links = False , encoding = None , ): \"\"\" Render a DataFrame as an HTML table. %(shared_params)s bold_rows : bool, default True Make the row labels bold in the output. classes : str or list or tuple, default None CSS class(es) to apply to the resulting html table. escape : bool, default True Convert the characters <, >, and & to HTML-safe sequences. notebook : {True, False}, default False Whether the generated HTML is for IPython Notebook. border : int A ``border=border`` attribute is included in the opening `<table>` tag. Default ``pd.options.display.html.border``. encoding : str, default \" utf - 8 \" Set character encoding. .. versionadded:: 1.0 table_id : str, optional A css id is included in the opening `<table>` tag if specified. .. versionadded:: 0.23.0 render_links : bool, default False Convert URLs to HTML links. .. versionadded:: 0.24.0 %(returns)s See Also -------- to_string : Convert DataFrame to a string. \"\"\" if justify is not None and justify not in fmt._VALID_JUSTIFY_PARAMETERS: raise ValueError(\"Invalid value for justify parameter\") formatter = fmt . DataFrameFormatter ( self , columns = columns , col_space = col_space , na_rep = na_rep , formatters = formatters , float_format = float_format , sparsify = sparsify , justify = justify , index_names = index_names , header = header , index = index , bold_rows = bold_rows , escape = escape , max_rows = max_rows , max_cols = max_cols , show_dimensions = show_dimensions , decimal = decimal , table_id = table_id , render_links = render_links , ) # TODO : a generic formatter wld b in DataFrameFormatter return formatter.to_html( buf = buf , classes = classes , notebook = notebook , border = border , encoding = encoding , )","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#to_json","text":"def to_json ( self , path_or_buf : Union [ str , pathlib . Path , IO [ ~ AnyStr ], NoneType ] = None , orient : Union [ str , NoneType ] = None , date_format : Union [ str , NoneType ] = None , double_precision : int = 10 , force_ascii : bool = True , date_unit : str = 'ms' , default_handler : Union [ Callable [[ Any ], Union [ str , int , float , bool , List , Dict , NoneType ]], NoneType ] = None , lines : bool = False , compression : Union [ str , NoneType ] = 'infer' , index : bool = True , indent : Union [ int , NoneType ] = None ) -> Union [ str , NoneType ] Convert the object to a JSON string. Note NaN's and None will be converted to null and datetime objects will be converted to UNIX timestamps.","title":"to_json"},{"location":"reference/hielen2/datalink_prova_df/#parameters_148","text":"path_or_buf : str or file handle, optional File path or object. If not specified, the result is returned as a string. orient : str Indication of expected JSON string format. * Series : - default is 'index' - allowed values are : { 'split' , 'records' , 'index' , 'table' } . * DataFrame : - default is 'columns' - allowed values are : { 'split' , 'records' , 'index' , 'columns' , 'values' , 'table' } . * The format of the JSON string : - 'split' : dict like { 'index' -> [ index ] , 'columns' -> [ columns ] , 'data' -> [ values ] } - 'records' : list like [ {column -> value}, ... , {column -> value} ] - 'index' : dict like { index -> { column -> value }} - 'columns' : dict like { column -> { index -> value }} - 'values' : just the values array - 'table' : dict like { 'schema' : { schema } , 'data' : { data }} Describing the data , where data component is like `` orient = 'records' `` . .. versionchanged : : 0.20.0 date_format : {None, 'epoch', 'iso'} Type of date conversion. 'epoch' = epoch milliseconds, 'iso' = ISO8601. The default depends on the orient . For orient='table' , the default is 'iso'. For all other orients, the default is 'epoch'. double_precision : int, default 10 The number of decimal places to use when encoding floating point values. force_ascii : bool, default True Force encoded string to be ASCII. date_unit : str, default 'ms' (milliseconds) The time unit to encode to, governs timestamp and ISO8601 precision. One of 's', 'ms', 'us', 'ns' for second, millisecond, microsecond, and nanosecond respectively. default_handler : callable, default None Handler to call if object cannot otherwise be converted to a suitable format for JSON. Should receive a single argument which is the object to convert and return a serialisable object. lines : bool, default False If 'orient' is 'records' write out line delimited json format. Will throw ValueError if incorrect 'orient' since others are not list like. compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None} A string representing the compression to use in the output file , only used when the first argument is a filename . By default , the compression is inferred from the filename . .. versionchanged :: 0 . 24 . 0 'infer' option added and set to default index : bool, default True Whether to include the index values in the JSON string. Not including the index ( index=False ) is only supported when orient is 'split' or 'table'. .. versionadded :: 0.23.0 indent : int, optional Length of whitespace used to indent each record. .. versionadded:: 1.0.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_159","text":"None or str If path_or_buf is None, returns the resulting json format as a string. Otherwise returns None.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_146","text":"read_json : Convert a JSON string to pandas object.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_73","text":"The behavior of indent=0 varies from the stdlib, which does not indent the output but does insert newlines. Currently, indent=0 and the default indent=None are equivalent in pandas, though this may change in a future release.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_147","text":"import json df = pd.DataFrame( ... [[\"a\", \"b\"], [\"c\", \"d\"]], ... index=[\"row 1\", \"row 2\"], ... columns=[\"col 1\", \"col 2\"], ... ) result = df.to_json(orient=\"split\") parsed = json.loads(result) json.dumps(parsed, indent=4) # doctest: +SKIP { \"columns\": [ \"col 1\", \"col 2\" ], \"index\": [ \"row 1\", \"row 2\" ], \"data\": [ [ \"a\", \"b\" ], [ \"c\", \"d\" ] ] } Encoding/decoding a Dataframe using 'records' formatted JSON. Note that index labels are not preserved with this encoding. result = df.to_json(orient=\"records\") parsed = json.loads(result) json.dumps(parsed, indent=4) # doctest: +SKIP [ { \"col 1\": \"a\", \"col 2\": \"b\" }, { \"col 1\": \"c\", \"col 2\": \"d\" } ] Encoding/decoding a Dataframe using 'index' formatted JSON: result = df.to_json(orient=\"index\") parsed = json.loads(result) json.dumps(parsed, indent=4) # doctest: +SKIP { \"row 1\": { \"col 1\": \"a\", \"col 2\": \"b\" }, \"row 2\": { \"col 1\": \"c\", \"col 2\": \"d\" } } Encoding/decoding a Dataframe using 'columns' formatted JSON: result = df.to_json(orient=\"columns\") parsed = json.loads(result) json.dumps(parsed, indent=4) # doctest: +SKIP { \"col 1\": { \"row 1\": \"a\", \"row 2\": \"c\" }, \"col 2\": { \"row 1\": \"b\", \"row 2\": \"d\" } } Encoding/decoding a Dataframe using 'values' formatted JSON: result = df.to_json(orient=\"values\") parsed = json.loads(result) json.dumps(parsed, indent=4) # doctest: +SKIP [ [ \"a\", \"b\" ], [ \"c\", \"d\" ] ] Encoding with Table Schema: result = df.to_json(orient=\"table\") parsed = json.loads(result) json.dumps(parsed, indent=4) # doctest: +SKIP { \"schema\": { \"fields\": [ { \"name\": \"index\", \"type\": \"string\" }, { \"name\": \"col 1\", \"type\": \"string\" }, { \"name\": \"col 2\", \"type\": \"string\" } ], \"primaryKey\": [ \"index\" ], \"pandas_version\": \"0.20.0\" }, \"data\": [ { \"index\": \"row 1\", \"col 1\": \"a\", \"col 2\": \"b\" }, { \"index\": \"row 2\", \"col 1\": \"c\", \"col 2\": \"d\" } ] } View Source def to_json ( self , path_or_buf : Optional [ FilePathOrBuffer ] = None , orient : Optional [ str ] = None , date_format : Optional [ str ] = None , double_precision : int = 10 , force_ascii : bool_t = True , date_unit : str = \"ms\" , default_handler : Optional [ Callable[[Any ] , JSONSerializable ]] = None , lines : bool_t = False , compression : Optional [ str ] = \"infer\" , index : bool_t = True , indent : Optional [ int ] = None , ) -> Optional [ str ] : \"\"\" Convert the object to a JSON string. Note NaN's and None will be converted to null and datetime objects will be converted to UNIX timestamps. Parameters ---------- path_or_buf : str or file handle, optional File path or object. If not specified, the result is returned as a string. orient : str Indication of expected JSON string format. * Series: - default is 'index' - allowed values are: {'split','records','index','table'}. * DataFrame: - default is 'columns' - allowed values are: {'split', 'records', 'index', 'columns', 'values', 'table'}. * The format of the JSON string: - 'split' : dict like {'index' -> [index], 'columns' -> [columns], 'data' -> [values]} - 'records' : list like [{column -> value}, ... , {column -> value}] - 'index' : dict like {index -> {column -> value}} - 'columns' : dict like {column -> {index -> value}} - 'values' : just the values array - 'table' : dict like {'schema': {schema}, 'data': {data}} Describing the data, where data component is like ``orient='records'``. .. versionchanged:: 0.20.0 date_format : {None, 'epoch', 'iso'} Type of date conversion. 'epoch' = epoch milliseconds, 'iso' = ISO8601. The default depends on the `orient`. For ``orient='table'``, the default is 'iso'. For all other orients, the default is 'epoch'. double_precision : int, default 10 The number of decimal places to use when encoding floating point values. force_ascii : bool, default True Force encoded string to be ASCII. date_unit : str, default 'ms' (milliseconds) The time unit to encode to, governs timestamp and ISO8601 precision. One of 's', 'ms', 'us', 'ns' for second, millisecond, microsecond, and nanosecond respectively. default_handler : callable, default None Handler to call if object cannot otherwise be converted to a suitable format for JSON. Should receive a single argument which is the object to convert and return a serialisable object. lines : bool, default False If 'orient' is 'records' write out line delimited json format. Will throw ValueError if incorrect 'orient' since others are not list like. compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None} A string representing the compression to use in the output file, only used when the first argument is a filename. By default, the compression is inferred from the filename. .. versionchanged:: 0.24.0 'infer' option added and set to default index : bool, default True Whether to include the index values in the JSON string. Not including the index (``index=False``) is only supported when orient is 'split' or 'table'. .. versionadded:: 0.23.0 indent : int, optional Length of whitespace used to indent each record. .. versionadded:: 1.0.0 Returns ------- None or str If path_or_buf is None, returns the resulting json format as a string. Otherwise returns None. See Also -------- read_json : Convert a JSON string to pandas object. Notes ----- The behavior of ``indent=0`` varies from the stdlib, which does not indent the output but does insert newlines. Currently, ``indent=0`` and the default ``indent=None`` are equivalent in pandas, though this may change in a future release. Examples -------- >>> import json >>> df = pd.DataFrame( ... [[\" a \", \" b \"], [\" c \", \" d \"]], ... index=[\" row 1 \", \" row 2 \"], ... columns=[\" col 1 \", \" col 2 \"], ... ) >>> result = df.to_json(orient=\" split \") >>> parsed = json.loads(result) >>> json.dumps(parsed, indent=4) # doctest: +SKIP { \" columns \": [ \" col 1 \", \" col 2 \" ], \" index \": [ \" row 1 \", \" row 2 \" ], \" data \": [ [ \" a \", \" b \" ], [ \" c \", \" d \" ] ] } Encoding/decoding a Dataframe using ``'records'`` formatted JSON. Note that index labels are not preserved with this encoding. >>> result = df.to_json(orient=\" records \") >>> parsed = json.loads(result) >>> json.dumps(parsed, indent=4) # doctest: +SKIP [ { \" col 1 \": \" a \", \" col 2 \": \" b \" }, { \" col 1 \": \" c \", \" col 2 \": \" d \" } ] Encoding/decoding a Dataframe using ``'index'`` formatted JSON: >>> result = df.to_json(orient=\" index \") >>> parsed = json.loads(result) >>> json.dumps(parsed, indent=4) # doctest: +SKIP { \" row 1 \": { \" col 1 \": \" a \", \" col 2 \": \" b \" }, \" row 2 \": { \" col 1 \": \" c \", \" col 2 \": \" d \" } } Encoding/decoding a Dataframe using ``'columns'`` formatted JSON: >>> result = df.to_json(orient=\" columns \") >>> parsed = json.loads(result) >>> json.dumps(parsed, indent=4) # doctest: +SKIP { \" col 1 \": { \" row 1 \": \" a \", \" row 2 \": \" c \" }, \" col 2 \": { \" row 1 \": \" b \", \" row 2 \": \" d \" } } Encoding/decoding a Dataframe using ``'values'`` formatted JSON: >>> result = df.to_json(orient=\" values \") >>> parsed = json.loads(result) >>> json.dumps(parsed, indent=4) # doctest: +SKIP [ [ \" a \", \" b \" ], [ \" c \", \" d \" ] ] Encoding with Table Schema: >>> result = df.to_json(orient=\" table \") >>> parsed = json.loads(result) >>> json.dumps(parsed, indent=4) # doctest: +SKIP { \" schema \": { \" fields \": [ { \" name \": \" index \", \" type \": \" string \" }, { \" name \": \" col 1 \", \" type \": \" string \" }, { \" name \": \" col 2 \", \" type \": \" string \" } ], \" primaryKey \": [ \" index \" ], \" pandas_version \": \" 0.20.0 \" }, \" data \": [ { \" index \": \" row 1 \", \" col 1 \": \" a \", \" col 2 \": \" b \" }, { \" index \": \" row 2 \", \" col 1 \": \" c \", \" col 2 \": \" d \" } ] } \"\"\" from pandas . io import json if date_format is None and orient == \"table\" : date_format = \"iso\" elif date_format is None : date_format = \"epoch\" config . is_nonnegative_int ( indent ) indent = indent or 0 return json . to_json ( path_or_buf = path_or_buf , obj = self , orient = orient , date_format = date_format , double_precision = double_precision , force_ascii = force_ascii , date_unit = date_unit , default_handler = default_handler , lines = lines , compression = compression , index = index , indent = indent , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_latex","text":"def to_latex ( self , buf = None , columns = None , col_space = None , header = True , index = True , na_rep = 'NaN' , formatters = None , float_format = None , sparsify = None , index_names = True , bold_rows = False , column_format = None , longtable = None , escape = None , encoding = None , decimal = '.' , multicolumn = None , multicolumn_format = None , multirow = None , caption = None , label = None ) Render object to a LaTeX tabular, longtable, or nested table/tabular. Requires \\usepackage{booktabs} . The output can be copy/pasted into a main LaTeX document or read from an external file with \\input{table.tex} . .. versionchanged:: 0.20.2 Added to Series. .. versionchanged:: 1.0.0 Added caption and label arguments.","title":"to_latex"},{"location":"reference/hielen2/datalink_prova_df/#parameters_149","text":"buf : str, Path or StringIO-like, optional, default None Buffer to write to. If None, the output is returned as a string. columns : list of label, optional The subset of columns to write. Writes all columns by default. col_space : int, optional The minimum width of each column. header : bool or list of str, default True Write out the column names. If a list of strings is given, it is assumed to be aliases for the column names. index : bool, default True Write row names (index). na_rep : str, default 'NaN' Missing data representation. formatters : list of functions or dict of {str: function}, optional Formatter functions to apply to columns' elements by position or name. The result of each function must be a unicode string. List must be of length equal to the number of columns. float_format : one-parameter function or str, optional, default None Formatter for floating point numbers. For example float_format=\"%.2f\" and float_format=\"{:0.2f}\".format will both result in 0.1234 being formatted as 0.12. sparsify : bool, optional Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row. By default, the value will be read from the config module. index_names : bool, default True Prints the names of the indexes. bold_rows : bool, default False Make the row labels bold in the output. column_format : str, optional The columns format as specified in LaTeX table format <https://en.wikibooks.org/wiki/LaTeX/Tables> __ e.g. 'rcl' for 3 columns. By default, 'l' will be used for all columns except columns of numbers, which default to 'r'. longtable : bool, optional By default, the value will be read from the pandas config module. Use a longtable environment instead of tabular. Requires adding a \\usepackage{longtable} to your LaTeX preamble. escape : bool, optional By default, the value will be read from the pandas config module. When set to False prevents from escaping latex special characters in column names. encoding : str, optional A string representing the encoding to use in the output file, defaults to 'utf-8'. decimal : str, default '.' Character recognized as decimal separator, e.g. ',' in Europe. multicolumn : bool, default True Use \\multicolumn to enhance MultiIndex columns. The default will be read from the config module. multicolumn_format : str, default 'l' The alignment for multicolumns, similar to column_format The default will be read from the config module. multirow : bool, default False Use \\multirow to enhance MultiIndex rows. Requires adding a \\usepackage{multirow} to your LaTeX preamble. Will print centered labels (instead of top-aligned) across the contained rows, separating groups via clines. The default will be read from the pandas config module. caption : str, optional The LaTeX caption to be placed inside \\caption{} in the output. .. versionadded :: 1.0.0 label : str, optional The LaTeX label to be placed inside \\label{} in the output. This is used with \\ref{} in the main .tex file. .. versionadded :: 1.0.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_160","text":"str or None If buf is None, returns the result as a string. Otherwise returns None.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_147","text":"DataFrame.to_string : Render a DataFrame to a console-friendly tabular output. DataFrame.to_html : Render a DataFrame as an HTML table.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_148","text":"df = pd.DataFrame({'name': ['Raphael', 'Donatello'], ... 'mask': ['red', 'purple'], ... 'weapon': ['sai', 'bo staff']}) print(df.to_latex(index=False)) # doctest: +NORMALIZE_WHITESPACE \\begin{tabular}{lll} \\toprule name & mask & weapon \\ \\midrule Raphael & red & sai \\ Donatello & purple & bo staff \\ \\bottomrule \\end{tabular} View Source @ Substitution ( returns = fmt . return_docstring ) def to_latex ( self , buf = None , columns = None , col_space = None , header = True , index = True , na_rep = \"NaN\" , formatters = None , float_format = None , sparsify = None , index_names = True , bold_rows = False , column_format = None , longtable = None , escape = None , encoding = None , decimal = \".\" , multicolumn = None , multicolumn_format = None , multirow = None , caption = None , label = None , ): r \"\"\" Render object to a LaTeX tabular, longtable, or nested table/tabular. Requires ``\\usepackage{booktabs}``. The output can be copy/pasted into a main LaTeX document or read from an external file with ``\\input{table.tex}``. .. versionchanged:: 0.20.2 Added to Series. .. versionchanged:: 1.0.0 Added caption and label arguments. Parameters ---------- buf : str, Path or StringIO-like, optional, default None Buffer to write to. If None, the output is returned as a string. columns : list of label, optional The subset of columns to write. Writes all columns by default. col_space : int, optional The minimum width of each column. header : bool or list of str, default True Write out the column names. If a list of strings is given, it is assumed to be aliases for the column names. index : bool, default True Write row names (index). na_rep : str, default 'NaN' Missing data representation. formatters : list of functions or dict of {str: function}, optional Formatter functions to apply to columns' elements by position or name. The result of each function must be a unicode string. List must be of length equal to the number of columns. float_format : one-parameter function or str, optional, default None Formatter for floating point numbers. For example ``float_format=\" %% . 2 f \"`` and ``float_format=\" { : 0 . 2 f } \".format`` will both result in 0.1234 being formatted as 0.12. sparsify : bool, optional Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row. By default, the value will be read from the config module. index_names : bool, default True Prints the names of the indexes. bold_rows : bool, default False Make the row labels bold in the output. column_format : str, optional The columns format as specified in `LaTeX table format <https://en.wikibooks.org/wiki/LaTeX/Tables>`__ e.g. 'rcl' for 3 columns. By default, 'l' will be used for all columns except columns of numbers, which default to 'r'. longtable : bool, optional By default, the value will be read from the pandas config module. Use a longtable environment instead of tabular. Requires adding a \\usepackage{longtable} to your LaTeX preamble. escape : bool, optional By default, the value will be read from the pandas config module. When set to False prevents from escaping latex special characters in column names. encoding : str, optional A string representing the encoding to use in the output file, defaults to 'utf-8'. decimal : str, default '.' Character recognized as decimal separator, e.g. ',' in Europe. multicolumn : bool, default True Use \\multicolumn to enhance MultiIndex columns. The default will be read from the config module. multicolumn_format : str, default 'l' The alignment for multicolumns, similar to `column_format` The default will be read from the config module. multirow : bool, default False Use \\multirow to enhance MultiIndex rows. Requires adding a \\usepackage{multirow} to your LaTeX preamble. Will print centered labels (instead of top-aligned) across the contained rows, separating groups via clines. The default will be read from the pandas config module. caption : str, optional The LaTeX caption to be placed inside ``\\caption{}`` in the output. .. versionadded:: 1.0.0 label : str, optional The LaTeX label to be placed inside ``\\label{}`` in the output. This is used with ``\\ref{}`` in the main ``.tex`` file. .. versionadded:: 1.0.0 %(returns)s See Also -------- DataFrame.to_string : Render a DataFrame to a console-friendly tabular output. DataFrame.to_html : Render a DataFrame as an HTML table. Examples -------- >>> df = pd.DataFrame({'name': ['Raphael', 'Donatello'], ... 'mask': ['red', 'purple'], ... 'weapon': ['sai', 'bo staff']}) >>> print(df.to_latex(index=False)) # doctest: +NORMALIZE_WHITESPACE \\begin{tabular}{lll} \\toprule name & mask & weapon \\\\ \\midrule Raphael & red & sai \\\\ Donatello & purple & bo staff \\\\ \\bottomrule \\end{tabular} \"\"\" # Get defaults from the pandas config if self . ndim == 1 : self = self . to_frame () if longtable is None : longtable = config . get_option ( \"display.latex.longtable\" ) if escape is None : escape = config . get_option ( \"display.latex.escape\" ) if multicolumn is None : multicolumn = config . get_option ( \"display.latex.multicolumn\" ) if multicolumn_format is None : multicolumn_format = config . get_option ( \"display.latex.multicolumn_format\" ) if multirow is None : multirow = config . get_option ( \"display.latex.multirow\" ) formatter = DataFrameFormatter ( self , columns = columns , col_space = col_space , na_rep = na_rep , header = header , index = index , formatters = formatters , float_format = float_format , bold_rows = bold_rows , sparsify = sparsify , index_names = index_names , escape = escape , decimal = decimal , ) return formatter . to_latex ( buf = buf , column_format = column_format , longtable = longtable , encoding = encoding , multicolumn = multicolumn , multicolumn_format = multicolumn_format , multirow = multirow , caption = caption , label = label , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_markdown","text":"def to_markdown ( self , buf : Union [ IO [ str ], NoneType ] = None , mode : Union [ str , NoneType ] = None , index : bool = True , ** kwargs ) -> Union [ str , NoneType ] Print DataFrame in Markdown-friendly format. .. versionadded:: 1.0.0","title":"to_markdown"},{"location":"reference/hielen2/datalink_prova_df/#parameters_150","text":"buf : str, Path or StringIO-like, optional, default None Buffer to write to. If None, the output is returned as a string. mode : str, optional Mode in which file is opened. index : bool, optional, default True Add index (row) labels. .. versionadded :: 1.1.0 **kwargs These parameters will be passed to tabulate <https://pypi.org/project/tabulate> _.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_161","text":"str DataFrame in Markdown-friendly format.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#examples_149","text":"s = pd.Series([\"elk\", \"pig\", \"dog\", \"quetzal\"], name=\"animal\") print(s.to_markdown()) | | animal | |---:|:---------| | 0 | elk | | 1 | pig | | 2 | dog | | 3 | quetzal | Output markdown with a tabulate option. print(s.to_markdown(tablefmt=\"grid\")) +----+----------+ | | animal | +====+==========+ | 0 | elk | +----+----------+ | 1 | pig | +----+----------+ | 2 | dog | +----+----------+ | 3 | quetzal | +----+----------+ View Source @ doc ( Series . to_markdown , klass= _ shared_doc_kwargs [ \"klass\" ], examples= \"\"\"Examples -------- >>> df = pd.DataFrame( ... data={\" animal_1 \": [\" elk \", \" pig \"], \" animal_2 \": [\" dog \", \" quetzal \"]} ... ) >>> print(df.to_markdown()) | | animal_1 | animal_2 | |---:|:-----------|:-----------| | 0 | elk | dog | | 1 | pig | quetzal | Output markdown with a tabulate option. >>> print(df.to_markdown(tablefmt=\" grid \")) +----+------------+------------+ | | animal_1 | animal_2 | +====+============+============+ | 0 | elk | dog | +----+------------+------------+ | 1 | pig | quetzal | +----+------------+------------+ \"\"\" , ) def to_markdown ( self , buf : Optional [ IO [ str ]] = None , mode : Optional [ str ] = None , index : bool = True , **kwargs , ) -> Optional [ str ] : if \"showindex\" in kwargs : warnings . warn ( \"'showindex' is deprecated. Only 'index' will be used \" \"in a future version. Use 'index' to silence this warning.\" , FutureWarning , stacklevel = 2 , ) kwargs . setdefault ( \"headers\" , \"keys\" ) kwargs . setdefault ( \"tablefmt\" , \"pipe\" ) kwargs . setdefault ( \"showindex\" , index ) tabulate = import_optional_dependency ( \"tabulate\" ) result = tabulate . tabulate ( self , **kwargs ) if buf is None : return result buf , _ , _ , _ = get_filepath_or_buffer ( buf , mode = mode ) assert buf is not None # Help mypy . buf . writelines ( result ) return None","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_numpy","text":"def to_numpy ( self , dtype = None , copy : bool = False , na_value =< object object at 0x7fb7e907ade0 > ) -> numpy . ndarray Convert the DataFrame to a NumPy array. .. versionadded:: 0.24.0 By default, the dtype of the returned array will be the common NumPy dtype of all types in the DataFrame. For example, if the dtypes are float16 and float32 , the results dtype will be float32 . This may require copying data and coercing values, which may be expensive.","title":"to_numpy"},{"location":"reference/hielen2/datalink_prova_df/#parameters_151","text":"dtype : str or numpy.dtype, optional The dtype to pass to :meth: numpy.asarray . copy : bool, default False Whether to ensure that the returned value is not a view on another array. Note that copy=False does not ensure that to_numpy() is no-copy. Rather, copy=True ensure that a copy is made, even if not strictly necessary. na_value : Any, optional The value to use for missing values. The default value depends on dtype and the dtypes of the DataFrame columns. .. versionadded :: 1.1.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_162","text":"numpy.ndarray","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_148","text":"Series.to_numpy : Similar method for Series.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_150","text":"pd.DataFrame({\"A\": [1, 2], \"B\": [3, 4]}).to_numpy() array([[1, 3], [2, 4]]) With heterogeneous data, the lowest common type will have to be used. df = pd.DataFrame({\"A\": [1, 2], \"B\": [3.0, 4.5]}) df.to_numpy() array([[1. , 3. ], [2. , 4.5]]) For a mix of numeric and non-numeric types, the output array will have object dtype. df['C'] = pd.date_range('2000', periods=2) df.to_numpy() array([[1, 3.0, Timestamp('2000-01-01 00:00:00')], [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object) View Source def to_numpy ( self , dtype = None , copy : bool = False , na_value = lib . no_default ) -> np . ndarray : \"\"\" Convert the DataFrame to a NumPy array. .. versionadded:: 0.24.0 By default, the dtype of the returned array will be the common NumPy dtype of all types in the DataFrame. For example, if the dtypes are ``float16`` and ``float32``, the results dtype will be ``float32``. This may require copying data and coercing values, which may be expensive. Parameters ---------- dtype : str or numpy.dtype, optional The dtype to pass to :meth:`numpy.asarray`. copy : bool, default False Whether to ensure that the returned value is not a view on another array. Note that ``copy=False`` does not *ensure* that ``to_numpy()`` is no-copy. Rather, ``copy=True`` ensure that a copy is made, even if not strictly necessary. na_value : Any, optional The value to use for missing values. The default value depends on `dtype` and the dtypes of the DataFrame columns. .. versionadded:: 1.1.0 Returns ------- numpy.ndarray See Also -------- Series.to_numpy : Similar method for Series. Examples -------- >>> pd.DataFrame({\" A \": [1, 2], \" B \": [3, 4]}).to_numpy() array([[1, 3], [2, 4]]) With heterogeneous data, the lowest common type will have to be used. >>> df = pd.DataFrame({\" A \": [1, 2], \" B \": [3.0, 4.5]}) >>> df.to_numpy() array([[1. , 3. ], [2. , 4.5]]) For a mix of numeric and non-numeric types, the output array will have object dtype. >>> df['C'] = pd.date_range('2000', periods=2) >>> df.to_numpy() array([[1, 3.0, Timestamp('2000-01-01 00:00:00')], [2, 4.5, Timestamp('2000-01-02 00:00:00')]], dtype=object) \"\"\" self . _consolidate_inplace () result = self . _mgr . as_array ( transpose = self . _AXIS_REVERSED , dtype = dtype , copy = copy , na_value = na_value ) if result . dtype is not dtype : result = np . array ( result , dtype = dtype , copy = False ) return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_parquet","text":"def to_parquet ( self , path : Union [ str , pathlib . Path , IO [ ~ AnyStr ]], engine : str = 'auto' , compression : Union [ str , NoneType ] = 'snappy' , index : Union [ bool , NoneType ] = None , partition_cols : Union [ List [ str ], NoneType ] = None , ** kwargs ) -> None Write a DataFrame to the binary parquet format. This function writes the dataframe as a parquet file <https://parquet.apache.org/> _. You can choose different parquet backends, and have the option of compression. See :ref: the user guide <io.parquet> for more details.","title":"to_parquet"},{"location":"reference/hielen2/datalink_prova_df/#parameters_152","text":"path : str or file-like object If a string, it will be used as Root Directory path when writing a partitioned dataset. By file-like object, we refer to objects with a write() method, such as a file handler (e.g. via builtin open function) or io.BytesIO. The engine fastparquet does not accept file-like objects. .. versionchanged :: 1.0.0 Previously this was \"fname\" engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto' Parquet library to use. If 'auto', then the option io.parquet.engine is used. The default io.parquet.engine behavior is to try 'pyarrow', falling back to 'fastparquet' if 'pyarrow' is unavailable. compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy' Name of the compression to use. Use None for no compression. index : bool, default None If True , include the dataframe's index(es) in the file output. If False , they will not be written to the file. If None , similar to True the dataframe's index(es) will be saved. However, instead of being saved as values, the RangeIndex will be stored as a range in the metadata so it doesn't require much space and is faster. Other indexes will be included as columns in the file output. .. versionadded :: 0.24.0 partition_cols : list, optional, default None Column names by which to partition the dataset. Columns are partitioned in the order they are given. Must be None if path is not a string. .. versionadded :: 0.24.0 **kwargs Additional arguments passed to the parquet library. See :ref: pandas io <io.parquet> for more details.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#see-also_149","text":"read_parquet : Read a parquet file. DataFrame.to_csv : Write a csv file. DataFrame.to_sql : Write to a sql table. DataFrame.to_hdf : Write to hdf.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_74","text":"This function requires either the fastparquet <https://pypi.org/project/fastparquet> or pyarrow <https://arrow.apache.org/docs/python/> library.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_151","text":"df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]}) df.to_parquet('df.parquet.gzip', ... compression='gzip') # doctest: +SKIP pd.read_parquet('df.parquet.gzip') # doctest: +SKIP col1 col2 0 1 3 1 2 4 If you want to get a buffer to the parquet content you can use a io.BytesIO object, as long as you don't use partition_cols, which creates multiple files. import io f = io.BytesIO() df.to_parquet(f) f.seek(0) 0 content = f.read() View Source @ deprecate_kwarg ( old_arg_name = \"fname\" , new_arg_name = \"path\" ) def to_parquet ( self , path : FilePathOrBuffer [ AnyStr ], engine : str = \"auto\" , compression : Optional [ str ] = \"snappy\" , index : Optional [ bool ] = None , partition_cols : Optional [ List [ str ]] = None , ** kwargs , ) -> None : \"\"\" Write a DataFrame to the binary parquet format. This function writes the dataframe as a `parquet file <https://parquet.apache.org/>`_. You can choose different parquet backends, and have the option of compression. See :ref:`the user guide <io.parquet>` for more details. Parameters ---------- path : str or file-like object If a string, it will be used as Root Directory path when writing a partitioned dataset. By file-like object, we refer to objects with a write() method, such as a file handler (e.g. via builtin open function) or io.BytesIO. The engine fastparquet does not accept file-like objects. .. versionchanged:: 1.0.0 Previously this was \" fname \" engine : {'auto', 'pyarrow', 'fastparquet'}, default 'auto' Parquet library to use. If 'auto', then the option ``io.parquet.engine`` is used. The default ``io.parquet.engine`` behavior is to try 'pyarrow', falling back to 'fastparquet' if 'pyarrow' is unavailable. compression : {'snappy', 'gzip', 'brotli', None}, default 'snappy' Name of the compression to use. Use ``None`` for no compression. index : bool, default None If ``True``, include the dataframe's index(es) in the file output. If ``False``, they will not be written to the file. If ``None``, similar to ``True`` the dataframe's index(es) will be saved. However, instead of being saved as values, the RangeIndex will be stored as a range in the metadata so it doesn't require much space and is faster. Other indexes will be included as columns in the file output. .. versionadded:: 0.24.0 partition_cols : list, optional, default None Column names by which to partition the dataset. Columns are partitioned in the order they are given. Must be None if path is not a string. .. versionadded:: 0.24.0 **kwargs Additional arguments passed to the parquet library. See :ref:`pandas io <io.parquet>` for more details. See Also -------- read_parquet : Read a parquet file. DataFrame.to_csv : Write a csv file. DataFrame.to_sql : Write to a sql table. DataFrame.to_hdf : Write to hdf. Notes ----- This function requires either the `fastparquet <https://pypi.org/project/fastparquet>`_ or `pyarrow <https://arrow.apache.org/docs/python/>`_ library. Examples -------- >>> df = pd.DataFrame(data={'col1': [1, 2], 'col2': [3, 4]}) >>> df.to_parquet('df.parquet.gzip', ... compression='gzip') # doctest: +SKIP >>> pd.read_parquet('df.parquet.gzip') # doctest: +SKIP col1 col2 0 1 3 1 2 4 If you want to get a buffer to the parquet content you can use a io.BytesIO object, as long as you don't use partition_cols, which creates multiple files. >>> import io >>> f = io.BytesIO() >>> df.to_parquet(f) >>> f.seek(0) 0 >>> content = f.read() \"\"\" from pandas . io . parquet import to_parquet to_parquet ( self , path , engine , compression = compression , index = index , partition_cols = partition_cols , ** kwargs , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_period","text":"def to_period ( self , freq = None , axis : Union [ str , int ] = 0 , copy : bool = True ) -> 'DataFrame' Convert DataFrame from DatetimeIndex to PeriodIndex. Convert DataFrame from DatetimeIndex to PeriodIndex with desired frequency (inferred from index if not passed).","title":"to_period"},{"location":"reference/hielen2/datalink_prova_df/#parameters_153","text":"freq : str, default Frequency of the PeriodIndex. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to convert (the index by default). copy : bool, default True If False then underlying input data is not copied.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_163","text":"DataFrame with PeriodIndex View Source def to_period ( self , freq = None , axis : Axis = 0 , copy : bool = True ) -> \"DataFrame\" : \"\"\" Convert DataFrame from DatetimeIndex to PeriodIndex. Convert DataFrame from DatetimeIndex to PeriodIndex with desired frequency (inferred from index if not passed). Parameters ---------- freq : str, default Frequency of the PeriodIndex. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to convert (the index by default). copy : bool, default True If False then underlying input data is not copied. Returns ------- DataFrame with PeriodIndex \"\"\" new_obj = self . copy ( deep = copy ) axis_name = self . _get_axis_name ( axis ) old_ax = getattr ( self , axis_name ) new_ax = old_ax . to_period ( freq = freq ) setattr ( new_obj , axis_name , new_ax ) return new_obj","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#to_pickle","text":"def to_pickle ( self , path , compression : Union [ str , NoneType ] = 'infer' , protocol : int = 5 ) -> None Pickle (serialize) object to file.","title":"to_pickle"},{"location":"reference/hielen2/datalink_prova_df/#parameters_154","text":"path : str File path where the pickled object will be stored. compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, default 'infer' A string representing the compression to use in the output file. By default, infers from the file extension in specified path. protocol : int Int which indicates which protocol should be used by the pickler, default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible values are 0, 1, 2, 3, 4. A negative value for the protocol parameter is equivalent to setting its value to HIGHEST_PROTOCOL. .. [1] https://docs.python.org/3/library/pickle.html.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#see-also_150","text":"read_pickle : Load pickled pandas object (or any object) from file. DataFrame.to_hdf : Write DataFrame to an HDF5 file. DataFrame.to_sql : Write DataFrame to a SQL database. DataFrame.to_parquet : Write a DataFrame to the binary parquet format.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_152","text":"original_df = pd.DataFrame({\"foo\": range(5), \"bar\": range(5, 10)}) original_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 original_df.to_pickle(\"./dummy.pkl\") unpickled_df = pd.read_pickle(\"./dummy.pkl\") unpickled_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 import os os.remove(\"./dummy.pkl\") View Source def to_pickle ( self , path , compression : Optional [ str ] = \"infer\" , protocol : int = pickle . HIGHEST_PROTOCOL , ) -> None : \"\"\" Pickle (serialize) object to file. Parameters ---------- path : str File path where the pickled object will be stored. compression : {'infer', 'gzip', 'bz2', 'zip', 'xz', None}, \\ default 'infer' A string representing the compression to use in the output file. By default, infers from the file extension in specified path. protocol : int Int which indicates which protocol should be used by the pickler, default HIGHEST_PROTOCOL (see [1]_ paragraph 12.1.2). The possible values are 0, 1, 2, 3, 4. A negative value for the protocol parameter is equivalent to setting its value to HIGHEST_PROTOCOL. .. [1] https://docs.python.org/3/library/pickle.html. See Also -------- read_pickle : Load pickled pandas object (or any object) from file. DataFrame.to_hdf : Write DataFrame to an HDF5 file. DataFrame.to_sql : Write DataFrame to a SQL database. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. Examples -------- >>> original_df = pd.DataFrame({\" foo \": range(5), \" bar \": range(5, 10)}) >>> original_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> original_df.to_pickle(\" . / dummy . pkl \") >>> unpickled_df = pd.read_pickle(\" . / dummy . pkl \") >>> unpickled_df foo bar 0 0 5 1 1 6 2 2 7 3 3 8 4 4 9 >>> import os >>> os.remove(\" . / dummy . pkl \") \"\"\" from pandas . io . pickle import to_pickle to_pickle ( self , path , compression = compression , protocol = protocol )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_records","text":"def to_records ( self , index = True , column_dtypes = None , index_dtypes = None ) -> numpy . recarray Convert DataFrame to a NumPy record array. Index will be included as the first field of the record array if requested.","title":"to_records"},{"location":"reference/hielen2/datalink_prova_df/#parameters_155","text":"index : bool, default True Include index in resulting record array, stored in 'index' field or using the index label, if set. column_dtypes : str, type, dict, default None .. versionadded:: 0.24.0 If a string or type, the data type to store all columns. If a dictionary, a mapping of column names and indices (zero-indexed) to specific data types. index_dtypes : str, type, dict, default None .. versionadded:: 0.24.0 If a string or type , the data type to store all index levels . If a dictionary , a mapping of index level names and indices ( zero - indexed ) to specific data types . This mapping is applied only if ` index = True ` .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_164","text":"numpy.recarray NumPy ndarray with the DataFrame labels as fields and each row of the DataFrame as entries.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_151","text":"DataFrame.from_records: Convert structured or record ndarray to DataFrame. numpy.recarray: An ndarray that allows field access using attributes, analogous to typed columns in a spreadsheet.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_153","text":"df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]}, ... index=['a', 'b']) df A B a 1 0.50 b 2 0.75 df.to_records() rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)], dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')]) If the DataFrame index has no label then the recarray field name is set to 'index'. If the index has a label then this is used as the field name: df.index = df.index.rename(\"I\") df.to_records() rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)], dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')]) The index can be excluded from the record array: df.to_records(index=False) rec.array([(1, 0.5 ), (2, 0.75)], dtype=[('A', '<i8'), ('B', '<f8')]) Data types can be specified for the columns: df.to_records(column_dtypes={\"A\": \"int32\"}) rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)], dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')]) As well as for the index: df.to_records(index_dtypes=\"<S2\") rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)], dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')]) index_dtypes = f\"<S{df.index.str.len().max()}\" df.to_records(index_dtypes=index_dtypes) rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)], dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')]) View Source def to_records ( self , index = True , column_dtypes = None , index_dtypes = None ) -> np . recarray : \"\"\" Convert DataFrame to a NumPy record array. Index will be included as the first field of the record array if requested. Parameters ---------- index : bool, default True Include index in resulting record array, stored in 'index' field or using the index label, if set. column_dtypes : str, type, dict, default None .. versionadded:: 0.24.0 If a string or type, the data type to store all columns. If a dictionary, a mapping of column names and indices (zero-indexed) to specific data types. index_dtypes : str, type, dict, default None .. versionadded:: 0.24.0 If a string or type, the data type to store all index levels. If a dictionary, a mapping of index level names and indices (zero-indexed) to specific data types. This mapping is applied only if `index=True`. Returns ------- numpy.recarray NumPy ndarray with the DataFrame labels as fields and each row of the DataFrame as entries. See Also -------- DataFrame.from_records: Convert structured or record ndarray to DataFrame. numpy.recarray: An ndarray that allows field access using attributes, analogous to typed columns in a spreadsheet. Examples -------- >>> df = pd.DataFrame({'A': [1, 2], 'B': [0.5, 0.75]}, ... index=['a', 'b']) >>> df A B a 1 0.50 b 2 0.75 >>> df.to_records() rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)], dtype=[('index', 'O'), ('A', '<i8'), ('B', '<f8')]) If the DataFrame index has no label then the recarray field name is set to 'index'. If the index has a label then this is used as the field name: >>> df.index = df.index.rename(\" I \") >>> df.to_records() rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)], dtype=[('I', 'O'), ('A', '<i8'), ('B', '<f8')]) The index can be excluded from the record array: >>> df.to_records(index=False) rec.array([(1, 0.5 ), (2, 0.75)], dtype=[('A', '<i8'), ('B', '<f8')]) Data types can be specified for the columns: >>> df.to_records(column_dtypes={\" A \": \" int32 \"}) rec.array([('a', 1, 0.5 ), ('b', 2, 0.75)], dtype=[('I', 'O'), ('A', '<i4'), ('B', '<f8')]) As well as for the index: >>> df.to_records(index_dtypes=\" < S2 \") rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)], dtype=[('I', 'S2'), ('A', '<i8'), ('B', '<f8')]) >>> index_dtypes = f\" < S { df . index . str . len (). max () } \" >>> df.to_records(index_dtypes=index_dtypes) rec.array([(b'a', 1, 0.5 ), (b'b', 2, 0.75)], dtype=[('I', 'S1'), ('A', '<i8'), ('B', '<f8')]) \"\"\" if index : if isinstance ( self . index , MultiIndex ) : # array of tuples to numpy cols . copy copy copy ix_vals = list ( map ( np . array , zip ( * self . index . _values ))) else : ix_vals = [ self.index.values ] arrays = ix_vals + [ np.asarray(self.iloc[:, i ] ) for i in range ( len ( self . columns )) ] count = 0 index_names = list ( self . index . names ) if isinstance ( self . index , MultiIndex ) : for i , n in enumerate ( index_names ) : if n is None : index_names [ i ] = f \"level_{count}\" count += 1 elif index_names [ 0 ] is None : index_names = [ \"index\" ] names = [ str(name) for name in itertools.chain(index_names, self.columns) ] else : arrays = [ np.asarray(self.iloc[:, i ] ) for i in range ( len ( self . columns )) ] names = [ str(c) for c in self.columns ] index_names = [] index_len = len ( index_names ) formats = [] for i , v in enumerate ( arrays ) : index = i # When the names and arrays are collected , we # first collect those in the DataFrame 's index, # followed by those in its columns. # # Thus, the total length of the array is: # len(index_names) + len(DataFrame.columns). # # This check allows us to see whether we are # handling a name / array in the index or column. if index < index_len: dtype_mapping = index_dtypes name = index_names[index] else: index -= index_len dtype_mapping = column_dtypes name = self.columns[index] # We have a dictionary, so we get the data type # associated with the index or column (which can # be denoted by its name in the DataFrame or its # position in DataFrame' s array of indices or # columns , whichever is applicable . if is_dict_like ( dtype_mapping ) : if name in dtype_mapping : dtype_mapping = dtype_mapping [ name ] elif index in dtype_mapping : dtype_mapping = dtype_mapping [ index ] else : dtype_mapping = None # If no mapping can be found , use the array ' s # dtype attribute for formatting . # # A valid dtype must either be a type or # string naming a type . if dtype_mapping is None : formats . append ( v . dtype ) elif isinstance ( dtype_mapping , ( type , np . dtype , str )) : formats . append ( dtype_mapping ) else : element = \"row\" if i < index_len else \"column\" msg = f \"Invalid dtype {dtype_mapping} specified for {element} {name}\" raise ValueError ( msg ) return np . rec . fromarrays ( arrays , dtype = { \"names\" : names , \"formats\" : formats } )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_sql","text":"def to_sql ( self , name : str , con , schema = None , if_exists : str = 'fail' , index : bool = True , index_label = None , chunksize = None , dtype = None , method = None ) -> None Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1]_ are supported. Tables can be newly created, appended to, or overwritten.","title":"to_sql"},{"location":"reference/hielen2/datalink_prova_df/#parameters_156","text":"name : str Name of SQL table. con : sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable See here <https://docs.sqlalchemy.org/en/13/core/connections.html> _. schema : str, optional Specify the schema (if database flavor supports this). If None, use default schema. if_exists : {'fail', 'replace', 'append'}, default 'fail' How to behave if the table already exists. * fail: Raise a ValueError. * replace: Drop the table before inserting new values. * append: Insert new values to the existing table. index : bool, default True Write DataFrame index as a column. Uses index_label as the column name in the table. index_label : str or sequence, default None Column label for index column(s). If None is given (default) and index is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksize : int, optional Specify the number of rows in each batch to be written at a time. By default, all rows will be written at once. dtype : dict or scalar, optional Specifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns. method : {None, 'multi', callable}, optional Controls the SQL insertion clause used: * None : Uses standard SQL `` INSERT `` clause ( one per row ). * 'multi' : Pass multiple values in a single `` INSERT `` clause . * callable with signature `` ( pd_table , conn , keys , data_iter ) `` . Details and a sample callable implementation can be found in the section : ref : `insert method <io.sql.method>` . .. versionadded :: 0 . 24 . 0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#raises_15","text":"ValueError When the table already exists and if_exists is 'fail' (the default).","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_152","text":"read_sql : Read a DataFrame from a table.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_75","text":"Timezone aware datetime columns will be written as Timestamp with timezone type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. .. versionadded:: 0.24.0","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#references","text":".. [1] https://docs.sqlalchemy.org .. [2] https://www.python.org/dev/peps/pep-0249/","title":"References"},{"location":"reference/hielen2/datalink_prova_df/#examples_154","text":"Create an in-memory SQLite database. from sqlalchemy import create_engine engine = create_engine('sqlite://', echo=False) Create a table from scratch with 3 rows. df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']}) df name 0 User 1 1 User 2 2 User 3 df.to_sql('users', con=engine) engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')] An sqlalchemy.engine.Connection can also be passed to to con : with engine.begin() as connection: ... df1 = pd.DataFrame({'name' : ['User 4', 'User 5']}) ... df1.to_sql('users', con=connection, if_exists='append') This is allowed to support operations that require that the same DBAPI connection is used for the entire operation. df2 = pd.DataFrame({'name' : ['User 6', 'User 7']}) df2.to_sql('users', con=engine, if_exists='append') engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'), (0, 'User 4'), (1, 'User 5'), (0, 'User 6'), (1, 'User 7')] Overwrite the table with just df2 . df2.to_sql('users', con=engine, if_exists='replace', ... index_label='id') engine.execute(\"SELECT * FROM users\").fetchall() [(0, 'User 6'), (1, 'User 7')] Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. df = pd.DataFrame({\"A\": [1, None, 2]}) df A 0 1.0 1 NaN 2 2.0 from sqlalchemy.types import Integer df.to_sql('integers', con=engine, index=False, ... dtype={\"A\": Integer()}) engine.execute(\"SELECT * FROM integers\").fetchall() [(1,), (None,), (2,)] View Source def to_sql ( self , name : str , con , schema = None , if_exists : str = \"fail\" , index : bool_t = True , index_label = None , chunksize = None , dtype = None , method = None , ) -> None : \"\"\" Write records stored in a DataFrame to a SQL database. Databases supported by SQLAlchemy [1]_ are supported. Tables can be newly created, appended to, or overwritten. Parameters ---------- name : str Name of SQL table. con : sqlalchemy.engine.(Engine or Connection) or sqlite3.Connection Using SQLAlchemy makes it possible to use any DB supported by that library. Legacy support is provided for sqlite3.Connection objects. The user is responsible for engine disposal and connection closure for the SQLAlchemy connectable See `here \\ <https://docs.sqlalchemy.org/en/13/core/connections.html>`_. schema : str, optional Specify the schema (if database flavor supports this). If None, use default schema. if_exists : {'fail', 'replace', 'append'}, default 'fail' How to behave if the table already exists. * fail: Raise a ValueError. * replace: Drop the table before inserting new values. * append: Insert new values to the existing table. index : bool, default True Write DataFrame index as a column. Uses `index_label` as the column name in the table. index_label : str or sequence, default None Column label for index column(s). If None is given (default) and `index` is True, then the index names are used. A sequence should be given if the DataFrame uses MultiIndex. chunksize : int, optional Specify the number of rows in each batch to be written at a time. By default, all rows will be written at once. dtype : dict or scalar, optional Specifying the datatype for columns. If a dictionary is used, the keys should be the column names and the values should be the SQLAlchemy types or strings for the sqlite3 legacy mode. If a scalar is provided, it will be applied to all columns. method : {None, 'multi', callable}, optional Controls the SQL insertion clause used: * None : Uses standard SQL ``INSERT`` clause (one per row). * 'multi': Pass multiple values in a single ``INSERT`` clause. * callable with signature ``(pd_table, conn, keys, data_iter)``. Details and a sample callable implementation can be found in the section :ref:`insert method <io.sql.method>`. .. versionadded:: 0.24.0 Raises ------ ValueError When the table already exists and `if_exists` is 'fail' (the default). See Also -------- read_sql : Read a DataFrame from a table. Notes ----- Timezone aware datetime columns will be written as ``Timestamp with timezone`` type with SQLAlchemy if supported by the database. Otherwise, the datetimes will be stored as timezone unaware timestamps local to the original timezone. .. versionadded:: 0.24.0 References ---------- .. [1] https://docs.sqlalchemy.org .. [2] https://www.python.org/dev/peps/pep-0249/ Examples -------- Create an in-memory SQLite database. >>> from sqlalchemy import create_engine >>> engine = create_engine('sqlite://', echo=False) Create a table from scratch with 3 rows. >>> df = pd.DataFrame({'name' : ['User 1', 'User 2', 'User 3']}) >>> df name 0 User 1 1 User 2 2 User 3 >>> df.to_sql('users', con=engine) >>> engine.execute(\" SELECT * FROM users \").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3')] An `sqlalchemy.engine.Connection` can also be passed to to `con`: >>> with engine.begin() as connection: ... df1 = pd.DataFrame({'name' : ['User 4', 'User 5']}) ... df1.to_sql('users', con=connection, if_exists='append') This is allowed to support operations that require that the same DBAPI connection is used for the entire operation. >>> df2 = pd.DataFrame({'name' : ['User 6', 'User 7']}) >>> df2.to_sql('users', con=engine, if_exists='append') >>> engine.execute(\" SELECT * FROM users \").fetchall() [(0, 'User 1'), (1, 'User 2'), (2, 'User 3'), (0, 'User 4'), (1, 'User 5'), (0, 'User 6'), (1, 'User 7')] Overwrite the table with just ``df2``. >>> df2.to_sql('users', con=engine, if_exists='replace', ... index_label='id') >>> engine.execute(\" SELECT * FROM users \").fetchall() [(0, 'User 6'), (1, 'User 7')] Specify the dtype (especially useful for integers with missing values). Notice that while pandas is forced to store the data as floating point, the database supports nullable integers. When fetching the data with Python, we get back integer scalars. >>> df = pd.DataFrame({\" A \": [1, None, 2]}) >>> df A 0 1.0 1 NaN 2 2.0 >>> from sqlalchemy.types import Integer >>> df.to_sql('integers', con=engine, index=False, ... dtype={\" A \": Integer()}) >>> engine.execute(\" SELECT * FROM integers \").fetchall() [(1,), (None,), (2,)] \"\"\" from pandas . io import sql sql . to_sql ( self , name , con , schema = schema , if_exists = if_exists , index = index , index_label = index_label , chunksize = chunksize , dtype = dtype , method = method , )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_stata","text":"def to_stata ( self , path : Union [ str , pathlib . Path , IO [ ~ AnyStr ]], convert_dates : Union [ Dict [ Union [ Hashable , NoneType ], str ], NoneType ] = None , write_index : bool = True , byteorder : Union [ str , NoneType ] = None , time_stamp : Union [ datetime . datetime , NoneType ] = None , data_label : Union [ str , NoneType ] = None , variable_labels : Union [ Dict [ Union [ Hashable , NoneType ], str ], NoneType ] = None , version : Union [ int , NoneType ] = 114 , convert_strl : Union [ Sequence [ Union [ Hashable , NoneType ]], NoneType ] = None , compression : Union [ str , Mapping [ str , str ], NoneType ] = 'infer' ) -> None Export DataFrame object to Stata dta format. Writes the DataFrame to a Stata dataset file. \"dta\" files contain a Stata dataset.","title":"to_stata"},{"location":"reference/hielen2/datalink_prova_df/#parameters_157","text":"path : str, buffer or path object String, path object (pathlib.Path or py._path.local.LocalPath) or object implementing a binary write() function. If using a buffer then the buffer will not be automatically closed after the file data has been written. .. versionchanged :: 1.0.0 Previously this was \"fname\" convert_dates : dict Dictionary mapping columns containing datetime types to stata internal format to use when writing the dates. Options are 'tc', 'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer or a name. Datetime columns that do not have a conversion type specified will be converted to 'tc'. Raises NotImplementedError if a datetime column has timezone information. write_index : bool Write the index to Stata dataset. byteorder : str Can be \">\", \"<\", \"little\", or \"big\". default is sys.byteorder . time_stamp : datetime A datetime to use as file creation date. Default is the current time. data_label : str, optional A label for the data set. Must be 80 characters or smaller. variable_labels : dict Dictionary containing columns as keys and variable labels as values. Each label must be 80 characters or smaller. version : {114, 117, 118, 119, None}, default 114 Version to use in the output dta file. Set to None to let pandas decide between 118 or 119 formats depending on the number of columns in the frame. Version 114 can be read by Stata 10 and later. Version 117 can be read by Stata 13 or later. Version 118 is supported in Stata 14 and later. Version 119 is supported in Stata 15 and later. Version 114 limits string variables to 244 characters or fewer while versions 117 and later allow strings with lengths up to 2,000,000 characters. Versions 118 and 119 support Unicode characters, and version 119 supports more than 32,767 variables. .. versionadded :: 0.23.0 .. versionchanged :: 1.0.0 Added support for formats 118 and 119. convert_strl : list, optional List of column names to convert to string columns to Stata StrL format. Only available if version is 117. Storing strings in the StrL format can produce smaller dta files if strings have more than 8 characters and values are repeated. .. versionadded :: 0.23.0 compression : str or dict, default 'infer' For on-the-fly compression of the output dta. If string, specifies compression mode. If dict, value at key 'method' specifies compression mode. Compression mode must be one of {'infer', 'gzip', 'bz2', 'zip', 'xz', None}. If compression mode is 'infer' and fname is path-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no compression). If dict and compression mode is one of {'zip', 'gzip', 'bz2'}, or inferred as one of the above, other entries passed as additional compression options. .. versionadded :: 1.1.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#raises_16","text":"NotImplementedError * If datetimes contain timezone information * Column dtype is not representable in Stata ValueError * Columns listed in convert_dates are neither datetime64[ns] or datetime.datetime * Column listed in convert_dates is not in DataFrame * Categorical label contains more than 32,000 characters","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_153","text":"read_stata : Import Stata data files. io.stata.StataWriter : Low-level writer for Stata data files. io.stata.StataWriter117 : Low-level writer for version 117 files.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_155","text":"df = pd.DataFrame({'animal': ['falcon', 'parrot', 'falcon', ... 'parrot'], ... 'speed': [350, 18, 361, 15]}) df.to_stata('animals.dta') # doctest: +SKIP View Source @deprecate_kwarg ( old_arg_name = \"fname\" , new_arg_name = \"path\" ) def to_stata ( self , path : FilePathOrBuffer , convert_dates : Optional [ Dict[Label, str ] ] = None , write_index : bool = True , byteorder : Optional [ str ] = None , time_stamp : Optional [ datetime.datetime ] = None , data_label : Optional [ str ] = None , variable_labels : Optional [ Dict[Label, str ] ] = None , version : Optional [ int ] = 114 , convert_strl : Optional [ Sequence[Label ] ] = None , compression : Union [ str, Mapping[str, str ] , None ] = \"infer\" , ) -> None : \"\"\" Export DataFrame object to Stata dta format. Writes the DataFrame to a Stata dataset file. \" dta \" files contain a Stata dataset. Parameters ---------- path : str, buffer or path object String, path object (pathlib.Path or py._path.local.LocalPath) or object implementing a binary write() function. If using a buffer then the buffer will not be automatically closed after the file data has been written. .. versionchanged:: 1.0.0 Previously this was \" fname \" convert_dates : dict Dictionary mapping columns containing datetime types to stata internal format to use when writing the dates. Options are 'tc', 'td', 'tm', 'tw', 'th', 'tq', 'ty'. Column can be either an integer or a name. Datetime columns that do not have a conversion type specified will be converted to 'tc'. Raises NotImplementedError if a datetime column has timezone information. write_index : bool Write the index to Stata dataset. byteorder : str Can be \" > \", \" < \", \" little \", or \" big \". default is `sys.byteorder`. time_stamp : datetime A datetime to use as file creation date. Default is the current time. data_label : str, optional A label for the data set. Must be 80 characters or smaller. variable_labels : dict Dictionary containing columns as keys and variable labels as values. Each label must be 80 characters or smaller. version : {114, 117, 118, 119, None}, default 114 Version to use in the output dta file. Set to None to let pandas decide between 118 or 119 formats depending on the number of columns in the frame. Version 114 can be read by Stata 10 and later. Version 117 can be read by Stata 13 or later. Version 118 is supported in Stata 14 and later. Version 119 is supported in Stata 15 and later. Version 114 limits string variables to 244 characters or fewer while versions 117 and later allow strings with lengths up to 2,000,000 characters. Versions 118 and 119 support Unicode characters, and version 119 supports more than 32,767 variables. .. versionadded:: 0.23.0 .. versionchanged:: 1.0.0 Added support for formats 118 and 119. convert_strl : list, optional List of column names to convert to string columns to Stata StrL format. Only available if version is 117. Storing strings in the StrL format can produce smaller dta files if strings have more than 8 characters and values are repeated. .. versionadded:: 0.23.0 compression : str or dict, default 'infer' For on-the-fly compression of the output dta. If string, specifies compression mode. If dict, value at key 'method' specifies compression mode. Compression mode must be one of {'infer', 'gzip', 'bz2', 'zip', 'xz', None}. If compression mode is 'infer' and `fname` is path-like, then detect compression from the following extensions: '.gz', '.bz2', '.zip', or '.xz' (otherwise no compression). If dict and compression mode is one of {'zip', 'gzip', 'bz2'}, or inferred as one of the above, other entries passed as additional compression options. .. versionadded:: 1.1.0 Raises ------ NotImplementedError * If datetimes contain timezone information * Column dtype is not representable in Stata ValueError * Columns listed in convert_dates are neither datetime64[ns] or datetime.datetime * Column listed in convert_dates is not in DataFrame * Categorical label contains more than 32,000 characters See Also -------- read_stata : Import Stata data files. io.stata.StataWriter : Low-level writer for Stata data files. io.stata.StataWriter117 : Low-level writer for version 117 files. Examples -------- >>> df = pd.DataFrame({'animal': ['falcon', 'parrot', 'falcon', ... 'parrot'], ... 'speed': [350, 18, 361, 15]}) >>> df.to_stata('animals.dta') # doctest: +SKIP \"\"\" if version not in ( 114 , 117 , 118 , 119 , None ) : raise ValueError ( \"Only formats 114, 117, 118 and 119 are supported.\" ) if version == 114 : if convert_strl is not None : raise ValueError ( \"strl is not supported in format 114\" ) from pandas . io . stata import StataWriter as statawriter elif version == 117 : # mypy : Name 'statawriter' already defined ( possibly by an import ) from pandas . io . stata import StataWriter117 as statawriter # type : ignore else : # versions 118 and 119 # mypy : Name 'statawriter' already defined ( possibly by an import ) from pandas . io . stata import StataWriterUTF8 as statawriter # type : ignore kwargs : Dict [ str, Any ] = {} if version is None or version >= 117 : # strl conversion is only supported >= 117 kwargs [ \"convert_strl\" ] = convert_strl if version is None or version >= 118 : # Specifying the version is only supported for UTF8 ( 118 or 119 ) kwargs [ \"version\" ] = version # mypy : Too many arguments for \"StataWriter\" writer = statawriter ( # type : ignore path , self , convert_dates = convert_dates , byteorder = byteorder , time_stamp = time_stamp , data_label = data_label , write_index = write_index , variable_labels = variable_labels , compression = compression , ** kwargs , ) writer . write_file ()","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_string","text":"def to_string ( self , buf : Union [ str , pathlib . Path , IO [ str ], NoneType ] = None , columns : Union [ Sequence [ str ], NoneType ] = None , col_space : Union [ int , NoneType ] = None , header : Union [ bool , Sequence [ str ]] = True , index : bool = True , na_rep : str = 'NaN' , formatters : Union [ List [ Callable ], Tuple [ Callable , ... ], Mapping [ Union [ str , int ], Callable ], NoneType ] = None , float_format : Union [ str , Callable , ForwardRef ( 'EngFormatter' ), NoneType ] = None , sparsify : Union [ bool , NoneType ] = None , index_names : bool = True , justify : Union [ str , NoneType ] = None , max_rows : Union [ int , NoneType ] = None , min_rows : Union [ int , NoneType ] = None , max_cols : Union [ int , NoneType ] = None , show_dimensions : bool = False , decimal : str = '.' , line_width : Union [ int , NoneType ] = None , max_colwidth : Union [ int , NoneType ] = None , encoding : Union [ str , NoneType ] = None ) -> Union [ str , NoneType ] Render a DataFrame to a console-friendly tabular output.","title":"to_string"},{"location":"reference/hielen2/datalink_prova_df/#parameters_158","text":"buf : str, Path or StringIO-like, optional, default None Buffer to write to. If None, the output is returned as a string. columns : sequence, optional, default None The subset of columns to write. Writes all columns by default. col_space : int, list or dict of int, optional The minimum width of each column. header : bool or sequence, optional Write out the column names. If a list of strings is given, it is assumed to be aliases for the column names. index : bool, optional, default True Whether to print index (row) labels. na_rep : str, optional, default 'NaN' String representation of NAN to use. formatters : list, tuple or dict of one-param. functions, optional Formatter functions to apply to columns' elements by position or name. The result of each function must be a unicode string. List/tuple must be of length equal to the number of columns. float_format : one-parameter function, optional, default None Formatter function to apply to columns' elements if they are floats. The result of this function must be a unicode string. sparsify : bool, optional, default True Set to False for a DataFrame with a hierarchical index to print every multiindex key at each row. index_names : bool, optional, default True Prints the names of the indexes. justify : str, default None How to justify the column labels. If None uses the option from the print configuration (controlled by set_option), 'right' out of the box. Valid values are * left * right * center * justify * justify-all * start * end * inherit * match-parent * initial * unset. max_rows : int, optional Maximum number of rows to display in the console. min_rows : int, optional The number of rows to display in the console in a truncated repr (when number of rows is above max_rows ). max_cols : int, optional Maximum number of columns to display in the console. show_dimensions : bool, default False Display DataFrame dimensions (number of rows by number of columns). decimal : str, default '.' Character recognized as decimal separator, e.g. ',' in Europe. line_width : int, optional Width to wrap a line in characters. max_colwidth : int, optional Max width to truncate each column in characters. By default, no limit. .. versionadded :: 1.0.0 encoding : str, default \"utf-8\" Set character encoding. .. versionadded :: 1.0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_165","text":"str or None If buf is None, returns the result as a string. Otherwise returns None.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_154","text":"to_html : Convert DataFrame to HTML.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_156","text":"d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]} df = pd.DataFrame(d) print(df.to_string()) col1 col2 0 1 4 1 2 5 2 3 6 View Source @Substitution ( header_type = \"bool or sequence\" , header = \"Write out the column names. If a list of strings \" \"is given, it is assumed to be aliases for the \" \"column names\" , col_space_type = \"int, list or dict of int\" , col_space = \"The minimum width of each column\" , ) @Substitution ( shared_params = fmt . common_docstring , returns = fmt . return_docstring ) def to_string ( self , buf : Optional [ FilePathOrBuffer[str ] ] = None , columns : Optional [ Sequence[str ] ] = None , col_space : Optional [ int ] = None , header : Union [ bool, Sequence[str ] ] = True , index : bool = True , na_rep : str = \"NaN\" , formatters : Optional [ fmt.FormattersType ] = None , float_format : Optional [ fmt.FloatFormatType ] = None , sparsify : Optional [ bool ] = None , index_names : bool = True , justify : Optional [ str ] = None , max_rows : Optional [ int ] = None , min_rows : Optional [ int ] = None , max_cols : Optional [ int ] = None , show_dimensions : bool = False , decimal : str = \".\" , line_width : Optional [ int ] = None , max_colwidth : Optional [ int ] = None , encoding : Optional [ str ] = None , ) -> Optional [ str ] : \"\"\" Render a DataFrame to a console-friendly tabular output. %(shared_params)s line_width : int, optional Width to wrap a line in characters. max_colwidth : int, optional Max width to truncate each column in characters. By default, no limit. .. versionadded:: 1.0.0 encoding : str, default \" utf - 8 \" Set character encoding. .. versionadded:: 1.0 %(returns)s See Also -------- to_html : Convert DataFrame to HTML. Examples -------- >>> d = {'col1': [1, 2, 3], 'col2': [4, 5, 6]} >>> df = pd.DataFrame(d) >>> print(df.to_string()) col1 col2 0 1 4 1 2 5 2 3 6 \"\"\" from pandas import option_context with option_context ( \"display.max_colwidth\" , max_colwidth ) : formatter = fmt . DataFrameFormatter ( self , columns = columns , col_space = col_space , na_rep = na_rep , formatters = formatters , float_format = float_format , sparsify = sparsify , justify = justify , index_names = index_names , header = header , index = index , min_rows = min_rows , max_rows = max_rows , max_cols = max_cols , show_dimensions = show_dimensions , decimal = decimal , line_width = line_width , ) return formatter . to_string ( buf = buf , encoding = encoding )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#to_timestamp","text":"def to_timestamp ( self , freq = None , how : str = 'start' , axis : Union [ str , int ] = 0 , copy : bool = True ) -> 'DataFrame' Cast to DatetimeIndex of timestamps, at beginning of period.","title":"to_timestamp"},{"location":"reference/hielen2/datalink_prova_df/#parameters_159","text":"freq : str, default frequency of PeriodIndex Desired frequency. how : {'s', 'e', 'start', 'end'} Convention for converting period to timestamp; start of period vs. end. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to convert (the index by default). copy : bool, default True If False then underlying input data is not copied.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_166","text":"DataFrame with DatetimeIndex View Source def to_timestamp ( self , freq = None , how : str = \"start\" , axis : Axis = 0 , copy : bool = True ) -> \"DataFrame\" : \"\"\" Cast to DatetimeIndex of timestamps, at *beginning* of period. Parameters ---------- freq : str, default frequency of PeriodIndex Desired frequency. how : {'s', 'e', 'start', 'end'} Convention for converting period to timestamp; start of period vs. end. axis : {0 or 'index', 1 or 'columns'}, default 0 The axis to convert (the index by default). copy : bool, default True If False then underlying input data is not copied. Returns ------- DataFrame with DatetimeIndex \"\"\" new_obj = self . copy ( deep = copy ) axis_name = self . _get_axis_name ( axis ) old_ax = getattr ( self , axis_name ) new_ax = old_ax . to_timestamp ( freq = freq , how = how ) setattr ( new_obj , axis_name , new_ax ) return new_obj","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#to_xarray","text":"def to_xarray ( self ) Return an xarray object from the pandas object.","title":"to_xarray"},{"location":"reference/hielen2/datalink_prova_df/#returns_167","text":"xarray.DataArray or xarray.Dataset Data in the pandas structure converted to Dataset if the object is a DataFrame, or a DataArray if the object is a Series.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_155","text":"DataFrame.to_hdf : Write DataFrame to an HDF5 file. DataFrame.to_parquet : Write a DataFrame to the binary parquet format.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_76","text":"See the xarray docs <https://xarray.pydata.org/en/stable/> __","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_157","text":"df = pd.DataFrame([('falcon', 'bird', 389.0, 2), ... ('parrot', 'bird', 24.0, 2), ... ('lion', 'mammal', 80.5, 4), ... ('monkey', 'mammal', np.nan, 4)], ... columns=['name', 'class', 'max_speed', ... 'num_legs']) df name class max_speed num_legs 0 falcon bird 389.0 2 1 parrot bird 24.0 2 2 lion mammal 80.5 4 3 monkey mammal NaN 4 df.to_xarray() Dimensions: (index: 4) Coordinates: * index (index) int64 0 1 2 3 Data variables: name (index) object 'falcon' 'parrot' 'lion' 'monkey' class (index) object 'bird' 'bird' 'mammal' 'mammal' max_speed (index) float64 389.0 24.0 80.5 nan num_legs (index) int64 2 2 4 4 df['max_speed'].to_xarray() array([389. , 24. , 80.5, nan]) Coordinates: * index (index) int64 0 1 2 3 dates = pd.to_datetime(['2018-01-01', '2018-01-01', ... '2018-01-02', '2018-01-02']) df_multiindex = pd.DataFrame({'date': dates, ... 'animal': ['falcon', 'parrot', ... 'falcon', 'parrot'], ... 'speed': [350, 18, 361, 15]}) df_multiindex = df_multiindex.set_index(['date', 'animal']) df_multiindex speed date animal 2018-01-01 falcon 350 parrot 18 2018-01-02 falcon 361 parrot 15 df_multiindex.to_xarray() Dimensions: (animal: 2, date: 2) Coordinates: * date (date) datetime64[ns] 2018-01-01 2018-01-02 * animal (animal) object 'falcon' 'parrot' Data variables: speed (date, animal) int64 350 18 361 15 View Source def to_xarray ( self ) : \"\"\" Return an xarray object from the pandas object. Returns ------- xarray.DataArray or xarray.Dataset Data in the pandas structure converted to Dataset if the object is a DataFrame, or a DataArray if the object is a Series. See Also -------- DataFrame.to_hdf : Write DataFrame to an HDF5 file. DataFrame.to_parquet : Write a DataFrame to the binary parquet format. Notes ----- See the `xarray docs <https://xarray.pydata.org/en/stable/>`__ Examples -------- >>> df = pd.DataFrame([('falcon', 'bird', 389.0, 2), ... ('parrot', 'bird', 24.0, 2), ... ('lion', 'mammal', 80.5, 4), ... ('monkey', 'mammal', np.nan, 4)], ... columns=['name', 'class', 'max_speed', ... 'num_legs']) >>> df name class max_speed num_legs 0 falcon bird 389.0 2 1 parrot bird 24.0 2 2 lion mammal 80.5 4 3 monkey mammal NaN 4 >>> df.to_xarray() <xarray.Dataset> Dimensions: (index: 4) Coordinates: * index (index) int64 0 1 2 3 Data variables: name (index) object 'falcon' 'parrot' 'lion' 'monkey' class (index) object 'bird' 'bird' 'mammal' 'mammal' max_speed (index) float64 389.0 24.0 80.5 nan num_legs (index) int64 2 2 4 4 >>> df['max_speed'].to_xarray() <xarray.DataArray 'max_speed' (index: 4)> array([389. , 24. , 80.5, nan]) Coordinates: * index (index) int64 0 1 2 3 >>> dates = pd.to_datetime(['2018-01-01', '2018-01-01', ... '2018-01-02', '2018-01-02']) >>> df_multiindex = pd.DataFrame({'date': dates, ... 'animal': ['falcon', 'parrot', ... 'falcon', 'parrot'], ... 'speed': [350, 18, 361, 15]}) >>> df_multiindex = df_multiindex.set_index(['date', 'animal']) >>> df_multiindex speed date animal 2018-01-01 falcon 350 parrot 18 2018-01-02 falcon 361 parrot 15 >>> df_multiindex.to_xarray() <xarray.Dataset> Dimensions: (animal: 2, date: 2) Coordinates: * date (date) datetime64[ns] 2018-01-01 2018-01-02 * animal (animal) object 'falcon' 'parrot' Data variables: speed (date, animal) int64 350 18 361 15 \"\"\" xarray = import_optional_dependency ( \"xarray\" ) if self . ndim == 1 : return xarray . DataArray . from_series ( self ) else : return xarray . Dataset . from_dataframe ( self )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#transform","text":"def transform ( self , func , axis = 0 , * args , ** kwargs ) -> 'DataFrame' Call func on self producing a DataFrame with transformed values. Produced DataFrame will have same axis length as self.","title":"transform"},{"location":"reference/hielen2/datalink_prova_df/#parameters_160","text":"func : function, str, list or dict Function to use for transforming the data. If a function, must either work when passed a DataFrame or when passed to DataFrame.apply. Accepted combinations are : - function - string function name - list of functions and / or function names , e . g . `` [ np . exp . 'sqrt' ] `` - dict of axis labels -> functions , function names or list of such . axis : {0 or 'index', 1 or 'columns'}, default 0 If 0 or 'index': apply function to each column. If 1 or 'columns': apply function to each row. args Positional arguments to pass to func . *kwargs Keyword arguments to pass to func .","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_168","text":"DataFrame A DataFrame that must have the same length as self.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_17","text":"ValueError : If the returned DataFrame has a different length than self.","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#see-also_156","text":"DataFrame.agg : Only perform aggregating type operations. DataFrame.apply : Invoke function on a DataFrame.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_158","text":"df = pd.DataFrame({'A': range(3), 'B': range(1, 4)}) df A B 0 0 1 1 1 2 2 2 3 df.transform(lambda x: x + 1) A B 0 1 2 1 2 3 2 3 4 Even though the resulting DataFrame must have the same length as the input DataFrame, it is possible to provide several input functions: s = pd.Series(range(3)) s 0 0 1 1 2 2 dtype: int64 s.transform([np.sqrt, np.exp]) sqrt exp 0 0.000000 1.000000 1 1.000000 2.718282 2 1.414214 7.389056 View Source @doc ( NDFrame . transform , klass = _shared_doc_kwargs [ \"klass\" ] , axis = _shared_doc_kwargs [ \"axis\" ] , ) def transform ( self , func , axis = 0 , * args , ** kwargs ) -> \"DataFrame\" : axis = self . _get_axis_number ( axis ) if axis == 1 : return self . T . transform ( func , * args , ** kwargs ). T return super (). transform ( func , * args , ** kwargs )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#transpose","text":"def transpose ( self , * args , copy : bool = False ) -> 'DataFrame' Transpose index and columns. Reflect the DataFrame over its main diagonal by writing rows as columns and vice-versa. The property :attr: .T is an accessor to the method :meth: transpose .","title":"transpose"},{"location":"reference/hielen2/datalink_prova_df/#parameters_161","text":"*args : tuple, optional Accepted for compatibility with NumPy. copy : bool, default False Whether to copy the data after transposing, even for DataFrames with a single dtype. Note that a copy is always required for mixed dtype DataFrames, or for DataFrames with any extension types.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_169","text":"DataFrame The transposed DataFrame.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_157","text":"numpy.transpose : Permute the dimensions of a given array.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_77","text":"Transposing a DataFrame with mixed dtypes will result in a homogeneous DataFrame with the object dtype. In such a case, a copy of the data is always made.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_159","text":"Square DataFrame with homogeneous dtype d1 = {'col1': [1, 2], 'col2': [3, 4]} df1 = pd.DataFrame(data=d1) df1 col1 col2 0 1 3 1 2 4 df1_transposed = df1.T # or df1.transpose() df1_transposed 0 1 col1 1 2 col2 3 4 When the dtype is homogeneous in the original DataFrame, we get a transposed DataFrame with the same dtype: df1.dtypes col1 int64 col2 int64 dtype: object df1_transposed.dtypes 0 int64 1 int64 dtype: object Non-square DataFrame with mixed dtypes d2 = {'name': ['Alice', 'Bob'], ... 'score': [9.5, 8], ... 'employed': [False, True], ... 'kids': [0, 0]} df2 = pd.DataFrame(data=d2) df2 name score employed kids 0 Alice 9.5 False 0 1 Bob 8.0 True 0 df2_transposed = df2.T # or df2.transpose() df2_transposed 0 1 name Alice Bob score 9.5 8 employed False True kids 0 0 When the DataFrame has mixed dtypes, we get a transposed DataFrame with the object dtype: df2.dtypes name object score float64 employed bool kids int64 dtype: object df2_transposed.dtypes 0 object 1 object dtype: object View Source def transpose ( self , * args , copy : bool = False ) -> \"DataFrame\" : \"\"\" Transpose index and columns. Reflect the DataFrame over its main diagonal by writing rows as columns and vice-versa. The property :attr:`.T` is an accessor to the method :meth:`transpose`. Parameters ---------- *args : tuple, optional Accepted for compatibility with NumPy. copy : bool, default False Whether to copy the data after transposing, even for DataFrames with a single dtype. Note that a copy is always required for mixed dtype DataFrames, or for DataFrames with any extension types. Returns ------- DataFrame The transposed DataFrame. See Also -------- numpy.transpose : Permute the dimensions of a given array. Notes ----- Transposing a DataFrame with mixed dtypes will result in a homogeneous DataFrame with the `object` dtype. In such a case, a copy of the data is always made. Examples -------- **Square DataFrame with homogeneous dtype** >>> d1 = {'col1': [1, 2], 'col2': [3, 4]} >>> df1 = pd.DataFrame(data=d1) >>> df1 col1 col2 0 1 3 1 2 4 >>> df1_transposed = df1.T # or df1.transpose() >>> df1_transposed 0 1 col1 1 2 col2 3 4 When the dtype is homogeneous in the original DataFrame, we get a transposed DataFrame with the same dtype: >>> df1.dtypes col1 int64 col2 int64 dtype: object >>> df1_transposed.dtypes 0 int64 1 int64 dtype: object **Non-square DataFrame with mixed dtypes** >>> d2 = {'name': ['Alice', 'Bob'], ... 'score': [9.5, 8], ... 'employed': [False, True], ... 'kids': [0, 0]} >>> df2 = pd.DataFrame(data=d2) >>> df2 name score employed kids 0 Alice 9.5 False 0 1 Bob 8.0 True 0 >>> df2_transposed = df2.T # or df2.transpose() >>> df2_transposed 0 1 name Alice Bob score 9.5 8 employed False True kids 0 0 When the DataFrame has mixed dtypes, we get a transposed DataFrame with the `object` dtype: >>> df2.dtypes name object score float64 employed bool kids int64 dtype: object >>> df2_transposed.dtypes 0 object 1 object dtype: object \"\"\" nv . validate_transpose ( args , dict ()) # construct the args dtypes = list ( self . dtypes ) if self . _is_homogeneous_type and dtypes and is_extension_array_dtype ( dtypes [ 0 ]): # We have EAs with the same dtype. We can preserve that dtype in transpose. dtype = dtypes [ 0 ] arr_type = dtype . construct_array_type () values = self . values new_values = [ arr_type . _from_sequence ( row , dtype = dtype ) for row in values ] result = self . _constructor ( dict ( zip ( self . index , new_values )), index = self . columns ) else : new_values = self . values . T if copy : new_values = new_values . copy () result = self . _constructor ( new_values , index = self . columns , columns = self . index ) return result . __finalize__ ( self , method = \"transpose\" )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#truediv","text":"def truediv ( self , other , axis = 'columns' , level = None , fill_value = None ) Get Floating division of dataframe and other, element-wise (binary operator truediv ). Equivalent to dataframe / other , but with support to substitute a fill_value for missing data in one of the inputs. With reverse version, rtruediv . Among flexible wrappers ( add , sub , mul , div , mod , pow ) to arithmetic operators: + , - , * , / , // , % , ** .","title":"truediv"},{"location":"reference/hielen2/datalink_prova_df/#parameters_162","text":"other : scalar, sequence, Series, or DataFrame Any single or multiple element data structure, or list-like object. axis : {0 or 'index', 1 or 'columns'} Whether to compare by the index (0 or 'index') or columns (1 or 'columns'). For Series input, axis to match Series index on. level : int or label Broadcast across a level, matching Index values on the passed MultiIndex level. fill_value : float or None, default None Fill existing missing (NaN) values, and any new element needed for successful DataFrame alignment, with this value before computation. If data in both corresponding DataFrame locations is missing the result will be missing.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_170","text":"DataFrame Result of the arithmetic operation.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_158","text":"DataFrame.add : Add DataFrames. DataFrame.sub : Subtract DataFrames. DataFrame.mul : Multiply DataFrames. DataFrame.div : Divide DataFrames (float division). DataFrame.truediv : Divide DataFrames (float division). DataFrame.floordiv : Divide DataFrames (integer division). DataFrame.mod : Calculate modulo (remainder after division). DataFrame.pow : Calculate exponential power.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_78","text":"Mismatched indices will be unioned together.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_160","text":"df = pd.DataFrame({'angles': [0, 3, 4], ... 'degrees': [360, 180, 360]}, ... index=['circle', 'triangle', 'rectangle']) df angles degrees circle 0 360 triangle 3 180 rectangle 4 360 Add a scalar with operator version which return the same results. df + 1 angles degrees circle 1 361 triangle 4 181 rectangle 5 361 df.add(1) angles degrees circle 1 361 triangle 4 181 rectangle 5 361 Divide by constant with reverse version. df.div(10) angles degrees circle 0.0 36.0 triangle 0.3 18.0 rectangle 0.4 36.0 df.rdiv(10) angles degrees circle inf 0.027778 triangle 3.333333 0.055556 rectangle 2.500000 0.027778 Subtract a list and Series by axis with operator version. df - [1, 2] angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub([1, 2], axis='columns') angles degrees circle -1 358 triangle 2 178 rectangle 3 358 df.sub(pd.Series([1, 1, 1], index=['circle', 'triangle', 'rectangle']), ... axis='index') angles degrees circle -1 359 triangle 2 179 rectangle 3 359 Multiply a DataFrame of different shape with operator version. other = pd.DataFrame({'angles': [0, 3, 4]}, ... index=['circle', 'triangle', 'rectangle']) other angles circle 0 triangle 3 rectangle 4 df * other angles degrees circle 0 NaN triangle 9 NaN rectangle 16 NaN df.mul(other, fill_value=0) angles degrees circle 0 0.0 triangle 9 0.0 rectangle 16 0.0 Divide by a MultiIndex by level. df_multindex = pd.DataFrame({'angles': [0, 3, 4, 4, 5, 6], ... 'degrees': [360, 180, 360, 360, 540, 720]}, ... index=[['A', 'A', 'A', 'B', 'B', 'B'], ... ['circle', 'triangle', 'rectangle', ... 'square', 'pentagon', 'hexagon']]) df_multindex angles degrees A circle 0 360 triangle 3 180 rectangle 4 360 B square 4 360 pentagon 5 540 hexagon 6 720 df.div(df_multindex, level=1, fill_value=0) angles degrees A circle NaN 1.0 triangle 1.0 1.0 rectangle 1.0 1.0 B square 0.0 0.0 pentagon 0.0 0.0 hexagon 0.0 0.0 View Source @Appender ( doc ) def f ( self , other , axis = default_axis , level = None , fill_value = None ) : if _should_reindex_frame_op ( self , other , op , axis , default_axis , fill_value , level ) : return _frame_arith_method_with_reindex ( self , other , op ) if isinstance ( other , ABCSeries ) and fill_value is not None : # TODO : We could allow this in cases where we end up going # through the DataFrame path raise NotImplementedError ( f \"fill_value {fill_value} not supported.\" ) axis = self . _get_axis_number ( axis ) if axis is not None else 1 # TODO : why are we passing flex = True instead of flex = not special ? # 15 tests fail if we pass flex = not special instead self , other = _align_method_FRAME ( self , other , axis , flex = True , level = level ) if isinstance ( other , ABCDataFrame ) : # Another DataFrame new_data = self . _combine_frame ( other , na_op , fill_value ) elif isinstance ( other , ABCSeries ) : new_data = dispatch_to_series ( self , other , op , axis = axis ) else : # in this case we always have ` np . ndim ( other ) == 0 ` if fill_value is not None : self = self . fillna ( fill_value ) new_data = dispatch_to_series ( self , other , op ) return self . _construct_result ( new_data )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#truncate","text":"def truncate ( self : ~ FrameOrSeries , before = None , after = None , axis = None , copy : bool = True ) -> ~ FrameOrSeries Truncate a Series or DataFrame before and after some index value. This is a useful shorthand for boolean indexing based on index values above or below certain thresholds.","title":"truncate"},{"location":"reference/hielen2/datalink_prova_df/#parameters_163","text":"before : date, str, int Truncate all rows before this index value. after : date, str, int Truncate all rows after this index value. axis : {0 or 'index', 1 or 'columns'}, optional Axis to truncate. Truncates the index (rows) by default. copy : bool, default is True, Return a copy of the truncated section.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_171","text":"type of caller The truncated Series or DataFrame.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_159","text":"DataFrame.loc : Select a subset of a DataFrame by label. DataFrame.iloc : Select a subset of a DataFrame by position.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_79","text":"If the index being truncated contains only datetime values, before and after may be specified as strings instead of Timestamps.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_161","text":"df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'], ... 'B': ['f', 'g', 'h', 'i', 'j'], ... 'C': ['k', 'l', 'm', 'n', 'o']}, ... index=[1, 2, 3, 4, 5]) df A B C 1 a f k 2 b g l 3 c h m 4 d i n 5 e j o df.truncate(before=2, after=4) A B C 2 b g l 3 c h m 4 d i n The columns of a DataFrame can be truncated. df.truncate(before=\"A\", after=\"B\", axis=\"columns\") A B 1 a f 2 b g 3 c h 4 d i 5 e j For Series, only rows can be truncated. df['A'].truncate(before=2, after=4) 2 b 3 c 4 d Name: A, dtype: object The index values in truncate can be datetimes or string dates. dates = pd.date_range('2016-01-01', '2016-02-01', freq='s') df = pd.DataFrame(index=dates, data={'A': 1}) df.tail() A 2016-01-31 23:59:56 1 2016-01-31 23:59:57 1 2016-01-31 23:59:58 1 2016-01-31 23:59:59 1 2016-02-01 00:00:00 1 df.truncate(before=pd.Timestamp('2016-01-05'), ... after=pd.Timestamp('2016-01-10')).tail() A 2016-01-09 23:59:56 1 2016-01-09 23:59:57 1 2016-01-09 23:59:58 1 2016-01-09 23:59:59 1 2016-01-10 00:00:00 1 Because the index is a DatetimeIndex containing only dates, we can specify before and after as strings. They will be coerced to Timestamps before truncation. df.truncate('2016-01-05', '2016-01-10').tail() A 2016-01-09 23:59:56 1 2016-01-09 23:59:57 1 2016-01-09 23:59:58 1 2016-01-09 23:59:59 1 2016-01-10 00:00:00 1 Note that truncate assumes a 0 value for any unspecified time component (midnight). This differs from partial string slicing, which returns any partially matching dates. df.loc['2016-01-05':'2016-01-10', :].tail() A 2016-01-10 23:59:55 1 2016-01-10 23:59:56 1 2016-01-10 23:59:57 1 2016-01-10 23:59:58 1 2016-01-10 23:59:59 1 View Source def truncate ( self : FrameOrSeries , before = None , after = None , axis = None , copy : bool_t = True ) -> FrameOrSeries : \"\"\" Truncate a Series or DataFrame before and after some index value. This is a useful shorthand for boolean indexing based on index values above or below certain thresholds. Parameters ---------- before : date, str, int Truncate all rows before this index value. after : date, str, int Truncate all rows after this index value. axis : {0 or 'index', 1 or 'columns'}, optional Axis to truncate. Truncates the index (rows) by default. copy : bool, default is True, Return a copy of the truncated section. Returns ------- type of caller The truncated Series or DataFrame. See Also -------- DataFrame.loc : Select a subset of a DataFrame by label. DataFrame.iloc : Select a subset of a DataFrame by position. Notes ----- If the index being truncated contains only datetime values, `before` and `after` may be specified as strings instead of Timestamps. Examples -------- >>> df = pd.DataFrame({'A': ['a', 'b', 'c', 'd', 'e'], ... 'B': ['f', 'g', 'h', 'i', 'j'], ... 'C': ['k', 'l', 'm', 'n', 'o']}, ... index=[1, 2, 3, 4, 5]) >>> df A B C 1 a f k 2 b g l 3 c h m 4 d i n 5 e j o >>> df.truncate(before=2, after=4) A B C 2 b g l 3 c h m 4 d i n The columns of a DataFrame can be truncated. >>> df.truncate(before=\" A \", after=\" B \", axis=\" columns \") A B 1 a f 2 b g 3 c h 4 d i 5 e j For Series, only rows can be truncated. >>> df['A'].truncate(before=2, after=4) 2 b 3 c 4 d Name: A, dtype: object The index values in ``truncate`` can be datetimes or string dates. >>> dates = pd.date_range('2016-01-01', '2016-02-01', freq='s') >>> df = pd.DataFrame(index=dates, data={'A': 1}) >>> df.tail() A 2016-01-31 23:59:56 1 2016-01-31 23:59:57 1 2016-01-31 23:59:58 1 2016-01-31 23:59:59 1 2016-02-01 00:00:00 1 >>> df.truncate(before=pd.Timestamp('2016-01-05'), ... after=pd.Timestamp('2016-01-10')).tail() A 2016-01-09 23:59:56 1 2016-01-09 23:59:57 1 2016-01-09 23:59:58 1 2016-01-09 23:59:59 1 2016-01-10 00:00:00 1 Because the index is a DatetimeIndex containing only dates, we can specify `before` and `after` as strings. They will be coerced to Timestamps before truncation. >>> df.truncate('2016-01-05', '2016-01-10').tail() A 2016-01-09 23:59:56 1 2016-01-09 23:59:57 1 2016-01-09 23:59:58 1 2016-01-09 23:59:59 1 2016-01-10 00:00:00 1 Note that ``truncate`` assumes a 0 value for any unspecified time component (midnight). This differs from partial string slicing, which returns any partially matching dates. >>> df.loc['2016-01-05':'2016-01-10', :].tail() A 2016-01-10 23:59:55 1 2016-01-10 23:59:56 1 2016-01-10 23:59:57 1 2016-01-10 23:59:58 1 2016-01-10 23:59:59 1 \"\"\" if axis is None : axis = self . _stat_axis_number axis = self . _get_axis_number ( axis ) ax = self . _get_axis ( axis ) # GH 17935 # Check that index is sorted if not ax . is_monotonic_increasing and not ax . is_monotonic_decreasing : raise ValueError ( \"truncate requires a sorted index\" ) # if we have a date index, convert to dates, otherwise # treat like a slice if ax . is_all_dates : from pandas . core . tools . datetimes import to_datetime before = to_datetime ( before ) after = to_datetime ( after ) if before is not None and after is not None : if before > after : raise ValueError ( f \"Truncate: {after} must be after {before}\" ) if len ( ax ) > 1 and ax . is_monotonic_decreasing : before , after = after , before slicer = [ slice ( None , None )] * self . _AXIS_LEN slicer [ axis ] = slice ( before , after ) result = self . loc [ tuple ( slicer )] if isinstance ( ax , MultiIndex ): setattr ( result , self . _get_axis_name ( axis ), ax . truncate ( before , after )) if copy : result = result . copy () return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#tshift","text":"def tshift ( self : ~ FrameOrSeries , periods : int = 1 , freq = None , axis : Union [ str , int ] = 0 ) -> ~ FrameOrSeries Shift the time index, using the index's frequency if available. .. deprecated:: 1.1.0 Use shift instead.","title":"tshift"},{"location":"reference/hielen2/datalink_prova_df/#parameters_164","text":"periods : int Number of periods to move, can be positive or negative. freq : DateOffset, timedelta, or str, default None Increment to use from the tseries module or time rule expressed as a string (e.g. 'EOM'). axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default 0 Corresponds to the axis that contains the Index.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_172","text":"shifted : Series/DataFrame","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#notes_80","text":"If freq is not specified then tries to use the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown View Source def tshift ( self : FrameOrSeries , periods : int = 1 , freq = None , axis : Axis = 0 ) -> FrameOrSeries : \"\"\" Shift the time index, using the index's frequency if available. .. deprecated:: 1.1.0 Use `shift` instead. Parameters ---------- periods : int Number of periods to move, can be positive or negative. freq : DateOffset, timedelta, or str, default None Increment to use from the tseries module or time rule expressed as a string (e.g. 'EOM'). axis : {0 or \u2018index\u2019, 1 or \u2018columns\u2019, None}, default 0 Corresponds to the axis that contains the Index. Returns ------- shifted : Series/DataFrame Notes ----- If freq is not specified then tries to use the freq or inferred_freq attributes of the index. If neither of those attributes exist, a ValueError is thrown \"\"\" warnings . warn ( ( \"tshift is deprecated and will be removed in a future version. \" \"Please use shift instead.\" ), FutureWarning , stacklevel = 2 , ) if freq is None : freq = \"infer\" return self . shift ( periods , freq , axis )","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#tz_convert","text":"def tz_convert ( self : ~ FrameOrSeries , tz , axis = 0 , level = None , copy : bool = True ) -> ~ FrameOrSeries Convert tz-aware axis to target time zone.","title":"tz_convert"},{"location":"reference/hielen2/datalink_prova_df/#parameters_165","text":"tz : str or tzinfo object axis : the axis to convert level : int, str, default None If axis is a MultiIndex, convert a specific level. Otherwise must be None. copy : bool, default True Also make a copy of the underlying data.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_173","text":"{klass} Object with time zone converted axis.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_18","text":"TypeError If the axis is tz-naive. View Source def tz_convert ( self : FrameOrSeries , tz , axis = 0 , level = None , copy : bool_t = True ) -> FrameOrSeries : \"\"\" Convert tz-aware axis to target time zone. Parameters ---------- tz : str or tzinfo object axis : the axis to convert level : int, str, default None If axis is a MultiIndex, convert a specific level. Otherwise must be None. copy : bool, default True Also make a copy of the underlying data. Returns ------- {klass} Object with time zone converted axis. Raises ------ TypeError If the axis is tz-naive. \"\"\" axis = self . _get_axis_number ( axis ) ax = self . _get_axis ( axis ) def _tz_convert ( ax , tz ) : if not hasattr ( ax , \"tz_convert\" ) : if len ( ax ) > 0 : ax_name = self . _get_axis_name ( axis ) raise TypeError ( f \"{ax_name} is not a valid DatetimeIndex or PeriodIndex\" ) else : ax = DatetimeIndex ( [] , tz = tz ) else : ax = ax . tz_convert ( tz ) return ax # if a level is given it must be a MultiIndex level or # equivalent to the axis name if isinstance ( ax , MultiIndex ) : level = ax . _get_level_number ( level ) new_level = _tz_convert ( ax . levels [ level ] , tz ) ax = ax . set_levels ( new_level , level = level ) else : if level not in ( None , 0 , ax . name ) : raise ValueError ( f \"The level {level} is not valid\" ) ax = _tz_convert ( ax , tz ) result = self . copy ( deep = copy ) result = result . set_axis ( ax , axis = axis , inplace = False ) return result . __finalize__ ( self , method = \"tz_convert\" )","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#tz_localize","text":"def tz_localize ( self : ~ FrameOrSeries , tz , axis = 0 , level = None , copy : bool = True , ambiguous = 'raise' , nonexistent : str = 'raise' ) -> ~ FrameOrSeries Localize tz-naive index of a Series or DataFrame to target time zone. This operation localizes the Index. To localize the values in a timezone-naive Series, use :meth: Series.dt.tz_localize .","title":"tz_localize"},{"location":"reference/hielen2/datalink_prova_df/#parameters_166","text":"tz : str or tzinfo axis : the axis to localize level : int, str, default None If axis ia a MultiIndex, localize a specific level. Otherwise must be None. copy : bool, default True Also make a copy of the underlying data. ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise' When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the ambiguous parameter dictates how ambiguous times should be handled. - 'infer' will attempt to infer fall dst-transition hours based on order - bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) - 'NaT' will return NaT where there are ambiguous times - 'raise' will raise an AmbiguousTimeError if there are ambiguous times. nonexistent : str, default 'raise' A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. Valid values are: - 'shift_forward' will shift the nonexistent time forward to the closest existing time - 'shift_backward' will shift the nonexistent time backward to the closest existing time - 'NaT' will return NaT where there are nonexistent times - timedelta objects will shift nonexistent times by the timedelta - 'raise' will raise an NonExistentTimeError if there are nonexistent times . .. versionadded :: 0 . 24 . 0","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_174","text":"Series or DataFrame Same type as the input.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#raises_19","text":"TypeError If the TimeSeries is tz-aware and tz is not None.","title":"Raises"},{"location":"reference/hielen2/datalink_prova_df/#examples_162","text":"Localize local times: s = pd.Series([1], ... index=pd.DatetimeIndex(['2018-09-15 01:30:00'])) s.tz_localize('CET') 2018-09-15 01:30:00+02:00 1 dtype: int64 Be careful with DST changes. When there is sequential data, pandas can infer the DST time: s = pd.Series(range(7), ... index=pd.DatetimeIndex(['2018-10-28 01:30:00', ... '2018-10-28 02:00:00', ... '2018-10-28 02:30:00', ... '2018-10-28 02:00:00', ... '2018-10-28 02:30:00', ... '2018-10-28 03:00:00', ... '2018-10-28 03:30:00'])) s.tz_localize('CET', ambiguous='infer') 2018-10-28 01:30:00+02:00 0 2018-10-28 02:00:00+02:00 1 2018-10-28 02:30:00+02:00 2 2018-10-28 02:00:00+01:00 3 2018-10-28 02:30:00+01:00 4 2018-10-28 03:00:00+01:00 5 2018-10-28 03:30:00+01:00 6 dtype: int64 In some cases, inferring the DST is impossible. In such cases, you can pass an ndarray to the ambiguous parameter to set the DST explicitly s = pd.Series(range(3), ... index=pd.DatetimeIndex(['2018-10-28 01:20:00', ... '2018-10-28 02:36:00', ... '2018-10-28 03:46:00'])) s.tz_localize('CET', ambiguous=np.array([True, True, False])) 2018-10-28 01:20:00+02:00 0 2018-10-28 02:36:00+02:00 1 2018-10-28 03:46:00+01:00 2 dtype: int64 If the DST transition causes nonexistent times, you can shift these dates forward or backward with a timedelta object or 'shift_forward' or 'shift_backward' . s = pd.Series(range(2), ... index=pd.DatetimeIndex(['2015-03-29 02:30:00', ... '2015-03-29 03:30:00'])) s.tz_localize('Europe/Warsaw', nonexistent='shift_forward') 2015-03-29 03:00:00+02:00 0 2015-03-29 03:30:00+02:00 1 dtype: int64 s.tz_localize('Europe/Warsaw', nonexistent='shift_backward') 2015-03-29 01:59:59.999999999+01:00 0 2015-03-29 03:30:00+02:00 1 dtype: int64 s.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H')) 2015-03-29 03:30:00+02:00 0 2015-03-29 03:30:00+02:00 1 dtype: int64 View Source def tz_localize ( self : FrameOrSeries , tz , axis = 0 , level = None , copy : bool_t = True , ambiguous = \"raise\" , nonexistent : str = \"raise\" , ) -> FrameOrSeries : \"\"\" Localize tz-naive index of a Series or DataFrame to target time zone. This operation localizes the Index. To localize the values in a timezone-naive Series, use :meth:`Series.dt.tz_localize`. Parameters ---------- tz : str or tzinfo axis : the axis to localize level : int, str, default None If axis ia a MultiIndex, localize a specific level. Otherwise must be None. copy : bool, default True Also make a copy of the underlying data. ambiguous : 'infer', bool-ndarray, 'NaT', default 'raise' When clocks moved backward due to DST, ambiguous times may arise. For example in Central European Time (UTC+01), when going from 03:00 DST to 02:00 non-DST, 02:30:00 local time occurs both at 00:30:00 UTC and at 01:30:00 UTC. In such a situation, the `ambiguous` parameter dictates how ambiguous times should be handled. - 'infer' will attempt to infer fall dst-transition hours based on order - bool-ndarray where True signifies a DST time, False designates a non-DST time (note that this flag is only applicable for ambiguous times) - 'NaT' will return NaT where there are ambiguous times - 'raise' will raise an AmbiguousTimeError if there are ambiguous times. nonexistent : str, default 'raise' A nonexistent time does not exist in a particular timezone where clocks moved forward due to DST. Valid values are: - 'shift_forward' will shift the nonexistent time forward to the closest existing time - 'shift_backward' will shift the nonexistent time backward to the closest existing time - 'NaT' will return NaT where there are nonexistent times - timedelta objects will shift nonexistent times by the timedelta - 'raise' will raise an NonExistentTimeError if there are nonexistent times. .. versionadded:: 0.24.0 Returns ------- Series or DataFrame Same type as the input. Raises ------ TypeError If the TimeSeries is tz-aware and tz is not None. Examples -------- Localize local times: >>> s = pd.Series([1], ... index=pd.DatetimeIndex(['2018-09-15 01:30:00'])) >>> s.tz_localize('CET') 2018-09-15 01:30:00+02:00 1 dtype: int64 Be careful with DST changes. When there is sequential data, pandas can infer the DST time: >>> s = pd.Series(range(7), ... index=pd.DatetimeIndex(['2018-10-28 01:30:00', ... '2018-10-28 02:00:00', ... '2018-10-28 02:30:00', ... '2018-10-28 02:00:00', ... '2018-10-28 02:30:00', ... '2018-10-28 03:00:00', ... '2018-10-28 03:30:00'])) >>> s.tz_localize('CET', ambiguous='infer') 2018-10-28 01:30:00+02:00 0 2018-10-28 02:00:00+02:00 1 2018-10-28 02:30:00+02:00 2 2018-10-28 02:00:00+01:00 3 2018-10-28 02:30:00+01:00 4 2018-10-28 03:00:00+01:00 5 2018-10-28 03:30:00+01:00 6 dtype: int64 In some cases, inferring the DST is impossible. In such cases, you can pass an ndarray to the ambiguous parameter to set the DST explicitly >>> s = pd.Series(range(3), ... index=pd.DatetimeIndex(['2018-10-28 01:20:00', ... '2018-10-28 02:36:00', ... '2018-10-28 03:46:00'])) >>> s.tz_localize('CET', ambiguous=np.array([True, True, False])) 2018-10-28 01:20:00+02:00 0 2018-10-28 02:36:00+02:00 1 2018-10-28 03:46:00+01:00 2 dtype: int64 If the DST transition causes nonexistent times, you can shift these dates forward or backward with a timedelta object or `'shift_forward'` or `'shift_backward'`. >>> s = pd.Series(range(2), ... index=pd.DatetimeIndex(['2015-03-29 02:30:00', ... '2015-03-29 03:30:00'])) >>> s.tz_localize('Europe/Warsaw', nonexistent='shift_forward') 2015-03-29 03:00:00+02:00 0 2015-03-29 03:30:00+02:00 1 dtype: int64 >>> s.tz_localize('Europe/Warsaw', nonexistent='shift_backward') 2015-03-29 01:59:59.999999999+01:00 0 2015-03-29 03:30:00+02:00 1 dtype: int64 >>> s.tz_localize('Europe/Warsaw', nonexistent=pd.Timedelta('1H')) 2015-03-29 03:30:00+02:00 0 2015-03-29 03:30:00+02:00 1 dtype: int64 \"\"\" nonexistent_options = ( \"raise\" , \"NaT\" , \"shift_forward\" , \"shift_backward\" ) if nonexistent not in nonexistent_options and not isinstance ( nonexistent , timedelta ): raise ValueError ( \"The nonexistent argument must be one of 'raise', \" \"'NaT', 'shift_forward', 'shift_backward' or \" \"a timedelta object\" ) axis = self . _get_axis_number ( axis ) ax = self . _get_axis ( axis ) def _tz_localize ( ax , tz , ambiguous , nonexistent ): if not hasattr ( ax , \"tz_localize\" ): if len ( ax ) > 0 : ax_name = self . _get_axis_name ( axis ) raise TypeError ( f \"{ax_name} is not a valid DatetimeIndex or PeriodIndex\" ) else : ax = DatetimeIndex ([], tz = tz ) else : ax = ax . tz_localize ( tz , ambiguous = ambiguous , nonexistent = nonexistent ) return ax # if a level is given it must be a MultiIndex level or # equivalent to the axis name if isinstance ( ax , MultiIndex ): level = ax . _get_level_number ( level ) new_level = _tz_localize ( ax . levels [ level ], tz , ambiguous , nonexistent ) ax = ax . set_levels ( new_level , level = level ) else : if level not in ( None , 0 , ax . name ): raise ValueError ( f \"The level {level} is not valid\" ) ax = _tz_localize ( ax , tz , ambiguous , nonexistent ) result = self . copy ( deep = copy ) result = result . set_axis ( ax , axis = axis , inplace = False ) return result . __finalize__ ( self , method = \"tz_localize\" )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#unstack","text":"def unstack ( self , level =- 1 , fill_value = None ) Pivot a level of the (necessarily hierarchical) index labels. Returns a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels. If the index is not a MultiIndex, the output will be a Series (the analogue of stack when the columns are not a MultiIndex).","title":"unstack"},{"location":"reference/hielen2/datalink_prova_df/#parameters_167","text":"level : int, str, or list of these, default -1 (last level) Level(s) of index to unstack, can pass level name. fill_value : int, str or dict Replace NaN with this value if the unstack produces missing values.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_175","text":"Series or DataFrame","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_160","text":"DataFrame.pivot : Pivot a table based on column values. DataFrame.stack : Pivot a level of the column labels (inverse operation from unstack ).","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#examples_163","text":"index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'), ... ('two', 'a'), ('two', 'b')]) s = pd.Series(np.arange(1.0, 5.0), index=index) s one a 1.0 b 2.0 two a 3.0 b 4.0 dtype: float64 s.unstack(level=-1) a b one 1.0 2.0 two 3.0 4.0 s.unstack(level=0) one two a 1.0 3.0 b 2.0 4.0 df = s.unstack(level=0) df.unstack() one a 1.0 b 2.0 two a 3.0 b 4.0 dtype: float64 View Source def unstack ( self , level =- 1 , fill_value = None ): \"\"\" Pivot a level of the (necessarily hierarchical) index labels. Returns a DataFrame having a new level of column labels whose inner-most level consists of the pivoted index labels. If the index is not a MultiIndex, the output will be a Series (the analogue of stack when the columns are not a MultiIndex). Parameters ---------- level : int, str, or list of these, default -1 (last level) Level(s) of index to unstack, can pass level name. fill_value : int, str or dict Replace NaN with this value if the unstack produces missing values. Returns ------- Series or DataFrame See Also -------- DataFrame.pivot : Pivot a table based on column values. DataFrame.stack : Pivot a level of the column labels (inverse operation from `unstack`). Examples -------- >>> index = pd.MultiIndex.from_tuples([('one', 'a'), ('one', 'b'), ... ('two', 'a'), ('two', 'b')]) >>> s = pd.Series(np.arange(1.0, 5.0), index=index) >>> s one a 1.0 b 2.0 two a 3.0 b 4.0 dtype: float64 >>> s.unstack(level=-1) a b one 1.0 2.0 two 3.0 4.0 >>> s.unstack(level=0) one two a 1.0 3.0 b 2.0 4.0 >>> df = s.unstack(level=0) >>> df.unstack() one a 1.0 b 2.0 two a 3.0 b 4.0 dtype: float64 \"\"\" from pandas . core . reshape . reshape import unstack return unstack ( self , level , fill_value )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#update","text":"def update ( self , value ) Needs to lock for writing json-database View Source def update ( self , value ): \"\"\" Needs to lock for writing json-database \"\"\" try : value = value . to_frame () except AttributeError as e : pass value = value . reset_index () value . columns = list ( range ( value . columns . __len__ ())) value = value . set_index ( value . columns [ 0 ]) error = None try : self . lock . acquire () try : self . __chk_and_reload_cache () self . drop ( value . index , axis = 0 , errors = 'ignore' , inplace = True ) self . append ( value , inplace = True ). sort_index ( inplace = True ) self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error","title":"update"},{"location":"reference/hielen2/datalink_prova_df/#value_counts","text":"def value_counts ( self , subset : Union [ Sequence [ Union [ Hashable , NoneType ]], NoneType ] = None , normalize : bool = False , sort : bool = True , ascending : bool = False ) Return a Series containing counts of unique rows in the DataFrame. .. versionadded:: 1.1.0","title":"value_counts"},{"location":"reference/hielen2/datalink_prova_df/#parameters_168","text":"subset : list-like, optional Columns to use when counting unique combinations. normalize : bool, default False Return proportions rather than frequencies. sort : bool, default True Sort by frequencies. ascending : bool, default False Sort in ascending order.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_176","text":"Series","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_161","text":"Series.value_counts: Equivalent method on Series.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_81","text":"The returned Series will have a MultiIndex with one level per input column. By default, rows that contain any NA values are omitted from the result. By default, the resulting Series will be in descending order so that the first element is the most frequently-occurring row.","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_164","text":"df = pd.DataFrame({'num_legs': [2, 4, 4, 6], ... 'num_wings': [2, 0, 0, 0]}, ... index=['falcon', 'dog', 'cat', 'ant']) df num_legs num_wings falcon 2 2 dog 4 0 cat 4 0 ant 6 0 df.value_counts() num_legs num_wings 4 0 2 6 0 1 2 2 1 dtype: int64 df.value_counts(sort=False) num_legs num_wings 2 2 1 4 0 2 6 0 1 dtype: int64 df.value_counts(ascending=True) num_legs num_wings 2 2 1 6 0 1 4 0 2 dtype: int64 df.value_counts(normalize=True) num_legs num_wings 4 0 0.50 6 0 0.25 2 2 0.25 dtype: float64 View Source def value_counts ( self , subset : Optional [ Sequence[Label ] ] = None , normalize : bool = False , sort : bool = True , ascending : bool = False , ) : \"\"\" Return a Series containing counts of unique rows in the DataFrame. .. versionadded:: 1.1.0 Parameters ---------- subset : list-like, optional Columns to use when counting unique combinations. normalize : bool, default False Return proportions rather than frequencies. sort : bool, default True Sort by frequencies. ascending : bool, default False Sort in ascending order. Returns ------- Series See Also -------- Series.value_counts: Equivalent method on Series. Notes ----- The returned Series will have a MultiIndex with one level per input column. By default, rows that contain any NA values are omitted from the result. By default, the resulting Series will be in descending order so that the first element is the most frequently-occurring row. Examples -------- >>> df = pd.DataFrame({'num_legs': [2, 4, 4, 6], ... 'num_wings': [2, 0, 0, 0]}, ... index=['falcon', 'dog', 'cat', 'ant']) >>> df num_legs num_wings falcon 2 2 dog 4 0 cat 4 0 ant 6 0 >>> df.value_counts() num_legs num_wings 4 0 2 6 0 1 2 2 1 dtype: int64 >>> df.value_counts(sort=False) num_legs num_wings 2 2 1 4 0 2 6 0 1 dtype: int64 >>> df.value_counts(ascending=True) num_legs num_wings 2 2 1 6 0 1 4 0 2 dtype: int64 >>> df.value_counts(normalize=True) num_legs num_wings 4 0 0.50 6 0 0.25 2 2 0.25 dtype: float64 \"\"\" if subset is None : subset = self . columns . tolist () counts = self . groupby ( subset ). grouper . size () if sort : counts = counts . sort_values ( ascending = ascending ) if normalize : counts /= counts . sum () # Force MultiIndex for single column if len ( subset ) == 1 : counts . index = MultiIndex . from_arrays ( [ counts.index ] , names =[ counts.index.name ] ) return counts","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#var","text":"def var ( self , axis = None , skipna = None , level = None , ddof = 1 , numeric_only = None , ** kwargs ) Return unbiased variance over requested axis. Normalized by N-1 by default. This can be changed using the ddof argument","title":"var"},{"location":"reference/hielen2/datalink_prova_df/#parameters_169","text":"axis : {index (0), columns (1)} skipna : bool, default True Exclude NA/null values. If an entire row/column is NA, the result will be NA. level : int or level name, default None If the axis is a MultiIndex (hierarchical), count along a particular level, collapsing into a Series. ddof : int, default 1 Delta Degrees of Freedom. The divisor used in calculations is N - ddof, where N represents the number of elements. numeric_only : bool, default None Include only float, int, boolean columns. If None, will attempt to use everything, then use only numeric data. Not implemented for Series.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_177","text":"Series or DataFrame (if level specified) View Source @Substitution ( desc = desc , name1 = name1 , name2 = name2 , axis_descr = axis_descr ) @Appender ( _num_ddof_doc ) def stat_func ( self , axis = None , skipna = None , level = None , ddof = 1 , numeric_only = None , ** kwargs ) : nv . validate_stat_ddof_func ( tuple (), kwargs , fname = name ) if skipna is None : skipna = True if axis is None : axis = self . _stat_axis_number if level is not None : return self . _agg_by_level ( name , axis = axis , level = level , skipna = skipna , ddof = ddof ) return self . _reduce ( func , name , axis = axis , numeric_only = numeric_only , skipna = skipna , ddof = ddof )","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#where","text":"def where ( self , cond , other = nan , inplace = False , axis = None , level = None , errors = 'raise' , try_cast = False ) Replace values where the condition is False.","title":"where"},{"location":"reference/hielen2/datalink_prova_df/#parameters_170","text":"cond : bool Series/DataFrame, array-like, or callable Where cond is True, keep the original value. Where False, replace with corresponding value from other . If cond is callable, it is computed on the Series/DataFrame and should return boolean Series/DataFrame or array. The callable must not change input Series/DataFrame (though pandas doesn't check it). other : scalar, Series/DataFrame, or callable Entries where cond is False are replaced with corresponding value from other . If other is callable, it is computed on the Series/DataFrame and should return scalar or Series/DataFrame. The callable must not change input Series/DataFrame (though pandas doesn't check it). inplace : bool, default False Whether to perform the operation in place on the data. axis : int, default None Alignment axis if needed. level : int, default None Alignment level if needed. errors : str, {'raise', 'ignore'}, default 'raise' Note that currently this parameter won't affect the results and will always coerce to a suitable dtype. - 'raise' : allow exceptions to be raised. - 'ignore' : suppress exceptions. On error return original object. try_cast : bool, default False Try to cast the result back to the input type (if possible).","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_178","text":"Same type as caller","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_162","text":":func: DataFrame.mask : Return an object of same shape as self.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_82","text":"The where method is an application of the if-then idiom. For each element in the calling DataFrame, if cond is True the element is used; otherwise the corresponding element from the DataFrame other is used. The signature for :func: DataFrame.where differs from :func: numpy.where . Roughly df1.where(m, df2) is equivalent to np.where(m, df1, df2) . For further details and examples see the where documentation in :ref: indexing <indexing.where_mask> .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_165","text":"s = pd.Series(range(5)) s.where(s > 0) 0 NaN 1 1.0 2 2.0 3 3.0 4 4.0 dtype: float64 s.mask(s > 0) 0 0.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 s.where(s > 1, 10) 0 10 1 10 2 2 3 3 4 4 dtype: int64 df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B']) df A B 0 0 1 1 2 3 2 4 5 3 6 7 4 8 9 m = df % 3 == 0 df.where(m, -df) A B 0 0 -1 1 -2 3 2 -4 -5 3 6 -7 4 -8 9 df.where(m, -df) == np.where(m, df, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True df.where(m, -df) == df.mask(~m, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True View Source @ doc ( klass = _shared_doc_kwargs [ \"klass\" ], cond = \"True\" , cond_rev = \"False\" , name = \"where\" , name_other = \"mask\" , ) def where ( self , cond , other = np . nan , inplace = False , axis = None , level = None , errors = \"raise\" , try_cast = False , ): \"\"\" Replace values where the condition is {cond_rev}. Parameters ---------- cond : bool {klass}, array-like, or callable Where `cond` is {cond}, keep the original value. Where {cond_rev}, replace with corresponding value from `other`. If `cond` is callable, it is computed on the {klass} and should return boolean {klass} or array. The callable must not change input {klass} (though pandas doesn't check it). other : scalar, {klass}, or callable Entries where `cond` is {cond_rev} are replaced with corresponding value from `other`. If other is callable, it is computed on the {klass} and should return scalar or {klass}. The callable must not change input {klass} (though pandas doesn't check it). inplace : bool, default False Whether to perform the operation in place on the data. axis : int, default None Alignment axis if needed. level : int, default None Alignment level if needed. errors : str, {{'raise', 'ignore'}}, default 'raise' Note that currently this parameter won't affect the results and will always coerce to a suitable dtype. - 'raise' : allow exceptions to be raised. - 'ignore' : suppress exceptions. On error return original object. try_cast : bool, default False Try to cast the result back to the input type (if possible). Returns ------- Same type as caller See Also -------- :func:`DataFrame.{name_other}` : Return an object of same shape as self. Notes ----- The {name} method is an application of the if-then idiom. For each element in the calling DataFrame, if ``cond`` is ``{cond}`` the element is used; otherwise the corresponding element from the DataFrame ``other`` is used. The signature for :func:`DataFrame.where` differs from :func:`numpy.where`. Roughly ``df1.where(m, df2)`` is equivalent to ``np.where(m, df1, df2)``. For further details and examples see the ``{name}`` documentation in :ref:`indexing <indexing.where_mask>`. Examples -------- >>> s = pd.Series(range(5)) >>> s.where(s > 0) 0 NaN 1 1.0 2 2.0 3 3.0 4 4.0 dtype: float64 >>> s.mask(s > 0) 0 0.0 1 NaN 2 NaN 3 NaN 4 NaN dtype: float64 >>> s.where(s > 1, 10) 0 10 1 10 2 2 3 3 4 4 dtype: int64 >>> df = pd.DataFrame(np.arange(10).reshape(-1, 2), columns=['A', 'B']) >>> df A B 0 0 1 1 2 3 2 4 5 3 6 7 4 8 9 >>> m = df % 3 == 0 >>> df.where(m, -df) A B 0 0 -1 1 -2 3 2 -4 -5 3 6 -7 4 -8 9 >>> df.where(m, -df) == np.where(m, df, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True >>> df.where(m, -df) == df.mask(~m, -df) A B 0 True True 1 True True 2 True True 3 True True 4 True True \"\"\" other = com . apply_if_callable ( other , self ) return self . _where ( cond , other , inplace , axis , level , errors = errors , try_cast = try_cast )","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#xs","text":"def xs ( self , key , axis = 0 , level = None , drop_level : bool = True ) Return cross-section from the Series/DataFrame. This method takes a key argument to select data at a particular level of a MultiIndex.","title":"xs"},{"location":"reference/hielen2/datalink_prova_df/#parameters_171","text":"key : label or tuple of label Label contained in the index, or partially in a MultiIndex. axis : {0 or 'index', 1 or 'columns'}, default 0 Axis to retrieve cross-section on. level : object, defaults to first n levels (n=1 or len(key)) In case of a key partially contained in a MultiIndex, indicate which levels are used. Levels can be referred by label or position. drop_level : bool, default True If False, returns object with same levels as self.","title":"Parameters"},{"location":"reference/hielen2/datalink_prova_df/#returns_179","text":"Series or DataFrame Cross-section from the original Series or DataFrame corresponding to the selected index levels.","title":"Returns"},{"location":"reference/hielen2/datalink_prova_df/#see-also_163","text":"DataFrame.loc : Access a group of rows and columns by label(s) or a boolean array. DataFrame.iloc : Purely integer-location based indexing for selection by position.","title":"See Also"},{"location":"reference/hielen2/datalink_prova_df/#notes_83","text":"xs can not be used to set values. MultiIndex Slicers is a generic way to get/set values on any level or levels. It is a superset of xs functionality, see :ref: MultiIndex Slicers <advanced.mi_slicers> .","title":"Notes"},{"location":"reference/hielen2/datalink_prova_df/#examples_166","text":"d = {'num_legs': [4, 4, 2, 2], ... 'num_wings': [0, 0, 2, 2], ... 'class': ['mammal', 'mammal', 'mammal', 'bird'], ... 'animal': ['cat', 'dog', 'bat', 'penguin'], ... 'locomotion': ['walks', 'walks', 'flies', 'walks']} df = pd.DataFrame(data=d) df = df.set_index(['class', 'animal', 'locomotion']) df num_legs num_wings class animal locomotion mammal cat walks 4 0 dog walks 4 0 bat flies 2 2 bird penguin walks 2 2 Get values at specified index df.xs('mammal') num_legs num_wings animal locomotion cat walks 4 0 dog walks 4 0 bat flies 2 2 Get values at several indexes df.xs(('mammal', 'dog')) num_legs num_wings locomotion walks 4 0 Get values at specified index and level df.xs('cat', level=1) num_legs num_wings class locomotion mammal walks 4 0 Get values at several indexes and levels df.xs(('bird', 'walks'), ... level=[0, 'locomotion']) num_legs num_wings animal penguin 2 2 Get values at specified column and axis df.xs('num_wings', axis=1) class animal locomotion mammal cat walks 0 dog walks 0 bat flies 2 bird penguin walks 2 Name: num_wings, dtype: int64 View Source def xs ( self , key , axis = 0 , level = None , drop_level : bool_t = True ) : \"\"\" Return cross-section from the Series/DataFrame. This method takes a `key` argument to select data at a particular level of a MultiIndex. Parameters ---------- key : label or tuple of label Label contained in the index, or partially in a MultiIndex. axis : {0 or 'index', 1 or 'columns'}, default 0 Axis to retrieve cross-section on. level : object, defaults to first n levels (n=1 or len(key)) In case of a key partially contained in a MultiIndex, indicate which levels are used. Levels can be referred by label or position. drop_level : bool, default True If False, returns object with same levels as self. Returns ------- Series or DataFrame Cross-section from the original Series or DataFrame corresponding to the selected index levels. See Also -------- DataFrame.loc : Access a group of rows and columns by label(s) or a boolean array. DataFrame.iloc : Purely integer-location based indexing for selection by position. Notes ----- `xs` can not be used to set values. MultiIndex Slicers is a generic way to get/set values on any level or levels. It is a superset of `xs` functionality, see :ref:`MultiIndex Slicers <advanced.mi_slicers>`. Examples -------- >>> d = {'num_legs': [4, 4, 2, 2], ... 'num_wings': [0, 0, 2, 2], ... 'class': ['mammal', 'mammal', 'mammal', 'bird'], ... 'animal': ['cat', 'dog', 'bat', 'penguin'], ... 'locomotion': ['walks', 'walks', 'flies', 'walks']} >>> df = pd.DataFrame(data=d) >>> df = df.set_index(['class', 'animal', 'locomotion']) >>> df num_legs num_wings class animal locomotion mammal cat walks 4 0 dog walks 4 0 bat flies 2 2 bird penguin walks 2 2 Get values at specified index >>> df.xs('mammal') num_legs num_wings animal locomotion cat walks 4 0 dog walks 4 0 bat flies 2 2 Get values at several indexes >>> df.xs(('mammal', 'dog')) num_legs num_wings locomotion walks 4 0 Get values at specified index and level >>> df.xs('cat', level=1) num_legs num_wings class locomotion mammal walks 4 0 Get values at several indexes and levels >>> df.xs(('bird', 'walks'), ... level=[0, 'locomotion']) num_legs num_wings animal penguin 2 2 Get values at specified column and axis >>> df.xs('num_wings', axis=1) class animal locomotion mammal cat walks 0 dog walks 0 bat flies 2 bird penguin walks 2 Name: num_wings, dtype: int64 \"\"\" axis = self . _get_axis_number ( axis ) labels = self . _get_axis ( axis ) if level is not None : if not isinstance ( labels , MultiIndex ) : raise TypeError ( \"Index must be a MultiIndex\" ) loc , new_ax = labels . get_loc_level ( key , level = level , drop_level = drop_level ) # create the tuple of the indexer _indexer = [ slice(None) ] * self . ndim _indexer [ axis ] = loc indexer = tuple ( _indexer ) result = self . iloc [ indexer ] setattr ( result , result . _get_axis_name ( axis ), new_ax ) return result if axis == 1 : return self [ key ] index = self . index if isinstance ( index , MultiIndex ) : loc , new_index = self . index . get_loc_level ( key , drop_level = drop_level ) else : loc = self . index . get_loc ( key ) if isinstance ( loc , np . ndarray ) : if loc . dtype == np . bool_ : ( inds ,) = loc . nonzero () return self . _take_with_is_copy ( inds , axis = axis ) else : return self . _take_with_is_copy ( loc , axis = axis ) if not is_scalar ( loc ) : new_index = self . index [ loc ] if is_scalar ( loc ) : # In this case loc should be an integer if self . ndim == 1 : # if we encounter an array - like and we only have 1 dim # that means that their are list / ndarrays inside the Series ! # so just return them ( GH 6394 ) return self . _values [ loc ] new_values = self . _mgr . fast_xs ( loc ) result = self . _constructor_sliced ( new_values , index = self . columns , name = self . index [ loc ] , dtype = new_values . dtype , ) else : result = self . iloc [ loc ] result . index = new_index # this could be a view # but only in a single - dtyped view sliceable case result . _set_is_copy ( self , copy = not result . _is_view ) return result","title":"Examples"},{"location":"reference/hielen2/datalink_prova_df/#db","text":"class DB ( connection ) Helper class that provides a standard way to create an ABC using inheritance. View Source class DB ( ABC ) : @abstractmethod def __init__ ( self , connection ) : pass @abstractmethod def __getitem__ ( self , key ) : pass @abstractmethod def __setitem__ ( self , key , value ) : pass @abstractmethod def pop ( self , key ) : pass","title":"DB"},{"location":"reference/hielen2/datalink_prova_df/#ancestors-in-mro_1","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/datalink_prova_df/#descendants","text":"hielen2.datalink_prova_df.JsonDB","title":"Descendants"},{"location":"reference/hielen2/datalink_prova_df/#methods_1","text":"","title":"Methods"},{"location":"reference/hielen2/datalink_prova_df/#pop_1","text":"def pop ( self , key ) View Source @abstractmethod def pop ( self , key ) : pass","title":"pop"},{"location":"reference/hielen2/datalink_prova_df/#jsondb","text":"class JsonDB ( connection , schema , lock_timeout_seconds = 10 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class JsonDB ( DB ) : def __init__ ( self , connection , schema , lock_timeout_seconds = 10 ) : self . jsonfile = connection self . lock = FileLock ( f \"{connection}.lock\" , timeout = lock_timeout_seconds ) self . md5file = f \"{connection}.md5\" self . md5 = None self . schema = schema self . __chk_and_reload_jsondb ( force = True ) def __brute_load_jsondb ( self ) : try : self . db = read_json ( self . jsonfile , orient = 'table' , convert_dates = False ) except Exception as e : self . db = DataFrame () if self . db . empty : self . db = DataFrame ( {} , columns = self . schema [ 'columns' ] ) self . db = self . db . set_index ( self . schema [ 'primary_key' ] ) def __chk_and_reload_jsondb ( self , force = False ) : \"\"\" Needs to check for json-database file changes in a thread safe way!! \"\"\" md5 = None error = None try : self . lock . acquire () try : if force : raise FileNotFoundError () with open ( self . md5file ) as o : md5 = o . read () if not md5 == self . md5 : self . md5 = md5 self . __brute_load_jsondb () except FileNotFoundError as e : ## refershing hash self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) self . __brute_load_jsondb () finally : self . lock . release () except Timeout : pass def save ( self ) : try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e def __write_jsondb ( self , key , value ) : \"\"\" Needs to lock for writing json-database \"\"\" item = None error = None try : self . lock . acquire () try : self . __chk_and_reload_jsondb () if value is None : # Request to remove key , raises KeyError item = self . __getitem__ ( key ) try : self . db = self . db . drop ( key , axis = 0 ) except KeyError : raise KeyError ( f \"key {key} to remove does not exist\" ) else : # Request to insert key , raises ValueError primarykey = self . schema [ 'primary_key' ] if not isinstance ( key ,( list , set , tuple )) : key =[ key ] if key . __len__ () < primarykey . __len__ () : raise ValueError ( f \"key {key!r} is not fully determinated\" ) keydict = dict ( zip ( self . schema [ 'primary_key' ] , key )) value . update ( keydict ) df = DataFrame ( [ value.values() ] ) df . columns = value . keys () df = df . set_index ( self . schema [ 'primary_key' ] ) try : self . db = self . db . append ( df , verify_integrity = True ). sort_index () except ValueError : raise ValueError ( f \"key {key} to insert exists\" ) self . db . replace ( { nan : None , NaT : None } , inplace = True ) item = self . __brute_getitem ( key ) self . save () except Exception as e : error = e finally : self . lock . release () except Timeout as e : error = e if error is not None : raise error return item def __brute_getitem ( self , key = None ) : out = None if key is None : out = self . db else : out = self . db . loc [ key ] if isinstance ( out , Series ) : out = out . to_frame (). T out . index . names = self . schema [ 'primary_key' ] out = out . reset_index (). to_dict ( orient = 'records' ) if out . __len__ () == 1 : out = out [ 0 ] return out def __getitem__ ( self , key = None ) : self . __chk_and_reload_jsondb () if isinstance ( key , list ) : try : key = list ( filter ( None , key )) except TypeError : pass return self . __brute_getitem ( key ) def pop ( self , key ) : return self . __write_jsondb ( key , None ) def __setitem__ ( self , key = None , value = None ) : self . __write_jsondb ( key , value )","title":"JsonDB"},{"location":"reference/hielen2/datalink_prova_df/#ancestors-in-mro_2","text":"hielen2.datalink_prova_df.DB abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/datalink_prova_df/#descendants_1","text":"hielen2.datalink_prova_df.fsHielenCache","title":"Descendants"},{"location":"reference/hielen2/datalink_prova_df/#methods_2","text":"","title":"Methods"},{"location":"reference/hielen2/datalink_prova_df/#pop_2","text":"def pop ( self , key ) View Source def pop ( self , key ): return self . __write_jsondb ( key , None )","title":"pop"},{"location":"reference/hielen2/datalink_prova_df/#save_1","text":"def save ( self ) View Source def save ( self ): try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e","title":"save"},{"location":"reference/hielen2/datalink_prova_df/#fshielencache","text":"class fsHielenCache ( connection , lock_timeout_seconds = 10 ) Helper class that provides a standard way to create an ABC using inheritance. View Source class fsHielenCache ( JsonDB ) : def __init__ ( self , connection , lock_timeout_seconds = 10 ) : self . cachepath = connection self . lts = lock_timeout_seconds schema = { \"columns\" : [ \"uid\",\"info\" ] , \"primary_key\" : [ \"uid\" ] } connfile = str ( Path ( connection ) / \"index.json\" ) super (). __init__ ( connfile , schema , self . lts ) def __getitem__ ( self , key ) : info = super (). __getitem__ ( key ) return CsvCache ( self . cachepath , key , self . lts ). get ( force_reload = True ) def __setitem__ ( self , key , value ) : if value is not None and not isinstance ( value ,( DataFrame , Series )) : raise ValueError ( \"pandas.DataFrame or pandas.Series required\" ) try : assert isinstance ( key , str ) assert key . __len__ () == 32 except AssertionError as e : raise ValueError ( f \"key {key} doesn't seems to match requirement format\" ) if value is not None : super (). __setitem__ ( key , {} ) item = CsvCache ( self . cachepath , key , self . lts ) os . makedirs ( item . cachepath , exist_ok = True ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ]= statistics else : super (). __setitem__ ( key , None ) try : item = CsvCache ( self . cachepath , key , self . lts ) item . cleanfs () except FileNotFoundError as e : pass def update ( self , key , value ) : if value is not None and not isinstance ( value ,( DataFrame , Series )) : #if value is not None and not isinstance ( value , DataFrame ) : raise ValueError ( \"pandas.DataFrame or pandas.Series required\" ) if value is not None : info = super (). __getitem__ ( key ) item = CsvCache ( self . cachepath , key , self . lts ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ]= statistics","title":"fsHielenCache"},{"location":"reference/hielen2/datalink_prova_df/#ancestors-in-mro_3","text":"hielen2.datalink_prova_df.JsonDB hielen2.datalink_prova_df.DB abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/datalink_prova_df/#methods_3","text":"","title":"Methods"},{"location":"reference/hielen2/datalink_prova_df/#pop_3","text":"def pop ( self , key ) View Source def pop ( self , key ): return self . __write_jsondb ( key , None )","title":"pop"},{"location":"reference/hielen2/datalink_prova_df/#save_2","text":"def save ( self ) View Source def save ( self ): try : self . lock . acquire () try : self . db . to_json ( self . jsonfile , orient = 'table' ) self . md5 = hashfile ( self . jsonfile ) with open ( self . md5file , \"w\" ) as o : o . write ( self . md5 ) finally : self . lock . release () except Timeout as e : # Just to remind Timout error here raise e","title":"save"},{"location":"reference/hielen2/datalink_prova_df/#update_1","text":"def update ( self , key , value ) View Source def update ( self , key , value ) : if value is not None and not isinstance ( value ,( DataFrame , Series )) : #if value is not None and not isinstance ( value , DataFrame ) : raise ValueError ( \"pandas.DataFrame or pandas.Series required\" ) if value is not None : info = super (). __getitem__ ( key ) item = CsvCache ( self . cachepath , key , self . lts ) item . update ( value ) #TODO MAKE STATITICS statistics = {} self . db . loc [ key ]= statistics","title":"update"},{"location":"reference/hielen2/datalink_prova_df/#seriescode","text":"class seriescode ( * args , ** kwargs ) View Source class seriescode (): def __init__ ( self ,* args ,** kwargs ): self . h =[ * args ] self . h . extend ( list ( kwargs . values ())) self . h = '' . join ([ str ( a ) for a in self . h ]) self . h = md5 ( f' { self . h }'. encode () ). hexdigest () def __repr__ ( self ): return self . h","title":"seriescode"},{"location":"reference/hielen2/source/","text":"Module hielen2.source View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 #!/usr/bin/env python # coding=utf-8 import os , re from glob import glob from pathlib import Path , os from inspect import ismodule from abc import ABC , abstractmethod from importlib import import_module from hielen2 import db , conf from hielen2.utils import getSchemaDict from marshmallow import Schema , fields , ValidationError from numpy import datetime64 def loadModule ( proto ): if ismodule ( proto ): return proto mod = db [ \"features_proto\" ][ proto ][ \"module\" ] try : return import_module ( mod ) except Exception as e : raise e return proto def moduleActions ( proto ): mod = loadModule ( proto ) try : return [ k . replace ( 'Schema' , '' ) . lower () for k in mod . __dict__ . keys () if 'Schema' in k ] except Exception as e : return [] def getActionSchemaClass ( proto , action ): mod = loadModule ( proto ) return mod . __getattribute__ ( f \" { action . capitalize () } Schema\" ) def getActionSchema ( proto , action ): return getSchemaDict ( getActionSchemaClass ( proto , action )()) def sourceFactory ( feat ): if isinstance ( feat , str ): feat = db [ 'features' ][ feat ] return loadModule ( feat [ 'type' ]) . Source ( feature = feat ) class SourceCache (): def __init__ ( self , syspath , subpath = '' ): self . cache = Path ( syspath ) / subpath def __truediv__ ( self , other ): other = str ( other ) . replace ( f \" { self . cache }{ os . sep } \" , \"\" ) return self . cache / other def mkdir ( self , path = '' ): outpath = self / path os . makedirs ( outpath , exist_ok = True ) return outpath def hasher ( self , * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return re . sub ( \"[^\\d]\" , \"\" , h ) def _agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t class StringTime ( fields . DateTime ): def _deserialize ( self , value , attr , data , ** kwargs ): return str ( super () . _deserialize ( value , attr , data , ** kwargs )) def _serialize ( self , value , attr , obj , ** kwargs ): return _agoodtime ( value ) class ActionSchema ( Schema ): ''' Minimal ActionSchema object. Used to define at least a timestamp ''' timestamp = StringTime ( required = True , allow_none = False ) class HielenSource ( ABC ): def __init__ ( self , feature ): self . __dict__ . update ( feature ) self . module = import_module ( self . __module__ ) #TODO possibili problemi di sicurezza for k , w in conf [ 'syscache' ] . items (): self . __setattr__ ( k , SourceCache ( w , self . uid ) ) def execAction ( self , action , ** kwargs ): aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass () . load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e ) def getActionSchema ( self , action ): return getActionSchema ( self . module , action ) def getActionValues ( self , action = None , timestamp = None ): if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self . uid , action , timestamp ] if not isinstance ( out , list ): out = [ out ] except KeyError : return [] return out def deleteActionValues ( self , action = None , timestamp = None ): out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ): out = [ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \" { a . capitalize () } Schema\" self . __getattribute__ ( f \"clean { a . capitalize () } \" )( t ) except Exception as e : pass try : db [ 'actions' ][ self . uid , a , t ] = None except Exception as e : raise ValueError ( e ) return out @abstractmethod def data ( timefrom = None , timeto = None , geom = None , ** kwargs ): pass Variables conf db Functions getActionSchema def getActionSchema ( proto , action ) View Source def getActionSchema ( proto , action ): return getSchemaDict ( getActionSchemaClass ( proto , action )()) getActionSchemaClass def getActionSchemaClass ( proto , action ) View Source def getActionSchemaClass ( proto , action ): mod = loadModule ( proto ) return mod . __getattribute__ ( f \"{action.capitalize()}Schema\" ) loadModule def loadModule ( proto ) View Source def loadModule ( proto ) : if ismodule ( proto ) : return proto mod = db [ \"features_proto\" ][ proto ][ \"module\" ] try : return import_module ( mod ) except Exception as e : raise e return proto moduleActions def moduleActions ( proto ) View Source def moduleActions ( proto ): mod = loadModule ( proto ) try : return [ k . replace ( 'Schema' , '' ). lower () for k in mod . __dict__ . keys () if 'Schema' in k ] except Exception as e : return [] sourceFactory def sourceFactory ( feat ) View Source def sourceFactory ( feat ) : if isinstance ( feat , str ) : feat = db [ 'features' ][ feat ] return loadModule ( feat [ 'type' ] ). Source ( feature = feat ) Classes ActionSchema class ActionSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) Minimal ActionSchema object. Used to define at least a timestamp View Source class ActionSchema ( Schema ): ''' Minimal ActionSchema object. Used to define at least a timestamp ''' timestamp = StringTime ( required = True , allow_none = False ) Ancestors (in MRO) marshmallow.schema.Schema marshmallow.base.SchemaABC Descendants hielen2.ext.source_photomonitoring.phm.ConfigSchema hielen2.ext.source_photomonitoring.phm.FeedSchema hielen2.ext.source_photomonitoring.phm_ok.ConfigSchema hielen2.ext.source_photomonitoring.phm_ok.FeedSchema hielen2.ext.source_tinsar.ConfigSchema hielen2.ext.source_tinsar.FeedSchema Class variables Meta OPTIONS_CLASS TYPE_MAPPING error_messages opts timestamp Static methods from_dict def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls Instance variables dict_class set_class Methods dump def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result dumps def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs ) get_attribute def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default ) handle_error def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass load def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True ) loads def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown ) on_bind_field def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None validate def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {} HielenSource class HielenSource ( feature ) Helper class that provides a standard way to create an ABC using inheritance. View Source class HielenSource ( ABC ) : def __init__ ( self , feature ) : self . __dict__ . update ( feature ) self . module = import_module ( self . __module__ ) #TODO possibili problemi di sicurezza for k , w in conf [ 'syscache' ] . items () : self . __setattr__ ( k , SourceCache ( w , self . uid ) ) def execAction ( self , action , ** kwargs ) : aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass (). load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e ) def getActionSchema ( self , action ) : return getActionSchema ( self . module , action ) def getActionValues ( self , action = None , timestamp = None ) : if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self.uid,action,timestamp ] if not isinstance ( out , list ) : out = [ out ] except KeyError : return [] return out def deleteActionValues ( self , action = None , timestamp = None ) : out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ) : out =[ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \"{a.capitalize()}Schema\" self . __getattribute__ ( f \"clean{a.capitalize()}\" )( t ) except Exception as e : pass try : db [ 'actions' ][ self.uid,a,t ]= None except Exception as e : raise ValueError ( e ) return out @abstractmethod def data ( timefrom = None , timeto = None , geom = None , ** kwargs ) : pass Ancestors (in MRO) abc.ABC Descendants hielen2.ext.source_photomonitoring.phm.Source hielen2.ext.source_photomonitoring.phm_ok.Source hielen2.ext.source_tinsar.Source Methods data def data ( timefrom = None , timeto = None , geom = None , ** kwargs ) View Source @abstractmethod def data ( timefrom = None , timeto = None , geom = None , ** kwargs ) : pass deleteActionValues def deleteActionValues ( self , action = None , timestamp = None ) View Source def deleteActionValues ( self , action = None , timestamp = None ) : out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ) : out =[ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \"{a.capitalize()}Schema\" self . __getattribute__ ( f \"clean{a.capitalize()}\" )( t ) except Exception as e : pass try : db [ 'actions' ][ self.uid,a,t ]= None except Exception as e : raise ValueError ( e ) return out execAction def execAction ( self , action , ** kwargs ) View Source def execAction ( self , action , ** kwargs ): aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass (). load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e ) getActionSchema def getActionSchema ( self , action ) View Source def getActionSchema ( self , action ): return getActionSchema ( self . module , action ) getActionValues def getActionValues ( self , action = None , timestamp = None ) View Source def getActionValues ( self , action = None , timestamp = None ) : if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self.uid,action,timestamp ] if not isinstance ( out , list ) : out = [ out ] except KeyError : return [] return out SourceCache class SourceCache ( syspath , subpath = '' ) View Source class SourceCache (): def __init__ ( self , syspath , subpath = '' ): self . cache = Path ( syspath ) / subpath def __truediv__ ( self , other ): other = str ( other ). replace ( f \"{self.cache}{os.sep}\" , \"\" ) return self . cache / other def mkdir ( self , path = '' ): outpath = self / path os . makedirs ( outpath , exist_ok = True ) return outpath def hasher ( self ,* args ,** kwargs ): h =[ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return re . sub ( \"[^\\d]\" , \"\" , h ) Methods hasher def hasher ( self , * args , ** kwargs ) View Source def hasher ( self , * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return re . sub ( \"[^\\d]\" , \"\" , h ) mkdir def mkdir ( self , path = '' ) View Source def mkdir ( self , path = '' ): outpath = self / path os . makedirs ( outpath , exist_ok = True ) return outpath StringTime class StringTime ( format : Union [ str , NoneType ] = None , ** kwargs ) A formatted datetime string. Example: '2014-12-22T03:12:58.019077+00:00' :param format: Either \"rfc\" (for RFC822), \"iso\" (for ISO8601), or a date format string. If None , defaults to \"iso\". :param kwargs: The same keyword arguments that :class: Field receives. .. versionchanged:: 3.0.0rc9 Does not modify timezone information on (de)serialization. View Source class StringTime ( fields . DateTime ): def _deserialize ( self , value , attr , data , ** kwargs ): return str ( super (). _deserialize ( value , attr , data , ** kwargs )) def _serialize ( self , value , attr , obj , ** kwargs ): return _agoodtime ( value ) Ancestors (in MRO) marshmallow.fields.DateTime marshmallow.fields.Field marshmallow.base.FieldABC Class variables DEFAULT_FORMAT DESERIALIZATION_FUNCS OBJ_TYPE SCHEMA_OPTS_VAR_NAME SERIALIZATION_FUNCS default_error_messages name parent Instance variables context The context dictionary for the parent :class: Schema . root Reference to the Schema that this field belongs to even if it is buried in a container field (e.g. List ). Return None for unbound fields. Methods deserialize def deserialize ( self , value : Any , attr : Union [ str , NoneType ] = None , data : Union [ Mapping [ str , Any ], NoneType ] = None , ** kwargs ) Deserialize value . :param value: The value to deserialize. :param attr: The attribute/key in data to deserialize. :param data: The raw input data passed to Schema.load . :param kwargs: Field-specific keyword arguments. :raise ValidationError: If an invalid value is passed or if a required value is missing. View Source def deserialize ( self , value : typing . Any , attr : typing . Optional [ str ] = None , data : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ** kwargs ): \"\"\"Deserialize ``value``. :param value: The value to deserialize. :param attr: The attribute/key in `data` to deserialize. :param data: The raw input data passed to `Schema.load`. :param kwargs: Field-specific keyword arguments. :raise ValidationError: If an invalid value is passed or if a required value is missing. \"\"\" # Validate required fields, deserialize, then validate # deserialized value self . _validate_missing ( value ) if value is missing_ : _miss = self . missing return _miss () if callable ( _miss ) else _miss if getattr ( self , \"allow_none\" , False ) is True and value is None : return None output = self . _deserialize ( value , attr , data , ** kwargs ) self . _validate ( output ) return output fail def fail ( self , key : str , ** kwargs ) Helper method that raises a ValidationError with an error message from self.error_messages . .. deprecated:: 3.0.0 Use make_error <marshmallow.fields.Field.make_error> instead. View Source def fail ( self , key : str , ** kwargs ): \"\"\"Helper method that raises a `ValidationError` with an error message from ``self.error_messages``. .. deprecated:: 3.0.0 Use `make_error <marshmallow.fields.Field.make_error>` instead. \"\"\" warnings . warn ( '`Field.fail` is deprecated. Use `raise self.make_error(\"{}\", ...)` instead.' . format ( key ), RemovedInMarshmallow4Warning , ) raise self . make_error ( key = key , ** kwargs ) get_value def get_value ( self , obj , attr , accessor = None , default =< marshmallow . missing > ) Return the value for a given key from an object. :param object obj: The object to get the value from. :param str attr: The attribute/key in obj to get the value from. :param callable accessor: A callable used to retrieve the value of attr from the object obj . Defaults to marshmallow.utils.get_value . View Source def get_value ( self , obj , attr , accessor = None , default = missing_ ): \"\"\"Return the value for a given key from an object. :param object obj: The object to get the value from. :param str attr: The attribute/key in `obj` to get the value from. :param callable accessor: A callable used to retrieve the value of `attr` from the object `obj`. Defaults to `marshmallow.utils.get_value`. \"\"\" # NOTE: Use getattr instead of direct attribute access here so that # subclasses aren't required to define `attribute` member attribute = getattr ( self , \"attribute\" , None ) accessor_func = accessor or utils . get_value check_key = attr if attribute is None else attribute return accessor_func ( obj , check_key , default ) make_error def make_error ( self , key : str , ** kwargs ) -> marshmallow . exceptions . ValidationError Helper method to make a ValidationError with an error message from self.error_messages . View Source def make_error ( self , key : str , ** kwargs ) -> ValidationError : \"\"\"Helper method to make a `ValidationError` with an error message from ``self.error_messages``. \"\"\" try : msg = self . error_messages [ key ] except KeyError as error : class_name = self . __class__ . __name__ message = ( \"ValidationError raised by `{class_name}`, but error key `{key}` does \" \"not exist in the `error_messages` dictionary.\" ). format ( class_name = class_name , key = key ) raise AssertionError ( message ) from error if isinstance ( msg , ( str , bytes )): msg = msg . format ( ** kwargs ) return ValidationError ( msg ) serialize def serialize ( self , attr : str , obj : Any , accessor : Union [ Callable [[ Any , str , Any ], Any ], NoneType ] = None , ** kwargs ) Pulls the value for the given key from the object, applies the field's formatting and returns the result. :param attr: The attribute/key to get from the object. :param obj: The object to access the attribute/key from. :param accessor: Function used to access values from obj . :param kwargs: Field-specific keyword arguments. View Source def serialize ( self , attr : str , obj : typing . Any , accessor : typing . Optional [ typing . Callable [[ typing . Any , str , typing . Any ], typing . Any ] ] = None , ** kwargs ): \"\"\"Pulls the value for the given key from the object, applies the field's formatting and returns the result. :param attr: The attribute/key to get from the object. :param obj: The object to access the attribute/key from. :param accessor: Function used to access values from ``obj``. :param kwargs: Field-specific keyword arguments. \"\"\" if self . _CHECK_ATTRIBUTE : value = self . get_value ( obj , attr , accessor = accessor ) if value is missing_ and hasattr ( self , \"default\" ): default = self . default value = default () if callable ( default ) else default if value is missing_ : return value else : value = None return self . _serialize ( value , attr , obj , ** kwargs )","title":"Source"},{"location":"reference/hielen2/source/#module-hielen2source","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 #!/usr/bin/env python # coding=utf-8 import os , re from glob import glob from pathlib import Path , os from inspect import ismodule from abc import ABC , abstractmethod from importlib import import_module from hielen2 import db , conf from hielen2.utils import getSchemaDict from marshmallow import Schema , fields , ValidationError from numpy import datetime64 def loadModule ( proto ): if ismodule ( proto ): return proto mod = db [ \"features_proto\" ][ proto ][ \"module\" ] try : return import_module ( mod ) except Exception as e : raise e return proto def moduleActions ( proto ): mod = loadModule ( proto ) try : return [ k . replace ( 'Schema' , '' ) . lower () for k in mod . __dict__ . keys () if 'Schema' in k ] except Exception as e : return [] def getActionSchemaClass ( proto , action ): mod = loadModule ( proto ) return mod . __getattribute__ ( f \" { action . capitalize () } Schema\" ) def getActionSchema ( proto , action ): return getSchemaDict ( getActionSchemaClass ( proto , action )()) def sourceFactory ( feat ): if isinstance ( feat , str ): feat = db [ 'features' ][ feat ] return loadModule ( feat [ 'type' ]) . Source ( feature = feat ) class SourceCache (): def __init__ ( self , syspath , subpath = '' ): self . cache = Path ( syspath ) / subpath def __truediv__ ( self , other ): other = str ( other ) . replace ( f \" { self . cache }{ os . sep } \" , \"\" ) return self . cache / other def mkdir ( self , path = '' ): outpath = self / path os . makedirs ( outpath , exist_ok = True ) return outpath def hasher ( self , * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return re . sub ( \"[^\\d]\" , \"\" , h ) def _agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t class StringTime ( fields . DateTime ): def _deserialize ( self , value , attr , data , ** kwargs ): return str ( super () . _deserialize ( value , attr , data , ** kwargs )) def _serialize ( self , value , attr , obj , ** kwargs ): return _agoodtime ( value ) class ActionSchema ( Schema ): ''' Minimal ActionSchema object. Used to define at least a timestamp ''' timestamp = StringTime ( required = True , allow_none = False ) class HielenSource ( ABC ): def __init__ ( self , feature ): self . __dict__ . update ( feature ) self . module = import_module ( self . __module__ ) #TODO possibili problemi di sicurezza for k , w in conf [ 'syscache' ] . items (): self . __setattr__ ( k , SourceCache ( w , self . uid ) ) def execAction ( self , action , ** kwargs ): aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass () . load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e ) def getActionSchema ( self , action ): return getActionSchema ( self . module , action ) def getActionValues ( self , action = None , timestamp = None ): if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self . uid , action , timestamp ] if not isinstance ( out , list ): out = [ out ] except KeyError : return [] return out def deleteActionValues ( self , action = None , timestamp = None ): out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ): out = [ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \" { a . capitalize () } Schema\" self . __getattribute__ ( f \"clean { a . capitalize () } \" )( t ) except Exception as e : pass try : db [ 'actions' ][ self . uid , a , t ] = None except Exception as e : raise ValueError ( e ) return out @abstractmethod def data ( timefrom = None , timeto = None , geom = None , ** kwargs ): pass","title":"Module hielen2.source"},{"location":"reference/hielen2/source/#variables","text":"conf db","title":"Variables"},{"location":"reference/hielen2/source/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/source/#getactionschema","text":"def getActionSchema ( proto , action ) View Source def getActionSchema ( proto , action ): return getSchemaDict ( getActionSchemaClass ( proto , action )())","title":"getActionSchema"},{"location":"reference/hielen2/source/#getactionschemaclass","text":"def getActionSchemaClass ( proto , action ) View Source def getActionSchemaClass ( proto , action ): mod = loadModule ( proto ) return mod . __getattribute__ ( f \"{action.capitalize()}Schema\" )","title":"getActionSchemaClass"},{"location":"reference/hielen2/source/#loadmodule","text":"def loadModule ( proto ) View Source def loadModule ( proto ) : if ismodule ( proto ) : return proto mod = db [ \"features_proto\" ][ proto ][ \"module\" ] try : return import_module ( mod ) except Exception as e : raise e return proto","title":"loadModule"},{"location":"reference/hielen2/source/#moduleactions","text":"def moduleActions ( proto ) View Source def moduleActions ( proto ): mod = loadModule ( proto ) try : return [ k . replace ( 'Schema' , '' ). lower () for k in mod . __dict__ . keys () if 'Schema' in k ] except Exception as e : return []","title":"moduleActions"},{"location":"reference/hielen2/source/#sourcefactory","text":"def sourceFactory ( feat ) View Source def sourceFactory ( feat ) : if isinstance ( feat , str ) : feat = db [ 'features' ][ feat ] return loadModule ( feat [ 'type' ] ). Source ( feature = feat )","title":"sourceFactory"},{"location":"reference/hielen2/source/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/source/#actionschema","text":"class ActionSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) Minimal ActionSchema object. Used to define at least a timestamp View Source class ActionSchema ( Schema ): ''' Minimal ActionSchema object. Used to define at least a timestamp ''' timestamp = StringTime ( required = True , allow_none = False )","title":"ActionSchema"},{"location":"reference/hielen2/source/#ancestors-in-mro","text":"marshmallow.schema.Schema marshmallow.base.SchemaABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/source/#descendants","text":"hielen2.ext.source_photomonitoring.phm.ConfigSchema hielen2.ext.source_photomonitoring.phm.FeedSchema hielen2.ext.source_photomonitoring.phm_ok.ConfigSchema hielen2.ext.source_photomonitoring.phm_ok.FeedSchema hielen2.ext.source_tinsar.ConfigSchema hielen2.ext.source_tinsar.FeedSchema","title":"Descendants"},{"location":"reference/hielen2/source/#class-variables","text":"Meta OPTIONS_CLASS TYPE_MAPPING error_messages opts timestamp","title":"Class variables"},{"location":"reference/hielen2/source/#static-methods","text":"","title":"Static methods"},{"location":"reference/hielen2/source/#from_dict","text":"def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls","title":"from_dict"},{"location":"reference/hielen2/source/#instance-variables","text":"dict_class set_class","title":"Instance variables"},{"location":"reference/hielen2/source/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/source/#dump","text":"def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result","title":"dump"},{"location":"reference/hielen2/source/#dumps","text":"def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs )","title":"dumps"},{"location":"reference/hielen2/source/#get_attribute","text":"def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default )","title":"get_attribute"},{"location":"reference/hielen2/source/#handle_error","text":"def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass","title":"handle_error"},{"location":"reference/hielen2/source/#load","text":"def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True )","title":"load"},{"location":"reference/hielen2/source/#loads","text":"def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown )","title":"loads"},{"location":"reference/hielen2/source/#on_bind_field","text":"def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None","title":"on_bind_field"},{"location":"reference/hielen2/source/#validate","text":"def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"validate"},{"location":"reference/hielen2/source/#hielensource","text":"class HielenSource ( feature ) Helper class that provides a standard way to create an ABC using inheritance. View Source class HielenSource ( ABC ) : def __init__ ( self , feature ) : self . __dict__ . update ( feature ) self . module = import_module ( self . __module__ ) #TODO possibili problemi di sicurezza for k , w in conf [ 'syscache' ] . items () : self . __setattr__ ( k , SourceCache ( w , self . uid ) ) def execAction ( self , action , ** kwargs ) : aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass (). load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e ) def getActionSchema ( self , action ) : return getActionSchema ( self . module , action ) def getActionValues ( self , action = None , timestamp = None ) : if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self.uid,action,timestamp ] if not isinstance ( out , list ) : out = [ out ] except KeyError : return [] return out def deleteActionValues ( self , action = None , timestamp = None ) : out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ) : out =[ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \"{a.capitalize()}Schema\" self . __getattribute__ ( f \"clean{a.capitalize()}\" )( t ) except Exception as e : pass try : db [ 'actions' ][ self.uid,a,t ]= None except Exception as e : raise ValueError ( e ) return out @abstractmethod def data ( timefrom = None , timeto = None , geom = None , ** kwargs ) : pass","title":"HielenSource"},{"location":"reference/hielen2/source/#ancestors-in-mro_1","text":"abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/source/#descendants_1","text":"hielen2.ext.source_photomonitoring.phm.Source hielen2.ext.source_photomonitoring.phm_ok.Source hielen2.ext.source_tinsar.Source","title":"Descendants"},{"location":"reference/hielen2/source/#methods_1","text":"","title":"Methods"},{"location":"reference/hielen2/source/#data","text":"def data ( timefrom = None , timeto = None , geom = None , ** kwargs ) View Source @abstractmethod def data ( timefrom = None , timeto = None , geom = None , ** kwargs ) : pass","title":"data"},{"location":"reference/hielen2/source/#deleteactionvalues","text":"def deleteActionValues ( self , action = None , timestamp = None ) View Source def deleteActionValues ( self , action = None , timestamp = None ) : out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ) : out =[ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \"{a.capitalize()}Schema\" self . __getattribute__ ( f \"clean{a.capitalize()}\" )( t ) except Exception as e : pass try : db [ 'actions' ][ self.uid,a,t ]= None except Exception as e : raise ValueError ( e ) return out","title":"deleteActionValues"},{"location":"reference/hielen2/source/#execaction","text":"def execAction ( self , action , ** kwargs ) View Source def execAction ( self , action , ** kwargs ): aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass (). load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e )","title":"execAction"},{"location":"reference/hielen2/source/#getactionschema_1","text":"def getActionSchema ( self , action ) View Source def getActionSchema ( self , action ): return getActionSchema ( self . module , action )","title":"getActionSchema"},{"location":"reference/hielen2/source/#getactionvalues","text":"def getActionValues ( self , action = None , timestamp = None ) View Source def getActionValues ( self , action = None , timestamp = None ) : if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self.uid,action,timestamp ] if not isinstance ( out , list ) : out = [ out ] except KeyError : return [] return out","title":"getActionValues"},{"location":"reference/hielen2/source/#sourcecache","text":"class SourceCache ( syspath , subpath = '' ) View Source class SourceCache (): def __init__ ( self , syspath , subpath = '' ): self . cache = Path ( syspath ) / subpath def __truediv__ ( self , other ): other = str ( other ). replace ( f \"{self.cache}{os.sep}\" , \"\" ) return self . cache / other def mkdir ( self , path = '' ): outpath = self / path os . makedirs ( outpath , exist_ok = True ) return outpath def hasher ( self ,* args ,** kwargs ): h =[ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return re . sub ( \"[^\\d]\" , \"\" , h )","title":"SourceCache"},{"location":"reference/hielen2/source/#methods_2","text":"","title":"Methods"},{"location":"reference/hielen2/source/#hasher","text":"def hasher ( self , * args , ** kwargs ) View Source def hasher ( self , * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return re . sub ( \"[^\\d]\" , \"\" , h )","title":"hasher"},{"location":"reference/hielen2/source/#mkdir","text":"def mkdir ( self , path = '' ) View Source def mkdir ( self , path = '' ): outpath = self / path os . makedirs ( outpath , exist_ok = True ) return outpath","title":"mkdir"},{"location":"reference/hielen2/source/#stringtime","text":"class StringTime ( format : Union [ str , NoneType ] = None , ** kwargs ) A formatted datetime string. Example: '2014-12-22T03:12:58.019077+00:00' :param format: Either \"rfc\" (for RFC822), \"iso\" (for ISO8601), or a date format string. If None , defaults to \"iso\". :param kwargs: The same keyword arguments that :class: Field receives. .. versionchanged:: 3.0.0rc9 Does not modify timezone information on (de)serialization. View Source class StringTime ( fields . DateTime ): def _deserialize ( self , value , attr , data , ** kwargs ): return str ( super (). _deserialize ( value , attr , data , ** kwargs )) def _serialize ( self , value , attr , obj , ** kwargs ): return _agoodtime ( value )","title":"StringTime"},{"location":"reference/hielen2/source/#ancestors-in-mro_2","text":"marshmallow.fields.DateTime marshmallow.fields.Field marshmallow.base.FieldABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/source/#class-variables_1","text":"DEFAULT_FORMAT DESERIALIZATION_FUNCS OBJ_TYPE SCHEMA_OPTS_VAR_NAME SERIALIZATION_FUNCS default_error_messages name parent","title":"Class variables"},{"location":"reference/hielen2/source/#instance-variables_1","text":"context The context dictionary for the parent :class: Schema . root Reference to the Schema that this field belongs to even if it is buried in a container field (e.g. List ). Return None for unbound fields.","title":"Instance variables"},{"location":"reference/hielen2/source/#methods_3","text":"","title":"Methods"},{"location":"reference/hielen2/source/#deserialize","text":"def deserialize ( self , value : Any , attr : Union [ str , NoneType ] = None , data : Union [ Mapping [ str , Any ], NoneType ] = None , ** kwargs ) Deserialize value . :param value: The value to deserialize. :param attr: The attribute/key in data to deserialize. :param data: The raw input data passed to Schema.load . :param kwargs: Field-specific keyword arguments. :raise ValidationError: If an invalid value is passed or if a required value is missing. View Source def deserialize ( self , value : typing . Any , attr : typing . Optional [ str ] = None , data : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ** kwargs ): \"\"\"Deserialize ``value``. :param value: The value to deserialize. :param attr: The attribute/key in `data` to deserialize. :param data: The raw input data passed to `Schema.load`. :param kwargs: Field-specific keyword arguments. :raise ValidationError: If an invalid value is passed or if a required value is missing. \"\"\" # Validate required fields, deserialize, then validate # deserialized value self . _validate_missing ( value ) if value is missing_ : _miss = self . missing return _miss () if callable ( _miss ) else _miss if getattr ( self , \"allow_none\" , False ) is True and value is None : return None output = self . _deserialize ( value , attr , data , ** kwargs ) self . _validate ( output ) return output","title":"deserialize"},{"location":"reference/hielen2/source/#fail","text":"def fail ( self , key : str , ** kwargs ) Helper method that raises a ValidationError with an error message from self.error_messages . .. deprecated:: 3.0.0 Use make_error <marshmallow.fields.Field.make_error> instead. View Source def fail ( self , key : str , ** kwargs ): \"\"\"Helper method that raises a `ValidationError` with an error message from ``self.error_messages``. .. deprecated:: 3.0.0 Use `make_error <marshmallow.fields.Field.make_error>` instead. \"\"\" warnings . warn ( '`Field.fail` is deprecated. Use `raise self.make_error(\"{}\", ...)` instead.' . format ( key ), RemovedInMarshmallow4Warning , ) raise self . make_error ( key = key , ** kwargs )","title":"fail"},{"location":"reference/hielen2/source/#get_value","text":"def get_value ( self , obj , attr , accessor = None , default =< marshmallow . missing > ) Return the value for a given key from an object. :param object obj: The object to get the value from. :param str attr: The attribute/key in obj to get the value from. :param callable accessor: A callable used to retrieve the value of attr from the object obj . Defaults to marshmallow.utils.get_value . View Source def get_value ( self , obj , attr , accessor = None , default = missing_ ): \"\"\"Return the value for a given key from an object. :param object obj: The object to get the value from. :param str attr: The attribute/key in `obj` to get the value from. :param callable accessor: A callable used to retrieve the value of `attr` from the object `obj`. Defaults to `marshmallow.utils.get_value`. \"\"\" # NOTE: Use getattr instead of direct attribute access here so that # subclasses aren't required to define `attribute` member attribute = getattr ( self , \"attribute\" , None ) accessor_func = accessor or utils . get_value check_key = attr if attribute is None else attribute return accessor_func ( obj , check_key , default )","title":"get_value"},{"location":"reference/hielen2/source/#make_error","text":"def make_error ( self , key : str , ** kwargs ) -> marshmallow . exceptions . ValidationError Helper method to make a ValidationError with an error message from self.error_messages . View Source def make_error ( self , key : str , ** kwargs ) -> ValidationError : \"\"\"Helper method to make a `ValidationError` with an error message from ``self.error_messages``. \"\"\" try : msg = self . error_messages [ key ] except KeyError as error : class_name = self . __class__ . __name__ message = ( \"ValidationError raised by `{class_name}`, but error key `{key}` does \" \"not exist in the `error_messages` dictionary.\" ). format ( class_name = class_name , key = key ) raise AssertionError ( message ) from error if isinstance ( msg , ( str , bytes )): msg = msg . format ( ** kwargs ) return ValidationError ( msg )","title":"make_error"},{"location":"reference/hielen2/source/#serialize","text":"def serialize ( self , attr : str , obj : Any , accessor : Union [ Callable [[ Any , str , Any ], Any ], NoneType ] = None , ** kwargs ) Pulls the value for the given key from the object, applies the field's formatting and returns the result. :param attr: The attribute/key to get from the object. :param obj: The object to access the attribute/key from. :param accessor: Function used to access values from obj . :param kwargs: Field-specific keyword arguments. View Source def serialize ( self , attr : str , obj : typing . Any , accessor : typing . Optional [ typing . Callable [[ typing . Any , str , typing . Any ], typing . Any ] ] = None , ** kwargs ): \"\"\"Pulls the value for the given key from the object, applies the field's formatting and returns the result. :param attr: The attribute/key to get from the object. :param obj: The object to access the attribute/key from. :param accessor: Function used to access values from ``obj``. :param kwargs: Field-specific keyword arguments. \"\"\" if self . _CHECK_ATTRIBUTE : value = self . get_value ( obj , attr , accessor = accessor ) if value is missing_ and hasattr ( self , \"default\" ): default = self . default value = default () if callable ( default ) else default if value is missing_ : return value else : value = None return self . _serialize ( value , attr , obj , ** kwargs )","title":"serialize"},{"location":"reference/hielen2/utils/","text":"Module hielen2.utils View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 #!/usr/bin/env python # coding=utf-8 from datetime import datetime from re import split , sub , findall from time import mktime import json from importlib import import_module from falcon import HTTPNotAcceptable from hashlib import md5 from marshmallow import Schema , fields def hug_output_format_conten_type ( handlers = [], error = \"The requested format does not match any of those allowed\" , ctpar = \"content_type\" , ): \"\"\"Returns a different handler depending on the input param ctpar If none match and no default is given falcon.HTTPNotAcceptable(error) is raised \"\"\" try : default = handlers [ 0 ] except Exception : default = None handlers = { h . content_type : h for h in handlers } def requested_output_type ( request = None ): try : par = request . _params [ ctpar ] handler = None for k , h in handlers . items (): if par . split ( \";\" )[ 0 ] == k . split ( \";\" )[ 0 ]: handler = h break except Exception : if default is not None : handler = default if handler is None : raise HTTPNotAcceptable ( error ) return handler def output_type ( data , request , response ): handler = requested_output_type ( request ) response . content_type = handler . content_type return handler ( data , request = request , response = response ) output_type . __doc__ = \"Supports any of the following formats: {0} \" . format ( \", \" . join ( function . __doc__ for function in handlers . values ()) ) output_type . content_type = \", \" . join ( handlers . keys ()) output_type . requested = requested_output_type return output_type def newinstanceof ( klass , * args , ** kwargs ): klass_ar = klass . split ( \".\" ) module = \".\" . join ( klass_ar [: - 1 ]) klass = klass_ar [ - 1 ] return getattr ( import_module ( module ), klass )( * args , ** kwargs ) def ut2isot ( u = None ): u = u or 1 return str ( datetime . fromtimestamp ( u )) def isot2ut ( t = None ): t = t or \"1970-01-01T01:00:01.00000Z\" dt = datetime ( * map ( int , split ( \"[^\\d]\" , sub ( \"[^\\d]$\" , \"\" , t )))) return int ( mktime ( dt . timetuple ())) def loadjsonfile ( filename ): with open ( filename ) as jf : return json . load ( jf ) def savejsonfile ( filename , struct ): with open ( filename , \"w\" ) as jf : json . dump ( struct , jf ) def eprint ( * args , fname = \"error\" , ** kwargs ): with open ( fname , \"a\" ) as f : print ( * args , file = f , ** kwargs ) def hasher ( * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return md5 ( f ' { h } ' . encode () ) . hexdigest () def hashfile ( filename ): BLOCKSIZE = 65536 hasher = md5 () with open ( filename , \"rb\" ) as afile : buf = afile . read ( BLOCKSIZE ) while len ( buf ) > 0 : hasher . update ( buf ) buf = afile . read ( BLOCKSIZE ) return hasher . hexdigest () ### MARSHMALLOW def getSchemaDict ( schema ): out = { \"fields\" :{}, \"required\" :[], \"hint\" : schema . __doc__ } for k , w in schema . dump_fields . items (): out [ 'fields' ][ k ] = w . __class__ . __name__ w . __dict__ [ 'required' ] and out [ \"required\" ] . append ( k ) return out class LocalFile ( fields . String ): \"\"\" Local Filepath manager used to identify a file into the system. Mainliy usefull for action Schema declaration in hielen2.HielenSource extention \"\"\" pass class Selection ( fields . String ): \"\"\" Provides python object which pertims selection on narry. It axcept a three filed \\ string separated by \":\". \":\" presence is managed as: \"start:stop:step\" ie.: \"start:stop\" - extracts from start to stop \"start:\" - extracts from start to max \"start\" - extract exactly start \"\"\" def _deserialize ( self , value , attr , data , ** kwargs ): try : if value is None or value == \"\" : #return [None,None,None] return slice ( None , None , None ) value = [ v or None for v in value . split ( ';' ) ] if value . __len__ () == 1 : return slice ( value [ 0 ], value [ 0 ]) return slice ( * value [ 0 : 3 ]) #return value[0:3] except Exception as e : raise ValueError ( e ) class JsonValidable : \"\"\" JSON Validator class. It is initailzed with a marshmallow.Schema instance. When __call__ function is invoked, \\ uses marshmallow facilities to validate the json and raise errors. Once initalized, changes __doc__ in order to descibe the json accepted. \"\"\" def __field_doc__ ( self , field ): required = field . required and \"!\" or \"\" allow_none = not field . allow_none and \"!\" or \"\" try : types = \"|\" . join ( self . TYPE_MAPPING [ field . __class__ ]) except KeyError : if field . __class__ is fields . List : f , required , allow_none = self . __field_doc__ ( field . inner ) types = f \"[ { f } ]\" elif field . __class__ is fields . Dict : kf , required , allow_none = self . __field_doc__ ( field . key_field ) vf , required , allow_none = self . __field_doc__ ( field . value_field ) types = f \" {{ { kf } , { vf } }} \" else : types = \"\" return ( types , required , allow_none ) def __schema_doc__ ( self ): flds = [] for n , f in self . schema . fields . items (): types , required , allow_none = self . __field_doc__ ( f ) # TODO formattare required e allow_none # flds.append( f\"**{n}**{required}{allow_none}: {types}\") flds . append ( f \"** { n } **: { types } \" ) fields = \", \" . join ( flds ) fields = f \" {{ { fields } }} \" if self . schema . many : fields = f \"[ { fields } ]\" return f \"JSON Schema { fields } \" def __init__ ( self , schema ): self . schema = schema self . TYPE_MAPPING = {} for k , w in self . schema . TYPE_MAPPING . items (): try : self . TYPE_MAPPING [ w ] . append ( findall ( r \"'(.*)'\" , str ( k ))[ 0 ]) except KeyError : self . TYPE_MAPPING [ w ] = [ findall ( r \"'(.*)'\" , str ( k ))[ 0 ]] self . __doc__ = str ( self . __schema_doc__ ()) def __call__ ( self , value ): if type ( value ) is list : # If Falcon is set to comma-separate entries, this segment joins them again. fixed_value = \",\" . join ( value ) else : fixed_value = value return self . schema . loads ( fixed_value ) Functions eprint def eprint ( * args , fname = 'error' , ** kwargs ) View Source def eprint ( * args , fname = \"error\" , ** kwargs ): with open ( fname , \"a\" ) as f : print ( * args , file = f , ** kwargs ) getSchemaDict def getSchemaDict ( schema ) View Source def getSchemaDict ( schema ) : out = { \"fields\" :{} , \"required\" :[] , \"hint\" : schema . __doc__ } for k , w in schema . dump_fields . items () : out [ 'fields' ][ k ]= w . __class__ . __name__ w . __dict__ [ 'required' ] and out [ \"required\" ] . append ( k ) return out hasher def hasher ( * args , ** kwargs ) View Source def hasher ( * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return md5 ( f '{h}' . encode () ). hexdigest () hashfile def hashfile ( filename ) View Source def hashfile ( filename ): BLOCKSIZE = 65536 hasher = md5 () with open ( filename , \"rb\" ) as afile : buf = afile . read ( BLOCKSIZE ) while len ( buf ) > 0 : hasher . update ( buf ) buf = afile . read ( BLOCKSIZE ) return hasher . hexdigest () hug_output_format_conten_type def hug_output_format_conten_type ( handlers = [], error = 'The requested format does not match any of those allowed' , ctpar = 'content_type' ) Returns a different handler depending on the input param ctpar If none match and no default is given falcon.HTTPNotAcceptable(error) is raised View Source def hug_output_format_conten_type ( handlers = [] , error = \"The requested format does not match any of those allowed\" , ctpar = \"content_type\" , ) : \"\"\"Returns a different handler depending on the input param ctpar If none match and no default is given falcon.HTTPNotAcceptable(error) is raised \"\"\" try : default = handlers [ 0 ] except Exception : default = None handlers = { h . content_type : h for h in handlers } def requested_output_type ( request = None ) : try : par = request . _params [ ctpar ] handler = None for k , h in handlers . items () : if par . split ( \";\" ) [ 0 ] == k . split ( \";\" ) [ 0 ] : handler = h break except Exception : if default is not None : handler = default if handler is None : raise HTTPNotAcceptable ( error ) return handler def output_type ( data , request , response ) : handler = requested_output_type ( request ) response . content_type = handler . content_type return handler ( data , request = request , response = response ) output_type . __doc__ = \"Supports any of the following formats: {0}\" . format ( \", \" . join ( function . __doc__ for function in handlers . values ()) ) output_type . content_type = \", \" . join ( handlers . keys ()) output_type . requested = requested_output_type return output_type isot2ut def isot2ut ( t = None ) View Source def isot2ut ( t = None ): t = t or \"1970-01-01T01:00:01.00000Z\" dt = datetime ( * map ( int , split ( \"[^\\d]\" , sub ( \"[^\\d]$\" , \"\" , t )))) return int ( mktime ( dt . timetuple ())) loadjsonfile def loadjsonfile ( filename ) View Source def loadjsonfile ( filename ): with open ( filename ) as jf : return json . load ( jf ) newinstanceof def newinstanceof ( klass , * args , ** kwargs ) View Source def newinstanceof ( klass , * args , **kwargs ) : klass_ar = klass . split ( \".\" ) module = \".\" . join ( klass_ar [:- 1 ]) klass = klass_ar [ - 1 ] return getattr ( import_module ( module ), klass )( * args , **kwargs ) savejsonfile def savejsonfile ( filename , struct ) View Source def savejsonfile ( filename , struct ): with open ( filename , \"w\" ) as jf : json . dump ( struct , jf ) ut2isot def ut2isot ( u = None ) View Source def ut2isot ( u = None ): u = u or 1 return str ( datetime . fromtimestamp ( u )) Classes JsonValidable class JsonValidable ( schema ) JSON Validator class. It is initailzed with a marshmallow.Schema instance. When call function is invoked, uses marshmallow facilities to validate the json and raise errors. Once initalized, changes doc in order to descibe the json accepted. View Source class JsonValidable : \"\"\" JSON Validator class. It is initailzed with a marshmallow.Schema instance. When __call__ function is invoked, \\ uses marshmallow facilities to validate the json and raise errors. Once initalized, changes __doc__ in order to descibe the json accepted. \"\"\" def __field_doc__ ( self , field ) : required = field . required and \"!\" or \"\" allow_none = not field . allow_none and \"!\" or \"\" try : types = \"|\" . join ( self . TYPE_MAPPING [ field.__class__ ] ) except KeyError : if field . __class__ is fields . List : f , required , allow_none = self . __field_doc__ ( field . inner ) types = f \"[{f}]\" elif field . __class__ is fields . Dict : kf , required , allow_none = self . __field_doc__ ( field . key_field ) vf , required , allow_none = self . __field_doc__ ( field . value_field ) types = f \"{{{kf},{vf}}}\" else : types = \"\" return ( types , required , allow_none ) def __schema_doc__ ( self ) : flds = [] for n , f in self . schema . fields . items () : types , required , allow_none = self . __field_doc__ ( f ) # TODO formattare required e allow_none # flds . append ( f \"**{n}**{required}{allow_none}: {types}\" ) flds . append ( f \"**{n}**: {types}\" ) fields = \", \" . join ( flds ) fields = f \"{{{fields}}}\" if self . schema . many : fields = f \"[{fields}]\" return f \"JSON Schema {fields}\" def __init__ ( self , schema ) : self . schema = schema self . TYPE_MAPPING = {} for k , w in self . schema . TYPE_MAPPING . items () : try : self . TYPE_MAPPING [ w ] . append ( findall ( r \"'(.*)'\" , str ( k )) [ 0 ] ) except KeyError : self . TYPE_MAPPING [ w ] = [ findall(r\"'(.*)'\", str(k))[0 ] ] self . __doc__ = str ( self . __schema_doc__ ()) def __call__ ( self , value ) : if type ( value ) is list : # If Falcon is set to comma - separate entries , this segment joins them again . fixed_value = \",\" . join ( value ) else : fixed_value = value return self . schema . loads ( fixed_value ) LocalFile class LocalFile ( * , default : Any = < marshmallow . missing > , missing : Any = < marshmallow . missing > , data_key : Union [ str , NoneType ] = None , attribute : Union [ str , NoneType ] = None , validate : Union [ Callable [[ Any ], Any ], Iterable [ Callable [[ Any ], Any ]], NoneType ] = None , required : bool = False , allow_none : Union [ bool , NoneType ] = None , load_only : bool = False , dump_only : bool = False , error_messages : Union [ Dict [ str , str ], NoneType ] = None , ** metadata ) Local Filepath manager used to identify a file into the system. Mainliy usefull for action Schema declaration in hielen2.HielenSource extention View Source class LocalFile ( fields . String ): \"\"\" Local Filepath manager used to identify a file into the system. Mainliy usefull for action Schema declaration in hielen2.HielenSource extention \"\"\" pass Ancestors (in MRO) marshmallow.fields.String marshmallow.fields.Field marshmallow.base.FieldABC Class variables default_error_messages name parent Instance variables context The context dictionary for the parent :class: Schema . root Reference to the Schema that this field belongs to even if it is buried in a container field (e.g. List ). Return None for unbound fields. Methods deserialize def deserialize ( self , value : Any , attr : Union [ str , NoneType ] = None , data : Union [ Mapping [ str , Any ], NoneType ] = None , ** kwargs ) Deserialize value . :param value: The value to deserialize. :param attr: The attribute/key in data to deserialize. :param data: The raw input data passed to Schema.load . :param kwargs: Field-specific keyword arguments. :raise ValidationError: If an invalid value is passed or if a required value is missing. View Source def deserialize ( self , value : typing . Any , attr : typing . Optional [ str ] = None , data : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ** kwargs ): \"\"\"Deserialize ``value``. :param value: The value to deserialize. :param attr: The attribute/key in `data` to deserialize. :param data: The raw input data passed to `Schema.load`. :param kwargs: Field-specific keyword arguments. :raise ValidationError: If an invalid value is passed or if a required value is missing. \"\"\" # Validate required fields, deserialize, then validate # deserialized value self . _validate_missing ( value ) if value is missing_ : _miss = self . missing return _miss () if callable ( _miss ) else _miss if getattr ( self , \"allow_none\" , False ) is True and value is None : return None output = self . _deserialize ( value , attr , data , ** kwargs ) self . _validate ( output ) return output fail def fail ( self , key : str , ** kwargs ) Helper method that raises a ValidationError with an error message from self.error_messages . .. deprecated:: 3.0.0 Use make_error <marshmallow.fields.Field.make_error> instead. View Source def fail ( self , key : str , ** kwargs ): \"\"\"Helper method that raises a `ValidationError` with an error message from ``self.error_messages``. .. deprecated:: 3.0.0 Use `make_error <marshmallow.fields.Field.make_error>` instead. \"\"\" warnings . warn ( '`Field.fail` is deprecated. Use `raise self.make_error(\"{}\", ...)` instead.' . format ( key ), RemovedInMarshmallow4Warning , ) raise self . make_error ( key = key , ** kwargs ) get_value def get_value ( self , obj , attr , accessor = None , default =< marshmallow . missing > ) Return the value for a given key from an object. :param object obj: The object to get the value from. :param str attr: The attribute/key in obj to get the value from. :param callable accessor: A callable used to retrieve the value of attr from the object obj . Defaults to marshmallow.utils.get_value . View Source def get_value ( self , obj , attr , accessor = None , default = missing_ ): \"\"\"Return the value for a given key from an object. :param object obj: The object to get the value from. :param str attr: The attribute/key in `obj` to get the value from. :param callable accessor: A callable used to retrieve the value of `attr` from the object `obj`. Defaults to `marshmallow.utils.get_value`. \"\"\" # NOTE: Use getattr instead of direct attribute access here so that # subclasses aren't required to define `attribute` member attribute = getattr ( self , \"attribute\" , None ) accessor_func = accessor or utils . get_value check_key = attr if attribute is None else attribute return accessor_func ( obj , check_key , default ) make_error def make_error ( self , key : str , ** kwargs ) -> marshmallow . exceptions . ValidationError Helper method to make a ValidationError with an error message from self.error_messages . View Source def make_error ( self , key : str , ** kwargs ) -> ValidationError : \"\"\"Helper method to make a `ValidationError` with an error message from ``self.error_messages``. \"\"\" try : msg = self . error_messages [ key ] except KeyError as error : class_name = self . __class__ . __name__ message = ( \"ValidationError raised by `{class_name}`, but error key `{key}` does \" \"not exist in the `error_messages` dictionary.\" ). format ( class_name = class_name , key = key ) raise AssertionError ( message ) from error if isinstance ( msg , ( str , bytes )): msg = msg . format ( ** kwargs ) return ValidationError ( msg ) serialize def serialize ( self , attr : str , obj : Any , accessor : Union [ Callable [[ Any , str , Any ], Any ], NoneType ] = None , ** kwargs ) Pulls the value for the given key from the object, applies the field's formatting and returns the result. :param attr: The attribute/key to get from the object. :param obj: The object to access the attribute/key from. :param accessor: Function used to access values from obj . :param kwargs: Field-specific keyword arguments. View Source def serialize ( self , attr : str , obj : typing . Any , accessor : typing . Optional [ typing . Callable [[ typing . Any , str , typing . Any ], typing . Any ] ] = None , ** kwargs ): \"\"\"Pulls the value for the given key from the object, applies the field's formatting and returns the result. :param attr: The attribute/key to get from the object. :param obj: The object to access the attribute/key from. :param accessor: Function used to access values from ``obj``. :param kwargs: Field-specific keyword arguments. \"\"\" if self . _CHECK_ATTRIBUTE : value = self . get_value ( obj , attr , accessor = accessor ) if value is missing_ and hasattr ( self , \"default\" ): default = self . default value = default () if callable ( default ) else default if value is missing_ : return value else : value = None return self . _serialize ( value , attr , obj , ** kwargs ) Selection class Selection ( * , default : Any = < marshmallow . missing > , missing : Any = < marshmallow . missing > , data_key : Union [ str , NoneType ] = None , attribute : Union [ str , NoneType ] = None , validate : Union [ Callable [[ Any ], Any ], Iterable [ Callable [[ Any ], Any ]], NoneType ] = None , required : bool = False , allow_none : Union [ bool , NoneType ] = None , load_only : bool = False , dump_only : bool = False , error_messages : Union [ Dict [ str , str ], NoneType ] = None , ** metadata ) Provides python object which pertims selection on narry. It axcept a three filed string separated by \":\". \":\" presence is managed as: \"start:stop:step\" ie.: \"start:stop\" - extracts from start to stop \"start:\" - extracts from start to max \"start\" - extract exactly start View Source class Selection ( fields . String ): \"\"\" Provides python object which pertims selection on narry. It axcept a three filed \\ string separated by \" : \". \" : \" presence is managed as: \" start:stop:step \" ie.: \" start:stop \" - extracts from start to stop \" start: \" - extracts from start to max \" start \" - extract exactly start \"\"\" def _deserialize ( self , value , attr , data , ** kwargs ): try: if value is None or value == \"\" : #return [None,None,None] return slice ( None , None , None ) value = [ v or None for v in value . split ( ';' ) ] if value . __len__ () == 1 : return slice ( value [ 0 ], value [ 0 ]) return slice (* value [ 0 : 3 ]) #return value[0:3] except Exception as e: raise ValueError ( e ) Ancestors (in MRO) marshmallow.fields.String marshmallow.fields.Field marshmallow.base.FieldABC Class variables default_error_messages name parent Instance variables context The context dictionary for the parent :class: Schema . root Reference to the Schema that this field belongs to even if it is buried in a container field (e.g. List ). Return None for unbound fields. Methods deserialize def deserialize ( self , value : Any , attr : Union [ str , NoneType ] = None , data : Union [ Mapping [ str , Any ], NoneType ] = None , ** kwargs ) Deserialize value . :param value: The value to deserialize. :param attr: The attribute/key in data to deserialize. :param data: The raw input data passed to Schema.load . :param kwargs: Field-specific keyword arguments. :raise ValidationError: If an invalid value is passed or if a required value is missing. View Source def deserialize ( self , value : typing . Any , attr : typing . Optional [ str ] = None , data : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ** kwargs ): \"\"\"Deserialize ``value``. :param value: The value to deserialize. :param attr: The attribute/key in `data` to deserialize. :param data: The raw input data passed to `Schema.load`. :param kwargs: Field-specific keyword arguments. :raise ValidationError: If an invalid value is passed or if a required value is missing. \"\"\" # Validate required fields, deserialize, then validate # deserialized value self . _validate_missing ( value ) if value is missing_ : _miss = self . missing return _miss () if callable ( _miss ) else _miss if getattr ( self , \"allow_none\" , False ) is True and value is None : return None output = self . _deserialize ( value , attr , data , ** kwargs ) self . _validate ( output ) return output fail def fail ( self , key : str , ** kwargs ) Helper method that raises a ValidationError with an error message from self.error_messages . .. deprecated:: 3.0.0 Use make_error <marshmallow.fields.Field.make_error> instead. View Source def fail ( self , key : str , ** kwargs ): \"\"\"Helper method that raises a `ValidationError` with an error message from ``self.error_messages``. .. deprecated:: 3.0.0 Use `make_error <marshmallow.fields.Field.make_error>` instead. \"\"\" warnings . warn ( '`Field.fail` is deprecated. Use `raise self.make_error(\"{}\", ...)` instead.' . format ( key ), RemovedInMarshmallow4Warning , ) raise self . make_error ( key = key , ** kwargs ) get_value def get_value ( self , obj , attr , accessor = None , default =< marshmallow . missing > ) Return the value for a given key from an object. :param object obj: The object to get the value from. :param str attr: The attribute/key in obj to get the value from. :param callable accessor: A callable used to retrieve the value of attr from the object obj . Defaults to marshmallow.utils.get_value . View Source def get_value ( self , obj , attr , accessor = None , default = missing_ ): \"\"\"Return the value for a given key from an object. :param object obj: The object to get the value from. :param str attr: The attribute/key in `obj` to get the value from. :param callable accessor: A callable used to retrieve the value of `attr` from the object `obj`. Defaults to `marshmallow.utils.get_value`. \"\"\" # NOTE: Use getattr instead of direct attribute access here so that # subclasses aren't required to define `attribute` member attribute = getattr ( self , \"attribute\" , None ) accessor_func = accessor or utils . get_value check_key = attr if attribute is None else attribute return accessor_func ( obj , check_key , default ) make_error def make_error ( self , key : str , ** kwargs ) -> marshmallow . exceptions . ValidationError Helper method to make a ValidationError with an error message from self.error_messages . View Source def make_error ( self , key : str , ** kwargs ) -> ValidationError : \"\"\"Helper method to make a `ValidationError` with an error message from ``self.error_messages``. \"\"\" try : msg = self . error_messages [ key ] except KeyError as error : class_name = self . __class__ . __name__ message = ( \"ValidationError raised by `{class_name}`, but error key `{key}` does \" \"not exist in the `error_messages` dictionary.\" ). format ( class_name = class_name , key = key ) raise AssertionError ( message ) from error if isinstance ( msg , ( str , bytes )): msg = msg . format ( ** kwargs ) return ValidationError ( msg ) serialize def serialize ( self , attr : str , obj : Any , accessor : Union [ Callable [[ Any , str , Any ], Any ], NoneType ] = None , ** kwargs ) Pulls the value for the given key from the object, applies the field's formatting and returns the result. :param attr: The attribute/key to get from the object. :param obj: The object to access the attribute/key from. :param accessor: Function used to access values from obj . :param kwargs: Field-specific keyword arguments. View Source def serialize ( self , attr : str , obj : typing . Any , accessor : typing . Optional [ typing . Callable [[ typing . Any , str , typing . Any ], typing . Any ] ] = None , ** kwargs ): \"\"\"Pulls the value for the given key from the object, applies the field's formatting and returns the result. :param attr: The attribute/key to get from the object. :param obj: The object to access the attribute/key from. :param accessor: Function used to access values from ``obj``. :param kwargs: Field-specific keyword arguments. \"\"\" if self . _CHECK_ATTRIBUTE : value = self . get_value ( obj , attr , accessor = accessor ) if value is missing_ and hasattr ( self , \"default\" ): default = self . default value = default () if callable ( default ) else default if value is missing_ : return value else : value = None return self . _serialize ( value , attr , obj , ** kwargs )","title":"Utils"},{"location":"reference/hielen2/utils/#module-hielen2utils","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 #!/usr/bin/env python # coding=utf-8 from datetime import datetime from re import split , sub , findall from time import mktime import json from importlib import import_module from falcon import HTTPNotAcceptable from hashlib import md5 from marshmallow import Schema , fields def hug_output_format_conten_type ( handlers = [], error = \"The requested format does not match any of those allowed\" , ctpar = \"content_type\" , ): \"\"\"Returns a different handler depending on the input param ctpar If none match and no default is given falcon.HTTPNotAcceptable(error) is raised \"\"\" try : default = handlers [ 0 ] except Exception : default = None handlers = { h . content_type : h for h in handlers } def requested_output_type ( request = None ): try : par = request . _params [ ctpar ] handler = None for k , h in handlers . items (): if par . split ( \";\" )[ 0 ] == k . split ( \";\" )[ 0 ]: handler = h break except Exception : if default is not None : handler = default if handler is None : raise HTTPNotAcceptable ( error ) return handler def output_type ( data , request , response ): handler = requested_output_type ( request ) response . content_type = handler . content_type return handler ( data , request = request , response = response ) output_type . __doc__ = \"Supports any of the following formats: {0} \" . format ( \", \" . join ( function . __doc__ for function in handlers . values ()) ) output_type . content_type = \", \" . join ( handlers . keys ()) output_type . requested = requested_output_type return output_type def newinstanceof ( klass , * args , ** kwargs ): klass_ar = klass . split ( \".\" ) module = \".\" . join ( klass_ar [: - 1 ]) klass = klass_ar [ - 1 ] return getattr ( import_module ( module ), klass )( * args , ** kwargs ) def ut2isot ( u = None ): u = u or 1 return str ( datetime . fromtimestamp ( u )) def isot2ut ( t = None ): t = t or \"1970-01-01T01:00:01.00000Z\" dt = datetime ( * map ( int , split ( \"[^\\d]\" , sub ( \"[^\\d]$\" , \"\" , t )))) return int ( mktime ( dt . timetuple ())) def loadjsonfile ( filename ): with open ( filename ) as jf : return json . load ( jf ) def savejsonfile ( filename , struct ): with open ( filename , \"w\" ) as jf : json . dump ( struct , jf ) def eprint ( * args , fname = \"error\" , ** kwargs ): with open ( fname , \"a\" ) as f : print ( * args , file = f , ** kwargs ) def hasher ( * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return md5 ( f ' { h } ' . encode () ) . hexdigest () def hashfile ( filename ): BLOCKSIZE = 65536 hasher = md5 () with open ( filename , \"rb\" ) as afile : buf = afile . read ( BLOCKSIZE ) while len ( buf ) > 0 : hasher . update ( buf ) buf = afile . read ( BLOCKSIZE ) return hasher . hexdigest () ### MARSHMALLOW def getSchemaDict ( schema ): out = { \"fields\" :{}, \"required\" :[], \"hint\" : schema . __doc__ } for k , w in schema . dump_fields . items (): out [ 'fields' ][ k ] = w . __class__ . __name__ w . __dict__ [ 'required' ] and out [ \"required\" ] . append ( k ) return out class LocalFile ( fields . String ): \"\"\" Local Filepath manager used to identify a file into the system. Mainliy usefull for action Schema declaration in hielen2.HielenSource extention \"\"\" pass class Selection ( fields . String ): \"\"\" Provides python object which pertims selection on narry. It axcept a three filed \\ string separated by \":\". \":\" presence is managed as: \"start:stop:step\" ie.: \"start:stop\" - extracts from start to stop \"start:\" - extracts from start to max \"start\" - extract exactly start \"\"\" def _deserialize ( self , value , attr , data , ** kwargs ): try : if value is None or value == \"\" : #return [None,None,None] return slice ( None , None , None ) value = [ v or None for v in value . split ( ';' ) ] if value . __len__ () == 1 : return slice ( value [ 0 ], value [ 0 ]) return slice ( * value [ 0 : 3 ]) #return value[0:3] except Exception as e : raise ValueError ( e ) class JsonValidable : \"\"\" JSON Validator class. It is initailzed with a marshmallow.Schema instance. When __call__ function is invoked, \\ uses marshmallow facilities to validate the json and raise errors. Once initalized, changes __doc__ in order to descibe the json accepted. \"\"\" def __field_doc__ ( self , field ): required = field . required and \"!\" or \"\" allow_none = not field . allow_none and \"!\" or \"\" try : types = \"|\" . join ( self . TYPE_MAPPING [ field . __class__ ]) except KeyError : if field . __class__ is fields . List : f , required , allow_none = self . __field_doc__ ( field . inner ) types = f \"[ { f } ]\" elif field . __class__ is fields . Dict : kf , required , allow_none = self . __field_doc__ ( field . key_field ) vf , required , allow_none = self . __field_doc__ ( field . value_field ) types = f \" {{ { kf } , { vf } }} \" else : types = \"\" return ( types , required , allow_none ) def __schema_doc__ ( self ): flds = [] for n , f in self . schema . fields . items (): types , required , allow_none = self . __field_doc__ ( f ) # TODO formattare required e allow_none # flds.append( f\"**{n}**{required}{allow_none}: {types}\") flds . append ( f \"** { n } **: { types } \" ) fields = \", \" . join ( flds ) fields = f \" {{ { fields } }} \" if self . schema . many : fields = f \"[ { fields } ]\" return f \"JSON Schema { fields } \" def __init__ ( self , schema ): self . schema = schema self . TYPE_MAPPING = {} for k , w in self . schema . TYPE_MAPPING . items (): try : self . TYPE_MAPPING [ w ] . append ( findall ( r \"'(.*)'\" , str ( k ))[ 0 ]) except KeyError : self . TYPE_MAPPING [ w ] = [ findall ( r \"'(.*)'\" , str ( k ))[ 0 ]] self . __doc__ = str ( self . __schema_doc__ ()) def __call__ ( self , value ): if type ( value ) is list : # If Falcon is set to comma-separate entries, this segment joins them again. fixed_value = \",\" . join ( value ) else : fixed_value = value return self . schema . loads ( fixed_value )","title":"Module hielen2.utils"},{"location":"reference/hielen2/utils/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/utils/#eprint","text":"def eprint ( * args , fname = 'error' , ** kwargs ) View Source def eprint ( * args , fname = \"error\" , ** kwargs ): with open ( fname , \"a\" ) as f : print ( * args , file = f , ** kwargs )","title":"eprint"},{"location":"reference/hielen2/utils/#getschemadict","text":"def getSchemaDict ( schema ) View Source def getSchemaDict ( schema ) : out = { \"fields\" :{} , \"required\" :[] , \"hint\" : schema . __doc__ } for k , w in schema . dump_fields . items () : out [ 'fields' ][ k ]= w . __class__ . __name__ w . __dict__ [ 'required' ] and out [ \"required\" ] . append ( k ) return out","title":"getSchemaDict"},{"location":"reference/hielen2/utils/#hasher","text":"def hasher ( * args , ** kwargs ) View Source def hasher ( * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return md5 ( f '{h}' . encode () ). hexdigest ()","title":"hasher"},{"location":"reference/hielen2/utils/#hashfile","text":"def hashfile ( filename ) View Source def hashfile ( filename ): BLOCKSIZE = 65536 hasher = md5 () with open ( filename , \"rb\" ) as afile : buf = afile . read ( BLOCKSIZE ) while len ( buf ) > 0 : hasher . update ( buf ) buf = afile . read ( BLOCKSIZE ) return hasher . hexdigest ()","title":"hashfile"},{"location":"reference/hielen2/utils/#hug_output_format_conten_type","text":"def hug_output_format_conten_type ( handlers = [], error = 'The requested format does not match any of those allowed' , ctpar = 'content_type' ) Returns a different handler depending on the input param ctpar If none match and no default is given falcon.HTTPNotAcceptable(error) is raised View Source def hug_output_format_conten_type ( handlers = [] , error = \"The requested format does not match any of those allowed\" , ctpar = \"content_type\" , ) : \"\"\"Returns a different handler depending on the input param ctpar If none match and no default is given falcon.HTTPNotAcceptable(error) is raised \"\"\" try : default = handlers [ 0 ] except Exception : default = None handlers = { h . content_type : h for h in handlers } def requested_output_type ( request = None ) : try : par = request . _params [ ctpar ] handler = None for k , h in handlers . items () : if par . split ( \";\" ) [ 0 ] == k . split ( \";\" ) [ 0 ] : handler = h break except Exception : if default is not None : handler = default if handler is None : raise HTTPNotAcceptable ( error ) return handler def output_type ( data , request , response ) : handler = requested_output_type ( request ) response . content_type = handler . content_type return handler ( data , request = request , response = response ) output_type . __doc__ = \"Supports any of the following formats: {0}\" . format ( \", \" . join ( function . __doc__ for function in handlers . values ()) ) output_type . content_type = \", \" . join ( handlers . keys ()) output_type . requested = requested_output_type return output_type","title":"hug_output_format_conten_type"},{"location":"reference/hielen2/utils/#isot2ut","text":"def isot2ut ( t = None ) View Source def isot2ut ( t = None ): t = t or \"1970-01-01T01:00:01.00000Z\" dt = datetime ( * map ( int , split ( \"[^\\d]\" , sub ( \"[^\\d]$\" , \"\" , t )))) return int ( mktime ( dt . timetuple ()))","title":"isot2ut"},{"location":"reference/hielen2/utils/#loadjsonfile","text":"def loadjsonfile ( filename ) View Source def loadjsonfile ( filename ): with open ( filename ) as jf : return json . load ( jf )","title":"loadjsonfile"},{"location":"reference/hielen2/utils/#newinstanceof","text":"def newinstanceof ( klass , * args , ** kwargs ) View Source def newinstanceof ( klass , * args , **kwargs ) : klass_ar = klass . split ( \".\" ) module = \".\" . join ( klass_ar [:- 1 ]) klass = klass_ar [ - 1 ] return getattr ( import_module ( module ), klass )( * args , **kwargs )","title":"newinstanceof"},{"location":"reference/hielen2/utils/#savejsonfile","text":"def savejsonfile ( filename , struct ) View Source def savejsonfile ( filename , struct ): with open ( filename , \"w\" ) as jf : json . dump ( struct , jf )","title":"savejsonfile"},{"location":"reference/hielen2/utils/#ut2isot","text":"def ut2isot ( u = None ) View Source def ut2isot ( u = None ): u = u or 1 return str ( datetime . fromtimestamp ( u ))","title":"ut2isot"},{"location":"reference/hielen2/utils/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/utils/#jsonvalidable","text":"class JsonValidable ( schema ) JSON Validator class. It is initailzed with a marshmallow.Schema instance. When call function is invoked, uses marshmallow facilities to validate the json and raise errors. Once initalized, changes doc in order to descibe the json accepted. View Source class JsonValidable : \"\"\" JSON Validator class. It is initailzed with a marshmallow.Schema instance. When __call__ function is invoked, \\ uses marshmallow facilities to validate the json and raise errors. Once initalized, changes __doc__ in order to descibe the json accepted. \"\"\" def __field_doc__ ( self , field ) : required = field . required and \"!\" or \"\" allow_none = not field . allow_none and \"!\" or \"\" try : types = \"|\" . join ( self . TYPE_MAPPING [ field.__class__ ] ) except KeyError : if field . __class__ is fields . List : f , required , allow_none = self . __field_doc__ ( field . inner ) types = f \"[{f}]\" elif field . __class__ is fields . Dict : kf , required , allow_none = self . __field_doc__ ( field . key_field ) vf , required , allow_none = self . __field_doc__ ( field . value_field ) types = f \"{{{kf},{vf}}}\" else : types = \"\" return ( types , required , allow_none ) def __schema_doc__ ( self ) : flds = [] for n , f in self . schema . fields . items () : types , required , allow_none = self . __field_doc__ ( f ) # TODO formattare required e allow_none # flds . append ( f \"**{n}**{required}{allow_none}: {types}\" ) flds . append ( f \"**{n}**: {types}\" ) fields = \", \" . join ( flds ) fields = f \"{{{fields}}}\" if self . schema . many : fields = f \"[{fields}]\" return f \"JSON Schema {fields}\" def __init__ ( self , schema ) : self . schema = schema self . TYPE_MAPPING = {} for k , w in self . schema . TYPE_MAPPING . items () : try : self . TYPE_MAPPING [ w ] . append ( findall ( r \"'(.*)'\" , str ( k )) [ 0 ] ) except KeyError : self . TYPE_MAPPING [ w ] = [ findall(r\"'(.*)'\", str(k))[0 ] ] self . __doc__ = str ( self . __schema_doc__ ()) def __call__ ( self , value ) : if type ( value ) is list : # If Falcon is set to comma - separate entries , this segment joins them again . fixed_value = \",\" . join ( value ) else : fixed_value = value return self . schema . loads ( fixed_value )","title":"JsonValidable"},{"location":"reference/hielen2/utils/#localfile","text":"class LocalFile ( * , default : Any = < marshmallow . missing > , missing : Any = < marshmallow . missing > , data_key : Union [ str , NoneType ] = None , attribute : Union [ str , NoneType ] = None , validate : Union [ Callable [[ Any ], Any ], Iterable [ Callable [[ Any ], Any ]], NoneType ] = None , required : bool = False , allow_none : Union [ bool , NoneType ] = None , load_only : bool = False , dump_only : bool = False , error_messages : Union [ Dict [ str , str ], NoneType ] = None , ** metadata ) Local Filepath manager used to identify a file into the system. Mainliy usefull for action Schema declaration in hielen2.HielenSource extention View Source class LocalFile ( fields . String ): \"\"\" Local Filepath manager used to identify a file into the system. Mainliy usefull for action Schema declaration in hielen2.HielenSource extention \"\"\" pass","title":"LocalFile"},{"location":"reference/hielen2/utils/#ancestors-in-mro","text":"marshmallow.fields.String marshmallow.fields.Field marshmallow.base.FieldABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/utils/#class-variables","text":"default_error_messages name parent","title":"Class variables"},{"location":"reference/hielen2/utils/#instance-variables","text":"context The context dictionary for the parent :class: Schema . root Reference to the Schema that this field belongs to even if it is buried in a container field (e.g. List ). Return None for unbound fields.","title":"Instance variables"},{"location":"reference/hielen2/utils/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/utils/#deserialize","text":"def deserialize ( self , value : Any , attr : Union [ str , NoneType ] = None , data : Union [ Mapping [ str , Any ], NoneType ] = None , ** kwargs ) Deserialize value . :param value: The value to deserialize. :param attr: The attribute/key in data to deserialize. :param data: The raw input data passed to Schema.load . :param kwargs: Field-specific keyword arguments. :raise ValidationError: If an invalid value is passed or if a required value is missing. View Source def deserialize ( self , value : typing . Any , attr : typing . Optional [ str ] = None , data : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ** kwargs ): \"\"\"Deserialize ``value``. :param value: The value to deserialize. :param attr: The attribute/key in `data` to deserialize. :param data: The raw input data passed to `Schema.load`. :param kwargs: Field-specific keyword arguments. :raise ValidationError: If an invalid value is passed or if a required value is missing. \"\"\" # Validate required fields, deserialize, then validate # deserialized value self . _validate_missing ( value ) if value is missing_ : _miss = self . missing return _miss () if callable ( _miss ) else _miss if getattr ( self , \"allow_none\" , False ) is True and value is None : return None output = self . _deserialize ( value , attr , data , ** kwargs ) self . _validate ( output ) return output","title":"deserialize"},{"location":"reference/hielen2/utils/#fail","text":"def fail ( self , key : str , ** kwargs ) Helper method that raises a ValidationError with an error message from self.error_messages . .. deprecated:: 3.0.0 Use make_error <marshmallow.fields.Field.make_error> instead. View Source def fail ( self , key : str , ** kwargs ): \"\"\"Helper method that raises a `ValidationError` with an error message from ``self.error_messages``. .. deprecated:: 3.0.0 Use `make_error <marshmallow.fields.Field.make_error>` instead. \"\"\" warnings . warn ( '`Field.fail` is deprecated. Use `raise self.make_error(\"{}\", ...)` instead.' . format ( key ), RemovedInMarshmallow4Warning , ) raise self . make_error ( key = key , ** kwargs )","title":"fail"},{"location":"reference/hielen2/utils/#get_value","text":"def get_value ( self , obj , attr , accessor = None , default =< marshmallow . missing > ) Return the value for a given key from an object. :param object obj: The object to get the value from. :param str attr: The attribute/key in obj to get the value from. :param callable accessor: A callable used to retrieve the value of attr from the object obj . Defaults to marshmallow.utils.get_value . View Source def get_value ( self , obj , attr , accessor = None , default = missing_ ): \"\"\"Return the value for a given key from an object. :param object obj: The object to get the value from. :param str attr: The attribute/key in `obj` to get the value from. :param callable accessor: A callable used to retrieve the value of `attr` from the object `obj`. Defaults to `marshmallow.utils.get_value`. \"\"\" # NOTE: Use getattr instead of direct attribute access here so that # subclasses aren't required to define `attribute` member attribute = getattr ( self , \"attribute\" , None ) accessor_func = accessor or utils . get_value check_key = attr if attribute is None else attribute return accessor_func ( obj , check_key , default )","title":"get_value"},{"location":"reference/hielen2/utils/#make_error","text":"def make_error ( self , key : str , ** kwargs ) -> marshmallow . exceptions . ValidationError Helper method to make a ValidationError with an error message from self.error_messages . View Source def make_error ( self , key : str , ** kwargs ) -> ValidationError : \"\"\"Helper method to make a `ValidationError` with an error message from ``self.error_messages``. \"\"\" try : msg = self . error_messages [ key ] except KeyError as error : class_name = self . __class__ . __name__ message = ( \"ValidationError raised by `{class_name}`, but error key `{key}` does \" \"not exist in the `error_messages` dictionary.\" ). format ( class_name = class_name , key = key ) raise AssertionError ( message ) from error if isinstance ( msg , ( str , bytes )): msg = msg . format ( ** kwargs ) return ValidationError ( msg )","title":"make_error"},{"location":"reference/hielen2/utils/#serialize","text":"def serialize ( self , attr : str , obj : Any , accessor : Union [ Callable [[ Any , str , Any ], Any ], NoneType ] = None , ** kwargs ) Pulls the value for the given key from the object, applies the field's formatting and returns the result. :param attr: The attribute/key to get from the object. :param obj: The object to access the attribute/key from. :param accessor: Function used to access values from obj . :param kwargs: Field-specific keyword arguments. View Source def serialize ( self , attr : str , obj : typing . Any , accessor : typing . Optional [ typing . Callable [[ typing . Any , str , typing . Any ], typing . Any ] ] = None , ** kwargs ): \"\"\"Pulls the value for the given key from the object, applies the field's formatting and returns the result. :param attr: The attribute/key to get from the object. :param obj: The object to access the attribute/key from. :param accessor: Function used to access values from ``obj``. :param kwargs: Field-specific keyword arguments. \"\"\" if self . _CHECK_ATTRIBUTE : value = self . get_value ( obj , attr , accessor = accessor ) if value is missing_ and hasattr ( self , \"default\" ): default = self . default value = default () if callable ( default ) else default if value is missing_ : return value else : value = None return self . _serialize ( value , attr , obj , ** kwargs )","title":"serialize"},{"location":"reference/hielen2/utils/#selection","text":"class Selection ( * , default : Any = < marshmallow . missing > , missing : Any = < marshmallow . missing > , data_key : Union [ str , NoneType ] = None , attribute : Union [ str , NoneType ] = None , validate : Union [ Callable [[ Any ], Any ], Iterable [ Callable [[ Any ], Any ]], NoneType ] = None , required : bool = False , allow_none : Union [ bool , NoneType ] = None , load_only : bool = False , dump_only : bool = False , error_messages : Union [ Dict [ str , str ], NoneType ] = None , ** metadata ) Provides python object which pertims selection on narry. It axcept a three filed string separated by \":\". \":\" presence is managed as: \"start:stop:step\" ie.: \"start:stop\" - extracts from start to stop \"start:\" - extracts from start to max \"start\" - extract exactly start View Source class Selection ( fields . String ): \"\"\" Provides python object which pertims selection on narry. It axcept a three filed \\ string separated by \" : \". \" : \" presence is managed as: \" start:stop:step \" ie.: \" start:stop \" - extracts from start to stop \" start: \" - extracts from start to max \" start \" - extract exactly start \"\"\" def _deserialize ( self , value , attr , data , ** kwargs ): try: if value is None or value == \"\" : #return [None,None,None] return slice ( None , None , None ) value = [ v or None for v in value . split ( ';' ) ] if value . __len__ () == 1 : return slice ( value [ 0 ], value [ 0 ]) return slice (* value [ 0 : 3 ]) #return value[0:3] except Exception as e: raise ValueError ( e )","title":"Selection"},{"location":"reference/hielen2/utils/#ancestors-in-mro_1","text":"marshmallow.fields.String marshmallow.fields.Field marshmallow.base.FieldABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/utils/#class-variables_1","text":"default_error_messages name parent","title":"Class variables"},{"location":"reference/hielen2/utils/#instance-variables_1","text":"context The context dictionary for the parent :class: Schema . root Reference to the Schema that this field belongs to even if it is buried in a container field (e.g. List ). Return None for unbound fields.","title":"Instance variables"},{"location":"reference/hielen2/utils/#methods_1","text":"","title":"Methods"},{"location":"reference/hielen2/utils/#deserialize_1","text":"def deserialize ( self , value : Any , attr : Union [ str , NoneType ] = None , data : Union [ Mapping [ str , Any ], NoneType ] = None , ** kwargs ) Deserialize value . :param value: The value to deserialize. :param attr: The attribute/key in data to deserialize. :param data: The raw input data passed to Schema.load . :param kwargs: Field-specific keyword arguments. :raise ValidationError: If an invalid value is passed or if a required value is missing. View Source def deserialize ( self , value : typing . Any , attr : typing . Optional [ str ] = None , data : typing . Optional [ typing . Mapping [ str , typing . Any ]] = None , ** kwargs ): \"\"\"Deserialize ``value``. :param value: The value to deserialize. :param attr: The attribute/key in `data` to deserialize. :param data: The raw input data passed to `Schema.load`. :param kwargs: Field-specific keyword arguments. :raise ValidationError: If an invalid value is passed or if a required value is missing. \"\"\" # Validate required fields, deserialize, then validate # deserialized value self . _validate_missing ( value ) if value is missing_ : _miss = self . missing return _miss () if callable ( _miss ) else _miss if getattr ( self , \"allow_none\" , False ) is True and value is None : return None output = self . _deserialize ( value , attr , data , ** kwargs ) self . _validate ( output ) return output","title":"deserialize"},{"location":"reference/hielen2/utils/#fail_1","text":"def fail ( self , key : str , ** kwargs ) Helper method that raises a ValidationError with an error message from self.error_messages . .. deprecated:: 3.0.0 Use make_error <marshmallow.fields.Field.make_error> instead. View Source def fail ( self , key : str , ** kwargs ): \"\"\"Helper method that raises a `ValidationError` with an error message from ``self.error_messages``. .. deprecated:: 3.0.0 Use `make_error <marshmallow.fields.Field.make_error>` instead. \"\"\" warnings . warn ( '`Field.fail` is deprecated. Use `raise self.make_error(\"{}\", ...)` instead.' . format ( key ), RemovedInMarshmallow4Warning , ) raise self . make_error ( key = key , ** kwargs )","title":"fail"},{"location":"reference/hielen2/utils/#get_value_1","text":"def get_value ( self , obj , attr , accessor = None , default =< marshmallow . missing > ) Return the value for a given key from an object. :param object obj: The object to get the value from. :param str attr: The attribute/key in obj to get the value from. :param callable accessor: A callable used to retrieve the value of attr from the object obj . Defaults to marshmallow.utils.get_value . View Source def get_value ( self , obj , attr , accessor = None , default = missing_ ): \"\"\"Return the value for a given key from an object. :param object obj: The object to get the value from. :param str attr: The attribute/key in `obj` to get the value from. :param callable accessor: A callable used to retrieve the value of `attr` from the object `obj`. Defaults to `marshmallow.utils.get_value`. \"\"\" # NOTE: Use getattr instead of direct attribute access here so that # subclasses aren't required to define `attribute` member attribute = getattr ( self , \"attribute\" , None ) accessor_func = accessor or utils . get_value check_key = attr if attribute is None else attribute return accessor_func ( obj , check_key , default )","title":"get_value"},{"location":"reference/hielen2/utils/#make_error_1","text":"def make_error ( self , key : str , ** kwargs ) -> marshmallow . exceptions . ValidationError Helper method to make a ValidationError with an error message from self.error_messages . View Source def make_error ( self , key : str , ** kwargs ) -> ValidationError : \"\"\"Helper method to make a `ValidationError` with an error message from ``self.error_messages``. \"\"\" try : msg = self . error_messages [ key ] except KeyError as error : class_name = self . __class__ . __name__ message = ( \"ValidationError raised by `{class_name}`, but error key `{key}` does \" \"not exist in the `error_messages` dictionary.\" ). format ( class_name = class_name , key = key ) raise AssertionError ( message ) from error if isinstance ( msg , ( str , bytes )): msg = msg . format ( ** kwargs ) return ValidationError ( msg )","title":"make_error"},{"location":"reference/hielen2/utils/#serialize_1","text":"def serialize ( self , attr : str , obj : Any , accessor : Union [ Callable [[ Any , str , Any ], Any ], NoneType ] = None , ** kwargs ) Pulls the value for the given key from the object, applies the field's formatting and returns the result. :param attr: The attribute/key to get from the object. :param obj: The object to access the attribute/key from. :param accessor: Function used to access values from obj . :param kwargs: Field-specific keyword arguments. View Source def serialize ( self , attr : str , obj : typing . Any , accessor : typing . Optional [ typing . Callable [[ typing . Any , str , typing . Any ], typing . Any ] ] = None , ** kwargs ): \"\"\"Pulls the value for the given key from the object, applies the field's formatting and returns the result. :param attr: The attribute/key to get from the object. :param obj: The object to access the attribute/key from. :param accessor: Function used to access values from ``obj``. :param kwargs: Field-specific keyword arguments. \"\"\" if self . _CHECK_ATTRIBUTE : value = self . get_value ( obj , attr , accessor = accessor ) if value is missing_ and hasattr ( self , \"default\" ): default = self . default value = default () if callable ( default ) else default if value is missing_ : return value else : value = None return self . _serialize ( value , attr , obj , ** kwargs )","title":"serialize"},{"location":"reference/hielen2/api/","text":"Module hielen2.api Sub-modules hielen2.api.actions hielen2.api.actionschemata hielen2.api.data hielen2.api.features hielen2.api.glob hielen2.api.mapping hielen2.api.parameters hielen2.api.prepare hielen2.api.prototypes","title":"Index"},{"location":"reference/hielen2/api/#module-hielen2api","text":"","title":"Module hielen2.api"},{"location":"reference/hielen2/api/#sub-modules","text":"hielen2.api.actions hielen2.api.actionschemata hielen2.api.data hielen2.api.features hielen2.api.glob hielen2.api.mapping hielen2.api.parameters hielen2.api.prepare hielen2.api.prototypes","title":"Sub-modules"},{"location":"reference/hielen2/api/actions/","text":"Module hielen2.api.actions View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 #!/usr/bin/env python # coding=utf-8 import hug import tempfile import falcon import os import time import json from hielen2 import db , conf import hielen2.source as sourceman from streaming_form_data import StreamingFormDataParser from streaming_form_data.targets import FileTarget , ValueTarget from himada.api import ResponseFormatter from urllib.parse import unquote from importlib import import_module import traceback @hug . get ( \"/ {feature} \" ) def features_actions_values ( feature , actions = None , timestamp = None , request = None , response = None ): \"\"\" **Recupero dello stato corrente delle azioni effettuate su una feature** L'intento di questa api \u00e8 quello di fornire i valori richiesti secondo lo schema dell'azione ___nota 1___: `actions` accetta valori multipli separati da virgola ___nota 2___: A seconda dell'action richiesta, alcuni parametri potrebbero essere utilizzati in fase \\ di input ma non registrati. Il che vuol dire che per quei parametri il valore di ritorno sar\u00e0 null viene restituito una struttura di questo tipo: [ { \"feature\"*:..., \"action_name*\":..., \"timestamp\": ..., \"value\":{...} }, { \"feature\"*:..., \"action_name*\":..., \"timestamp\": ..., \"value\":{...} }, ... ] ___nota 3___ :(*) I campi \"feature\" e \"action\" potrebbero non essere restituiti nella struttura \\ nel caso in cui essi risultino non ambigui. \"timestamp\" e \"value\" vengono sempre restituiti Possibili risposte: - _404 Not Found_: Nel non venga trovata la feature richiesta o essa abbia un problema di \\ configurazione \"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db[\"features\"][feature] featobj = sourceman . sourceFactory ( feature ) out . data = featobj . getActionValues ( actions , timestamp ) if not isinstance ( out . data , list ): out . data = [ out . data ] except Exception as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature ' { feature } ' does not exists or it is misconfigured: { e } \" out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return @hug . get ( \"/ {feature} / {action} \" ) def feature_action_values ( feature , action , timestamp = None , request = None , response = None ): \"\"\" **Recupero dello stato corrente per una specifica azione di una specifica feature**\"\"\" return features_actions_values ( feature , action , timestamp , request = request , response = response ) @hug . delete ( \"/ {feature} / {action} \" ) def feature_action_delete ( feature , action , timestamp , request = None , response = None ): \"\"\" **Eliminazione di una determinata azione di una specifica feature**\"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db[\"features\"][feature] featobj = sourceman . sourceFactory ( feature ) out . data = featobj . deleteActionValues ( action , timestamp ) except Exception as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature ' { feature } ' does not exists or it is misconfigured: { e } \" out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return @hug . post ( \"/ {feature} / {action} \" , parse_body = False ) @hug . default_input_format ( content_type = \"multipart/form-data\" ) def make_action ( feature , action , request = None , response = None ): \"\"\" **Esecuzione delle azioni** Richiede l'esecuzione di una specifica azione su una feature, fornendo tutte le informazioni \\ necessarie attraverso una form dinamica dedicata. - Oltre ai due parametri `feature` e `form`, `timestamp`, indicati nella url, accetta un \\ _multipart/form-data_ basato sulla specifica form, selezionata tramite i due parametri espliciti. - Tutto il content \u00e8 scaricato attarverso i chunk dello stream ('100 continue') per evitare il \\ timeout dei workers in caso di contenuti di grandi dimensioni. Possibili risposte: - _200 OK_: Nel caso in cui l'azione vada a buon fine. L'azione richiesta viene presa in carico ma \\ potrebbe avere un tempo di esecuzione arbitrario. L'azione quindi viene splittata su un altro processo. - _404 Not Found_: Nel caso la feature non esista o non sia definita per essa l'azione richiesta. - _500 Internal Server Error_: Nel caso pessimo che il modulo dichiarato non esista. - _501 Not Implemented'_: Nel caso la tipologia non fornisse ancora l'iplementazione di uno o tutti \\ i moduli di gestione E' stato implementato il meccanismo minimo di gestione che prevede il salvataggio delle info \\ fornite che possono essere fornite tali e quali in uscita (vedi metodo GET dell'api). Questo \\ meccanismo permette di svluppare i moduli a partire da un template con risposta di default. \"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db[\"features\"][feature] featobj = sourceman . sourceFactory ( feature ) except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature ' { feature } ' does not exists or it is misconfigured: { e } \" out . format ( request = request , response = response ) return try : schema = featobj . getActionSchema ( action ) except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_IMPLEMENTED out . message = f \"Prototype ' { featobj . type } ' actions not implemented.\" out . format ( request = request , response = response ) return except ModuleNotFoundError as e : traceback . print_exc () out . status = falcon . HTTP_INTERNAL_SERVER_ERROR out . message = f \"Prototype ' { featobj . type } ' module not found.\" out . format ( request = request , response = response ) return parser = StreamingFormDataParser ( headers = request . headers ) values = {} # TODO Differenziazione delle tipologie di input for k , w in schema [ \"fields\" ] . items (): if w == \"LocalFile\" : timenow = time . perf_counter () filepath = os . path . join ( tempfile . gettempdir (), f \" { feature } . { k } . { timenow } .part\" ) target = FileTarget ( filepath ) parser . register ( k , target ) values [ k ] = filepath else : target = ValueTarget () parser . register ( k , target ) values [ k ] = target while True : chunk = request . stream . read ( 8192 ) if not chunk : break parser . data_received ( chunk ) kwargs = {} for k , w in values . items (): if isinstance ( w , str ): v = os . path . exists ( w ) and w or None else : v = unquote ( w . value . decode ( \"utf8\" )) or None kwargs [ k ] = v m = [ m for m in schema [ \"required\" ] if kwargs [ m ] is None ] if m . __len__ (): out . status = falcon . HTTP_BAD_REQUEST out . message = f \"Required parameters { m } not supplied\" out . format ( request = request , response = response ) return # CHECKS request checks ALL RIGHT. Continuing with code loading # Trying to initialize feature action manager module try : result = featobj . execAction ( action , ** kwargs ) except AttributeError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_IMPLEMENTED out . message = f \"Action ' { action } ' not implemented.\" out . format ( request = request , response = response ) return except ValueError as e : traceback . print_exc () out . status = falcon . HTTP_BAD_REQUEST out . message = f \"Action values error: { e } .\" out . format ( request = request , response = response ) return except Exception as e : traceback . print_exc () try : db [ \"actions\" ][ feature , action , result [ 'timestamp' ]] = { \"value\" : result } db [ \"features\" ] . save () except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_INTERNAL_SERVER_ERROR out . message = str ( e ) out . format ( request = request , response = response ) except ValueError as e : traceback . print_exc () out . status = falcon . HTTP_BAD_REQUEST out . message = str ( e ) out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return Variables conf db Functions feature_action_delete def feature_action_delete ( feature , action , timestamp , request = None , response = None ) Eliminazione di una determinata azione di una specifica feature View Source @hug . delete ( \"/{feature}/{action}\" ) def feature_action_delete ( feature , action , timestamp , request = None , response = None ) : \"\"\" **Eliminazione di una determinata azione di una specifica feature**\"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db [ \"features\" ][ feature ] featobj = sourceman . sourceFactory ( feature ) out . data = featobj . deleteActionValues ( action , timestamp ) except Exception as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature '{feature}' does not exists or it is misconfigured: {e}\" out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return feature_action_values def feature_action_values ( feature , action , timestamp = None , request = None , response = None ) Recupero dello stato corrente per una specifica azione di una specifica feature View Source @hug . get ( \"/{feature}/{action}\" ) def feature_action_values ( feature , action , timestamp = None , request = None , response = None ) : \"\"\" **Recupero dello stato corrente per una specifica azione di una specifica feature**\"\"\" return features_actions_values ( feature , action , timestamp , request = request , response = response ) features_actions_values def features_actions_values ( feature , actions = None , timestamp = None , request = None , response = None ) Recupero dello stato corrente delle azioni effettuate su una feature L'intento di questa api \u00e8 quello di fornire i valori richiesti secondo lo schema dell'azione nota 1 : actions accetta valori multipli separati da virgola nota 2 : A seconda dell'action richiesta, alcuni parametri potrebbero essere utilizzati in fase di input ma non registrati. Il che vuol dire che per quei parametri il valore di ritorno sar\u00e0 null viene restituito una struttura di questo tipo: [ { \"feature\"*:..., \"action_name*\":..., \"timestamp\": ..., \"value\":{...} }, { \"feature\"*:..., \"action_name*\":..., \"timestamp\": ..., \"value\":{...} }, ... ] nota 3 :(*) I campi \"feature\" e \"action\" potrebbero non essere restituiti nella struttura nel caso in cui essi risultino non ambigui. \"timestamp\" e \"value\" vengono sempre restituiti Possibili risposte: 404 Not Found : Nel non venga trovata la feature richiesta o essa abbia un problema di configurazione View Source @hug . get ( \"/{feature}\" ) def features_actions_values ( feature , actions = None , timestamp = None , request = None , response = None ) : \"\"\" **Recupero dello stato corrente delle azioni effettuate su una feature** L'intento di questa api \u00e8 quello di fornire i valori richiesti secondo lo schema dell'azione ___nota 1___: `actions` accetta valori multipli separati da virgola ___nota 2___: A seconda dell'action richiesta, alcuni parametri potrebbero essere utilizzati in fase \\ di input ma non registrati. Il che vuol dire che per quei parametri il valore di ritorno sar\u00e0 null viene restituito una struttura di questo tipo: [ { \" feature \"*:..., \" action_name * \":..., \" timestamp \": ..., \" value \":{...} }, { \" feature \"*:..., \" action_name * \":..., \" timestamp \": ..., \" value \":{...} }, ... ] ___nota 3___ :(*) I campi \" feature \" e \" action \" potrebbero non essere restituiti nella struttura \\ nel caso in cui essi risultino non ambigui. \" timestamp \" e \" value \" vengono sempre restituiti Possibili risposte: - _404 Not Found_: Nel non venga trovata la feature richiesta o essa abbia un problema di \\ configurazione \"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db [ \"features\" ][ feature ] featobj = sourceman . sourceFactory ( feature ) out . data = featobj . getActionValues ( actions , timestamp ) if not isinstance ( out . data , list ) : out . data =[ out.data ] except Exception as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature '{feature}' does not exists or it is misconfigured: {e}\" out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return make_action def make_action ( feature , action , request = None , response = None ) Esecuzione delle azioni Richiede l'esecuzione di una specifica azione su una feature, fornendo tutte le informazioni necessarie attraverso una form dinamica dedicata. Oltre ai due parametri feature e form , timestamp , indicati nella url, accetta un multipart/form-data basato sulla specifica form, selezionata tramite i due parametri espliciti. Tutto il content \u00e8 scaricato attarverso i chunk dello stream ('100 continue') per evitare il timeout dei workers in caso di contenuti di grandi dimensioni. Possibili risposte: 200 OK : Nel caso in cui l'azione vada a buon fine. L'azione richiesta viene presa in carico ma potrebbe avere un tempo di esecuzione arbitrario. L'azione quindi viene splittata su un altro processo. 404 Not Found : Nel caso la feature non esista o non sia definita per essa l'azione richiesta. 500 Internal Server Error : Nel caso pessimo che il modulo dichiarato non esista. 501 Not Implemented' : Nel caso la tipologia non fornisse ancora l'iplementazione di uno o tutti i moduli di gestione E' stato implementato il meccanismo minimo di gestione che prevede il salvataggio delle info fornite che possono essere fornite tali e quali in uscita (vedi metodo GET dell'api). Questo meccanismo permette di svluppare i moduli a partire da un template con risposta di default. View Source @hug . post ( \"/{feature}/{action}\" , parse_body = False ) @hug . default_input_format ( content_type = \"multipart/form-data\" ) def make_action ( feature , action , request = None , response = None ) : \"\"\" **Esecuzione delle azioni** Richiede l'esecuzione di una specifica azione su una feature, fornendo tutte le informazioni \\ necessarie attraverso una form dinamica dedicata. - Oltre ai due parametri `feature` e `form`, `timestamp`, indicati nella url, accetta un \\ _multipart/form-data_ basato sulla specifica form, selezionata tramite i due parametri espliciti. - Tutto il content \u00e8 scaricato attarverso i chunk dello stream ('100 continue') per evitare il \\ timeout dei workers in caso di contenuti di grandi dimensioni. Possibili risposte: - _200 OK_: Nel caso in cui l'azione vada a buon fine. L'azione richiesta viene presa in carico ma \\ potrebbe avere un tempo di esecuzione arbitrario. L'azione quindi viene splittata su un altro processo. - _404 Not Found_: Nel caso la feature non esista o non sia definita per essa l'azione richiesta. - _500 Internal Server Error_: Nel caso pessimo che il modulo dichiarato non esista. - _501 Not Implemented'_: Nel caso la tipologia non fornisse ancora l'iplementazione di uno o tutti \\ i moduli di gestione E' stato implementato il meccanismo minimo di gestione che prevede il salvataggio delle info \\ fornite che possono essere fornite tali e quali in uscita (vedi metodo GET dell'api). Questo \\ meccanismo permette di svluppare i moduli a partire da un template con risposta di default. \"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db [ \"features\" ][ feature ] featobj = sourceman . sourceFactory ( feature ) except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature '{feature}' does not exists or it is misconfigured: {e}\" out . format ( request = request , response = response ) return try : schema = featobj . getActionSchema ( action ) except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_IMPLEMENTED out . message = f \"Prototype '{featobj.type}' actions not implemented.\" out . format ( request = request , response = response ) return except ModuleNotFoundError as e : traceback . print_exc () out . status = falcon . HTTP_INTERNAL_SERVER_ERROR out . message = f \"Prototype '{featobj.type}' module not found.\" out . format ( request = request , response = response ) return parser = StreamingFormDataParser ( headers = request . headers ) values = {} # TODO Differenziazione delle tipologie di input for k , w in schema [ \"fields\" ] . items () : if w == \"LocalFile\" : timenow = time . perf_counter () filepath = os . path . join ( tempfile . gettempdir (), f \"{feature}.{k}.{timenow}.part\" ) target = FileTarget ( filepath ) parser . register ( k , target ) values [ k ] = filepath else : target = ValueTarget () parser . register ( k , target ) values [ k ] = target while True : chunk = request . stream . read ( 8192 ) if not chunk : break parser . data_received ( chunk ) kwargs = {} for k , w in values . items () : if isinstance ( w , str ) : v = os . path . exists ( w ) and w or None else : v = unquote ( w . value . decode ( \"utf8\" )) or None kwargs [ k ] = v m = [ m for m in schema[\"required\" ] if kwargs [ m ] is None ] if m . __len__ () : out . status = falcon . HTTP_BAD_REQUEST out . message = f \"Required parameters {m} not supplied\" out . format ( request = request , response = response ) return # CHECKS request checks ALL RIGHT . Continuing with code loading # Trying to initialize feature action manager module try : result = featobj . execAction ( action , ** kwargs ) except AttributeError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_IMPLEMENTED out . message = f \"Action '{action}' not implemented.\" out . format ( request = request , response = response ) return except ValueError as e : traceback . print_exc () out . status = falcon . HTTP_BAD_REQUEST out . message = f \"Action values error: {e}.\" out . format ( request = request , response = response ) return except Exception as e : traceback . print_exc () try : db [ \"actions\" ][ feature,action,result['timestamp' ] ] = { \"value\" : result } db [ \"features\" ] . save () except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_INTERNAL_SERVER_ERROR out . message = str ( e ) out . format ( request = request , response = response ) except ValueError as e : traceback . print_exc () out . status = falcon . HTTP_BAD_REQUEST out . message = str ( e ) out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return","title":"Actions"},{"location":"reference/hielen2/api/actions/#module-hielen2apiactions","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 #!/usr/bin/env python # coding=utf-8 import hug import tempfile import falcon import os import time import json from hielen2 import db , conf import hielen2.source as sourceman from streaming_form_data import StreamingFormDataParser from streaming_form_data.targets import FileTarget , ValueTarget from himada.api import ResponseFormatter from urllib.parse import unquote from importlib import import_module import traceback @hug . get ( \"/ {feature} \" ) def features_actions_values ( feature , actions = None , timestamp = None , request = None , response = None ): \"\"\" **Recupero dello stato corrente delle azioni effettuate su una feature** L'intento di questa api \u00e8 quello di fornire i valori richiesti secondo lo schema dell'azione ___nota 1___: `actions` accetta valori multipli separati da virgola ___nota 2___: A seconda dell'action richiesta, alcuni parametri potrebbero essere utilizzati in fase \\ di input ma non registrati. Il che vuol dire che per quei parametri il valore di ritorno sar\u00e0 null viene restituito una struttura di questo tipo: [ { \"feature\"*:..., \"action_name*\":..., \"timestamp\": ..., \"value\":{...} }, { \"feature\"*:..., \"action_name*\":..., \"timestamp\": ..., \"value\":{...} }, ... ] ___nota 3___ :(*) I campi \"feature\" e \"action\" potrebbero non essere restituiti nella struttura \\ nel caso in cui essi risultino non ambigui. \"timestamp\" e \"value\" vengono sempre restituiti Possibili risposte: - _404 Not Found_: Nel non venga trovata la feature richiesta o essa abbia un problema di \\ configurazione \"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db[\"features\"][feature] featobj = sourceman . sourceFactory ( feature ) out . data = featobj . getActionValues ( actions , timestamp ) if not isinstance ( out . data , list ): out . data = [ out . data ] except Exception as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature ' { feature } ' does not exists or it is misconfigured: { e } \" out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return @hug . get ( \"/ {feature} / {action} \" ) def feature_action_values ( feature , action , timestamp = None , request = None , response = None ): \"\"\" **Recupero dello stato corrente per una specifica azione di una specifica feature**\"\"\" return features_actions_values ( feature , action , timestamp , request = request , response = response ) @hug . delete ( \"/ {feature} / {action} \" ) def feature_action_delete ( feature , action , timestamp , request = None , response = None ): \"\"\" **Eliminazione di una determinata azione di una specifica feature**\"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db[\"features\"][feature] featobj = sourceman . sourceFactory ( feature ) out . data = featobj . deleteActionValues ( action , timestamp ) except Exception as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature ' { feature } ' does not exists or it is misconfigured: { e } \" out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return @hug . post ( \"/ {feature} / {action} \" , parse_body = False ) @hug . default_input_format ( content_type = \"multipart/form-data\" ) def make_action ( feature , action , request = None , response = None ): \"\"\" **Esecuzione delle azioni** Richiede l'esecuzione di una specifica azione su una feature, fornendo tutte le informazioni \\ necessarie attraverso una form dinamica dedicata. - Oltre ai due parametri `feature` e `form`, `timestamp`, indicati nella url, accetta un \\ _multipart/form-data_ basato sulla specifica form, selezionata tramite i due parametri espliciti. - Tutto il content \u00e8 scaricato attarverso i chunk dello stream ('100 continue') per evitare il \\ timeout dei workers in caso di contenuti di grandi dimensioni. Possibili risposte: - _200 OK_: Nel caso in cui l'azione vada a buon fine. L'azione richiesta viene presa in carico ma \\ potrebbe avere un tempo di esecuzione arbitrario. L'azione quindi viene splittata su un altro processo. - _404 Not Found_: Nel caso la feature non esista o non sia definita per essa l'azione richiesta. - _500 Internal Server Error_: Nel caso pessimo che il modulo dichiarato non esista. - _501 Not Implemented'_: Nel caso la tipologia non fornisse ancora l'iplementazione di uno o tutti \\ i moduli di gestione E' stato implementato il meccanismo minimo di gestione che prevede il salvataggio delle info \\ fornite che possono essere fornite tali e quali in uscita (vedi metodo GET dell'api). Questo \\ meccanismo permette di svluppare i moduli a partire da un template con risposta di default. \"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db[\"features\"][feature] featobj = sourceman . sourceFactory ( feature ) except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature ' { feature } ' does not exists or it is misconfigured: { e } \" out . format ( request = request , response = response ) return try : schema = featobj . getActionSchema ( action ) except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_IMPLEMENTED out . message = f \"Prototype ' { featobj . type } ' actions not implemented.\" out . format ( request = request , response = response ) return except ModuleNotFoundError as e : traceback . print_exc () out . status = falcon . HTTP_INTERNAL_SERVER_ERROR out . message = f \"Prototype ' { featobj . type } ' module not found.\" out . format ( request = request , response = response ) return parser = StreamingFormDataParser ( headers = request . headers ) values = {} # TODO Differenziazione delle tipologie di input for k , w in schema [ \"fields\" ] . items (): if w == \"LocalFile\" : timenow = time . perf_counter () filepath = os . path . join ( tempfile . gettempdir (), f \" { feature } . { k } . { timenow } .part\" ) target = FileTarget ( filepath ) parser . register ( k , target ) values [ k ] = filepath else : target = ValueTarget () parser . register ( k , target ) values [ k ] = target while True : chunk = request . stream . read ( 8192 ) if not chunk : break parser . data_received ( chunk ) kwargs = {} for k , w in values . items (): if isinstance ( w , str ): v = os . path . exists ( w ) and w or None else : v = unquote ( w . value . decode ( \"utf8\" )) or None kwargs [ k ] = v m = [ m for m in schema [ \"required\" ] if kwargs [ m ] is None ] if m . __len__ (): out . status = falcon . HTTP_BAD_REQUEST out . message = f \"Required parameters { m } not supplied\" out . format ( request = request , response = response ) return # CHECKS request checks ALL RIGHT. Continuing with code loading # Trying to initialize feature action manager module try : result = featobj . execAction ( action , ** kwargs ) except AttributeError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_IMPLEMENTED out . message = f \"Action ' { action } ' not implemented.\" out . format ( request = request , response = response ) return except ValueError as e : traceback . print_exc () out . status = falcon . HTTP_BAD_REQUEST out . message = f \"Action values error: { e } .\" out . format ( request = request , response = response ) return except Exception as e : traceback . print_exc () try : db [ \"actions\" ][ feature , action , result [ 'timestamp' ]] = { \"value\" : result } db [ \"features\" ] . save () except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_INTERNAL_SERVER_ERROR out . message = str ( e ) out . format ( request = request , response = response ) except ValueError as e : traceback . print_exc () out . status = falcon . HTTP_BAD_REQUEST out . message = str ( e ) out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return","title":"Module hielen2.api.actions"},{"location":"reference/hielen2/api/actions/#variables","text":"conf db","title":"Variables"},{"location":"reference/hielen2/api/actions/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/api/actions/#feature_action_delete","text":"def feature_action_delete ( feature , action , timestamp , request = None , response = None ) Eliminazione di una determinata azione di una specifica feature View Source @hug . delete ( \"/{feature}/{action}\" ) def feature_action_delete ( feature , action , timestamp , request = None , response = None ) : \"\"\" **Eliminazione di una determinata azione di una specifica feature**\"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db [ \"features\" ][ feature ] featobj = sourceman . sourceFactory ( feature ) out . data = featobj . deleteActionValues ( action , timestamp ) except Exception as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature '{feature}' does not exists or it is misconfigured: {e}\" out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return","title":"feature_action_delete"},{"location":"reference/hielen2/api/actions/#feature_action_values","text":"def feature_action_values ( feature , action , timestamp = None , request = None , response = None ) Recupero dello stato corrente per una specifica azione di una specifica feature View Source @hug . get ( \"/{feature}/{action}\" ) def feature_action_values ( feature , action , timestamp = None , request = None , response = None ) : \"\"\" **Recupero dello stato corrente per una specifica azione di una specifica feature**\"\"\" return features_actions_values ( feature , action , timestamp , request = request , response = response )","title":"feature_action_values"},{"location":"reference/hielen2/api/actions/#features_actions_values","text":"def features_actions_values ( feature , actions = None , timestamp = None , request = None , response = None ) Recupero dello stato corrente delle azioni effettuate su una feature L'intento di questa api \u00e8 quello di fornire i valori richiesti secondo lo schema dell'azione nota 1 : actions accetta valori multipli separati da virgola nota 2 : A seconda dell'action richiesta, alcuni parametri potrebbero essere utilizzati in fase di input ma non registrati. Il che vuol dire che per quei parametri il valore di ritorno sar\u00e0 null viene restituito una struttura di questo tipo: [ { \"feature\"*:..., \"action_name*\":..., \"timestamp\": ..., \"value\":{...} }, { \"feature\"*:..., \"action_name*\":..., \"timestamp\": ..., \"value\":{...} }, ... ] nota 3 :(*) I campi \"feature\" e \"action\" potrebbero non essere restituiti nella struttura nel caso in cui essi risultino non ambigui. \"timestamp\" e \"value\" vengono sempre restituiti Possibili risposte: 404 Not Found : Nel non venga trovata la feature richiesta o essa abbia un problema di configurazione View Source @hug . get ( \"/{feature}\" ) def features_actions_values ( feature , actions = None , timestamp = None , request = None , response = None ) : \"\"\" **Recupero dello stato corrente delle azioni effettuate su una feature** L'intento di questa api \u00e8 quello di fornire i valori richiesti secondo lo schema dell'azione ___nota 1___: `actions` accetta valori multipli separati da virgola ___nota 2___: A seconda dell'action richiesta, alcuni parametri potrebbero essere utilizzati in fase \\ di input ma non registrati. Il che vuol dire che per quei parametri il valore di ritorno sar\u00e0 null viene restituito una struttura di questo tipo: [ { \" feature \"*:..., \" action_name * \":..., \" timestamp \": ..., \" value \":{...} }, { \" feature \"*:..., \" action_name * \":..., \" timestamp \": ..., \" value \":{...} }, ... ] ___nota 3___ :(*) I campi \" feature \" e \" action \" potrebbero non essere restituiti nella struttura \\ nel caso in cui essi risultino non ambigui. \" timestamp \" e \" value \" vengono sempre restituiti Possibili risposte: - _404 Not Found_: Nel non venga trovata la feature richiesta o essa abbia un problema di \\ configurazione \"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db [ \"features\" ][ feature ] featobj = sourceman . sourceFactory ( feature ) out . data = featobj . getActionValues ( actions , timestamp ) if not isinstance ( out . data , list ) : out . data =[ out.data ] except Exception as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature '{feature}' does not exists or it is misconfigured: {e}\" out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return","title":"features_actions_values"},{"location":"reference/hielen2/api/actions/#make_action","text":"def make_action ( feature , action , request = None , response = None ) Esecuzione delle azioni Richiede l'esecuzione di una specifica azione su una feature, fornendo tutte le informazioni necessarie attraverso una form dinamica dedicata. Oltre ai due parametri feature e form , timestamp , indicati nella url, accetta un multipart/form-data basato sulla specifica form, selezionata tramite i due parametri espliciti. Tutto il content \u00e8 scaricato attarverso i chunk dello stream ('100 continue') per evitare il timeout dei workers in caso di contenuti di grandi dimensioni. Possibili risposte: 200 OK : Nel caso in cui l'azione vada a buon fine. L'azione richiesta viene presa in carico ma potrebbe avere un tempo di esecuzione arbitrario. L'azione quindi viene splittata su un altro processo. 404 Not Found : Nel caso la feature non esista o non sia definita per essa l'azione richiesta. 500 Internal Server Error : Nel caso pessimo che il modulo dichiarato non esista. 501 Not Implemented' : Nel caso la tipologia non fornisse ancora l'iplementazione di uno o tutti i moduli di gestione E' stato implementato il meccanismo minimo di gestione che prevede il salvataggio delle info fornite che possono essere fornite tali e quali in uscita (vedi metodo GET dell'api). Questo meccanismo permette di svluppare i moduli a partire da un template con risposta di default. View Source @hug . post ( \"/{feature}/{action}\" , parse_body = False ) @hug . default_input_format ( content_type = \"multipart/form-data\" ) def make_action ( feature , action , request = None , response = None ) : \"\"\" **Esecuzione delle azioni** Richiede l'esecuzione di una specifica azione su una feature, fornendo tutte le informazioni \\ necessarie attraverso una form dinamica dedicata. - Oltre ai due parametri `feature` e `form`, `timestamp`, indicati nella url, accetta un \\ _multipart/form-data_ basato sulla specifica form, selezionata tramite i due parametri espliciti. - Tutto il content \u00e8 scaricato attarverso i chunk dello stream ('100 continue') per evitare il \\ timeout dei workers in caso di contenuti di grandi dimensioni. Possibili risposte: - _200 OK_: Nel caso in cui l'azione vada a buon fine. L'azione richiesta viene presa in carico ma \\ potrebbe avere un tempo di esecuzione arbitrario. L'azione quindi viene splittata su un altro processo. - _404 Not Found_: Nel caso la feature non esista o non sia definita per essa l'azione richiesta. - _500 Internal Server Error_: Nel caso pessimo che il modulo dichiarato non esista. - _501 Not Implemented'_: Nel caso la tipologia non fornisse ancora l'iplementazione di uno o tutti \\ i moduli di gestione E' stato implementato il meccanismo minimo di gestione che prevede il salvataggio delle info \\ fornite che possono essere fornite tali e quali in uscita (vedi metodo GET dell'api). Questo \\ meccanismo permette di svluppare i moduli a partire da un template con risposta di default. \"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db [ \"features\" ][ feature ] featobj = sourceman . sourceFactory ( feature ) except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature '{feature}' does not exists or it is misconfigured: {e}\" out . format ( request = request , response = response ) return try : schema = featobj . getActionSchema ( action ) except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_IMPLEMENTED out . message = f \"Prototype '{featobj.type}' actions not implemented.\" out . format ( request = request , response = response ) return except ModuleNotFoundError as e : traceback . print_exc () out . status = falcon . HTTP_INTERNAL_SERVER_ERROR out . message = f \"Prototype '{featobj.type}' module not found.\" out . format ( request = request , response = response ) return parser = StreamingFormDataParser ( headers = request . headers ) values = {} # TODO Differenziazione delle tipologie di input for k , w in schema [ \"fields\" ] . items () : if w == \"LocalFile\" : timenow = time . perf_counter () filepath = os . path . join ( tempfile . gettempdir (), f \"{feature}.{k}.{timenow}.part\" ) target = FileTarget ( filepath ) parser . register ( k , target ) values [ k ] = filepath else : target = ValueTarget () parser . register ( k , target ) values [ k ] = target while True : chunk = request . stream . read ( 8192 ) if not chunk : break parser . data_received ( chunk ) kwargs = {} for k , w in values . items () : if isinstance ( w , str ) : v = os . path . exists ( w ) and w or None else : v = unquote ( w . value . decode ( \"utf8\" )) or None kwargs [ k ] = v m = [ m for m in schema[\"required\" ] if kwargs [ m ] is None ] if m . __len__ () : out . status = falcon . HTTP_BAD_REQUEST out . message = f \"Required parameters {m} not supplied\" out . format ( request = request , response = response ) return # CHECKS request checks ALL RIGHT . Continuing with code loading # Trying to initialize feature action manager module try : result = featobj . execAction ( action , ** kwargs ) except AttributeError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_IMPLEMENTED out . message = f \"Action '{action}' not implemented.\" out . format ( request = request , response = response ) return except ValueError as e : traceback . print_exc () out . status = falcon . HTTP_BAD_REQUEST out . message = f \"Action values error: {e}.\" out . format ( request = request , response = response ) return except Exception as e : traceback . print_exc () try : db [ \"actions\" ][ feature,action,result['timestamp' ] ] = { \"value\" : result } db [ \"features\" ] . save () except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_INTERNAL_SERVER_ERROR out . message = str ( e ) out . format ( request = request , response = response ) except ValueError as e : traceback . print_exc () out . status = falcon . HTTP_BAD_REQUEST out . message = str ( e ) out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return","title":"make_action"},{"location":"reference/hielen2/api/actionschemata/","text":"Module hielen2.api.actionschemata View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 #!/usr/bin/env python # coding=utf-8 import hug import falcon from hielen2 import db , source from himada.api import ResponseFormatter @hug . get ( \"/\" ) def get_protos_schemata ( prototypes = None , actions = None , request = None , response = None ): \"\"\" **Recupero dello schema dei parametri per inizializare le forms delle azioni** ritorna una struttura json di questo tipo: { \"NomePrototipo1\": { \"action1\": { \"args\": { \"arg1.1\": \"type_arg1.1\", \"arg1.2\": \"type_arg1.2\", ... }, \"mandatory\": [ args keys sublist ] }, \"action2\": { \"args\": { \"arg2.1\": \"type_arg2.1\", \"arg2.2\": \"type_arg2.2\", ... }, }, ... }, \"NomePrototipo3\": { ... }, ... }, \"\"\" out = ResponseFormatter () out . data = {} try : if actions is not None and actions is not list : actions = [ actions ] protos = db [ \"features_proto\" ][ prototypes ] if not isinstance ( protos , list ): protos = [ protos ] for p in protos : uid = p [ 'uid' ] out . data [ uid ] = {} for a in [ act for act in source . moduleActions ( uid ) if actions is None or act in actions ]: out . data [ uid ][ a ] = source . getActionSchema ( uid , a ) except KeyError as e : out . status = out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) raise e response = out . format ( response = response , request = request ) @hug . get ( \"/ {prototype} \" ) def get_proto_schemata ( prototype , actions = None , request = None , response = None ): \"\"\" **Alias per il recupero di tutte le informazioni di uno specifico prototipo** \"\"\" return get_protos_schemata ( prototype , actions , request , response ) @hug . get ( \"/ {prototype} / {action} \" ) def get_proto_schema ( prototype , action , request = None , response = None ): \"\"\" **Alias per il recupero di tutte le informazioni delle form di uno specifico prototipo** \"\"\" return get_protos_schemata ( prototype , action , request , response ) Variables db Functions get_proto_schema def get_proto_schema ( prototype , action , request = None , response = None ) Alias per il recupero di tutte le informazioni delle form di uno specifico prototipo View Source @hug . get ( \"/{prototype}/{action}\" ) def get_proto_schema ( prototype , action , request = None , response = None ) : \"\"\" **Alias per il recupero di tutte le informazioni delle form di uno specifico prototipo** \"\"\" return get_protos_schemata ( prototype , action , request , response ) get_proto_schemata def get_proto_schemata ( prototype , actions = None , request = None , response = None ) Alias per il recupero di tutte le informazioni di uno specifico prototipo View Source @hug . get ( \"/{prototype}\" ) def get_proto_schemata ( prototype , actions = None , request = None , response = None ) : \"\"\" **Alias per il recupero di tutte le informazioni di uno specifico prototipo** \"\"\" return get_protos_schemata ( prototype , actions , request , response ) get_protos_schemata def get_protos_schemata ( prototypes = None , actions = None , request = None , response = None ) Recupero dello schema dei parametri per inizializare le forms delle azioni ritorna una struttura json di questo tipo: { \"NomePrototipo1\": { \"action1\": { \"args\": { \"arg1.1\": \"type_arg1.1\", \"arg1.2\": \"type_arg1.2\", ... }, \"mandatory\": [ args keys sublist ] }, \"action2\": { \"args\": { \"arg2.1\": \"type_arg2.1\", \"arg2.2\": \"type_arg2.2\", ... }, }, ... }, \"NomePrototipo3\": { ... }, ... }, View Source @hug . get ( \"/\" ) def get_protos_schemata ( prototypes = None , actions = None , request = None , response = None ) : \"\"\" **Recupero dello schema dei parametri per inizializare le forms delle azioni** ritorna una struttura json di questo tipo: { \" NomePrototipo1 \": { \" action1 \": { \" args \": { \" arg1 .1 \": \" type_arg1 .1 \", \" arg1 .2 \": \" type_arg1 .2 \", ... }, \" mandatory \": [ args keys sublist ] }, \" action2 \": { \" args \": { \" arg2 .1 \": \" type_arg2 .1 \", \" arg2 .2 \": \" type_arg2 .2 \", ... }, }, ... }, \" NomePrototipo3 \": { ... }, ... }, \"\"\" out = ResponseFormatter () out . data = {} try : if actions is not None and actions is not list : actions =[ actions ] protos = db [ \"features_proto\" ][ prototypes ] if not isinstance ( protos , list ) : protos =[ protos ] for p in protos : uid = p [ 'uid' ] out . data [ uid ]= {} for a in [ act for act in source.moduleActions(uid) if actions is None or act in actions ] : out . data [ uid ][ a ]= source . getActionSchema ( uid , a ) except KeyError as e : out . status = out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) raise e response = out . format ( response = response , request = request )","title":"Actionschemata"},{"location":"reference/hielen2/api/actionschemata/#module-hielen2apiactionschemata","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 #!/usr/bin/env python # coding=utf-8 import hug import falcon from hielen2 import db , source from himada.api import ResponseFormatter @hug . get ( \"/\" ) def get_protos_schemata ( prototypes = None , actions = None , request = None , response = None ): \"\"\" **Recupero dello schema dei parametri per inizializare le forms delle azioni** ritorna una struttura json di questo tipo: { \"NomePrototipo1\": { \"action1\": { \"args\": { \"arg1.1\": \"type_arg1.1\", \"arg1.2\": \"type_arg1.2\", ... }, \"mandatory\": [ args keys sublist ] }, \"action2\": { \"args\": { \"arg2.1\": \"type_arg2.1\", \"arg2.2\": \"type_arg2.2\", ... }, }, ... }, \"NomePrototipo3\": { ... }, ... }, \"\"\" out = ResponseFormatter () out . data = {} try : if actions is not None and actions is not list : actions = [ actions ] protos = db [ \"features_proto\" ][ prototypes ] if not isinstance ( protos , list ): protos = [ protos ] for p in protos : uid = p [ 'uid' ] out . data [ uid ] = {} for a in [ act for act in source . moduleActions ( uid ) if actions is None or act in actions ]: out . data [ uid ][ a ] = source . getActionSchema ( uid , a ) except KeyError as e : out . status = out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) raise e response = out . format ( response = response , request = request ) @hug . get ( \"/ {prototype} \" ) def get_proto_schemata ( prototype , actions = None , request = None , response = None ): \"\"\" **Alias per il recupero di tutte le informazioni di uno specifico prototipo** \"\"\" return get_protos_schemata ( prototype , actions , request , response ) @hug . get ( \"/ {prototype} / {action} \" ) def get_proto_schema ( prototype , action , request = None , response = None ): \"\"\" **Alias per il recupero di tutte le informazioni delle form di uno specifico prototipo** \"\"\" return get_protos_schemata ( prototype , action , request , response )","title":"Module hielen2.api.actionschemata"},{"location":"reference/hielen2/api/actionschemata/#variables","text":"db","title":"Variables"},{"location":"reference/hielen2/api/actionschemata/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/api/actionschemata/#get_proto_schema","text":"def get_proto_schema ( prototype , action , request = None , response = None ) Alias per il recupero di tutte le informazioni delle form di uno specifico prototipo View Source @hug . get ( \"/{prototype}/{action}\" ) def get_proto_schema ( prototype , action , request = None , response = None ) : \"\"\" **Alias per il recupero di tutte le informazioni delle form di uno specifico prototipo** \"\"\" return get_protos_schemata ( prototype , action , request , response )","title":"get_proto_schema"},{"location":"reference/hielen2/api/actionschemata/#get_proto_schemata","text":"def get_proto_schemata ( prototype , actions = None , request = None , response = None ) Alias per il recupero di tutte le informazioni di uno specifico prototipo View Source @hug . get ( \"/{prototype}\" ) def get_proto_schemata ( prototype , actions = None , request = None , response = None ) : \"\"\" **Alias per il recupero di tutte le informazioni di uno specifico prototipo** \"\"\" return get_protos_schemata ( prototype , actions , request , response )","title":"get_proto_schemata"},{"location":"reference/hielen2/api/actionschemata/#get_protos_schemata","text":"def get_protos_schemata ( prototypes = None , actions = None , request = None , response = None ) Recupero dello schema dei parametri per inizializare le forms delle azioni ritorna una struttura json di questo tipo: { \"NomePrototipo1\": { \"action1\": { \"args\": { \"arg1.1\": \"type_arg1.1\", \"arg1.2\": \"type_arg1.2\", ... }, \"mandatory\": [ args keys sublist ] }, \"action2\": { \"args\": { \"arg2.1\": \"type_arg2.1\", \"arg2.2\": \"type_arg2.2\", ... }, }, ... }, \"NomePrototipo3\": { ... }, ... }, View Source @hug . get ( \"/\" ) def get_protos_schemata ( prototypes = None , actions = None , request = None , response = None ) : \"\"\" **Recupero dello schema dei parametri per inizializare le forms delle azioni** ritorna una struttura json di questo tipo: { \" NomePrototipo1 \": { \" action1 \": { \" args \": { \" arg1 .1 \": \" type_arg1 .1 \", \" arg1 .2 \": \" type_arg1 .2 \", ... }, \" mandatory \": [ args keys sublist ] }, \" action2 \": { \" args \": { \" arg2 .1 \": \" type_arg2 .1 \", \" arg2 .2 \": \" type_arg2 .2 \", ... }, }, ... }, \" NomePrototipo3 \": { ... }, ... }, \"\"\" out = ResponseFormatter () out . data = {} try : if actions is not None and actions is not list : actions =[ actions ] protos = db [ \"features_proto\" ][ prototypes ] if not isinstance ( protos , list ) : protos =[ protos ] for p in protos : uid = p [ 'uid' ] out . data [ uid ]= {} for a in [ act for act in source.moduleActions(uid) if actions is None or act in actions ] : out . data [ uid ][ a ]= source . getActionSchema ( uid , a ) except KeyError as e : out . status = out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) raise e response = out . format ( response = response , request = request )","title":"get_protos_schemata"},{"location":"reference/hielen2/api/data/","text":"Module hielen2.api.data View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 #!/usr/bin/env python # coding=utf-8 import hug import falcon import json from marshmallow import Schema , fields from numpy import nan , unique from pandas import DataFrame , to_datetime from hielen2 import db from hielen2.data.data_access_layer import Series from hielen2.utils import hug_output_format_conten_type , JsonValidable from himada.api import ResponseFormatter import asyncio data_out_handler = hug_output_format_conten_type ( [ hug . output_format . text , hug . output_format . json ] ) CSV = \"text/plain; charset=utf-8\" JSON = \"application/json; charset=utf-8\" class DataMapSchema ( Schema ): \"\"\"\"\"\" timefrom = fields . Str ( default = None , required = False ) timeto = fields . Str ( default = None , reuired = False ) series = fields . List ( fields . Str , default = []) ####### API DATATABLE ####### @hug . get ( \"/\" , examples = \"\" , output = data_out_handler ) def tabular_data ( datamap : JsonValidable ( DataMapSchema ( many = True )), content_type = None , request = None , response = None , ): series = {} for s in datamap : try : timefrom = s [ \"timefrom\" ] except KeyError : timefrom = None try : timeto = s [ \"timeto\" ] except KeyError : timeto = None for p in s [ \"series\" ]: if p not in series . keys (): series [ p ] = [] try : series [ p ] . append ( Series ( p ) . thdata ( timefrom = timefrom , timeto = timeto )) except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return out = DataFrame () for param , sers in series . items (): ser = None for r in sers : s = r . result () if ser is None : ser = s else : ser = ser . append ( s ) . sort_index () idx = unique ( ser . index . values , return_index = True )[ 1 ] ser = ser . iloc [ idx ] ser . columns = [ param ] out = out . join ( ser , how = \"outer\" ) out . index . name = \"timestamp\" requested = data_out_handler . requested ( request ) . content_type if requested == CSV : return hug . types . text ( out . to_csv ()) if requested == JSON : return hug . types . json ( out . to_json ( orient = \"table\" )) @hug . get ( \"/ {feature} /\" , output = data_out_handler ) def tabular_data_el ( feature , par = None , timefrom = None , timeto = None , content_type = None , request = None , response = None , ): try : ft = db [ \"features\" ][ feature ] except KeyError : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( feature ) + \" not found\" response = out . format ( response = response , request = request ) return try : if par is None : series = list ( ft [ \"parameters\" ] . values ()) else : series = [ ft [ \"parameters\" ][ par ]] except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return datamap = dict ( series = series ) if timefrom is not None : datamap [ \"timefrom\" ] = timefrom if timeto is not None : datamap [ \"timeto\" ] = timeto return tabular_data ( datamap = [ datamap ], request = request , response = response ) @hug . get ( \"/ {feature} / {par} \" , output = data_out_handler ) def tabular_data_par ( feature = None , par = None , timefrom = None , timeto = None , content_type = None , request = None , response = None , ): return tabular_data_el ( feature = feature , par = par , timefrom = timefrom , timeto = timeto , request = request , response = response , ) Variables CSV JSON db nan Functions data_out_handler def data_out_handler ( data , request , response ) Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) View Source def output_type ( data , request , response ): handler = requested_output_type ( request ) response . content_type = handler . content_type return handler ( data , request = request , response = response ) tabular_data def tabular_data ( datamap : < hielen2 . utils . JsonValidable object at 0x7fb7d5003ee0 > , content_type = None , request = None , response = None ) View Source @hug . get ( \"/\" , examples = \"\" , output = data_out_handler ) def tabular_data ( datamap : JsonValidable ( DataMapSchema ( many = True )), content_type = None , request = None , response = None , ) : series = {} for s in datamap : try : timefrom = s [ \"timefrom\" ] except KeyError : timefrom = None try : timeto = s [ \"timeto\" ] except KeyError : timeto = None for p in s [ \"series\" ] : if p not in series . keys () : series [ p ] = [] try : series [ p ] . append ( Series ( p ). thdata ( timefrom = timefrom , timeto = timeto )) except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return out = DataFrame () for param , sers in series . items () : ser = None for r in sers : s = r . result () if ser is None : ser = s else : ser = ser . append ( s ). sort_index () idx = unique ( ser . index . values , return_index = True ) [ 1 ] ser = ser . iloc [ idx ] ser . columns = [ param ] out = out . join ( ser , how = \"outer\" ) out . index . name = \"timestamp\" requested = data_out_handler . requested ( request ). content_type if requested == CSV : return hug . types . text ( out . to_csv ()) if requested == JSON : return hug . types . json ( out . to_json ( orient = \"table\" )) tabular_data_el def tabular_data_el ( feature , par = None , timefrom = None , timeto = None , content_type = None , request = None , response = None ) View Source @hug . get ( \"/{feature}/\" , output = data_out_handler ) def tabular_data_el ( feature , par = None , timefrom = None , timeto = None , content_type = None , request = None , response = None , ) : try : ft = db [ \"features\" ][ feature ] except KeyError : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( feature ) + \" not found\" response = out . format ( response = response , request = request ) return try : if par is None : series = list ( ft [ \"parameters\" ] . values ()) else : series = [ ft[\"parameters\" ][ par ] ] except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return datamap = dict ( series = series ) if timefrom is not None : datamap [ \"timefrom\" ] = timefrom if timeto is not None : datamap [ \"timeto\" ] = timeto return tabular_data ( datamap =[ datamap ] , request = request , response = response ) tabular_data_par def tabular_data_par ( feature = None , par = None , timefrom = None , timeto = None , content_type = None , request = None , response = None ) View Source @hug . get ( \"/{feature}/{par}\" , output = data_out_handler ) def tabular_data_par ( feature = None , par = None , timefrom = None , timeto = None , content_type = None , request = None , response = None , ) : return tabular_data_el ( feature = feature , par = par , timefrom = timefrom , timeto = timeto , request = request , response = response , ) Classes DataMapSchema class DataMapSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) View Source class DataMapSchema ( Schema ): \"\"\"\"\"\" timefrom = fields . Str ( default = None , required = False ) timeto = fields . Str ( default = None , reuired = False ) series = fields . List ( fields . Str , default =[]) Ancestors (in MRO) marshmallow.schema.Schema marshmallow.base.SchemaABC Class variables Meta OPTIONS_CLASS TYPE_MAPPING error_messages opts series timefrom timeto Static methods from_dict def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls Instance variables dict_class set_class Methods dump def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result dumps def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs ) get_attribute def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default ) handle_error def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass load def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True ) loads def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown ) on_bind_field def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None validate def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"Data"},{"location":"reference/hielen2/api/data/#module-hielen2apidata","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 #!/usr/bin/env python # coding=utf-8 import hug import falcon import json from marshmallow import Schema , fields from numpy import nan , unique from pandas import DataFrame , to_datetime from hielen2 import db from hielen2.data.data_access_layer import Series from hielen2.utils import hug_output_format_conten_type , JsonValidable from himada.api import ResponseFormatter import asyncio data_out_handler = hug_output_format_conten_type ( [ hug . output_format . text , hug . output_format . json ] ) CSV = \"text/plain; charset=utf-8\" JSON = \"application/json; charset=utf-8\" class DataMapSchema ( Schema ): \"\"\"\"\"\" timefrom = fields . Str ( default = None , required = False ) timeto = fields . Str ( default = None , reuired = False ) series = fields . List ( fields . Str , default = []) ####### API DATATABLE ####### @hug . get ( \"/\" , examples = \"\" , output = data_out_handler ) def tabular_data ( datamap : JsonValidable ( DataMapSchema ( many = True )), content_type = None , request = None , response = None , ): series = {} for s in datamap : try : timefrom = s [ \"timefrom\" ] except KeyError : timefrom = None try : timeto = s [ \"timeto\" ] except KeyError : timeto = None for p in s [ \"series\" ]: if p not in series . keys (): series [ p ] = [] try : series [ p ] . append ( Series ( p ) . thdata ( timefrom = timefrom , timeto = timeto )) except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return out = DataFrame () for param , sers in series . items (): ser = None for r in sers : s = r . result () if ser is None : ser = s else : ser = ser . append ( s ) . sort_index () idx = unique ( ser . index . values , return_index = True )[ 1 ] ser = ser . iloc [ idx ] ser . columns = [ param ] out = out . join ( ser , how = \"outer\" ) out . index . name = \"timestamp\" requested = data_out_handler . requested ( request ) . content_type if requested == CSV : return hug . types . text ( out . to_csv ()) if requested == JSON : return hug . types . json ( out . to_json ( orient = \"table\" )) @hug . get ( \"/ {feature} /\" , output = data_out_handler ) def tabular_data_el ( feature , par = None , timefrom = None , timeto = None , content_type = None , request = None , response = None , ): try : ft = db [ \"features\" ][ feature ] except KeyError : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( feature ) + \" not found\" response = out . format ( response = response , request = request ) return try : if par is None : series = list ( ft [ \"parameters\" ] . values ()) else : series = [ ft [ \"parameters\" ][ par ]] except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return datamap = dict ( series = series ) if timefrom is not None : datamap [ \"timefrom\" ] = timefrom if timeto is not None : datamap [ \"timeto\" ] = timeto return tabular_data ( datamap = [ datamap ], request = request , response = response ) @hug . get ( \"/ {feature} / {par} \" , output = data_out_handler ) def tabular_data_par ( feature = None , par = None , timefrom = None , timeto = None , content_type = None , request = None , response = None , ): return tabular_data_el ( feature = feature , par = par , timefrom = timefrom , timeto = timeto , request = request , response = response , )","title":"Module hielen2.api.data"},{"location":"reference/hielen2/api/data/#variables","text":"CSV JSON db nan","title":"Variables"},{"location":"reference/hielen2/api/data/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/api/data/#data_out_handler","text":"def data_out_handler ( data , request , response ) Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) View Source def output_type ( data , request , response ): handler = requested_output_type ( request ) response . content_type = handler . content_type return handler ( data , request = request , response = response )","title":"data_out_handler"},{"location":"reference/hielen2/api/data/#tabular_data","text":"def tabular_data ( datamap : < hielen2 . utils . JsonValidable object at 0x7fb7d5003ee0 > , content_type = None , request = None , response = None ) View Source @hug . get ( \"/\" , examples = \"\" , output = data_out_handler ) def tabular_data ( datamap : JsonValidable ( DataMapSchema ( many = True )), content_type = None , request = None , response = None , ) : series = {} for s in datamap : try : timefrom = s [ \"timefrom\" ] except KeyError : timefrom = None try : timeto = s [ \"timeto\" ] except KeyError : timeto = None for p in s [ \"series\" ] : if p not in series . keys () : series [ p ] = [] try : series [ p ] . append ( Series ( p ). thdata ( timefrom = timefrom , timeto = timeto )) except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return out = DataFrame () for param , sers in series . items () : ser = None for r in sers : s = r . result () if ser is None : ser = s else : ser = ser . append ( s ). sort_index () idx = unique ( ser . index . values , return_index = True ) [ 1 ] ser = ser . iloc [ idx ] ser . columns = [ param ] out = out . join ( ser , how = \"outer\" ) out . index . name = \"timestamp\" requested = data_out_handler . requested ( request ). content_type if requested == CSV : return hug . types . text ( out . to_csv ()) if requested == JSON : return hug . types . json ( out . to_json ( orient = \"table\" ))","title":"tabular_data"},{"location":"reference/hielen2/api/data/#tabular_data_el","text":"def tabular_data_el ( feature , par = None , timefrom = None , timeto = None , content_type = None , request = None , response = None ) View Source @hug . get ( \"/{feature}/\" , output = data_out_handler ) def tabular_data_el ( feature , par = None , timefrom = None , timeto = None , content_type = None , request = None , response = None , ) : try : ft = db [ \"features\" ][ feature ] except KeyError : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( feature ) + \" not found\" response = out . format ( response = response , request = request ) return try : if par is None : series = list ( ft [ \"parameters\" ] . values ()) else : series = [ ft[\"parameters\" ][ par ] ] except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return datamap = dict ( series = series ) if timefrom is not None : datamap [ \"timefrom\" ] = timefrom if timeto is not None : datamap [ \"timeto\" ] = timeto return tabular_data ( datamap =[ datamap ] , request = request , response = response )","title":"tabular_data_el"},{"location":"reference/hielen2/api/data/#tabular_data_par","text":"def tabular_data_par ( feature = None , par = None , timefrom = None , timeto = None , content_type = None , request = None , response = None ) View Source @hug . get ( \"/{feature}/{par}\" , output = data_out_handler ) def tabular_data_par ( feature = None , par = None , timefrom = None , timeto = None , content_type = None , request = None , response = None , ) : return tabular_data_el ( feature = feature , par = par , timefrom = timefrom , timeto = timeto , request = request , response = response , )","title":"tabular_data_par"},{"location":"reference/hielen2/api/data/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/api/data/#datamapschema","text":"class DataMapSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) View Source class DataMapSchema ( Schema ): \"\"\"\"\"\" timefrom = fields . Str ( default = None , required = False ) timeto = fields . Str ( default = None , reuired = False ) series = fields . List ( fields . Str , default =[])","title":"DataMapSchema"},{"location":"reference/hielen2/api/data/#ancestors-in-mro","text":"marshmallow.schema.Schema marshmallow.base.SchemaABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/api/data/#class-variables","text":"Meta OPTIONS_CLASS TYPE_MAPPING error_messages opts series timefrom timeto","title":"Class variables"},{"location":"reference/hielen2/api/data/#static-methods","text":"","title":"Static methods"},{"location":"reference/hielen2/api/data/#from_dict","text":"def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls","title":"from_dict"},{"location":"reference/hielen2/api/data/#instance-variables","text":"dict_class set_class","title":"Instance variables"},{"location":"reference/hielen2/api/data/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/api/data/#dump","text":"def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result","title":"dump"},{"location":"reference/hielen2/api/data/#dumps","text":"def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs )","title":"dumps"},{"location":"reference/hielen2/api/data/#get_attribute","text":"def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default )","title":"get_attribute"},{"location":"reference/hielen2/api/data/#handle_error","text":"def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass","title":"handle_error"},{"location":"reference/hielen2/api/data/#load","text":"def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True )","title":"load"},{"location":"reference/hielen2/api/data/#loads","text":"def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown )","title":"loads"},{"location":"reference/hielen2/api/data/#on_bind_field","text":"def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None","title":"on_bind_field"},{"location":"reference/hielen2/api/data/#validate","text":"def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"validate"},{"location":"reference/hielen2/api/features/","text":"Module hielen2.api.features View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 #!/usr/bin/env python # coding=utf-8 import hug import falcon from hielen2 import db from hielen2.utils import JsonValidable , hasher import hielen2.source as sourceman from marshmallow import Schema , fields from himada.api import ResponseFormatter from marshmallow_geojson import GeoJSONSchema import traceback class FeaturePropertiesSchema ( Schema ): context = fields . Str ( default = \"no-context\" , allow_none = False ) label = fields . Str ( default = None ) description = fields . Str ( default = None ) location = fields . Str ( default = None ) style = fields . Str ( default = None ) status = fields . Str ( default = None ) timestamp = fields . Str ( default = None ) @hug . post ( \"/\" ) def create_feature ( uid , prototype , properties : JsonValidable ( FeaturePropertiesSchema ()) = {}, geometry : JsonValidable ( GeoJSONSchema ()) = {}, request = None , response = None , ): \"\"\" **Creazione delle Features.** Ogni feature deve avere il suo codice univoco `uid` e il suo prototipo `prototype`. Questi due \\ campi sono immutabli (vedi PUT `/feature/{uid}`). Il prototipo della feature forisce informazioni per l'inizializazione della struttura. Il parametro `geometry` deve essere un GeoJson Se la feature viene creata correttamente ne restituisce la struttura Possibili risposte: - _409 Conflict_: Nel caso in cui il uid fornito esista gi\u00e0. - _404 Not Found_: Nel caso in cui il prototipo richiesto non esista. - _201 Created_: Nel caso in cui la feature venga creata correttamente. \"\"\" out = ResponseFormatter ( status = falcon . HTTP_CREATED ) try : # feature = db[\"features_proto\"][prototype]['struct'] feature = db [ \"features_proto\" ][ prototype ] feature [ \"geometry\" ] = geometry feature [ \"uid\" ] = uid feature [ \"type\" ] = prototype try : assert properties [ \"context\" ] is not None except Exception : properties [ \"context\" ] = \"no-context\" feature . update ( properties ) try : db [ \"features\" ][ uid ] raise ValueError ( f \"feature { uid } exists\" ) except KeyError : pass params = {} for v in feature [ 'parameters' ]: param = v [ 'param' ] struct = v [ 'struct' ] suid = hasher ( uid , param ) try : db [ 'series' ][ suid ] raise ValueError ( f \"serires ' { uid } _ { param } ' ( { suid } ) exists\" ) except KeyError : pass for k , w in struct . pop ( 'init_operands' ) . items (): struct [ 'operands' ][ k ] = eval ( w . replace ( '{{new}}' , 'feature' )) db [ 'series' ][ suid ] = struct params [ param ] = suid feature [ 'parameters' ] = params db [ \"features\" ][ uid ] = feature out . data = db [ \"features\" ][ uid ] except KeyError as e : traceback . print_exc () out . message = f \"prototype ' { prototype } ' not found.\" out . status = falcon . HTTP_NOT_FOUND except ValueError as e : traceback . print_exc () out . message = str ( e ) out . status = falcon . HTTP_CONFLICT response = out . format ( response = response , request = request ) return @hug . get ( \"/\" ) def features_info ( uids = None , cntxt = None , request = None , response = None ): \"\"\" **Recupero delle informazioni delle features.** __nota__: uids accetta valori multipli separati da virgola viene restituito una struttura di questo tipo: { \"features\": [ { \"type\": \"Feature\", \"properties\": { ... }, \"geometry\": <GeoJson Validable> }, ... ] } ___nota___: Al contrario di quanto detto nel TODO non viene inserito il context a livello \\ \"features\" perch\u00e8 in effetti \u00e8 una informazione sempre conosciuta a priori (se si lavora \\ per commesse). Al contrario se si lavora per uids allora ha senso inserie questa info all' \\ interno delle properties delle singole features. Possibili risposte: - _404 Not Found_: Nel caso in cui nessuna feature risponda ai criteri \"\"\" out = ResponseFormatter () try : out . data = { \"features\" :[]} extract = db [ \"features\" ][ uids ] if not isinstance ( extract , list ): extract = [ extract ] for v in extract : if cntxt is None or v [ \"context\" ] == cntxt : out . data [ 'features' ] . append ({ \"type\" : \"Feature\" , \"geometry\" : v . pop ( \"geometry\" ), \"parameters\" : v . pop ( \"parameters\" ), \"properties\" : v }) except KeyError as e : out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) response = out . format ( response = response , request = request ) return @hug . get ( \"/ {uid} \" ) def feature_info ( uid , cntxt = None , request = None , response = None ): \"\"\" **Alias di recupero informazioni della specifica feature**\"\"\" return features_info ( uid , cntxt , request , response ) @hug . put ( \"/ {uid} \" ) def update_feature ( uid , properties : JsonValidable ( FeaturePropertiesSchema ()) = {}, geometry : JsonValidable ( GeoJSONSchema ()) = {}, request = None , response = None , ): \"\"\" **Modifica delle properties di una feature** Possibili risposte: - _404 Not Found_: Nel caso in cui il prototipo richiesto non esista. - _202 Accepted_: Nel caso in cui la feature venga modificata correttamente. \"\"\" out = ResponseFormatter ( status = falcon . HTTP_ACCEPTED ) if uid is None : out . status = falcon . HTTP_BAD_REQUEST out . message = \"None value not allowed\" try : feat = db [ \"features\" ][ uid ] feat . update ( properties ) feat [ \"geometry\" ] . update ( geometry ) db [ \"features\" ][ uid ] = None db [ \"features\" ][ uid ] = feat out . data = db [ \"features\" ][ uid ] except KeyError as e : out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature ' { uid } ' not foud.\" response = out . format ( response = response , request = request ) return @hug . delete ( \"/ {uid} \" ) def del_feature ( uid , request = None , response = None ): \"\"\" **Cancellazione delle Features** Se la feature viene cancellata correttamente ne restituisce la struttura Possibili risposte: - _404 Not Found_: Nel caso in cui il prototipo richiesto non esista. - _202 Accepted_: Nel caso in cui la feature venga eliminata correttamente. \"\"\" out = ResponseFormatter ( falcon . HTTP_ACCEPTED ) if uid is None : out . status = falcon . HTTP_BAD_REQUEST out . message = \"None value not allowed\" try : out . data = db [ \"features\" ][ uid ] for v in out . data [ 'parameters' ] . values (): if v is not None : try : db [ 'datacache' ][ v ] = None except KeyError as e : pass try : db [ 'series' ][ v ] = None except KeyError as e : pass sourceman . sourceFactory ( uid ) . deleteActionValues () db [ \"features\" ][ uid ] = None except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature ' { uid } ' not foud.\" response = out . format ( response = response , request = request ) return Variables db Functions create_feature def create_feature ( uid , prototype , properties : < hielen2 . utils . JsonValidable object at 0x7fb7d50088b0 > = {}, geometry : < hielen2 . utils . JsonValidable object at 0x7fb7d5008b80 > = {}, request = None , response = None ) Creazione delle Features. Ogni feature deve avere il suo codice univoco uid e il suo prototipo prototype . Questi due campi sono immutabli (vedi PUT /feature/{uid} ). Il prototipo della feature forisce informazioni per l'inizializazione della struttura. Il parametro geometry deve essere un GeoJson Se la feature viene creata correttamente ne restituisce la struttura Possibili risposte: 409 Conflict : Nel caso in cui il uid fornito esista gi\u00e0. 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 201 Created : Nel caso in cui la feature venga creata correttamente. View Source @hug . post ( \"/\" ) def create_feature ( uid , prototype , properties : JsonValidable ( FeaturePropertiesSchema ()) = {} , geometry : JsonValidable ( GeoJSONSchema ()) = {} , request = None , response = None , ) : \"\"\" **Creazione delle Features.** Ogni feature deve avere il suo codice univoco `uid` e il suo prototipo `prototype`. Questi due \\ campi sono immutabli (vedi PUT `/feature/{uid}`). Il prototipo della feature forisce informazioni per l'inizializazione della struttura. Il parametro `geometry` deve essere un GeoJson Se la feature viene creata correttamente ne restituisce la struttura Possibili risposte: - _409 Conflict_: Nel caso in cui il uid fornito esista gi\u00e0. - _404 Not Found_: Nel caso in cui il prototipo richiesto non esista. - _201 Created_: Nel caso in cui la feature venga creata correttamente. \"\"\" out = ResponseFormatter ( status = falcon . HTTP_CREATED ) try : # feature = db [ \"features_proto\" ][ prototype ][ 'struct' ] feature = db [ \"features_proto\" ][ prototype ] feature [ \"geometry\" ]= geometry feature [ \"uid\" ]= uid feature [ \"type\" ]= prototype try : assert properties [ \"context\" ] is not None except Exception : properties [ \"context\" ] = \"no-context\" feature . update ( properties ) try : db [ \"features\" ][ uid ] raise ValueError ( f \"feature {uid} exists\" ) except KeyError : pass params = {} for v in feature [ 'parameters' ] : param = v [ 'param' ] struct = v [ 'struct' ] suid = hasher ( uid , param ) try : db [ 'series' ][ suid ] raise ValueError ( f \"serires '{uid}_{param}' ({suid}) exists\" ) except KeyError : pass for k , w in struct . pop ( 'init_operands' ). items () : struct [ 'operands' ][ k ]= eval ( w . replace ( '{{new}}' , 'feature' )) db [ 'series' ][ suid ]= struct params [ param ]= suid feature [ 'parameters' ]= params db [ \"features\" ][ uid ]= feature out . data = db [ \"features\" ][ uid ] except KeyError as e : traceback . print_exc () out . message = f \"prototype '{prototype}' not found.\" out . status = falcon . HTTP_NOT_FOUND except ValueError as e : traceback . print_exc () out . message = str ( e ) out . status = falcon . HTTP_CONFLICT response = out . format ( response = response , request = request ) return del_feature def del_feature ( uid , request = None , response = None ) Cancellazione delle Features Se la feature viene cancellata correttamente ne restituisce la struttura Possibili risposte: 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 202 Accepted : Nel caso in cui la feature venga eliminata correttamente. View Source @hug . delete ( \"/{uid}\" ) def del_feature ( uid , request = None , response = None ) : \"\"\" **Cancellazione delle Features** Se la feature viene cancellata correttamente ne restituisce la struttura Possibili risposte: - _404 Not Found_: Nel caso in cui il prototipo richiesto non esista. - _202 Accepted_: Nel caso in cui la feature venga eliminata correttamente. \"\"\" out = ResponseFormatter ( falcon . HTTP_ACCEPTED ) if uid is None : out . status = falcon . HTTP_BAD_REQUEST out . message = \"None value not allowed\" try : out . data = db [ \"features\" ][ uid ] for v in out . data [ 'parameters' ] . values () : if v is not None : try : db [ 'datacache' ][ v ] = None except KeyError as e : pass try : db [ 'series' ][ v ] = None except KeyError as e : pass sourceman . sourceFactory ( uid ). deleteActionValues () db [ \"features\" ][ uid ] = None except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature '{uid}' not foud.\" response = out . format ( response = response , request = request ) return feature_info def feature_info ( uid , cntxt = None , request = None , response = None ) Alias di recupero informazioni della specifica feature View Source @hug . get ( \"/{uid}\" ) def feature_info ( uid , cntxt = None , request = None , response = None ) : \"\"\" **Alias di recupero informazioni della specifica feature**\"\"\" return features_info ( uid , cntxt , request , response ) features_info def features_info ( uids = None , cntxt = None , request = None , response = None ) Recupero delle informazioni delle features. nota : uids accetta valori multipli separati da virgola viene restituito una struttura di questo tipo: { \"features\": [ { \"type\": \"Feature\", \"properties\": { ... }, \"geometry\": <GeoJson Validable> }, ... ] } nota : Al contrario di quanto detto nel TODO non viene inserito il context a livello \"features\" perch\u00e8 in effetti \u00e8 una informazione sempre conosciuta a priori (se si lavora per commesse). Al contrario se si lavora per uids allora ha senso inserie questa info all' interno delle properties delle singole features. Possibili risposte: 404 Not Found : Nel caso in cui nessuna feature risponda ai criteri View Source @hug . get ( \"/\" ) def features_info ( uids = None , cntxt = None , request = None , response = None ) : \"\"\" **Recupero delle informazioni delle features.** __nota__: uids accetta valori multipli separati da virgola viene restituito una struttura di questo tipo: { \" features \": [ { \" type \": \" Feature \", \" properties \": { ... }, \" geometry \": <GeoJson Validable> }, ... ] } ___nota___: Al contrario di quanto detto nel TODO non viene inserito il context a livello \\ \" features \" perch\u00e8 in effetti \u00e8 una informazione sempre conosciuta a priori (se si lavora \\ per commesse). Al contrario se si lavora per uids allora ha senso inserie questa info all' \\ interno delle properties delle singole features. Possibili risposte: - _404 Not Found_: Nel caso in cui nessuna feature risponda ai criteri \"\"\" out = ResponseFormatter () try : out . data = { \"features\" :[]} extract = db [ \"features\" ][ uids ] if not isinstance ( extract , list ) : extract =[ extract ] for v in extract : if cntxt is None or v [ \"context\" ] == cntxt : out . data [ 'features' ] . append ( { \"type\" : \"Feature\" , \"geometry\" : v . pop ( \"geometry\" ), \"parameters\" : v . pop ( \"parameters\" ), \"properties\" : v } ) except KeyError as e : out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) response = out . format ( response = response , request = request ) return update_feature def update_feature ( uid , properties : < hielen2 . utils . JsonValidable object at 0x7fb7d5008cd0 > = {}, geometry : < hielen2 . utils . JsonValidable object at 0x7fb7d4f9af10 > = {}, request = None , response = None ) Modifica delle properties di una feature Possibili risposte: 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 202 Accepted : Nel caso in cui la feature venga modificata correttamente. View Source @hug . put ( \"/{uid}\" ) def update_feature ( uid , properties : JsonValidable ( FeaturePropertiesSchema ()) = {} , geometry : JsonValidable ( GeoJSONSchema ()) = {} , request = None , response = None , ) : \"\"\" **Modifica delle properties di una feature** Possibili risposte: - _404 Not Found_: Nel caso in cui il prototipo richiesto non esista. - _202 Accepted_: Nel caso in cui la feature venga modificata correttamente. \"\"\" out = ResponseFormatter ( status = falcon . HTTP_ACCEPTED ) if uid is None : out . status = falcon . HTTP_BAD_REQUEST out . message = \"None value not allowed\" try : feat = db [ \"features\" ][ uid ] feat . update ( properties ) feat [ \"geometry\" ] . update ( geometry ) db [ \"features\" ][ uid ] = None db [ \"features\" ][ uid ] = feat out . data = db [ \"features\" ][ uid ] except KeyError as e : out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature '{uid}' not foud.\" response = out . format ( response = response , request = request ) return Classes FeaturePropertiesSchema class FeaturePropertiesSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) Base schema class with which to define custom schemas. Example usage: .. code-block:: python import datetime as dt from dataclasses import dataclass from marshmallow import Schema , fields @dataclass class Album : title : str release_date : dt . date class AlbumSchema ( Schema ): title = fields . Str () release_date = fields . Date () album = Album ( \"Beggars Banquet\" , dt . date ( 1968 , 12 , 6 )) schema = AlbumSchema () data = schema . dump ( album ) data # {'release_date': '1968-12-06', 'title': 'Beggars Banquet'} :param only: Whitelist of the declared fields to select when instantiating the Schema. If None, all fields are used. Nested fields can be represented with dot delimiters. :param exclude: Blacklist of the declared fields to exclude when instantiating the Schema. If a field appears in both only and exclude , it is not used. Nested fields can be represented with dot delimiters. :param many: Should be set to True if obj is a collection so that the object will be serialized to a list. :param context: Optional context passed to :class: fields.Method and :class: fields.Function fields. :param load_only: Fields to skip during serialization (write-only fields) :param dump_only: Fields to skip during deserialization (read-only fields) :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . .. versionchanged:: 3.0.0 prefix parameter removed. .. versionchanged:: 2.0.0 __validators__ , __preprocessors__ , and __data_handlers__ are removed in favor of marshmallow.decorators.validates_schema , marshmallow.decorators.pre_load and marshmallow.decorators.post_dump . __accessor__ and __error_handler__ are deprecated. Implement the handle_error and get_attribute methods instead. View Source class FeaturePropertiesSchema ( Schema ): context = fields . Str ( default = \"no-context\" , allow_none = False ) label = fields . Str ( default = None ) description = fields . Str ( default = None ) location = fields . Str ( default = None ) style = fields . Str ( default = None ) status = fields . Str ( default = None ) timestamp = fields . Str ( default = None ) Ancestors (in MRO) marshmallow.schema.Schema marshmallow.base.SchemaABC Class variables Meta OPTIONS_CLASS TYPE_MAPPING context description error_messages label location opts status style timestamp Static methods from_dict def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls Instance variables dict_class set_class Methods dump def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result dumps def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs ) get_attribute def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default ) handle_error def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass load def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True ) loads def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown ) on_bind_field def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None validate def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"Features"},{"location":"reference/hielen2/api/features/#module-hielen2apifeatures","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 #!/usr/bin/env python # coding=utf-8 import hug import falcon from hielen2 import db from hielen2.utils import JsonValidable , hasher import hielen2.source as sourceman from marshmallow import Schema , fields from himada.api import ResponseFormatter from marshmallow_geojson import GeoJSONSchema import traceback class FeaturePropertiesSchema ( Schema ): context = fields . Str ( default = \"no-context\" , allow_none = False ) label = fields . Str ( default = None ) description = fields . Str ( default = None ) location = fields . Str ( default = None ) style = fields . Str ( default = None ) status = fields . Str ( default = None ) timestamp = fields . Str ( default = None ) @hug . post ( \"/\" ) def create_feature ( uid , prototype , properties : JsonValidable ( FeaturePropertiesSchema ()) = {}, geometry : JsonValidable ( GeoJSONSchema ()) = {}, request = None , response = None , ): \"\"\" **Creazione delle Features.** Ogni feature deve avere il suo codice univoco `uid` e il suo prototipo `prototype`. Questi due \\ campi sono immutabli (vedi PUT `/feature/{uid}`). Il prototipo della feature forisce informazioni per l'inizializazione della struttura. Il parametro `geometry` deve essere un GeoJson Se la feature viene creata correttamente ne restituisce la struttura Possibili risposte: - _409 Conflict_: Nel caso in cui il uid fornito esista gi\u00e0. - _404 Not Found_: Nel caso in cui il prototipo richiesto non esista. - _201 Created_: Nel caso in cui la feature venga creata correttamente. \"\"\" out = ResponseFormatter ( status = falcon . HTTP_CREATED ) try : # feature = db[\"features_proto\"][prototype]['struct'] feature = db [ \"features_proto\" ][ prototype ] feature [ \"geometry\" ] = geometry feature [ \"uid\" ] = uid feature [ \"type\" ] = prototype try : assert properties [ \"context\" ] is not None except Exception : properties [ \"context\" ] = \"no-context\" feature . update ( properties ) try : db [ \"features\" ][ uid ] raise ValueError ( f \"feature { uid } exists\" ) except KeyError : pass params = {} for v in feature [ 'parameters' ]: param = v [ 'param' ] struct = v [ 'struct' ] suid = hasher ( uid , param ) try : db [ 'series' ][ suid ] raise ValueError ( f \"serires ' { uid } _ { param } ' ( { suid } ) exists\" ) except KeyError : pass for k , w in struct . pop ( 'init_operands' ) . items (): struct [ 'operands' ][ k ] = eval ( w . replace ( '{{new}}' , 'feature' )) db [ 'series' ][ suid ] = struct params [ param ] = suid feature [ 'parameters' ] = params db [ \"features\" ][ uid ] = feature out . data = db [ \"features\" ][ uid ] except KeyError as e : traceback . print_exc () out . message = f \"prototype ' { prototype } ' not found.\" out . status = falcon . HTTP_NOT_FOUND except ValueError as e : traceback . print_exc () out . message = str ( e ) out . status = falcon . HTTP_CONFLICT response = out . format ( response = response , request = request ) return @hug . get ( \"/\" ) def features_info ( uids = None , cntxt = None , request = None , response = None ): \"\"\" **Recupero delle informazioni delle features.** __nota__: uids accetta valori multipli separati da virgola viene restituito una struttura di questo tipo: { \"features\": [ { \"type\": \"Feature\", \"properties\": { ... }, \"geometry\": <GeoJson Validable> }, ... ] } ___nota___: Al contrario di quanto detto nel TODO non viene inserito il context a livello \\ \"features\" perch\u00e8 in effetti \u00e8 una informazione sempre conosciuta a priori (se si lavora \\ per commesse). Al contrario se si lavora per uids allora ha senso inserie questa info all' \\ interno delle properties delle singole features. Possibili risposte: - _404 Not Found_: Nel caso in cui nessuna feature risponda ai criteri \"\"\" out = ResponseFormatter () try : out . data = { \"features\" :[]} extract = db [ \"features\" ][ uids ] if not isinstance ( extract , list ): extract = [ extract ] for v in extract : if cntxt is None or v [ \"context\" ] == cntxt : out . data [ 'features' ] . append ({ \"type\" : \"Feature\" , \"geometry\" : v . pop ( \"geometry\" ), \"parameters\" : v . pop ( \"parameters\" ), \"properties\" : v }) except KeyError as e : out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) response = out . format ( response = response , request = request ) return @hug . get ( \"/ {uid} \" ) def feature_info ( uid , cntxt = None , request = None , response = None ): \"\"\" **Alias di recupero informazioni della specifica feature**\"\"\" return features_info ( uid , cntxt , request , response ) @hug . put ( \"/ {uid} \" ) def update_feature ( uid , properties : JsonValidable ( FeaturePropertiesSchema ()) = {}, geometry : JsonValidable ( GeoJSONSchema ()) = {}, request = None , response = None , ): \"\"\" **Modifica delle properties di una feature** Possibili risposte: - _404 Not Found_: Nel caso in cui il prototipo richiesto non esista. - _202 Accepted_: Nel caso in cui la feature venga modificata correttamente. \"\"\" out = ResponseFormatter ( status = falcon . HTTP_ACCEPTED ) if uid is None : out . status = falcon . HTTP_BAD_REQUEST out . message = \"None value not allowed\" try : feat = db [ \"features\" ][ uid ] feat . update ( properties ) feat [ \"geometry\" ] . update ( geometry ) db [ \"features\" ][ uid ] = None db [ \"features\" ][ uid ] = feat out . data = db [ \"features\" ][ uid ] except KeyError as e : out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature ' { uid } ' not foud.\" response = out . format ( response = response , request = request ) return @hug . delete ( \"/ {uid} \" ) def del_feature ( uid , request = None , response = None ): \"\"\" **Cancellazione delle Features** Se la feature viene cancellata correttamente ne restituisce la struttura Possibili risposte: - _404 Not Found_: Nel caso in cui il prototipo richiesto non esista. - _202 Accepted_: Nel caso in cui la feature venga eliminata correttamente. \"\"\" out = ResponseFormatter ( falcon . HTTP_ACCEPTED ) if uid is None : out . status = falcon . HTTP_BAD_REQUEST out . message = \"None value not allowed\" try : out . data = db [ \"features\" ][ uid ] for v in out . data [ 'parameters' ] . values (): if v is not None : try : db [ 'datacache' ][ v ] = None except KeyError as e : pass try : db [ 'series' ][ v ] = None except KeyError as e : pass sourceman . sourceFactory ( uid ) . deleteActionValues () db [ \"features\" ][ uid ] = None except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature ' { uid } ' not foud.\" response = out . format ( response = response , request = request ) return","title":"Module hielen2.api.features"},{"location":"reference/hielen2/api/features/#variables","text":"db","title":"Variables"},{"location":"reference/hielen2/api/features/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/api/features/#create_feature","text":"def create_feature ( uid , prototype , properties : < hielen2 . utils . JsonValidable object at 0x7fb7d50088b0 > = {}, geometry : < hielen2 . utils . JsonValidable object at 0x7fb7d5008b80 > = {}, request = None , response = None ) Creazione delle Features. Ogni feature deve avere il suo codice univoco uid e il suo prototipo prototype . Questi due campi sono immutabli (vedi PUT /feature/{uid} ). Il prototipo della feature forisce informazioni per l'inizializazione della struttura. Il parametro geometry deve essere un GeoJson Se la feature viene creata correttamente ne restituisce la struttura Possibili risposte: 409 Conflict : Nel caso in cui il uid fornito esista gi\u00e0. 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 201 Created : Nel caso in cui la feature venga creata correttamente. View Source @hug . post ( \"/\" ) def create_feature ( uid , prototype , properties : JsonValidable ( FeaturePropertiesSchema ()) = {} , geometry : JsonValidable ( GeoJSONSchema ()) = {} , request = None , response = None , ) : \"\"\" **Creazione delle Features.** Ogni feature deve avere il suo codice univoco `uid` e il suo prototipo `prototype`. Questi due \\ campi sono immutabli (vedi PUT `/feature/{uid}`). Il prototipo della feature forisce informazioni per l'inizializazione della struttura. Il parametro `geometry` deve essere un GeoJson Se la feature viene creata correttamente ne restituisce la struttura Possibili risposte: - _409 Conflict_: Nel caso in cui il uid fornito esista gi\u00e0. - _404 Not Found_: Nel caso in cui il prototipo richiesto non esista. - _201 Created_: Nel caso in cui la feature venga creata correttamente. \"\"\" out = ResponseFormatter ( status = falcon . HTTP_CREATED ) try : # feature = db [ \"features_proto\" ][ prototype ][ 'struct' ] feature = db [ \"features_proto\" ][ prototype ] feature [ \"geometry\" ]= geometry feature [ \"uid\" ]= uid feature [ \"type\" ]= prototype try : assert properties [ \"context\" ] is not None except Exception : properties [ \"context\" ] = \"no-context\" feature . update ( properties ) try : db [ \"features\" ][ uid ] raise ValueError ( f \"feature {uid} exists\" ) except KeyError : pass params = {} for v in feature [ 'parameters' ] : param = v [ 'param' ] struct = v [ 'struct' ] suid = hasher ( uid , param ) try : db [ 'series' ][ suid ] raise ValueError ( f \"serires '{uid}_{param}' ({suid}) exists\" ) except KeyError : pass for k , w in struct . pop ( 'init_operands' ). items () : struct [ 'operands' ][ k ]= eval ( w . replace ( '{{new}}' , 'feature' )) db [ 'series' ][ suid ]= struct params [ param ]= suid feature [ 'parameters' ]= params db [ \"features\" ][ uid ]= feature out . data = db [ \"features\" ][ uid ] except KeyError as e : traceback . print_exc () out . message = f \"prototype '{prototype}' not found.\" out . status = falcon . HTTP_NOT_FOUND except ValueError as e : traceback . print_exc () out . message = str ( e ) out . status = falcon . HTTP_CONFLICT response = out . format ( response = response , request = request ) return","title":"create_feature"},{"location":"reference/hielen2/api/features/#del_feature","text":"def del_feature ( uid , request = None , response = None ) Cancellazione delle Features Se la feature viene cancellata correttamente ne restituisce la struttura Possibili risposte: 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 202 Accepted : Nel caso in cui la feature venga eliminata correttamente. View Source @hug . delete ( \"/{uid}\" ) def del_feature ( uid , request = None , response = None ) : \"\"\" **Cancellazione delle Features** Se la feature viene cancellata correttamente ne restituisce la struttura Possibili risposte: - _404 Not Found_: Nel caso in cui il prototipo richiesto non esista. - _202 Accepted_: Nel caso in cui la feature venga eliminata correttamente. \"\"\" out = ResponseFormatter ( falcon . HTTP_ACCEPTED ) if uid is None : out . status = falcon . HTTP_BAD_REQUEST out . message = \"None value not allowed\" try : out . data = db [ \"features\" ][ uid ] for v in out . data [ 'parameters' ] . values () : if v is not None : try : db [ 'datacache' ][ v ] = None except KeyError as e : pass try : db [ 'series' ][ v ] = None except KeyError as e : pass sourceman . sourceFactory ( uid ). deleteActionValues () db [ \"features\" ][ uid ] = None except KeyError as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature '{uid}' not foud.\" response = out . format ( response = response , request = request ) return","title":"del_feature"},{"location":"reference/hielen2/api/features/#feature_info","text":"def feature_info ( uid , cntxt = None , request = None , response = None ) Alias di recupero informazioni della specifica feature View Source @hug . get ( \"/{uid}\" ) def feature_info ( uid , cntxt = None , request = None , response = None ) : \"\"\" **Alias di recupero informazioni della specifica feature**\"\"\" return features_info ( uid , cntxt , request , response )","title":"feature_info"},{"location":"reference/hielen2/api/features/#features_info","text":"def features_info ( uids = None , cntxt = None , request = None , response = None ) Recupero delle informazioni delle features. nota : uids accetta valori multipli separati da virgola viene restituito una struttura di questo tipo: { \"features\": [ { \"type\": \"Feature\", \"properties\": { ... }, \"geometry\": <GeoJson Validable> }, ... ] } nota : Al contrario di quanto detto nel TODO non viene inserito il context a livello \"features\" perch\u00e8 in effetti \u00e8 una informazione sempre conosciuta a priori (se si lavora per commesse). Al contrario se si lavora per uids allora ha senso inserie questa info all' interno delle properties delle singole features. Possibili risposte: 404 Not Found : Nel caso in cui nessuna feature risponda ai criteri View Source @hug . get ( \"/\" ) def features_info ( uids = None , cntxt = None , request = None , response = None ) : \"\"\" **Recupero delle informazioni delle features.** __nota__: uids accetta valori multipli separati da virgola viene restituito una struttura di questo tipo: { \" features \": [ { \" type \": \" Feature \", \" properties \": { ... }, \" geometry \": <GeoJson Validable> }, ... ] } ___nota___: Al contrario di quanto detto nel TODO non viene inserito il context a livello \\ \" features \" perch\u00e8 in effetti \u00e8 una informazione sempre conosciuta a priori (se si lavora \\ per commesse). Al contrario se si lavora per uids allora ha senso inserie questa info all' \\ interno delle properties delle singole features. Possibili risposte: - _404 Not Found_: Nel caso in cui nessuna feature risponda ai criteri \"\"\" out = ResponseFormatter () try : out . data = { \"features\" :[]} extract = db [ \"features\" ][ uids ] if not isinstance ( extract , list ) : extract =[ extract ] for v in extract : if cntxt is None or v [ \"context\" ] == cntxt : out . data [ 'features' ] . append ( { \"type\" : \"Feature\" , \"geometry\" : v . pop ( \"geometry\" ), \"parameters\" : v . pop ( \"parameters\" ), \"properties\" : v } ) except KeyError as e : out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) response = out . format ( response = response , request = request ) return","title":"features_info"},{"location":"reference/hielen2/api/features/#update_feature","text":"def update_feature ( uid , properties : < hielen2 . utils . JsonValidable object at 0x7fb7d5008cd0 > = {}, geometry : < hielen2 . utils . JsonValidable object at 0x7fb7d4f9af10 > = {}, request = None , response = None ) Modifica delle properties di una feature Possibili risposte: 404 Not Found : Nel caso in cui il prototipo richiesto non esista. 202 Accepted : Nel caso in cui la feature venga modificata correttamente. View Source @hug . put ( \"/{uid}\" ) def update_feature ( uid , properties : JsonValidable ( FeaturePropertiesSchema ()) = {} , geometry : JsonValidable ( GeoJSONSchema ()) = {} , request = None , response = None , ) : \"\"\" **Modifica delle properties di una feature** Possibili risposte: - _404 Not Found_: Nel caso in cui il prototipo richiesto non esista. - _202 Accepted_: Nel caso in cui la feature venga modificata correttamente. \"\"\" out = ResponseFormatter ( status = falcon . HTTP_ACCEPTED ) if uid is None : out . status = falcon . HTTP_BAD_REQUEST out . message = \"None value not allowed\" try : feat = db [ \"features\" ][ uid ] feat . update ( properties ) feat [ \"geometry\" ] . update ( geometry ) db [ \"features\" ][ uid ] = None db [ \"features\" ][ uid ] = feat out . data = db [ \"features\" ][ uid ] except KeyError as e : out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature '{uid}' not foud.\" response = out . format ( response = response , request = request ) return","title":"update_feature"},{"location":"reference/hielen2/api/features/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/api/features/#featurepropertiesschema","text":"class FeaturePropertiesSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) Base schema class with which to define custom schemas. Example usage: .. code-block:: python import datetime as dt from dataclasses import dataclass from marshmallow import Schema , fields @dataclass class Album : title : str release_date : dt . date class AlbumSchema ( Schema ): title = fields . Str () release_date = fields . Date () album = Album ( \"Beggars Banquet\" , dt . date ( 1968 , 12 , 6 )) schema = AlbumSchema () data = schema . dump ( album ) data # {'release_date': '1968-12-06', 'title': 'Beggars Banquet'} :param only: Whitelist of the declared fields to select when instantiating the Schema. If None, all fields are used. Nested fields can be represented with dot delimiters. :param exclude: Blacklist of the declared fields to exclude when instantiating the Schema. If a field appears in both only and exclude , it is not used. Nested fields can be represented with dot delimiters. :param many: Should be set to True if obj is a collection so that the object will be serialized to a list. :param context: Optional context passed to :class: fields.Method and :class: fields.Function fields. :param load_only: Fields to skip during serialization (write-only fields) :param dump_only: Fields to skip during deserialization (read-only fields) :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . .. versionchanged:: 3.0.0 prefix parameter removed. .. versionchanged:: 2.0.0 __validators__ , __preprocessors__ , and __data_handlers__ are removed in favor of marshmallow.decorators.validates_schema , marshmallow.decorators.pre_load and marshmallow.decorators.post_dump . __accessor__ and __error_handler__ are deprecated. Implement the handle_error and get_attribute methods instead. View Source class FeaturePropertiesSchema ( Schema ): context = fields . Str ( default = \"no-context\" , allow_none = False ) label = fields . Str ( default = None ) description = fields . Str ( default = None ) location = fields . Str ( default = None ) style = fields . Str ( default = None ) status = fields . Str ( default = None ) timestamp = fields . Str ( default = None )","title":"FeaturePropertiesSchema"},{"location":"reference/hielen2/api/features/#ancestors-in-mro","text":"marshmallow.schema.Schema marshmallow.base.SchemaABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/api/features/#class-variables","text":"Meta OPTIONS_CLASS TYPE_MAPPING context description error_messages label location opts status style timestamp","title":"Class variables"},{"location":"reference/hielen2/api/features/#static-methods","text":"","title":"Static methods"},{"location":"reference/hielen2/api/features/#from_dict","text":"def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls","title":"from_dict"},{"location":"reference/hielen2/api/features/#instance-variables","text":"dict_class set_class","title":"Instance variables"},{"location":"reference/hielen2/api/features/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/api/features/#dump","text":"def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result","title":"dump"},{"location":"reference/hielen2/api/features/#dumps","text":"def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs )","title":"dumps"},{"location":"reference/hielen2/api/features/#get_attribute","text":"def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default )","title":"get_attribute"},{"location":"reference/hielen2/api/features/#handle_error","text":"def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass","title":"handle_error"},{"location":"reference/hielen2/api/features/#load","text":"def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True )","title":"load"},{"location":"reference/hielen2/api/features/#loads","text":"def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown )","title":"loads"},{"location":"reference/hielen2/api/features/#on_bind_field","text":"def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None","title":"on_bind_field"},{"location":"reference/hielen2/api/features/#validate","text":"def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"validate"},{"location":"reference/hielen2/api/glob/","text":"Module hielen2.api.glob View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 #!/usr/bin/env python # coding=utf-8 import hug from . import parameters , prototypes , data , features , actions , actionschemata , mapping import falcon \"\"\" @hug.not_found() def not_found(): return {'error': { 'status': falcon.status.HTTP_NOT_FOUND, 'description': 'URL is invalid.', }} api = hug.get(on_invalid=hug.redirect.not_found) \"\"\" @hug . extend_api ( \"/parameters\" ) def elemman (): \"\"\" parameters manager \"\"\" return [ parameters ] @hug . extend_api ( \"/prototypes\" ) def protoman (): \"\"\" Prototypes manager \"\"\" return [ prototypes ] @hug . extend_api ( \"/data\" ) def dataman (): \"\"\" Series manager \"\"\" return [ data ] @hug . extend_api ( \"/features\" ) def featman (): \"\"\" Series manager \"\"\" return [ features ] @hug . extend_api ( \"/actions\" ) def actiman (): \"\"\" Series manager \"\"\" return [ actions ] @hug . extend_api ( \"/actionschemata\" ) def scheman (): \"\"\" Series manager \"\"\" return [ actionschemata ] @hug . extend_api ( \"/mapping\" ) def mapsman (): \"\"\" Series manager \"\"\" return [ mapping ] Functions actiman def actiman ( ) Series manager View Source @hug . extend_api ( \"/actions\" ) def actiman () : \"\"\" Series manager \"\"\" return [ actions ] dataman def dataman ( ) Series manager View Source @hug . extend_api ( \"/data\" ) def dataman () : \"\"\" Series manager \"\"\" return [ data ] elemman def elemman ( ) parameters manager View Source @hug . extend_api ( \"/parameters\" ) def elemman () : \"\"\" parameters manager \"\"\" return [ parameters ] featman def featman ( ) Series manager View Source @hug . extend_api ( \"/features\" ) def featman () : \"\"\" Series manager \"\"\" return [ features ] mapsman def mapsman ( ) Series manager View Source @hug . extend_api ( \"/mapping\" ) def mapsman () : \"\"\" Series manager \"\"\" return [ mapping ] protoman def protoman ( ) Prototypes manager View Source @hug . extend_api ( \"/prototypes\" ) def protoman () : \"\"\" Prototypes manager \"\"\" return [ prototypes ] scheman def scheman ( ) Series manager View Source @hug . extend_api ( \"/actionschemata\" ) def scheman () : \"\"\" Series manager \"\"\" return [ actionschemata ]","title":"Glob"},{"location":"reference/hielen2/api/glob/#module-hielen2apiglob","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 #!/usr/bin/env python # coding=utf-8 import hug from . import parameters , prototypes , data , features , actions , actionschemata , mapping import falcon \"\"\" @hug.not_found() def not_found(): return {'error': { 'status': falcon.status.HTTP_NOT_FOUND, 'description': 'URL is invalid.', }} api = hug.get(on_invalid=hug.redirect.not_found) \"\"\" @hug . extend_api ( \"/parameters\" ) def elemman (): \"\"\" parameters manager \"\"\" return [ parameters ] @hug . extend_api ( \"/prototypes\" ) def protoman (): \"\"\" Prototypes manager \"\"\" return [ prototypes ] @hug . extend_api ( \"/data\" ) def dataman (): \"\"\" Series manager \"\"\" return [ data ] @hug . extend_api ( \"/features\" ) def featman (): \"\"\" Series manager \"\"\" return [ features ] @hug . extend_api ( \"/actions\" ) def actiman (): \"\"\" Series manager \"\"\" return [ actions ] @hug . extend_api ( \"/actionschemata\" ) def scheman (): \"\"\" Series manager \"\"\" return [ actionschemata ] @hug . extend_api ( \"/mapping\" ) def mapsman (): \"\"\" Series manager \"\"\" return [ mapping ]","title":"Module hielen2.api.glob"},{"location":"reference/hielen2/api/glob/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/api/glob/#actiman","text":"def actiman ( ) Series manager View Source @hug . extend_api ( \"/actions\" ) def actiman () : \"\"\" Series manager \"\"\" return [ actions ]","title":"actiman"},{"location":"reference/hielen2/api/glob/#dataman","text":"def dataman ( ) Series manager View Source @hug . extend_api ( \"/data\" ) def dataman () : \"\"\" Series manager \"\"\" return [ data ]","title":"dataman"},{"location":"reference/hielen2/api/glob/#elemman","text":"def elemman ( ) parameters manager View Source @hug . extend_api ( \"/parameters\" ) def elemman () : \"\"\" parameters manager \"\"\" return [ parameters ]","title":"elemman"},{"location":"reference/hielen2/api/glob/#featman","text":"def featman ( ) Series manager View Source @hug . extend_api ( \"/features\" ) def featman () : \"\"\" Series manager \"\"\" return [ features ]","title":"featman"},{"location":"reference/hielen2/api/glob/#mapsman","text":"def mapsman ( ) Series manager View Source @hug . extend_api ( \"/mapping\" ) def mapsman () : \"\"\" Series manager \"\"\" return [ mapping ]","title":"mapsman"},{"location":"reference/hielen2/api/glob/#protoman","text":"def protoman ( ) Prototypes manager View Source @hug . extend_api ( \"/prototypes\" ) def protoman () : \"\"\" Prototypes manager \"\"\" return [ prototypes ]","title":"protoman"},{"location":"reference/hielen2/api/glob/#scheman","text":"def scheman ( ) Series manager View Source @hug . extend_api ( \"/actionschemata\" ) def scheman () : \"\"\" Series manager \"\"\" return [ actionschemata ]","title":"scheman"},{"location":"reference/hielen2/api/mapping/","text":"Module hielen2.api.mapping View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 #!/usr/bin/env python # coding=utf-8 import hug import falcon import json from marshmallow import Schema , fields from numpy import nan , unique from pandas import DataFrame , to_datetime from hielen2 import db from hielen2.maps.data_access_layer import Series from hielen2.utils import hug_output_format_conten_type , JsonValidable , Selection from himada.api import ResponseFormatter import asyncio data_out_handler = hug_output_format_conten_type ( [ hug . output_format . text , hug . output_format . json ] ) CSV = \"text/plain; charset=utf-8\" JSON = \"application/json; charset=utf-8\" class DataMapSchema ( Schema ): \"\"\"\"\"\" times = Selection ( missing = slice ( None ), default = slice ( None ), required = False , allow_none = True ) timeref = fields . Str ( default = None , reuired = False , allow_none = True ) series = fields . List ( fields . Str , default = []) refresh = fields . Bool ( default = False , required = False , allow_none = True ) ####### API DATATABLE ####### @hug . get ( \"/\" , examples = \"\" , output = data_out_handler ) def tabular_data ( datamap : JsonValidable ( DataMapSchema ( many = True )), content_type = None , request = None , response = None , ): series = {} for s in datamap : for p in s [ \"series\" ]: if p not in series . keys (): series [ p ] = [] try : series [ p ] . append ( Series ( p ) . thdata ( s [ 'times' ], s [ 'timeref' ], s [ 'refresh' ])) except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return out = DataFrame () for param , sers in series . items (): ser = None for r in sers : s = r . result () if ser is None : ser = s else : ser = ser . append ( s ) . sort_index () idx = unique ( ser . index . values , return_index = True )[ 1 ] ser = ser . iloc [ idx ] #ser.columns = [param] ser . name = param out = out . join ( ser , how = \"outer\" ) out . index . name = \"timestamp\" requested = data_out_handler . requested ( request ) . content_type if requested == CSV : return hug . types . text ( out . to_csv ( sep = ';' )) if requested == JSON : return hug . types . json ( out . to_json ( orient = \"table\" )) @hug . get ( \"/ {feature} /\" , output = data_out_handler ) def tabular_data_el ( feature , par = None , times = None , timeref = None , refresh = None , content_type = None , request = None , response = None , ): try : ft = db [ \"features\" ][ feature ] except KeyError : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( feature ) + \" not found\" response = out . format ( response = response , request = request ) return try : if par is None : series = list ( ft [ \"parameters\" ] . values ()) else : series = [ ft [ \"parameters\" ][ par ]] except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return datamap = dict ( series = series , times = times , timeref = timeref , refresh = refresh ) datamap = DataMapSchema () . loads ( json . dumps ( datamap )) return tabular_data ( datamap = [ datamap ], request = request , response = response ) @hug . get ( \"/ {feature} / {par} \" , output = data_out_handler ) def tabular_data_par ( feature = None , par = None , times = None , timeref = None , refresh = None , content_type = None , request = None , response = None , ): return tabular_data_el ( feature = feature , par = par , times = times , timeref = timeref , refresh = refresh , request = request , response = response , ) Variables CSV JSON db nan Functions data_out_handler def data_out_handler ( data , request , response ) Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) View Source def output_type ( data , request , response ): handler = requested_output_type ( request ) response . content_type = handler . content_type return handler ( data , request = request , response = response ) tabular_data def tabular_data ( datamap : < hielen2 . utils . JsonValidable object at 0x7fb7d4f59550 > , content_type = None , request = None , response = None ) View Source @hug . get ( \"/\" , examples = \"\" , output = data_out_handler ) def tabular_data ( datamap : JsonValidable ( DataMapSchema ( many = True )), content_type = None , request = None , response = None , ) : series = {} for s in datamap : for p in s [ \"series\" ] : if p not in series . keys () : series [ p ] = [] try : series [ p ] . append ( Series ( p ). thdata ( s [ 'times' ] , s [ 'timeref' ] , s [ 'refresh' ] )) except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return out = DataFrame () for param , sers in series . items () : ser = None for r in sers : s = r . result () if ser is None : ser = s else : ser = ser . append ( s ). sort_index () idx = unique ( ser . index . values , return_index = True ) [ 1 ] ser = ser . iloc [ idx ] #ser . columns = [ param ] ser . name = param out = out . join ( ser , how = \"outer\" ) out . index . name = \"timestamp\" requested = data_out_handler . requested ( request ). content_type if requested == CSV : return hug . types . text ( out . to_csv ( sep = ';' )) if requested == JSON : return hug . types . json ( out . to_json ( orient = \"table\" )) tabular_data_el def tabular_data_el ( feature , par = None , times = None , timeref = None , refresh = None , content_type = None , request = None , response = None ) View Source @hug . get ( \"/{feature}/\" , output = data_out_handler ) def tabular_data_el ( feature , par = None , times = None , timeref = None , refresh = None , content_type = None , request = None , response = None , ) : try : ft = db [ \"features\" ][ feature ] except KeyError : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( feature ) + \" not found\" response = out . format ( response = response , request = request ) return try : if par is None : series = list ( ft [ \"parameters\" ] . values ()) else : series = [ ft[\"parameters\" ][ par ] ] except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return datamap = dict ( series = series , times = times , timeref = timeref , refresh = refresh ) datamap = DataMapSchema (). loads ( json . dumps ( datamap )) return tabular_data ( datamap =[ datamap ] , request = request , response = response ) tabular_data_par def tabular_data_par ( feature = None , par = None , times = None , timeref = None , refresh = None , content_type = None , request = None , response = None ) View Source @hug . get ( \"/{feature}/{par}\" , output = data_out_handler ) def tabular_data_par ( feature = None , par = None , times = None , timeref = None , refresh = None , content_type = None , request = None , response = None , ) : return tabular_data_el ( feature = feature , par = par , times = times , timeref = timeref , refresh = refresh , request = request , response = response , ) Classes DataMapSchema class DataMapSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) View Source class DataMapSchema ( Schema ): \"\"\"\"\"\" times = Selection ( missing = slice ( None ), default = slice ( None ), required = False , allow_none = True ) timeref = fields . Str ( default = None , reuired = False , allow_none = True ) series = fields . List ( fields . Str , default =[]) refresh = fields . Bool ( default = False , required = False , allow_none = True ) Ancestors (in MRO) marshmallow.schema.Schema marshmallow.base.SchemaABC Class variables Meta OPTIONS_CLASS TYPE_MAPPING error_messages opts refresh series timeref times Static methods from_dict def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls Instance variables dict_class set_class Methods dump def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result dumps def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs ) get_attribute def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default ) handle_error def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass load def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True ) loads def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown ) on_bind_field def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None validate def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"Mapping"},{"location":"reference/hielen2/api/mapping/#module-hielen2apimapping","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 #!/usr/bin/env python # coding=utf-8 import hug import falcon import json from marshmallow import Schema , fields from numpy import nan , unique from pandas import DataFrame , to_datetime from hielen2 import db from hielen2.maps.data_access_layer import Series from hielen2.utils import hug_output_format_conten_type , JsonValidable , Selection from himada.api import ResponseFormatter import asyncio data_out_handler = hug_output_format_conten_type ( [ hug . output_format . text , hug . output_format . json ] ) CSV = \"text/plain; charset=utf-8\" JSON = \"application/json; charset=utf-8\" class DataMapSchema ( Schema ): \"\"\"\"\"\" times = Selection ( missing = slice ( None ), default = slice ( None ), required = False , allow_none = True ) timeref = fields . Str ( default = None , reuired = False , allow_none = True ) series = fields . List ( fields . Str , default = []) refresh = fields . Bool ( default = False , required = False , allow_none = True ) ####### API DATATABLE ####### @hug . get ( \"/\" , examples = \"\" , output = data_out_handler ) def tabular_data ( datamap : JsonValidable ( DataMapSchema ( many = True )), content_type = None , request = None , response = None , ): series = {} for s in datamap : for p in s [ \"series\" ]: if p not in series . keys (): series [ p ] = [] try : series [ p ] . append ( Series ( p ) . thdata ( s [ 'times' ], s [ 'timeref' ], s [ 'refresh' ])) except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return out = DataFrame () for param , sers in series . items (): ser = None for r in sers : s = r . result () if ser is None : ser = s else : ser = ser . append ( s ) . sort_index () idx = unique ( ser . index . values , return_index = True )[ 1 ] ser = ser . iloc [ idx ] #ser.columns = [param] ser . name = param out = out . join ( ser , how = \"outer\" ) out . index . name = \"timestamp\" requested = data_out_handler . requested ( request ) . content_type if requested == CSV : return hug . types . text ( out . to_csv ( sep = ';' )) if requested == JSON : return hug . types . json ( out . to_json ( orient = \"table\" )) @hug . get ( \"/ {feature} /\" , output = data_out_handler ) def tabular_data_el ( feature , par = None , times = None , timeref = None , refresh = None , content_type = None , request = None , response = None , ): try : ft = db [ \"features\" ][ feature ] except KeyError : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( feature ) + \" not found\" response = out . format ( response = response , request = request ) return try : if par is None : series = list ( ft [ \"parameters\" ] . values ()) else : series = [ ft [ \"parameters\" ][ par ]] except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return datamap = dict ( series = series , times = times , timeref = timeref , refresh = refresh ) datamap = DataMapSchema () . loads ( json . dumps ( datamap )) return tabular_data ( datamap = [ datamap ], request = request , response = response ) @hug . get ( \"/ {feature} / {par} \" , output = data_out_handler ) def tabular_data_par ( feature = None , par = None , times = None , timeref = None , refresh = None , content_type = None , request = None , response = None , ): return tabular_data_el ( feature = feature , par = par , times = times , timeref = timeref , refresh = refresh , request = request , response = response , )","title":"Module hielen2.api.mapping"},{"location":"reference/hielen2/api/mapping/#variables","text":"CSV JSON db nan","title":"Variables"},{"location":"reference/hielen2/api/mapping/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/api/mapping/#data_out_handler","text":"def data_out_handler ( data , request , response ) Supports any of the following formats: Free form UTF-8 text, JSON (Javascript Serialized Object Notation) View Source def output_type ( data , request , response ): handler = requested_output_type ( request ) response . content_type = handler . content_type return handler ( data , request = request , response = response )","title":"data_out_handler"},{"location":"reference/hielen2/api/mapping/#tabular_data","text":"def tabular_data ( datamap : < hielen2 . utils . JsonValidable object at 0x7fb7d4f59550 > , content_type = None , request = None , response = None ) View Source @hug . get ( \"/\" , examples = \"\" , output = data_out_handler ) def tabular_data ( datamap : JsonValidable ( DataMapSchema ( many = True )), content_type = None , request = None , response = None , ) : series = {} for s in datamap : for p in s [ \"series\" ] : if p not in series . keys () : series [ p ] = [] try : series [ p ] . append ( Series ( p ). thdata ( s [ 'times' ] , s [ 'timeref' ] , s [ 'refresh' ] )) except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return out = DataFrame () for param , sers in series . items () : ser = None for r in sers : s = r . result () if ser is None : ser = s else : ser = ser . append ( s ). sort_index () idx = unique ( ser . index . values , return_index = True ) [ 1 ] ser = ser . iloc [ idx ] #ser . columns = [ param ] ser . name = param out = out . join ( ser , how = \"outer\" ) out . index . name = \"timestamp\" requested = data_out_handler . requested ( request ). content_type if requested == CSV : return hug . types . text ( out . to_csv ( sep = ';' )) if requested == JSON : return hug . types . json ( out . to_json ( orient = \"table\" ))","title":"tabular_data"},{"location":"reference/hielen2/api/mapping/#tabular_data_el","text":"def tabular_data_el ( feature , par = None , times = None , timeref = None , refresh = None , content_type = None , request = None , response = None ) View Source @hug . get ( \"/{feature}/\" , output = data_out_handler ) def tabular_data_el ( feature , par = None , times = None , timeref = None , refresh = None , content_type = None , request = None , response = None , ) : try : ft = db [ \"features\" ][ feature ] except KeyError : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( feature ) + \" not found\" response = out . format ( response = response , request = request ) return try : if par is None : series = list ( ft [ \"parameters\" ] . values ()) else : series = [ ft[\"parameters\" ][ par ] ] except KeyError as e : out = ResponseFormatter ( status = falcon . HTTP_NOT_FOUND ) out . message = str ( e ) + \" not found\" response = out . format ( response = response , request = request ) return datamap = dict ( series = series , times = times , timeref = timeref , refresh = refresh ) datamap = DataMapSchema (). loads ( json . dumps ( datamap )) return tabular_data ( datamap =[ datamap ] , request = request , response = response )","title":"tabular_data_el"},{"location":"reference/hielen2/api/mapping/#tabular_data_par","text":"def tabular_data_par ( feature = None , par = None , times = None , timeref = None , refresh = None , content_type = None , request = None , response = None ) View Source @hug . get ( \"/{feature}/{par}\" , output = data_out_handler ) def tabular_data_par ( feature = None , par = None , times = None , timeref = None , refresh = None , content_type = None , request = None , response = None , ) : return tabular_data_el ( feature = feature , par = par , times = times , timeref = timeref , refresh = refresh , request = request , response = response , )","title":"tabular_data_par"},{"location":"reference/hielen2/api/mapping/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/api/mapping/#datamapschema","text":"class DataMapSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) View Source class DataMapSchema ( Schema ): \"\"\"\"\"\" times = Selection ( missing = slice ( None ), default = slice ( None ), required = False , allow_none = True ) timeref = fields . Str ( default = None , reuired = False , allow_none = True ) series = fields . List ( fields . Str , default =[]) refresh = fields . Bool ( default = False , required = False , allow_none = True )","title":"DataMapSchema"},{"location":"reference/hielen2/api/mapping/#ancestors-in-mro","text":"marshmallow.schema.Schema marshmallow.base.SchemaABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/api/mapping/#class-variables","text":"Meta OPTIONS_CLASS TYPE_MAPPING error_messages opts refresh series timeref times","title":"Class variables"},{"location":"reference/hielen2/api/mapping/#static-methods","text":"","title":"Static methods"},{"location":"reference/hielen2/api/mapping/#from_dict","text":"def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls","title":"from_dict"},{"location":"reference/hielen2/api/mapping/#instance-variables","text":"dict_class set_class","title":"Instance variables"},{"location":"reference/hielen2/api/mapping/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/api/mapping/#dump","text":"def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result","title":"dump"},{"location":"reference/hielen2/api/mapping/#dumps","text":"def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs )","title":"dumps"},{"location":"reference/hielen2/api/mapping/#get_attribute","text":"def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default )","title":"get_attribute"},{"location":"reference/hielen2/api/mapping/#handle_error","text":"def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass","title":"handle_error"},{"location":"reference/hielen2/api/mapping/#load","text":"def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True )","title":"load"},{"location":"reference/hielen2/api/mapping/#loads","text":"def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown )","title":"loads"},{"location":"reference/hielen2/api/mapping/#on_bind_field","text":"def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None","title":"on_bind_field"},{"location":"reference/hielen2/api/mapping/#validate","text":"def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"validate"},{"location":"reference/hielen2/api/parameters/","text":"Module hielen2.api.parameters View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 #!/usr/bin/env python # coding=utf-8 import hug import falcon from hielen2 import db from hielen2.utils import JsonValidable from marshmallow import Schema , fields from himada.api import ResponseFormatter @hug . get ( \"/\" , examples = \"\" ) def features_params ( cntxt = None , uids = None , params = None , request = None , response = None ): \"\"\" **Ricerca dei parametri associati alle features**. __nota__: uid accetta valori multipli separati da virgola viene restituita una struttura di questo tipo: { \"<fetUID>\":[ { \"series\":\"<series_UID>\", \"param\":\"<param_name>\", \"um\":\"<mearurement_unit>\" } ... ] ... } Possibili risposte: - _404 Not Found_: Nel caso in cui nessun parametro risponda ai criteri \"\"\" def _format ( param , series ): if series is None : return None parameters = [] try : parameters . append ({ \"series\" : series , \"name\" : param , \"unit\" : db [ \"series\" ][ series ][ \"mu\" ], }) except AttributeError as e : pass except TypeError as e : pass return parameters out = ResponseFormatter () try : if not isinstance ( params , ( list , set )) and params is not None : params = [ params ] feats = db [ \"features\" ][ uids ] if not isinstance ( feats , list ): feats = [ feats ] out . data = {} for f in feats : if cntxt is None or f [ \"context\" ] == cntxt : parameters = [] try : for p , s in f [ 'parameters' ] . items (): if s is not None and ( params is None or p in params ): parameters . append ({ \"series\" : s , \"param\" : p , \"unit\" : db [ \"series\" ][ s ][ \"mu\" ]}) except AttributeError : pass out . data [ f [ 'uid' ]] = parameters except KeyError as e : out . status = falcon . HTTP_OK out . message = str ( e ) response = out . format ( response = response , request = request ) return @hug . get ( \"/ {cntxt} \" ) def context_features_params ( cntxt = None , uids = None , params = None , request = None , response = None ): \"\"\" **Alias di ricerca dei Parametri nello lo specifico contesto**\"\"\" return features_params ( cntxt , uids , params , request , response ) @hug . get ( \"/ {cntxt} / {uid} \" ) def context_feature_params ( cntxt = None , uid = None , params = None , request = None , response = None ): \"\"\" **Alias di ricerca dei Parametri della specifica Feature lo specifico contesto**\"\"\" return features_params ( cntxt , uid , params , request , response ) @hug . get ( \"/ {cntxt} / {uid} / {param} \" ) def context_feature_params ( cntxt = None , uid = None , param = None , request = None , response = None ): \"\"\" **Alias di ricerca dello specifico Parametro della specifica Feature lo specifico contesto**\"\"\" return features_params ( cntxt , uid , param , request , response ) Variables db Functions context_feature_params def context_feature_params ( cntxt = None , uid = None , param = None , request = None , response = None ) Alias di ricerca dello specifico Parametro della specifica Feature lo specifico contesto View Source @hug . get ( \"/{cntxt}/{uid}/{param}\" ) def context_feature_params ( cntxt = None , uid = None , param = None , request = None , response = None ) : \"\"\" **Alias di ricerca dello specifico Parametro della specifica Feature lo specifico contesto**\"\"\" return features_params ( cntxt , uid , param , request , response ) context_features_params def context_features_params ( cntxt = None , uids = None , params = None , request = None , response = None ) Alias di ricerca dei Parametri nello lo specifico contesto View Source @hug . get ( \"/{cntxt}\" ) def context_features_params ( cntxt = None , uids = None , params = None , request = None , response = None ) : \"\"\" **Alias di ricerca dei Parametri nello lo specifico contesto**\"\"\" return features_params ( cntxt , uids , params , request , response ) features_params def features_params ( cntxt = None , uids = None , params = None , request = None , response = None ) Ricerca dei parametri associati alle features . nota : uid accetta valori multipli separati da virgola viene restituita una struttura di questo tipo: { \"<fetUID>\":[ { \"series\":\"<series_UID>\", \"param\":\"<param_name>\", \"um\":\"<mearurement_unit>\" } ... ] ... } Possibili risposte: 404 Not Found : Nel caso in cui nessun parametro risponda ai criteri View Source @hug . get ( \"/\" , examples = \"\" ) def features_params ( cntxt = None , uids = None , params = None , request = None , response = None ) : \"\"\" **Ricerca dei parametri associati alle features**. __nota__: uid accetta valori multipli separati da virgola viene restituita una struttura di questo tipo: { \" < fetUID > \":[ { \" series \":\" < series_UID > \", \" param \":\" < param_name > \", \" um \":\" < mearurement_unit > \" } ... ] ... } Possibili risposte: - _404 Not Found_: Nel caso in cui nessun parametro risponda ai criteri \"\"\" def _format ( param , series ) : if series is None : return None parameters = [] try : parameters . append ( { \"series\" : series , \"name\" : param , \"unit\" : db [ \"series\" ][ series ][ \"mu\" ] , } ) except AttributeError as e : pass except TypeError as e : pass return parameters out = ResponseFormatter () try : if not isinstance ( params , ( list , set )) and params is not None : params = [ params ] feats = db [ \"features\" ][ uids ] if not isinstance ( feats , list ) : feats =[ feats ] out . data = {} for f in feats : if cntxt is None or f [ \"context\" ] == cntxt : parameters = [] try : for p , s in f [ 'parameters' ] . items () : if s is not None and ( params is None or p in params ) : parameters . append ( { \"series\" : s , \"param\" : p , \"unit\" : db [ \"series\" ][ s ][ \"mu\" ] } ) except AttributeError : pass out . data [ f['uid' ] ] = parameters except KeyError as e : out . status = falcon . HTTP_OK out . message = str ( e ) response = out . format ( response = response , request = request ) return","title":"Parameters"},{"location":"reference/hielen2/api/parameters/#module-hielen2apiparameters","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 #!/usr/bin/env python # coding=utf-8 import hug import falcon from hielen2 import db from hielen2.utils import JsonValidable from marshmallow import Schema , fields from himada.api import ResponseFormatter @hug . get ( \"/\" , examples = \"\" ) def features_params ( cntxt = None , uids = None , params = None , request = None , response = None ): \"\"\" **Ricerca dei parametri associati alle features**. __nota__: uid accetta valori multipli separati da virgola viene restituita una struttura di questo tipo: { \"<fetUID>\":[ { \"series\":\"<series_UID>\", \"param\":\"<param_name>\", \"um\":\"<mearurement_unit>\" } ... ] ... } Possibili risposte: - _404 Not Found_: Nel caso in cui nessun parametro risponda ai criteri \"\"\" def _format ( param , series ): if series is None : return None parameters = [] try : parameters . append ({ \"series\" : series , \"name\" : param , \"unit\" : db [ \"series\" ][ series ][ \"mu\" ], }) except AttributeError as e : pass except TypeError as e : pass return parameters out = ResponseFormatter () try : if not isinstance ( params , ( list , set )) and params is not None : params = [ params ] feats = db [ \"features\" ][ uids ] if not isinstance ( feats , list ): feats = [ feats ] out . data = {} for f in feats : if cntxt is None or f [ \"context\" ] == cntxt : parameters = [] try : for p , s in f [ 'parameters' ] . items (): if s is not None and ( params is None or p in params ): parameters . append ({ \"series\" : s , \"param\" : p , \"unit\" : db [ \"series\" ][ s ][ \"mu\" ]}) except AttributeError : pass out . data [ f [ 'uid' ]] = parameters except KeyError as e : out . status = falcon . HTTP_OK out . message = str ( e ) response = out . format ( response = response , request = request ) return @hug . get ( \"/ {cntxt} \" ) def context_features_params ( cntxt = None , uids = None , params = None , request = None , response = None ): \"\"\" **Alias di ricerca dei Parametri nello lo specifico contesto**\"\"\" return features_params ( cntxt , uids , params , request , response ) @hug . get ( \"/ {cntxt} / {uid} \" ) def context_feature_params ( cntxt = None , uid = None , params = None , request = None , response = None ): \"\"\" **Alias di ricerca dei Parametri della specifica Feature lo specifico contesto**\"\"\" return features_params ( cntxt , uid , params , request , response ) @hug . get ( \"/ {cntxt} / {uid} / {param} \" ) def context_feature_params ( cntxt = None , uid = None , param = None , request = None , response = None ): \"\"\" **Alias di ricerca dello specifico Parametro della specifica Feature lo specifico contesto**\"\"\" return features_params ( cntxt , uid , param , request , response )","title":"Module hielen2.api.parameters"},{"location":"reference/hielen2/api/parameters/#variables","text":"db","title":"Variables"},{"location":"reference/hielen2/api/parameters/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/api/parameters/#context_feature_params","text":"def context_feature_params ( cntxt = None , uid = None , param = None , request = None , response = None ) Alias di ricerca dello specifico Parametro della specifica Feature lo specifico contesto View Source @hug . get ( \"/{cntxt}/{uid}/{param}\" ) def context_feature_params ( cntxt = None , uid = None , param = None , request = None , response = None ) : \"\"\" **Alias di ricerca dello specifico Parametro della specifica Feature lo specifico contesto**\"\"\" return features_params ( cntxt , uid , param , request , response )","title":"context_feature_params"},{"location":"reference/hielen2/api/parameters/#context_features_params","text":"def context_features_params ( cntxt = None , uids = None , params = None , request = None , response = None ) Alias di ricerca dei Parametri nello lo specifico contesto View Source @hug . get ( \"/{cntxt}\" ) def context_features_params ( cntxt = None , uids = None , params = None , request = None , response = None ) : \"\"\" **Alias di ricerca dei Parametri nello lo specifico contesto**\"\"\" return features_params ( cntxt , uids , params , request , response )","title":"context_features_params"},{"location":"reference/hielen2/api/parameters/#features_params","text":"def features_params ( cntxt = None , uids = None , params = None , request = None , response = None ) Ricerca dei parametri associati alle features . nota : uid accetta valori multipli separati da virgola viene restituita una struttura di questo tipo: { \"<fetUID>\":[ { \"series\":\"<series_UID>\", \"param\":\"<param_name>\", \"um\":\"<mearurement_unit>\" } ... ] ... } Possibili risposte: 404 Not Found : Nel caso in cui nessun parametro risponda ai criteri View Source @hug . get ( \"/\" , examples = \"\" ) def features_params ( cntxt = None , uids = None , params = None , request = None , response = None ) : \"\"\" **Ricerca dei parametri associati alle features**. __nota__: uid accetta valori multipli separati da virgola viene restituita una struttura di questo tipo: { \" < fetUID > \":[ { \" series \":\" < series_UID > \", \" param \":\" < param_name > \", \" um \":\" < mearurement_unit > \" } ... ] ... } Possibili risposte: - _404 Not Found_: Nel caso in cui nessun parametro risponda ai criteri \"\"\" def _format ( param , series ) : if series is None : return None parameters = [] try : parameters . append ( { \"series\" : series , \"name\" : param , \"unit\" : db [ \"series\" ][ series ][ \"mu\" ] , } ) except AttributeError as e : pass except TypeError as e : pass return parameters out = ResponseFormatter () try : if not isinstance ( params , ( list , set )) and params is not None : params = [ params ] feats = db [ \"features\" ][ uids ] if not isinstance ( feats , list ) : feats =[ feats ] out . data = {} for f in feats : if cntxt is None or f [ \"context\" ] == cntxt : parameters = [] try : for p , s in f [ 'parameters' ] . items () : if s is not None and ( params is None or p in params ) : parameters . append ( { \"series\" : s , \"param\" : p , \"unit\" : db [ \"series\" ][ s ][ \"mu\" ] } ) except AttributeError : pass out . data [ f['uid' ] ] = parameters except KeyError as e : out . status = falcon . HTTP_OK out . message = str ( e ) response = out . format ( response = response , request = request ) return","title":"features_params"},{"location":"reference/hielen2/api/prepare/","text":"Module hielen2.api.prepare View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 #!/usr/bin/env python # coding=utf-8 import hug import tempfile import falcon import os import time import json from hielen2 import db , conf import hielen2.source as sourceman from himada.api import ResponseFormatter import traceback @hug . get ( '/map/' ) def map ( features , timestamp = None , paramser = None , timeref = None , request = None , response = None ): \"\"\" \"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db[\"features\"][features] featobj = sourceman . sourceFactory ( feature ) print ( \" \\n\\n\\n \" ) print ( \"FEATS\" , features ) print ( \"TIMESTAMP\" , timestamp ) print ( \"TIMEREF\" , timeref ) print ( \"PARAMSER\" , paramser ) print ( \" \\n\\n\\n \" ) out . data = featobj . map ( timestamp , timeref , paramser ) except Exception as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature ' { features } ' does not exists or it is misconfigured: { e } \" out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return @hug . get ( '/map/ {feature} ' ) def map_feat ( feature , timestamp = None , paramser = None , timeref = None , request = None , response = None ): return map ( features = feature , timestamp = timestamp , paramser = paramser , timeref = timeref , request = request , response = response ) \"\"\" @hug.get('/map/{feature}/{param}') def map_feat_param(): pass @hug.get('/timeline') def timeline(): pass @hug.get('/timeline/{feature}') def timeline_feat(): pass @hug.get('/timeline/{feature}/{param}') def timeline_feat(): pass \"\"\" Variables conf db Functions map def map ( features , timestamp = None , paramser = None , timeref = None , request = None , response = None ) View Source @hug . get ( '/map/' ) def map ( features , timestamp = None , paramser = None , timeref = None , request = None , response = None ) : \"\"\" \"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db [ \"features\" ][ features ] featobj = sourceman . sourceFactory ( feature ) print ( \"\\n\\n\\n\" ) print ( \"FEATS\" , features ) print ( \"TIMESTAMP\" , timestamp ) print ( \"TIMEREF\" , timeref ) print ( \"PARAMSER\" , paramser ) print ( \"\\n\\n\\n\" ) out . data = featobj . map ( timestamp , timeref , paramser ) except Exception as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature '{features}' does not exists or it is misconfigured: {e}\" out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return map_feat def map_feat ( feature , timestamp = None , paramser = None , timeref = None , request = None , response = None ) View Source @hug . get ( '/map/{feature}' ) def map_feat ( feature , timestamp = None , paramser = None , timeref = None , request = None , response = None ) : return map ( features = feature , timestamp = timestamp , paramser = paramser , timeref = timeref , request = request , response = response )","title":"Prepare"},{"location":"reference/hielen2/api/prepare/#module-hielen2apiprepare","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 #!/usr/bin/env python # coding=utf-8 import hug import tempfile import falcon import os import time import json from hielen2 import db , conf import hielen2.source as sourceman from himada.api import ResponseFormatter import traceback @hug . get ( '/map/' ) def map ( features , timestamp = None , paramser = None , timeref = None , request = None , response = None ): \"\"\" \"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db[\"features\"][features] featobj = sourceman . sourceFactory ( feature ) print ( \" \\n\\n\\n \" ) print ( \"FEATS\" , features ) print ( \"TIMESTAMP\" , timestamp ) print ( \"TIMEREF\" , timeref ) print ( \"PARAMSER\" , paramser ) print ( \" \\n\\n\\n \" ) out . data = featobj . map ( timestamp , timeref , paramser ) except Exception as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature ' { features } ' does not exists or it is misconfigured: { e } \" out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return @hug . get ( '/map/ {feature} ' ) def map_feat ( feature , timestamp = None , paramser = None , timeref = None , request = None , response = None ): return map ( features = feature , timestamp = timestamp , paramser = paramser , timeref = timeref , request = request , response = response ) \"\"\" @hug.get('/map/{feature}/{param}') def map_feat_param(): pass @hug.get('/timeline') def timeline(): pass @hug.get('/timeline/{feature}') def timeline_feat(): pass @hug.get('/timeline/{feature}/{param}') def timeline_feat(): pass \"\"\"","title":"Module hielen2.api.prepare"},{"location":"reference/hielen2/api/prepare/#variables","text":"conf db","title":"Variables"},{"location":"reference/hielen2/api/prepare/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/api/prepare/#map","text":"def map ( features , timestamp = None , paramser = None , timeref = None , request = None , response = None ) View Source @hug . get ( '/map/' ) def map ( features , timestamp = None , paramser = None , timeref = None , request = None , response = None ) : \"\"\" \"\"\" out = ResponseFormatter () # Trying to manage income feature request and its prototype configuration try : #feat = db [ \"features\" ][ features ] featobj = sourceman . sourceFactory ( feature ) print ( \"\\n\\n\\n\" ) print ( \"FEATS\" , features ) print ( \"TIMESTAMP\" , timestamp ) print ( \"TIMEREF\" , timeref ) print ( \"PARAMSER\" , paramser ) print ( \"\\n\\n\\n\" ) out . data = featobj . map ( timestamp , timeref , paramser ) except Exception as e : traceback . print_exc () out . status = falcon . HTTP_NOT_FOUND out . message = f \"feature '{features}' does not exists or it is misconfigured: {e}\" out . format ( request = request , response = response ) return out . format ( request = request , response = response ) return","title":"map"},{"location":"reference/hielen2/api/prepare/#map_feat","text":"def map_feat ( feature , timestamp = None , paramser = None , timeref = None , request = None , response = None ) View Source @hug . get ( '/map/{feature}' ) def map_feat ( feature , timestamp = None , paramser = None , timeref = None , request = None , response = None ) : return map ( features = feature , timestamp = timestamp , paramser = paramser , timeref = timeref , request = request , response = response )","title":"map_feat"},{"location":"reference/hielen2/api/prototypes/","text":"Module hielen2.api.prototypes View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 #!/usr/bin/env python # coding=utf-8 import hug import falcon from hielen2 import db from himada.api import ResponseFormatter @hug . post ( \"/\" ) def new_protptype ( prototype , request = None , response = None ): \"\"\" ** Definizione di nuovi prototipi ** _PLACEHOLDER: Non ancora implementato_ \"\"\" return \"not yet implemented\" @hug . get ( \"/\" ) def prototypes ( request = None , response = None ): \"\"\" **Recupero di tutte le informazioni dei prototipi** ritorna una struttura json di questo tipo: { { \"uid1\": ..., \"module1\": ..., \"struct\": { \"classification\": ..., \"type\": ..., \"parameters\": { \"par1_1\": { \"type\": ..., \"operands\": { \"output\": ... } }, \"...\", \"par1_N\": { \"type\": ..., \"operands\": { \"output\": ... } } } } }, { \"uid2\": ..., \"module2\": ..., \"struct\": { \"classification\": ..., \"type\": ..., \"parameters\": { \"par2_1\": { \"type\": ..., \"operands\": { \"output\": ... } }, \"...\", \"par2_N\": { \"type\": ..., \"operands\": { \"output\": ... } } } } } } \"\"\" out = ResponseFormatter () try : out . data = db [ \"features_proto\" ][ None ] except KeyError as e : out . status = out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) response = out . format ( response = response , request = request ) @hug . get ( \"/ {prototype} /struct\" ) def prototype_struct ( prototype , request = None , response = None ): \"\"\" **Alias per il recupero delle info di inizializzazione delle features legate ad uno specifico \\ prototipo** \"\"\" out = ResponseFormatter () try : out . data = db [ \"features_proto\" ][ prototype ][ \"struct\" ] except KeyError as e : out . status = out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) response = out . format ( response = response , request = request ) Variables db Functions new_protptype def new_protptype ( prototype , request = None , response = None ) Definizione di nuovi prototipi PLACEHOLDER: Non ancora implementato View Source @hug . post ( \"/\" ) def new_protptype ( prototype , request = None , response = None ) : \"\"\" ** Definizione di nuovi prototipi ** _PLACEHOLDER: Non ancora implementato_ \"\"\" return \"not yet implemented\" prototype_struct def prototype_struct ( prototype , request = None , response = None ) Alias per il recupero delle info di inizializzazione delle features legate ad uno specifico prototipo View Source @hug . get ( \"/{prototype}/struct\" ) def prototype_struct ( prototype , request = None , response = None ) : \"\"\" **Alias per il recupero delle info di inizializzazione delle features legate ad uno specifico \\ prototipo** \"\"\" out = ResponseFormatter () try : out . data = db [ \"features_proto\" ][ prototype ][ \"struct\" ] except KeyError as e : out . status = out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) response = out . format ( response = response , request = request ) prototypes def prototypes ( request = None , response = None ) Recupero di tutte le informazioni dei prototipi ritorna una struttura json di questo tipo: { { \"uid1\": ..., \"module1\": ..., \"struct\": { \"classification\": ..., \"type\": ..., \"parameters\": { \"par1_1\": { \"type\": ..., \"operands\": { \"output\": ... } }, \"...\", \"par1_N\": { \"type\": ..., \"operands\": { \"output\": ... } } } } }, { \"uid2\": ..., \"module2\": ..., \"struct\": { \"classification\": ..., \"type\": ..., \"parameters\": { \"par2_1\": { \"type\": ..., \"operands\": { \"output\": ... } }, \"...\", \"par2_N\": { \"type\": ..., \"operands\": { \"output\": ... } } } } } } View Source @hug . get ( \"/\" ) def prototypes ( request = None , response = None ) : \"\"\" **Recupero di tutte le informazioni dei prototipi** ritorna una struttura json di questo tipo: { { \" uid1 \": ..., \" module1 \": ..., \" struct \": { \" classification \": ..., \" type \": ..., \" parameters \": { \" par1_1 \": { \" type \": ..., \" operands \": { \" output \": ... } }, \" ... \", \" par1_N \": { \" type \": ..., \" operands \": { \" output \": ... } } } } }, { \" uid2 \": ..., \" module2 \": ..., \" struct \": { \" classification \": ..., \" type \": ..., \" parameters \": { \" par2_1 \": { \" type \": ..., \" operands \": { \" output \": ... } }, \" ... \", \" par2_N \": { \" type \": ..., \" operands \": { \" output \": ... } } } } } } \"\"\" out = ResponseFormatter () try : out . data = db [ \"features_proto\" ][ None ] except KeyError as e : out . status = out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) response = out . format ( response = response , request = request )","title":"Prototypes"},{"location":"reference/hielen2/api/prototypes/#module-hielen2apiprototypes","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 #!/usr/bin/env python # coding=utf-8 import hug import falcon from hielen2 import db from himada.api import ResponseFormatter @hug . post ( \"/\" ) def new_protptype ( prototype , request = None , response = None ): \"\"\" ** Definizione di nuovi prototipi ** _PLACEHOLDER: Non ancora implementato_ \"\"\" return \"not yet implemented\" @hug . get ( \"/\" ) def prototypes ( request = None , response = None ): \"\"\" **Recupero di tutte le informazioni dei prototipi** ritorna una struttura json di questo tipo: { { \"uid1\": ..., \"module1\": ..., \"struct\": { \"classification\": ..., \"type\": ..., \"parameters\": { \"par1_1\": { \"type\": ..., \"operands\": { \"output\": ... } }, \"...\", \"par1_N\": { \"type\": ..., \"operands\": { \"output\": ... } } } } }, { \"uid2\": ..., \"module2\": ..., \"struct\": { \"classification\": ..., \"type\": ..., \"parameters\": { \"par2_1\": { \"type\": ..., \"operands\": { \"output\": ... } }, \"...\", \"par2_N\": { \"type\": ..., \"operands\": { \"output\": ... } } } } } } \"\"\" out = ResponseFormatter () try : out . data = db [ \"features_proto\" ][ None ] except KeyError as e : out . status = out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) response = out . format ( response = response , request = request ) @hug . get ( \"/ {prototype} /struct\" ) def prototype_struct ( prototype , request = None , response = None ): \"\"\" **Alias per il recupero delle info di inizializzazione delle features legate ad uno specifico \\ prototipo** \"\"\" out = ResponseFormatter () try : out . data = db [ \"features_proto\" ][ prototype ][ \"struct\" ] except KeyError as e : out . status = out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) response = out . format ( response = response , request = request )","title":"Module hielen2.api.prototypes"},{"location":"reference/hielen2/api/prototypes/#variables","text":"db","title":"Variables"},{"location":"reference/hielen2/api/prototypes/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/api/prototypes/#new_protptype","text":"def new_protptype ( prototype , request = None , response = None ) Definizione di nuovi prototipi PLACEHOLDER: Non ancora implementato View Source @hug . post ( \"/\" ) def new_protptype ( prototype , request = None , response = None ) : \"\"\" ** Definizione di nuovi prototipi ** _PLACEHOLDER: Non ancora implementato_ \"\"\" return \"not yet implemented\"","title":"new_protptype"},{"location":"reference/hielen2/api/prototypes/#prototype_struct","text":"def prototype_struct ( prototype , request = None , response = None ) Alias per il recupero delle info di inizializzazione delle features legate ad uno specifico prototipo View Source @hug . get ( \"/{prototype}/struct\" ) def prototype_struct ( prototype , request = None , response = None ) : \"\"\" **Alias per il recupero delle info di inizializzazione delle features legate ad uno specifico \\ prototipo** \"\"\" out = ResponseFormatter () try : out . data = db [ \"features_proto\" ][ prototype ][ \"struct\" ] except KeyError as e : out . status = out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) response = out . format ( response = response , request = request )","title":"prototype_struct"},{"location":"reference/hielen2/api/prototypes/#prototypes","text":"def prototypes ( request = None , response = None ) Recupero di tutte le informazioni dei prototipi ritorna una struttura json di questo tipo: { { \"uid1\": ..., \"module1\": ..., \"struct\": { \"classification\": ..., \"type\": ..., \"parameters\": { \"par1_1\": { \"type\": ..., \"operands\": { \"output\": ... } }, \"...\", \"par1_N\": { \"type\": ..., \"operands\": { \"output\": ... } } } } }, { \"uid2\": ..., \"module2\": ..., \"struct\": { \"classification\": ..., \"type\": ..., \"parameters\": { \"par2_1\": { \"type\": ..., \"operands\": { \"output\": ... } }, \"...\", \"par2_N\": { \"type\": ..., \"operands\": { \"output\": ... } } } } } } View Source @hug . get ( \"/\" ) def prototypes ( request = None , response = None ) : \"\"\" **Recupero di tutte le informazioni dei prototipi** ritorna una struttura json di questo tipo: { { \" uid1 \": ..., \" module1 \": ..., \" struct \": { \" classification \": ..., \" type \": ..., \" parameters \": { \" par1_1 \": { \" type \": ..., \" operands \": { \" output \": ... } }, \" ... \", \" par1_N \": { \" type \": ..., \" operands \": { \" output \": ... } } } } }, { \" uid2 \": ..., \" module2 \": ..., \" struct \": { \" classification \": ..., \" type \": ..., \" parameters \": { \" par2_1 \": { \" type \": ..., \" operands \": { \" output \": ... } }, \" ... \", \" par2_N \": { \" type \": ..., \" operands \": { \" output \": ... } } } } } } \"\"\" out = ResponseFormatter () try : out . data = db [ \"features_proto\" ][ None ] except KeyError as e : out . status = out . status = falcon . HTTP_NOT_FOUND out . message = str ( e ) response = out . format ( response = response , request = request )","title":"prototypes"},{"location":"reference/hielen2/data/","text":"Module hielen2.data Sub-modules hielen2.data.calculation hielen2.data.data_access_layer","title":"Index"},{"location":"reference/hielen2/data/#module-hielen2data","text":"","title":"Module hielen2.data"},{"location":"reference/hielen2/data/#sub-modules","text":"hielen2.data.calculation hielen2.data.data_access_layer","title":"Sub-modules"},{"location":"reference/hielen2/data/calculation/","text":"Module hielen2.data.calculation View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 #!/usr/bin/env python # coding=utf-8 __name__ = \"hielen2.series.calculation\" __version__ = \"0.0.1\" __author__ = \"Alessandro Modesti\" __email__ = \"it@img-srl.com\" __description__ = \"hub for hielen calculations\" __license__ = \"MIT\" __uri__ = \"\" from pandas import DataFrame , Series import math import numpy as np #### CUSTOM LIBRARY #### def poly_trans ( S0 , ** kwargs ): def _parse ( k , w ): k = k . replace ( \"E\" , \"\" ) return f \" { w } *S0** { k } \" operator = \"+\" . join ( _parse ( * x ) for x in kwargs . items () if x [ 0 ][ 0 ] in [ \"E\" , \"e\" ]) return eval ( operator ) def slope ( S0 , unit , radius ): if unit == \"\u00b0\" : S0 = S0 [ 0 ] . apply ( lambda x : math . sin ( math . radians ( x ))) return S0 * radius def aligned ( func ): def wrap_align ( left , right ): left = left . copy () right = right . copy () try : left . columns = list ( range ( len ( left . columns ))) except AttributeError : left . name = 0 try : right . columns = list ( range ( len ( right . columns ))) except AttributeError : right . name = 0 left , right = left . align ( right , axis = 0 , copy = False ) mask = left . notna ()[ 0 ] right = right . fillna ( method = \"pad\" ) return func ( left [ mask ], right [ mask ]) return wrap_align @aligned def add ( left , right ): right = right . fillna ( 0 ) return left + right @aligned def sub ( left , right ): right = right . fillna ( 0 ) return left - right def filter ( b ): d = abs ( b - b . rolling ( window = 50 , center = True , min_periods = 1 ) . apply ( np . mean )) std = abs ( b . rolling ( window = 50 , center = True , min_periods = 1 ) . apply ( np . std )) return b [ d < 3 * std ] def int_or_str ( value ): try : return int ( value ) except ValueError : return value VERSION = tuple ( map ( int_or_str , __version__ . split ( \".\" ))) __all__ = [ \"poly_trans\" , \"add\" , \"sub\" , \"slope\" ] Functions add def add ( left , right ) View Source def wrap_align ( left , right ) : left = left . copy () right = right . copy () try : left . columns = list ( range ( len ( left . columns ))) except AttributeError : left . name = 0 try : right . columns = list ( range ( len ( right . columns ))) except AttributeError : right . name = 0 left , right = left . align ( right , axis = 0 , copy = False ) mask = left . notna () [ 0 ] right = right . fillna ( method = \"pad\" ) return func ( left [ mask ] , right [ mask ] ) poly_trans def poly_trans ( S0 , ** kwargs ) View Source def poly_trans ( S0 , ** kwargs ): def _parse ( k , w ): k = k . replace ( \"E\" , \"\" ) return f \"{w}*S0**{k}\" operator = \"+\" . join ( _parse ( * x ) for x in kwargs . items () if x [ 0 ][ 0 ] in [ \"E\" , \"e\" ]) return eval ( operator ) slope def slope ( S0 , unit , radius ) View Source def slope ( S0 , unit , radius ): if unit == \"\u00b0\" : S0 = S0 [ 0 ]. apply ( lambda x : math . sin ( math . radians ( x ))) return S0 * radius sub def sub ( left , right ) View Source def wrap_align ( left , right ) : left = left . copy () right = right . copy () try : left . columns = list ( range ( len ( left . columns ))) except AttributeError : left . name = 0 try : right . columns = list ( range ( len ( right . columns ))) except AttributeError : right . name = 0 left , right = left . align ( right , axis = 0 , copy = False ) mask = left . notna () [ 0 ] right = right . fillna ( method = \"pad\" ) return func ( left [ mask ] , right [ mask ] )","title":"Calculation"},{"location":"reference/hielen2/data/calculation/#module-hielen2datacalculation","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 #!/usr/bin/env python # coding=utf-8 __name__ = \"hielen2.series.calculation\" __version__ = \"0.0.1\" __author__ = \"Alessandro Modesti\" __email__ = \"it@img-srl.com\" __description__ = \"hub for hielen calculations\" __license__ = \"MIT\" __uri__ = \"\" from pandas import DataFrame , Series import math import numpy as np #### CUSTOM LIBRARY #### def poly_trans ( S0 , ** kwargs ): def _parse ( k , w ): k = k . replace ( \"E\" , \"\" ) return f \" { w } *S0** { k } \" operator = \"+\" . join ( _parse ( * x ) for x in kwargs . items () if x [ 0 ][ 0 ] in [ \"E\" , \"e\" ]) return eval ( operator ) def slope ( S0 , unit , radius ): if unit == \"\u00b0\" : S0 = S0 [ 0 ] . apply ( lambda x : math . sin ( math . radians ( x ))) return S0 * radius def aligned ( func ): def wrap_align ( left , right ): left = left . copy () right = right . copy () try : left . columns = list ( range ( len ( left . columns ))) except AttributeError : left . name = 0 try : right . columns = list ( range ( len ( right . columns ))) except AttributeError : right . name = 0 left , right = left . align ( right , axis = 0 , copy = False ) mask = left . notna ()[ 0 ] right = right . fillna ( method = \"pad\" ) return func ( left [ mask ], right [ mask ]) return wrap_align @aligned def add ( left , right ): right = right . fillna ( 0 ) return left + right @aligned def sub ( left , right ): right = right . fillna ( 0 ) return left - right def filter ( b ): d = abs ( b - b . rolling ( window = 50 , center = True , min_periods = 1 ) . apply ( np . mean )) std = abs ( b . rolling ( window = 50 , center = True , min_periods = 1 ) . apply ( np . std )) return b [ d < 3 * std ] def int_or_str ( value ): try : return int ( value ) except ValueError : return value VERSION = tuple ( map ( int_or_str , __version__ . split ( \".\" ))) __all__ = [ \"poly_trans\" , \"add\" , \"sub\" , \"slope\" ]","title":"Module hielen2.data.calculation"},{"location":"reference/hielen2/data/calculation/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/data/calculation/#add","text":"def add ( left , right ) View Source def wrap_align ( left , right ) : left = left . copy () right = right . copy () try : left . columns = list ( range ( len ( left . columns ))) except AttributeError : left . name = 0 try : right . columns = list ( range ( len ( right . columns ))) except AttributeError : right . name = 0 left , right = left . align ( right , axis = 0 , copy = False ) mask = left . notna () [ 0 ] right = right . fillna ( method = \"pad\" ) return func ( left [ mask ] , right [ mask ] )","title":"add"},{"location":"reference/hielen2/data/calculation/#poly_trans","text":"def poly_trans ( S0 , ** kwargs ) View Source def poly_trans ( S0 , ** kwargs ): def _parse ( k , w ): k = k . replace ( \"E\" , \"\" ) return f \"{w}*S0**{k}\" operator = \"+\" . join ( _parse ( * x ) for x in kwargs . items () if x [ 0 ][ 0 ] in [ \"E\" , \"e\" ]) return eval ( operator )","title":"poly_trans"},{"location":"reference/hielen2/data/calculation/#slope","text":"def slope ( S0 , unit , radius ) View Source def slope ( S0 , unit , radius ): if unit == \"\u00b0\" : S0 = S0 [ 0 ]. apply ( lambda x : math . sin ( math . radians ( x ))) return S0 * radius","title":"slope"},{"location":"reference/hielen2/data/calculation/#sub","text":"def sub ( left , right ) View Source def wrap_align ( left , right ) : left = left . copy () right = right . copy () try : left . columns = list ( range ( len ( left . columns ))) except AttributeError : left . name = 0 try : right . columns = list ( range ( len ( right . columns ))) except AttributeError : right . name = 0 left , right = left . align ( right , axis = 0 , copy = False ) mask = left . notna () [ 0 ] right = right . fillna ( method = \"pad\" ) return func ( left [ mask ] , right [ mask ] )","title":"sub"},{"location":"reference/hielen2/data/data_access_layer/","text":"Module hielen2.data.data_access_layer View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 #!/usr/bin/env python # coding=utf-8 from pandas import DataFrame from time import time from concurrent.futures import ThreadPoolExecutor from functools import wraps from numpy import nan , unique from importlib import import_module from hielen2 import db from hielen2.utils import isot2ut , ut2isot def _threadpool ( f ): @wraps ( f ) def wrap ( * args , ** kwargs ): return ThreadPoolExecutor () . submit ( f , * args , ** kwargs ) return wrap class Series : def __init__ ( self , uid ): series_info = db [ \"series\" ][ uid ] self . __dict__ . update ( series_info ) geninfo = dict ( ( k , w ) for k , w in series_info . items () if k in ( \"modules\" , \"operator\" , \"operands\" ) ) self . generator = Generator ( ** geninfo ) @_threadpool def thdata ( self , timefrom = None , timeto = None , * args , ** kwargs ): return self . data ( timefrom , timeto ) def data ( self , timefrom = None , timeto = None ): if timefrom is not None : if self . first is not None : timefrom = max ( self . first , timefrom ) else : timefrom = self . first if timeto is not None : if self . last is not None : timeto = min ( self . last , timeto ) else : timeto = self . last try : out = db [ \"datacache\" ][ self . uid ] . to_frame () if timefrom is not None and out . index . max () < timefrom : out = out . tail ( 1 ) else : out = out [ timefrom : timeto ] except KeyError : out = DataFrame () timefrom2 = out . index . max () if timefrom2 is nan : timefrom2 = timefrom else : # timefrom2 = max(isot2ut(timefrom2),isot2ut(timefrom) or 1) timefrom2 = max ( isot2ut ( timefrom2 ), isot2ut ( timefrom ) or 1 ) timefrom2 = ut2isot ( timefrom2 ) gen = self . generator . _generate ( timefrom = timefrom2 , timeto = timeto ) try : gen = gen . to_frame () except AttributeError : pass gen . columns = list ( range ( gen . columns . __len__ ())) out = out . append ( gen ) . sort_index () idx = unique ( out . index . values , return_index = True )[ 1 ] out = out . iloc [ idx ] out . index . name = \"timestamp\" return out class Generator : def __init__ ( self , modules = None , operator = None , operands = None ): self . operator = operator or \"DataFrame()\" self . modules = {} if not modules is None : for k , m in modules . items (): self . operator = self . operator . replace ( k , f \"self.modules[ { k !r} ]\" ) self . modules [ k ] = import_module ( m ) self . operands = {} if operands is not None : self . operands = dict ( Generator . _parse_operand ( * op ) for op in operands . items () ) def _parse_operand ( key , value ): \"\"\" trying to extract a series \"\"\" try : return ( key , Series ( value )) except KeyError : pass \"\"\" trying to extract element attribute \"\"\" #TODO modificare per utilizare configurazione try : v = value . split ( \".\" ) assert v . __len__ () == 2 return ( key , db [ \"features\" ][ v [ 0 ]][ \"properties\" ][ v [ 1 ]]) except Exception : pass \"\"\" giving up. It should be a scalar. return it \"\"\" return ( key , value ) def _generate ( self , timefrom , timeto ): operands = dict ( timefrom = timefrom , timeto = timeto ) operands . update ( { k : w for k , w in self . operands . items () if not isinstance ( w , Series )} ) runners = { k : w . thdata ( timefrom , timeto ) for k , w in self . operands . items () if isinstance ( w , Series ) } operands . update ({ k : w . result () for k , w in runners . items ()}) # operands.update( { k:w.data(timefrom,timeto) for k,w in self.operands.items() if isinstance(w,Series) } ) # print('OPERANDI',operands) # print('OPERATORE', self.operator) return eval ( self . operator ) Variables db nan Classes Generator class Generator ( modules = None , operator = None , operands = None ) View Source class Generator : def __init__ ( self , modules = None , operator = None , operands = None ) : self . operator = operator or \"DataFrame()\" self . modules = {} if not modules is None : for k , m in modules . items () : self . operator = self . operator . replace ( k , f \"self.modules[{k!r}]\" ) self . modules [ k ] = import_module ( m ) self . operands = {} if operands is not None : self . operands = dict ( Generator . _parse_operand ( * op ) for op in operands . items () ) def _parse_operand ( key , value ) : \"\"\" trying to extract a series \"\"\" try : return ( key , Series ( value )) except KeyError : pass \"\"\" trying to extract element attribute \"\"\" #TODO modificare per utilizare configurazione try : v = value . split ( \".\" ) assert v . __len__ () == 2 return ( key , db [ \"features\" ][ v[0 ] ] [ \"properties\" ][ v[1 ] ] ) except Exception : pass \"\"\" giving up. It should be a scalar. return it \"\"\" return ( key , value ) def _generate ( self , timefrom , timeto ) : operands = dict ( timefrom = timefrom , timeto = timeto ) operands . update ( { k : w for k , w in self . operands . items () if not isinstance ( w , Series ) } ) runners = { k : w . thdata ( timefrom , timeto ) for k , w in self . operands . items () if isinstance ( w , Series ) } operands . update ( { k : w . result () for k , w in runners . items () } ) # operands . update ( { k : w . data ( timefrom , timeto ) for k , w in self . operands . items () if isinstance ( w , Series ) } ) # print ( 'OPERANDI' , operands ) # print ( 'OPERATORE' , self . operator ) return eval ( self . operator ) Series class Series ( uid ) View Source class Series : def __init__ ( self , uid ) : series_info = db [ \"series\" ][ uid ] self . __dict__ . update ( series_info ) geninfo = dict ( ( k , w ) for k , w in series_info . items () if k in ( \"modules\" , \"operator\" , \"operands\" ) ) self . generator = Generator ( ** geninfo ) @_threadpool def thdata ( self , timefrom = None , timeto = None , * args , ** kwargs ) : return self . data ( timefrom , timeto ) def data ( self , timefrom = None , timeto = None ) : if timefrom is not None : if self . first is not None : timefrom = max ( self . first , timefrom ) else : timefrom = self . first if timeto is not None : if self . last is not None : timeto = min ( self . last , timeto ) else : timeto = self . last try : out = db [ \"datacache\" ][ self.uid ] . to_frame () if timefrom is not None and out . index . max () < timefrom : out = out . tail ( 1 ) else : out = out [ timefrom:timeto ] except KeyError : out = DataFrame () timefrom2 = out . index . max () if timefrom2 is nan : timefrom2 = timefrom else : # timefrom2 = max ( isot2ut ( timefrom2 ), isot2ut ( timefrom ) or 1 ) timefrom2 = max ( isot2ut ( timefrom2 ), isot2ut ( timefrom ) or 1 ) timefrom2 = ut2isot ( timefrom2 ) gen = self . generator . _generate ( timefrom = timefrom2 , timeto = timeto ) try : gen = gen . to_frame () except AttributeError : pass gen . columns = list ( range ( gen . columns . __len__ ())) out = out . append ( gen ). sort_index () idx = unique ( out . index . values , return_index = True ) [ 1 ] out = out . iloc [ idx ] out . index . name = \"timestamp\" return out Methods data def data ( self , timefrom = None , timeto = None ) View Source def data ( self , timefrom = None , timeto = None ) : if timefrom is not None : if self . first is not None : timefrom = max ( self . first , timefrom ) else : timefrom = self . first if timeto is not None : if self . last is not None : timeto = min ( self . last , timeto ) else : timeto = self . last try : out = db [ \"datacache\" ][ self.uid ] . to_frame () if timefrom is not None and out . index . max () < timefrom : out = out . tail ( 1 ) else : out = out [ timefrom:timeto ] except KeyError : out = DataFrame () timefrom2 = out . index . max () if timefrom2 is nan : timefrom2 = timefrom else : # timefrom2 = max ( isot2ut ( timefrom2 ), isot2ut ( timefrom ) or 1 ) timefrom2 = max ( isot2ut ( timefrom2 ), isot2ut ( timefrom ) or 1 ) timefrom2 = ut2isot ( timefrom2 ) gen = self . generator . _generate ( timefrom = timefrom2 , timeto = timeto ) try : gen = gen . to_frame () except AttributeError : pass gen . columns = list ( range ( gen . columns . __len__ ())) out = out . append ( gen ). sort_index () idx = unique ( out . index . values , return_index = True ) [ 1 ] out = out . iloc [ idx ] out . index . name = \"timestamp\" return out thdata def thdata ( self , timefrom = None , timeto = None , * args , ** kwargs ) View Source @_threadpool def thdata ( self , timefrom = None , timeto = None , * args , ** kwargs ) : return self . data ( timefrom , timeto )","title":"Data Access Layer"},{"location":"reference/hielen2/data/data_access_layer/#module-hielen2datadata_access_layer","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 #!/usr/bin/env python # coding=utf-8 from pandas import DataFrame from time import time from concurrent.futures import ThreadPoolExecutor from functools import wraps from numpy import nan , unique from importlib import import_module from hielen2 import db from hielen2.utils import isot2ut , ut2isot def _threadpool ( f ): @wraps ( f ) def wrap ( * args , ** kwargs ): return ThreadPoolExecutor () . submit ( f , * args , ** kwargs ) return wrap class Series : def __init__ ( self , uid ): series_info = db [ \"series\" ][ uid ] self . __dict__ . update ( series_info ) geninfo = dict ( ( k , w ) for k , w in series_info . items () if k in ( \"modules\" , \"operator\" , \"operands\" ) ) self . generator = Generator ( ** geninfo ) @_threadpool def thdata ( self , timefrom = None , timeto = None , * args , ** kwargs ): return self . data ( timefrom , timeto ) def data ( self , timefrom = None , timeto = None ): if timefrom is not None : if self . first is not None : timefrom = max ( self . first , timefrom ) else : timefrom = self . first if timeto is not None : if self . last is not None : timeto = min ( self . last , timeto ) else : timeto = self . last try : out = db [ \"datacache\" ][ self . uid ] . to_frame () if timefrom is not None and out . index . max () < timefrom : out = out . tail ( 1 ) else : out = out [ timefrom : timeto ] except KeyError : out = DataFrame () timefrom2 = out . index . max () if timefrom2 is nan : timefrom2 = timefrom else : # timefrom2 = max(isot2ut(timefrom2),isot2ut(timefrom) or 1) timefrom2 = max ( isot2ut ( timefrom2 ), isot2ut ( timefrom ) or 1 ) timefrom2 = ut2isot ( timefrom2 ) gen = self . generator . _generate ( timefrom = timefrom2 , timeto = timeto ) try : gen = gen . to_frame () except AttributeError : pass gen . columns = list ( range ( gen . columns . __len__ ())) out = out . append ( gen ) . sort_index () idx = unique ( out . index . values , return_index = True )[ 1 ] out = out . iloc [ idx ] out . index . name = \"timestamp\" return out class Generator : def __init__ ( self , modules = None , operator = None , operands = None ): self . operator = operator or \"DataFrame()\" self . modules = {} if not modules is None : for k , m in modules . items (): self . operator = self . operator . replace ( k , f \"self.modules[ { k !r} ]\" ) self . modules [ k ] = import_module ( m ) self . operands = {} if operands is not None : self . operands = dict ( Generator . _parse_operand ( * op ) for op in operands . items () ) def _parse_operand ( key , value ): \"\"\" trying to extract a series \"\"\" try : return ( key , Series ( value )) except KeyError : pass \"\"\" trying to extract element attribute \"\"\" #TODO modificare per utilizare configurazione try : v = value . split ( \".\" ) assert v . __len__ () == 2 return ( key , db [ \"features\" ][ v [ 0 ]][ \"properties\" ][ v [ 1 ]]) except Exception : pass \"\"\" giving up. It should be a scalar. return it \"\"\" return ( key , value ) def _generate ( self , timefrom , timeto ): operands = dict ( timefrom = timefrom , timeto = timeto ) operands . update ( { k : w for k , w in self . operands . items () if not isinstance ( w , Series )} ) runners = { k : w . thdata ( timefrom , timeto ) for k , w in self . operands . items () if isinstance ( w , Series ) } operands . update ({ k : w . result () for k , w in runners . items ()}) # operands.update( { k:w.data(timefrom,timeto) for k,w in self.operands.items() if isinstance(w,Series) } ) # print('OPERANDI',operands) # print('OPERATORE', self.operator) return eval ( self . operator )","title":"Module hielen2.data.data_access_layer"},{"location":"reference/hielen2/data/data_access_layer/#variables","text":"db nan","title":"Variables"},{"location":"reference/hielen2/data/data_access_layer/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/data/data_access_layer/#generator","text":"class Generator ( modules = None , operator = None , operands = None ) View Source class Generator : def __init__ ( self , modules = None , operator = None , operands = None ) : self . operator = operator or \"DataFrame()\" self . modules = {} if not modules is None : for k , m in modules . items () : self . operator = self . operator . replace ( k , f \"self.modules[{k!r}]\" ) self . modules [ k ] = import_module ( m ) self . operands = {} if operands is not None : self . operands = dict ( Generator . _parse_operand ( * op ) for op in operands . items () ) def _parse_operand ( key , value ) : \"\"\" trying to extract a series \"\"\" try : return ( key , Series ( value )) except KeyError : pass \"\"\" trying to extract element attribute \"\"\" #TODO modificare per utilizare configurazione try : v = value . split ( \".\" ) assert v . __len__ () == 2 return ( key , db [ \"features\" ][ v[0 ] ] [ \"properties\" ][ v[1 ] ] ) except Exception : pass \"\"\" giving up. It should be a scalar. return it \"\"\" return ( key , value ) def _generate ( self , timefrom , timeto ) : operands = dict ( timefrom = timefrom , timeto = timeto ) operands . update ( { k : w for k , w in self . operands . items () if not isinstance ( w , Series ) } ) runners = { k : w . thdata ( timefrom , timeto ) for k , w in self . operands . items () if isinstance ( w , Series ) } operands . update ( { k : w . result () for k , w in runners . items () } ) # operands . update ( { k : w . data ( timefrom , timeto ) for k , w in self . operands . items () if isinstance ( w , Series ) } ) # print ( 'OPERANDI' , operands ) # print ( 'OPERATORE' , self . operator ) return eval ( self . operator )","title":"Generator"},{"location":"reference/hielen2/data/data_access_layer/#series","text":"class Series ( uid ) View Source class Series : def __init__ ( self , uid ) : series_info = db [ \"series\" ][ uid ] self . __dict__ . update ( series_info ) geninfo = dict ( ( k , w ) for k , w in series_info . items () if k in ( \"modules\" , \"operator\" , \"operands\" ) ) self . generator = Generator ( ** geninfo ) @_threadpool def thdata ( self , timefrom = None , timeto = None , * args , ** kwargs ) : return self . data ( timefrom , timeto ) def data ( self , timefrom = None , timeto = None ) : if timefrom is not None : if self . first is not None : timefrom = max ( self . first , timefrom ) else : timefrom = self . first if timeto is not None : if self . last is not None : timeto = min ( self . last , timeto ) else : timeto = self . last try : out = db [ \"datacache\" ][ self.uid ] . to_frame () if timefrom is not None and out . index . max () < timefrom : out = out . tail ( 1 ) else : out = out [ timefrom:timeto ] except KeyError : out = DataFrame () timefrom2 = out . index . max () if timefrom2 is nan : timefrom2 = timefrom else : # timefrom2 = max ( isot2ut ( timefrom2 ), isot2ut ( timefrom ) or 1 ) timefrom2 = max ( isot2ut ( timefrom2 ), isot2ut ( timefrom ) or 1 ) timefrom2 = ut2isot ( timefrom2 ) gen = self . generator . _generate ( timefrom = timefrom2 , timeto = timeto ) try : gen = gen . to_frame () except AttributeError : pass gen . columns = list ( range ( gen . columns . __len__ ())) out = out . append ( gen ). sort_index () idx = unique ( out . index . values , return_index = True ) [ 1 ] out = out . iloc [ idx ] out . index . name = \"timestamp\" return out","title":"Series"},{"location":"reference/hielen2/data/data_access_layer/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/data/data_access_layer/#data","text":"def data ( self , timefrom = None , timeto = None ) View Source def data ( self , timefrom = None , timeto = None ) : if timefrom is not None : if self . first is not None : timefrom = max ( self . first , timefrom ) else : timefrom = self . first if timeto is not None : if self . last is not None : timeto = min ( self . last , timeto ) else : timeto = self . last try : out = db [ \"datacache\" ][ self.uid ] . to_frame () if timefrom is not None and out . index . max () < timefrom : out = out . tail ( 1 ) else : out = out [ timefrom:timeto ] except KeyError : out = DataFrame () timefrom2 = out . index . max () if timefrom2 is nan : timefrom2 = timefrom else : # timefrom2 = max ( isot2ut ( timefrom2 ), isot2ut ( timefrom ) or 1 ) timefrom2 = max ( isot2ut ( timefrom2 ), isot2ut ( timefrom ) or 1 ) timefrom2 = ut2isot ( timefrom2 ) gen = self . generator . _generate ( timefrom = timefrom2 , timeto = timeto ) try : gen = gen . to_frame () except AttributeError : pass gen . columns = list ( range ( gen . columns . __len__ ())) out = out . append ( gen ). sort_index () idx = unique ( out . index . values , return_index = True ) [ 1 ] out = out . iloc [ idx ] out . index . name = \"timestamp\" return out","title":"data"},{"location":"reference/hielen2/data/data_access_layer/#thdata","text":"def thdata ( self , timefrom = None , timeto = None , * args , ** kwargs ) View Source @_threadpool def thdata ( self , timefrom = None , timeto = None , * args , ** kwargs ) : return self . data ( timefrom , timeto )","title":"thdata"},{"location":"reference/hielen2/ext/","text":"Module hielen2.ext Sub-modules hielen2.ext.source_csv hielen2.ext.source_photomonitoring hielen2.ext.source_smori hielen2.ext.source_tinsar hielen2.ext.source_winecap","title":"Index"},{"location":"reference/hielen2/ext/#module-hielen2ext","text":"","title":"Module hielen2.ext"},{"location":"reference/hielen2/ext/#sub-modules","text":"hielen2.ext.source_csv hielen2.ext.source_photomonitoring hielen2.ext.source_smori hielen2.ext.source_tinsar hielen2.ext.source_winecap","title":"Sub-modules"},{"location":"reference/hielen2/ext/source_csv/","text":"Module hielen2.ext.source_csv View Source # coding: utf-8 from pandas import DataFrame , Series , to_datetime , read_csv import json import requests def get_ch ( path = \"./incomes\" , restype = None , resource = None , filename = \"last_load.csv\" , column = None , timefrom = None , timeto = None , ): return GWO ( path , restype , filename ) . getDataSeries ( resource = resource , column = column , timefrom = timefrom , timeto = timeto ) class GWO : def __init__ ( self , path = \"./incomes\" , restype = None , filename = \"last_load.csv\" ): self . path = path self . restype = restype self . filename = filename def getDataSeries ( self , resource = None , column = None , timefrom = None , timeto = None ): out = read_csv ( f \"{self.path}/{self.restype}/{resource}/{self.filename}\" , header = None , index_col = [ 0 ], )[ column ] # out.index=to_datetime(out.index) out = out . loc [ timefrom : timeto ] return out Functions get_ch def get_ch ( path = './incomes' , restype = None , resource = None , filename = 'last_load.csv' , column = None , timefrom = None , timeto = None ) View Source def get_ch ( path = \"./incomes\" , restype = None , resource = None , filename = \"last_load.csv\" , column = None , timefrom = None , timeto = None , ): return GWO ( path , restype , filename ). getDataSeries ( resource = resource , column = column , timefrom = timefrom , timeto = timeto ) Classes GWO class GWO ( path = './incomes' , restype = None , filename = 'last_load.csv' ) View Source class GWO : def __init__ ( self , path = \"./incomes\" , restype = None , filename = \"last_load.csv\" ) : self . path = path self . restype = restype self . filename = filename def getDataSeries ( self , resource = None , column = None , timefrom = None , timeto = None ) : out = read_csv ( f \"{self.path}/{self.restype}/{resource}/{self.filename}\" , header = None , index_col =[ 0 ] , ) [ column ] # out . index = to_datetime ( out . index ) out = out . loc [ timefrom:timeto ] return out Methods getDataSeries def getDataSeries ( self , resource = None , column = None , timefrom = None , timeto = None ) View Source def getDataSeries ( self , resource = None , column = None , timefrom = None , timeto = None ) : out = read_csv ( f \"{self.path}/{self.restype}/{resource}/{self.filename}\" , header = None , index_col =[ 0 ] , ) [ column ] # out . index = to_datetime ( out . index ) out = out . loc [ timefrom:timeto ] return out","title":"Source Csv"},{"location":"reference/hielen2/ext/source_csv/#module-hielen2extsource_csv","text":"View Source # coding: utf-8 from pandas import DataFrame , Series , to_datetime , read_csv import json import requests def get_ch ( path = \"./incomes\" , restype = None , resource = None , filename = \"last_load.csv\" , column = None , timefrom = None , timeto = None , ): return GWO ( path , restype , filename ) . getDataSeries ( resource = resource , column = column , timefrom = timefrom , timeto = timeto ) class GWO : def __init__ ( self , path = \"./incomes\" , restype = None , filename = \"last_load.csv\" ): self . path = path self . restype = restype self . filename = filename def getDataSeries ( self , resource = None , column = None , timefrom = None , timeto = None ): out = read_csv ( f \"{self.path}/{self.restype}/{resource}/{self.filename}\" , header = None , index_col = [ 0 ], )[ column ] # out.index=to_datetime(out.index) out = out . loc [ timefrom : timeto ] return out","title":"Module hielen2.ext.source_csv"},{"location":"reference/hielen2/ext/source_csv/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/ext/source_csv/#get_ch","text":"def get_ch ( path = './incomes' , restype = None , resource = None , filename = 'last_load.csv' , column = None , timefrom = None , timeto = None ) View Source def get_ch ( path = \"./incomes\" , restype = None , resource = None , filename = \"last_load.csv\" , column = None , timefrom = None , timeto = None , ): return GWO ( path , restype , filename ). getDataSeries ( resource = resource , column = column , timefrom = timefrom , timeto = timeto )","title":"get_ch"},{"location":"reference/hielen2/ext/source_csv/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/ext/source_csv/#gwo","text":"class GWO ( path = './incomes' , restype = None , filename = 'last_load.csv' ) View Source class GWO : def __init__ ( self , path = \"./incomes\" , restype = None , filename = \"last_load.csv\" ) : self . path = path self . restype = restype self . filename = filename def getDataSeries ( self , resource = None , column = None , timefrom = None , timeto = None ) : out = read_csv ( f \"{self.path}/{self.restype}/{resource}/{self.filename}\" , header = None , index_col =[ 0 ] , ) [ column ] # out . index = to_datetime ( out . index ) out = out . loc [ timefrom:timeto ] return out","title":"GWO"},{"location":"reference/hielen2/ext/source_csv/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_csv/#getdataseries","text":"def getDataSeries ( self , resource = None , column = None , timefrom = None , timeto = None ) View Source def getDataSeries ( self , resource = None , column = None , timefrom = None , timeto = None ) : out = read_csv ( f \"{self.path}/{self.restype}/{resource}/{self.filename}\" , header = None , index_col =[ 0 ] , ) [ column ] # out . index = to_datetime ( out . index ) out = out . loc [ timefrom:timeto ] return out","title":"getDataSeries"},{"location":"reference/hielen2/ext/source_smori/","text":"Module hielen2.ext.source_smori View Source # coding: utf-8 from pandas import DataFrame , Series , to_datetime import json import requests def get_ch ( sito = None , id_stazione = None , id_unita = None , id_sensore = None , aggr = \"avg\" , timefrom = None , timeto = None , ): return GWO () . getDataSeries ( sito = sito , stazione = id_stazione , unita = id_unita , sensore = id_sensore , aggr = \"avg\" , timefrom = timefrom , timeto = timeto , ) class GWO : def __init__ ( self , uri = \"https://www.smori.it/tisma/api/v1/sensor_data.php\" ): self . uri = uri def getDataSeries ( self , sito = None , stazione = None , unita = None , sensore = None , aggr = \"avg\" , timefrom = None , timeto = None , ): params = dict ( sito = sito , stazione = stazione , unita = unita , sensore = sensore , ) if aggr is not None : params [ \"aggr\" ] = aggr if timefrom is not None : params [ \"dal\" ] = timefrom if timefrom is not None : params [ \"al\" ] = timeto r = requests . get ( url = self . uri , params = params ) out = DataFrame ( json . loads ( r . text )[ \"data\" ]) # print (r.url) if out . empty : return out out = out . set_index ([ \"timestamp\" ])[ \"valore\" ] out = out . astype ( float , copy = False , errors = \"ignore\" ) out . name = f \"{stazione}_{unita}_{sensore}\" out . index = to_datetime ( out . index ) return out Functions get_ch def get_ch ( sito = None , id_stazione = None , id_unita = None , id_sensore = None , aggr = 'avg' , timefrom = None , timeto = None ) View Source def get_ch ( sito = None , id_stazione = None , id_unita = None , id_sensore = None , aggr = \"avg\" , timefrom = None , timeto = None , ): return GWO (). getDataSeries ( sito = sito , stazione = id_stazione , unita = id_unita , sensore = id_sensore , aggr = \"avg\" , timefrom = timefrom , timeto = timeto , ) Classes GWO class GWO ( uri = 'https://www.smori.it/tisma/api/v1/sensor_data.php' ) View Source class GWO: def __init__ ( self , uri = \"https://www.smori.it/tisma/api/v1/sensor_data.php\" ): self . uri = uri def getDataSeries ( self , sito = None , stazione = None , unita = None , sensore = None , aggr = \"avg\" , timefrom = None , timeto = None , ): params = dict ( sito = sito , stazione = stazione , unita = unita , sensore = sensore , ) if aggr is not None: params [ \"aggr\" ] = aggr if timefrom is not None: params [ \"dal\" ] = timefrom if timefrom is not None: params [ \"al\" ] = timeto r = requests . get ( url = self . uri , params = params ) out = DataFrame ( json . loads ( r . text )[ \"data\" ]) # print (r.url) if out . empty: return out out = out . set_index ([ \"timestamp\" ])[ \"valore\" ] out = out . astype ( float , copy = False , errors = \"ignore\" ) out . name = f \"{stazione}_{unita}_{sensore}\" out . index = to_datetime ( out . index ) return out Methods getDataSeries def getDataSeries ( self , sito = None , stazione = None , unita = None , sensore = None , aggr = 'avg' , timefrom = None , timeto = None ) View Source def getDataSeries ( self , sito = None , stazione = None , unita = None , sensore = None , aggr = \"avg\" , timefrom = None , timeto = None , ): params = dict ( sito = sito , stazione = stazione , unita = unita , sensore = sensore , ) if aggr is not None : params [ \"aggr\" ] = aggr if timefrom is not None : params [ \"dal\" ] = timefrom if timefrom is not None : params [ \"al\" ] = timeto r = requests . get ( url = self . uri , params = params ) out = DataFrame ( json . loads ( r . text )[ \"data\" ]) # print ( r . url ) if out . empty : return out out = out . set_index ([ \"timestamp\" ])[ \"valore\" ] out = out . astype ( float , copy = False , errors = \"ignore\" ) out . name = f \"{stazione}_{unita}_{sensore}\" out . index = to_datetime ( out . index ) return out","title":"Source Smori"},{"location":"reference/hielen2/ext/source_smori/#module-hielen2extsource_smori","text":"View Source # coding: utf-8 from pandas import DataFrame , Series , to_datetime import json import requests def get_ch ( sito = None , id_stazione = None , id_unita = None , id_sensore = None , aggr = \"avg\" , timefrom = None , timeto = None , ): return GWO () . getDataSeries ( sito = sito , stazione = id_stazione , unita = id_unita , sensore = id_sensore , aggr = \"avg\" , timefrom = timefrom , timeto = timeto , ) class GWO : def __init__ ( self , uri = \"https://www.smori.it/tisma/api/v1/sensor_data.php\" ): self . uri = uri def getDataSeries ( self , sito = None , stazione = None , unita = None , sensore = None , aggr = \"avg\" , timefrom = None , timeto = None , ): params = dict ( sito = sito , stazione = stazione , unita = unita , sensore = sensore , ) if aggr is not None : params [ \"aggr\" ] = aggr if timefrom is not None : params [ \"dal\" ] = timefrom if timefrom is not None : params [ \"al\" ] = timeto r = requests . get ( url = self . uri , params = params ) out = DataFrame ( json . loads ( r . text )[ \"data\" ]) # print (r.url) if out . empty : return out out = out . set_index ([ \"timestamp\" ])[ \"valore\" ] out = out . astype ( float , copy = False , errors = \"ignore\" ) out . name = f \"{stazione}_{unita}_{sensore}\" out . index = to_datetime ( out . index ) return out","title":"Module hielen2.ext.source_smori"},{"location":"reference/hielen2/ext/source_smori/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/ext/source_smori/#get_ch","text":"def get_ch ( sito = None , id_stazione = None , id_unita = None , id_sensore = None , aggr = 'avg' , timefrom = None , timeto = None ) View Source def get_ch ( sito = None , id_stazione = None , id_unita = None , id_sensore = None , aggr = \"avg\" , timefrom = None , timeto = None , ): return GWO (). getDataSeries ( sito = sito , stazione = id_stazione , unita = id_unita , sensore = id_sensore , aggr = \"avg\" , timefrom = timefrom , timeto = timeto , )","title":"get_ch"},{"location":"reference/hielen2/ext/source_smori/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/ext/source_smori/#gwo","text":"class GWO ( uri = 'https://www.smori.it/tisma/api/v1/sensor_data.php' ) View Source class GWO: def __init__ ( self , uri = \"https://www.smori.it/tisma/api/v1/sensor_data.php\" ): self . uri = uri def getDataSeries ( self , sito = None , stazione = None , unita = None , sensore = None , aggr = \"avg\" , timefrom = None , timeto = None , ): params = dict ( sito = sito , stazione = stazione , unita = unita , sensore = sensore , ) if aggr is not None: params [ \"aggr\" ] = aggr if timefrom is not None: params [ \"dal\" ] = timefrom if timefrom is not None: params [ \"al\" ] = timeto r = requests . get ( url = self . uri , params = params ) out = DataFrame ( json . loads ( r . text )[ \"data\" ]) # print (r.url) if out . empty: return out out = out . set_index ([ \"timestamp\" ])[ \"valore\" ] out = out . astype ( float , copy = False , errors = \"ignore\" ) out . name = f \"{stazione}_{unita}_{sensore}\" out . index = to_datetime ( out . index ) return out","title":"GWO"},{"location":"reference/hielen2/ext/source_smori/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_smori/#getdataseries","text":"def getDataSeries ( self , sito = None , stazione = None , unita = None , sensore = None , aggr = 'avg' , timefrom = None , timeto = None ) View Source def getDataSeries ( self , sito = None , stazione = None , unita = None , sensore = None , aggr = \"avg\" , timefrom = None , timeto = None , ): params = dict ( sito = sito , stazione = stazione , unita = unita , sensore = sensore , ) if aggr is not None : params [ \"aggr\" ] = aggr if timefrom is not None : params [ \"dal\" ] = timefrom if timefrom is not None : params [ \"al\" ] = timeto r = requests . get ( url = self . uri , params = params ) out = DataFrame ( json . loads ( r . text )[ \"data\" ]) # print ( r . url ) if out . empty : return out out = out . set_index ([ \"timestamp\" ])[ \"valore\" ] out = out . astype ( float , copy = False , errors = \"ignore\" ) out . name = f \"{stazione}_{unita}_{sensore}\" out . index = to_datetime ( out . index ) return out","title":"getDataSeries"},{"location":"reference/hielen2/ext/source_winecap/","text":"Module hielen2.ext.source_winecap View Source # coding: utf-8 from pandas import DataFrame , Series , to_datetime from zeep import Client from zeep.helpers import serialize_object from concurrent.futures import ThreadPoolExecutor from functools import wraps from time import time from hielen.utils import isot2ut \"\"\" sudo apt-get install libxml2-dev libxslt1-dev pip install lxml==4.2.5 zeep \"\"\" # key='80d373db820fea6f8c5f57d125eb509d' key = \"04a71268d386d61801824863ad7e2a5d\" GWOmac = \"00009DEA\" def get_ch ( GW = None , LG = None , CH = None , timefrom = None , timeto = None ): return GWO ( mac = GW ) . getDataSeries ( mac = LG , ch = CH , timefrom = timefrom , timeto = timeto ) def threadpool ( f , executor = None ): @wraps ( f ) def wrap ( * args , ** kwargs ): return ThreadPoolExecutor () . submit ( f , * args , ** kwargs ) return wrap class GWO : def __init__ ( self , key = key , mac = GWOmac , wsdl = \"http://www.winecap.it/winecapws.wsdl\" ): self . key = key self . mac = mac self . client = Client ( wsdl = wsdl ) self . _gch = self . client . service . getChannelHistory self . _gsh = self . client . service . getSystemHistory self . _gsl = self . client . service . getSensorList def getSensorsList ( self ): return DataFrame ( serialize_object ( self . _gsl ( self . key , self . mac ))) def getDataSeries ( self , mac , ch , timefrom = None , timeto = None ): if not isinstance ( timefrom , int ): timefrom = isot2ut ( timefrom ) if timeto is None : timeto = int ( time ()) if not isinstance ( timeto , int ): timeto = isot2ut ( timeto ) ahead = True out = Series () while ahead : u = DataFrame ( serialize_object ( self . _gch ( self . key , self . mac , mac , ch , timefrom , timeto ) ) ) if u . __len__ () < 1024 : ahead = False if u . __len__ () > 0 : u = u . set_index ([ \"timeStamp\" ])[ \"value\" ] u . index . names = [ \"timestamp\" ] timefrom = u . index . max () + 1 out = out . append ( u ) out = out . sort_index () out . name = f \"{mac}_{ch}\" out . sort_index () out . index = to_datetime ( out . index , unit = \"s\" ) return out @threadpool def getThreadedSeries ( self , * args , ** kwargs ): return self . getDataSeries ( * args , ** kwargs ) def getDataFrame ( self , reqser = [], timefrom = None , timeto = None ): thds = [ self . getThreadedSeries ( * x , timefrom , timeto ) for x in reqser ] return [ x . result () for x in thds ] def getDataFrameSE ( self , reqser = [], timefrom = None , timeto = None ): return [ self . getDataSeries ( * x , timefrom , timeto ) for x in reqser ] Variables GWOmac key Functions get_ch def get_ch ( GW = None , LG = None , CH = None , timefrom = None , timeto = None ) View Source def get_ch ( GW = None , LG = None , CH = None , timefrom = None , timeto = None ): return GWO ( mac = GW ). getDataSeries ( mac = LG , ch = CH , timefrom = timefrom , timeto = timeto ) threadpool def threadpool ( f , executor = None ) View Source def threadpool ( f , executor = None ) : @wraps ( f ) def wrap ( * args , ** kwargs ) : return ThreadPoolExecutor (). submit ( f , * args , ** kwargs ) return wrap Classes GWO class GWO ( key = '04a71268d386d61801824863ad7e2a5d' , mac = '00009DEA' , wsdl = 'http://www.winecap.it/winecapws.wsdl' ) View Source class GWO : def __init__ ( self , key = key , mac = GWOmac , wsdl = \"http://www.winecap.it/winecapws.wsdl\" ) : self . key = key self . mac = mac self . client = Client ( wsdl = wsdl ) self . _gch = self . client . service . getChannelHistory self . _gsh = self . client . service . getSystemHistory self . _gsl = self . client . service . getSensorList def getSensorsList ( self ) : return DataFrame ( serialize_object ( self . _gsl ( self . key , self . mac ))) def getDataSeries ( self , mac , ch , timefrom = None , timeto = None ) : if not isinstance ( timefrom , int ) : timefrom = isot2ut ( timefrom ) if timeto is None : timeto = int ( time ()) if not isinstance ( timeto , int ) : timeto = isot2ut ( timeto ) ahead = True out = Series () while ahead : u = DataFrame ( serialize_object ( self . _gch ( self . key , self . mac , mac , ch , timefrom , timeto ) ) ) if u . __len__ () < 1024 : ahead = False if u . __len__ () > 0 : u = u . set_index ( [ \"timeStamp\" ] ) [ \"value\" ] u . index . names = [ \"timestamp\" ] timefrom = u . index . max () + 1 out = out . append ( u ) out = out . sort_index () out . name = f \"{mac}_{ch}\" out . sort_index () out . index = to_datetime ( out . index , unit = \"s\" ) return out @threadpool def getThreadedSeries ( self , * args , ** kwargs ) : return self . getDataSeries ( * args , ** kwargs ) def getDataFrame ( self , reqser = [] , timefrom = None , timeto = None ) : thds = [ self.getThreadedSeries(*x, timefrom, timeto) for x in reqser ] return [ x.result() for x in thds ] def getDataFrameSE ( self , reqser = [] , timefrom = None , timeto = None ) : return [ self.getDataSeries(*x, timefrom, timeto) for x in reqser ] Methods getDataFrame def getDataFrame ( self , reqser = [], timefrom = None , timeto = None ) View Source def getDataFrame ( self , reqser = [], timefrom = None , timeto = None ): thds = [ self . getThreadedSeries ( * x , timefrom , timeto ) for x in reqser ] return [ x . result () for x in thds ] getDataFrameSE def getDataFrameSE ( self , reqser = [], timefrom = None , timeto = None ) View Source def getDataFrameSE ( self , reqser = [], timefrom = None , timeto = None ): return [ self . getDataSeries ( * x , timefrom , timeto ) for x in reqser ] getDataSeries def getDataSeries ( self , mac , ch , timefrom = None , timeto = None ) View Source def getDataSeries ( self , mac , ch , timefrom = None , timeto = None ): if not isinstance ( timefrom , int ): timefrom = isot2ut ( timefrom ) if timeto is None : timeto = int ( time ()) if not isinstance ( timeto , int ): timeto = isot2ut ( timeto ) ahead = True out = Series () while ahead : u = DataFrame ( serialize_object ( self . _gch ( self . key , self . mac , mac , ch , timefrom , timeto ) ) ) if u . __len__ () < 1024 : ahead = False if u . __len__ () > 0 : u = u . set_index ([ \"timeStamp\" ])[ \"value\" ] u . index . names = [ \"timestamp\" ] timefrom = u . index . max () + 1 out = out . append ( u ) out = out . sort_index () out . name = f \"{mac}_{ch}\" out . sort_index () out . index = to_datetime ( out . index , unit = \"s\" ) return out getSensorsList def getSensorsList ( self ) View Source def getSensorsList ( self ): return DataFrame ( serialize_object ( self . _gsl ( self . key , self . mac ))) getThreadedSeries def getThreadedSeries ( self , * args , ** kwargs ) View Source @threadpool def getThreadedSeries ( self , * args , ** kwargs ) : return self . getDataSeries ( * args , ** kwargs )","title":"Source Winecap"},{"location":"reference/hielen2/ext/source_winecap/#module-hielen2extsource_winecap","text":"View Source # coding: utf-8 from pandas import DataFrame , Series , to_datetime from zeep import Client from zeep.helpers import serialize_object from concurrent.futures import ThreadPoolExecutor from functools import wraps from time import time from hielen.utils import isot2ut \"\"\" sudo apt-get install libxml2-dev libxslt1-dev pip install lxml==4.2.5 zeep \"\"\" # key='80d373db820fea6f8c5f57d125eb509d' key = \"04a71268d386d61801824863ad7e2a5d\" GWOmac = \"00009DEA\" def get_ch ( GW = None , LG = None , CH = None , timefrom = None , timeto = None ): return GWO ( mac = GW ) . getDataSeries ( mac = LG , ch = CH , timefrom = timefrom , timeto = timeto ) def threadpool ( f , executor = None ): @wraps ( f ) def wrap ( * args , ** kwargs ): return ThreadPoolExecutor () . submit ( f , * args , ** kwargs ) return wrap class GWO : def __init__ ( self , key = key , mac = GWOmac , wsdl = \"http://www.winecap.it/winecapws.wsdl\" ): self . key = key self . mac = mac self . client = Client ( wsdl = wsdl ) self . _gch = self . client . service . getChannelHistory self . _gsh = self . client . service . getSystemHistory self . _gsl = self . client . service . getSensorList def getSensorsList ( self ): return DataFrame ( serialize_object ( self . _gsl ( self . key , self . mac ))) def getDataSeries ( self , mac , ch , timefrom = None , timeto = None ): if not isinstance ( timefrom , int ): timefrom = isot2ut ( timefrom ) if timeto is None : timeto = int ( time ()) if not isinstance ( timeto , int ): timeto = isot2ut ( timeto ) ahead = True out = Series () while ahead : u = DataFrame ( serialize_object ( self . _gch ( self . key , self . mac , mac , ch , timefrom , timeto ) ) ) if u . __len__ () < 1024 : ahead = False if u . __len__ () > 0 : u = u . set_index ([ \"timeStamp\" ])[ \"value\" ] u . index . names = [ \"timestamp\" ] timefrom = u . index . max () + 1 out = out . append ( u ) out = out . sort_index () out . name = f \"{mac}_{ch}\" out . sort_index () out . index = to_datetime ( out . index , unit = \"s\" ) return out @threadpool def getThreadedSeries ( self , * args , ** kwargs ): return self . getDataSeries ( * args , ** kwargs ) def getDataFrame ( self , reqser = [], timefrom = None , timeto = None ): thds = [ self . getThreadedSeries ( * x , timefrom , timeto ) for x in reqser ] return [ x . result () for x in thds ] def getDataFrameSE ( self , reqser = [], timefrom = None , timeto = None ): return [ self . getDataSeries ( * x , timefrom , timeto ) for x in reqser ]","title":"Module hielen2.ext.source_winecap"},{"location":"reference/hielen2/ext/source_winecap/#variables","text":"GWOmac key","title":"Variables"},{"location":"reference/hielen2/ext/source_winecap/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/ext/source_winecap/#get_ch","text":"def get_ch ( GW = None , LG = None , CH = None , timefrom = None , timeto = None ) View Source def get_ch ( GW = None , LG = None , CH = None , timefrom = None , timeto = None ): return GWO ( mac = GW ). getDataSeries ( mac = LG , ch = CH , timefrom = timefrom , timeto = timeto )","title":"get_ch"},{"location":"reference/hielen2/ext/source_winecap/#threadpool","text":"def threadpool ( f , executor = None ) View Source def threadpool ( f , executor = None ) : @wraps ( f ) def wrap ( * args , ** kwargs ) : return ThreadPoolExecutor (). submit ( f , * args , ** kwargs ) return wrap","title":"threadpool"},{"location":"reference/hielen2/ext/source_winecap/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/ext/source_winecap/#gwo","text":"class GWO ( key = '04a71268d386d61801824863ad7e2a5d' , mac = '00009DEA' , wsdl = 'http://www.winecap.it/winecapws.wsdl' ) View Source class GWO : def __init__ ( self , key = key , mac = GWOmac , wsdl = \"http://www.winecap.it/winecapws.wsdl\" ) : self . key = key self . mac = mac self . client = Client ( wsdl = wsdl ) self . _gch = self . client . service . getChannelHistory self . _gsh = self . client . service . getSystemHistory self . _gsl = self . client . service . getSensorList def getSensorsList ( self ) : return DataFrame ( serialize_object ( self . _gsl ( self . key , self . mac ))) def getDataSeries ( self , mac , ch , timefrom = None , timeto = None ) : if not isinstance ( timefrom , int ) : timefrom = isot2ut ( timefrom ) if timeto is None : timeto = int ( time ()) if not isinstance ( timeto , int ) : timeto = isot2ut ( timeto ) ahead = True out = Series () while ahead : u = DataFrame ( serialize_object ( self . _gch ( self . key , self . mac , mac , ch , timefrom , timeto ) ) ) if u . __len__ () < 1024 : ahead = False if u . __len__ () > 0 : u = u . set_index ( [ \"timeStamp\" ] ) [ \"value\" ] u . index . names = [ \"timestamp\" ] timefrom = u . index . max () + 1 out = out . append ( u ) out = out . sort_index () out . name = f \"{mac}_{ch}\" out . sort_index () out . index = to_datetime ( out . index , unit = \"s\" ) return out @threadpool def getThreadedSeries ( self , * args , ** kwargs ) : return self . getDataSeries ( * args , ** kwargs ) def getDataFrame ( self , reqser = [] , timefrom = None , timeto = None ) : thds = [ self.getThreadedSeries(*x, timefrom, timeto) for x in reqser ] return [ x.result() for x in thds ] def getDataFrameSE ( self , reqser = [] , timefrom = None , timeto = None ) : return [ self.getDataSeries(*x, timefrom, timeto) for x in reqser ]","title":"GWO"},{"location":"reference/hielen2/ext/source_winecap/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_winecap/#getdataframe","text":"def getDataFrame ( self , reqser = [], timefrom = None , timeto = None ) View Source def getDataFrame ( self , reqser = [], timefrom = None , timeto = None ): thds = [ self . getThreadedSeries ( * x , timefrom , timeto ) for x in reqser ] return [ x . result () for x in thds ]","title":"getDataFrame"},{"location":"reference/hielen2/ext/source_winecap/#getdataframese","text":"def getDataFrameSE ( self , reqser = [], timefrom = None , timeto = None ) View Source def getDataFrameSE ( self , reqser = [], timefrom = None , timeto = None ): return [ self . getDataSeries ( * x , timefrom , timeto ) for x in reqser ]","title":"getDataFrameSE"},{"location":"reference/hielen2/ext/source_winecap/#getdataseries","text":"def getDataSeries ( self , mac , ch , timefrom = None , timeto = None ) View Source def getDataSeries ( self , mac , ch , timefrom = None , timeto = None ): if not isinstance ( timefrom , int ): timefrom = isot2ut ( timefrom ) if timeto is None : timeto = int ( time ()) if not isinstance ( timeto , int ): timeto = isot2ut ( timeto ) ahead = True out = Series () while ahead : u = DataFrame ( serialize_object ( self . _gch ( self . key , self . mac , mac , ch , timefrom , timeto ) ) ) if u . __len__ () < 1024 : ahead = False if u . __len__ () > 0 : u = u . set_index ([ \"timeStamp\" ])[ \"value\" ] u . index . names = [ \"timestamp\" ] timefrom = u . index . max () + 1 out = out . append ( u ) out = out . sort_index () out . name = f \"{mac}_{ch}\" out . sort_index () out . index = to_datetime ( out . index , unit = \"s\" ) return out","title":"getDataSeries"},{"location":"reference/hielen2/ext/source_winecap/#getsensorslist","text":"def getSensorsList ( self ) View Source def getSensorsList ( self ): return DataFrame ( serialize_object ( self . _gsl ( self . key , self . mac )))","title":"getSensorsList"},{"location":"reference/hielen2/ext/source_winecap/#getthreadedseries","text":"def getThreadedSeries ( self , * args , ** kwargs ) View Source @threadpool def getThreadedSeries ( self , * args , ** kwargs ) : return self . getDataSeries ( * args , ** kwargs )","title":"getThreadedSeries"},{"location":"reference/hielen2/ext/source_photomonitoring/","text":"Module hielen2.ext.source_photomonitoring View Source # coding=utf-8 __name__ = \"Source_Photomonitoring\" __version__ = \"0.0.1\" __author__ = \"Alessandro Modesti\" __email__ = \"it@img-srl.com\" __description__ = \"HielenSource extensione\" __license__ = \"MIT\" __uri__ = \"\" from .phm import Source , ConfigSchema , FeedSchema , map __all__ = [ \"Source\" , \"ConfigSchema\" , \"FeedSchema\" , \"map\" ] Sub-modules hielen2.ext.source_photomonitoring.phm hielen2.ext.source_photomonitoring.phm_ok hielen2.ext.source_photomonitoring.rendering hielen2.ext.source_photomonitoring.struct hielen2.ext.source_photomonitoring.struct_ok Functions map def map ( feature , times = None , timeref = None , output = 'RV' ) View Source def map ( feature , times = None , timeref = None , output = \"RV\" ) : feature = sourceFactory ( feature ) timestamp = None if isinstance ( times , slice ) : timestamp = times . stop else : timestamp = times conf = feature . config_last_before ( timestamp ) reftimestamp = timeref or conf [ 'timestamp' ] ncfile = feature . ncfile_path ( conf [ 'timestamp' ] ) mapfile = feature . mapfile_path ( conf [ 'timestamp' ] ) conf = conf [ 'value' ] h = conf [ 'meta' ][ 'height' ] w = conf [ 'meta' ][ 'width' ] wsc = int ( conf [ 'window_size_change' ] ) imgout = zeros ( [ h,w,4 ] ) timestamp , imagearray = generate_map ( ncfile , timestamp = timestamp , timeref = timeref , param = output , step_size = conf [ 'step_size' ] ) imgout [ wsc:,wsc: ]= imagearray [ :h-wsc,:w-wsc ] imgname = f \"{feature.hasher(timestamp)}_{feature.hasher(reftimestamp)}_{output}.tiff\" path_image = feature . mapcache / imgname conf [ 'meta' ][ 'count' ]= 3 conf [ 'meta' ][ 'compress' ]= 'LZW' conf [ 'meta' ][ 'driver' ]= 'GTiff' conf [ 'meta' ][ 'dtype' ]= 'uint8' imagearray = imagearray [ :h-wsc,:w-wsc,0:conf['meta' ][ 'count' ] ] with rasterio . open ( path_image , 'w' , ** conf [ 'meta' ] ) as dst : for i in range ( 0 , conf [ 'meta' ][ 'count' ] ) : dst . write ( imagearray [ :,:,i ] , i + 1 ) url =[ \"http://localhost:8081/maps/mapserv\", \"?map=\"+ str(mapfile), \"&SERVICE=WMS&VERSION=1.1.1\", \"&imgfile=\"+ str(imgname), \"&layers=imglyr\", \"&transparent=true\", \"&format=image/png\", \"&mode=tile\", \"&tilemode=gmap\", \"&tile=364+214+10\", ] url = \"\" . join ( url ) ser = Series ( [ url ] , index = DatetimeIndex ( [ timestamp ] )) return ser Classes ConfigSchema class ConfigSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) 'master_image' (required): the base image used as reference grid for elaboration. It can be any image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent elaboration images. It can be a standard world file (six lines text file) according to http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the 'geo_regerence_file' and/or embeded into the 'master_image' View Source class ConfigSchema ( ActionSchema ): \"\"\"'master_image' (required): the base image used as reference grid for elaboration. It can be any \\ image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based \\ on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. \\ (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected \\ for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent \\ elaboration images. It can be a standard world file (six lines text file) according to \\ http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to \\ https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm \\ (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones \\ possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' \\ (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the \\ 'geo_regerence_file' and/or embeded into the 'master_image' \"\"\" master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Number ( required = False , default = 1 , allow_none = True , as_string = False ) window_size_change = fields . Number ( required = False , default = 0 , allow_none = True , as_string = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True ) Ancestors (in MRO) hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC Class variables Meta OPTIONS_CLASS TYPE_MAPPING crs error_messages geo_reference_file master_image opts step_size window_size_change Static methods from_dict def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls Instance variables dict_class set_class Methods dump def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result dumps def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs ) get_attribute def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default ) handle_error def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass load def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True ) loads def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown ) on_bind_field def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None validate def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {} FeedSchema class FeedSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) reference_time: timestamp of the reference \"master_image\". If Null assumes last \"master_image\" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info View Source class FeedSchema ( ActionSchema ): \"\"\"reference_time: timestamp of the reference \" master_image \". If Null assumes last \\ \" master_image \" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info \"\"\" NS_displacement = LocalFile ( required = True , allow_none = False ) EW_displacement = LocalFile ( required = True , allow_none = False ) CORR = LocalFile ( required = False , allow_none = True ) Ancestors (in MRO) hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC Class variables CORR EW_displacement Meta NS_displacement OPTIONS_CLASS TYPE_MAPPING error_messages opts Static methods from_dict def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls Instance variables dict_class set_class Methods dump def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result dumps def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs ) get_attribute def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default ) handle_error def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass load def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True ) loads def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown ) on_bind_field def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None validate def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {} Source class Source ( feature ) PhotoMonitoring source manager View Source class Source ( HielenSource ) : ''' PhotoMonitoring source manager ''' def hasher ( self , * args , **kwargs ) : h = [ * args ] h . extend ( list ( kwargs . values ())) h='' . join ([ str ( a ) for a in h ]) return re . sub ( \" [ ^\\d ] \",\"\",h)[0:14] def config_last_before(self,timestamp): c=self.getActionValues('config',slice(None,timestamp)) try: return c[-1] except Exception as e: return None def ncfile_path(self,timestamp): return self.filecache / f\" { self . hasher ( timestamp )}. nc \" def masterimg_path(self,timestamp): return self.mapcache / f\" { self . hasher ( timestamp )}. tiff \" def mapfile_path(self,timestamp): return self.mapcache / f\" { self . hasher ( timestamp )}. map \" def config(self, **kwargs): out={} timestamp=kwargs['timestamp'] #Temporary image path path_temp_image=Path(kwargs[\" master_image \"]) self.filecache.mkdir() self.mapcache.mkdir() path_masterimg=self.masterimg_path(timestamp) path_mapfile=self.mapfile_path(timestamp) path_geo_ref=None try: name=str(path_temp_image).split(\" . \") #Temporary georef file path_temp_ref=Path(kwargs[\" geo_reference_file \"]) with open(path_temp_ref) as trf: ''' trying to match reference file type (wld or aux.wml) if exists ''' try: float(trf.readline()) path_geo_ref=Path(\" . \".join([*name[0:-1],\" wld \"])) except Exception as e: path_geo_ref=Path(\" . \".join([*name,\" aux \",\" xml \"])) path_temp_ref.replace(path_geo_ref) except Exception as e: pass # traceback.print_exc() try: ''' trying to define crs from income parameters ''' crs=rasterio.crs.CRS.from_string(kwargs['crs']) except Exception: crs=None try: with rasterio.open(path_temp_image) as src: meta = src.meta.copy() if crs is not None: meta['crs']=crs meta['transform']=list(meta['transform'])[0:6] try: meta['crs']=meta['crs'].to_string() except AttributeError: meta['crs']=None meta['count']=3 meta['compress']='lzw' meta['dtype']='uint8' if src.count == 1: #trasformare da gray scale a rgb rgb = src.read(1).copy() rgb = (rgb/2**16)*255 rgb = rgb.astype('uint8') rgb = [rgb,rgb,rgb] else: rgb = src.read() with rasterio.open(path_masterimg, 'w', **meta) as dst: for i in range(0, rgb.__len__()): dst.write(rgb[i],i+1) bands=dst.meta['count'] outcrs=dst.meta['crs'] outum=outcrs.linear_units #Master_image is ok. Macking mapfile setMFparams(path_mapfile, bands=bands, crs=outcrs, um=outum) except Exception as e: raise ValueError(e) x_offset=y_offset=kwargs['window_size_change'] or 0 step_size=kwargs['step_size'] or 1 out['master_image']=magic.from_file(str(path_masterimg)) out['timestamp']=timestamp out['step_size']=step_size out['window_size_change']=x_offset out['meta']=meta x_values=arange(y_offset,meta['width'],step_size)*meta['transform'][0]+meta['transform'][2] y_values=arange(x_offset,meta['height'],step_size)*meta['transform'][4]+meta['transform'][5] ncpath=self.ncfile_path(timestamp) config_NC(ncpath,timestamp,x_values,y_values).close() return out def cleanConfig(self,timestamp): os.unlink(self.ncfile_path(timestamp)) os.unlink(self.masterimg_path(timestamp)) os.unlink(self.mapfile_path(timestamp)) def feed(self, **kwargs): fileNS=Path(kwargs[\" NS_displacement \"]) fileEW=Path(kwargs[\" EW_displacement \"]) try: fileCORR=Path(kwargs[\" CORR \"]) except Exception as e: fileCORR=None timestamp=kwargs[\" timestamp \"] reftime=self.config_last_before(timestamp)['timestamp'] ncpath=self.ncfile_path(reftime) frames={\" ns \":None,\" ew \":None,\" corr \":None} frames[\" ns \"] = read_csv(fileNS,header=None) frames[\" ew \"] = read_csv(fileEW,header=None) if fileCORR is None: frames[\" corr \"] = DataFrame(full((frames[\" ns \"].shape),0.99)) else: frames[\" corr \" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , **frames ). close () return kwargs def cleanFeed ( timestamp ) : pass def data ( feat , timefrom = None , timeto = None , geom = None , **kwargs ) : return kwargs Ancestors (in MRO) hielen2.source.HielenSource abc.ABC Methods cleanConfig def cleanConfig ( self , timestamp ) View Source def cleanConfig ( self , timestamp ): os . unlink ( self . ncfile_path ( timestamp )) os . unlink ( self . masterimg_path ( timestamp )) os . unlink ( self . mapfile_path ( timestamp )) cleanFeed def cleanFeed ( timestamp ) View Source def cleanFeed ( timestamp ): pass config def config ( self , ** kwargs ) View Source def config ( self , **kwargs ) : out= {} timestamp = kwargs [ 'timestamp' ] # Temporary image path path_temp_image = Path ( kwargs [ \"master_image\" ]) self . filecache . mkdir () self . mapcache . mkdir () path_masterimg = self . masterimg_path ( timestamp ) path_mapfile = self . mapfile_path ( timestamp ) path_geo_ref = None try : name = str ( path_temp_image ). split ( \".\" ) # Temporary georef file path_temp_ref = Path ( kwargs [ \"geo_reference_file\" ]) with open ( path_temp_ref ) as trf : ''' trying to match reference file type (wld or aux.wml) if exists ''' try : float ( trf . readline ()) path_geo_ref = Path ( \".\" . join ([ * name [ 0 :- 1 ], \"wld\" ])) except Exception as e : path_geo_ref = Path ( \".\" . join ([ * name , \"aux\" , \"xml\" ])) path_temp_ref . replace ( path_geo_ref ) except Exception as e : pass # traceback . print_exc () try : ''' trying to define crs from income parameters ''' crs = rasterio . crs . CRS . from_string ( kwargs [ 'crs' ]) except Exception : crs = None try : with rasterio . open ( path_temp_image ) as src : meta = src . meta . copy () if crs is not None : meta [ 'crs' ] = crs meta [ 'transform' ] = list ( meta [ 'transform' ])[ 0 : 6 ] try : meta [ 'crs' ] = meta [ 'crs' ]. to_string () except AttributeError : meta [ 'crs' ] = None meta [ 'count' ] = 3 meta [ 'compress' ] ='lzw' meta [ 'dtype' ] ='uint8' if src . count == 1 : #trasformare da gray scale a rgb rgb = src . read ( 1 ). copy () rgb = ( rgb / 2 ** 16 ) * 255 rgb = rgb . astype ( 'uint8' ) rgb = [ rgb , rgb , rgb ] else : rgb = src . read () with rasterio . open ( path_masterimg , 'w' , **meta ) as dst : for i in range ( 0 , rgb . __ len__ ()) : dst . write ( rgb [ i ], i + 1 ) bands = dst . meta [ 'count' ] outcrs = dst . meta [ 'crs' ] outum = outcrs . linear_units # Master_image is ok . Macking mapfile setMFparams ( path_mapfile , bands = bands , crs = outcrs , um = outum ) except Exception as e : raise ValueError ( e ) x_offset = y_offset = kwargs [ 'window_size_change' ] or 0 step_size = kwargs [ 'step_size' ] or 1 out [ 'master_image' ] = magic . from_file ( str ( path_masterimg )) out [ 'timestamp' ] = timestamp out [ 'step_size' ] = step_size out [ 'window_size_change' ] = x_offset out [ 'meta' ] = meta x_values = arange ( y_offset , meta [ 'width' ], step_size ) * meta [ 'transform' ][ 0 ] + meta [ 'transform' ][ 2 ] y_values = arange ( x_offset , meta [ 'height' ], step_size ) * meta [ 'transform' ][ 4 ] + meta [ 'transform' ][ 5 ] ncpath = self . ncfile_path ( timestamp ) config_NC ( ncpath , timestamp , x_values , y_values ). close () return out config_last_before def config_last_before ( self , timestamp ) View Source def config_last_before ( self , timestamp ): c = self . getActionValues ( 'config' , slice ( None , timestamp )) try : return c [ - 1 ] except Exception as e : return None data def data ( feat , timefrom = None , timeto = None , geom = None , ** kwargs ) View Source def data ( feat , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs deleteActionValues def deleteActionValues ( self , action = None , timestamp = None ) View Source def deleteActionValues ( self , action = None , timestamp = None ) : out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ) : out =[ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \"{a.capitalize()}Schema\" self . __getattribute__ ( f \"clean{a.capitalize()}\" )( t ) except Exception as e : pass try : db [ 'actions' ][ self.uid,a,t ]= None except Exception as e : raise ValueError ( e ) return out execAction def execAction ( self , action , ** kwargs ) View Source def execAction ( self , action , ** kwargs ): aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass (). load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e ) feed def feed ( self , ** kwargs ) View Source def feed ( self , ** kwargs ): fileNS = Path ( kwargs [ \"NS_displacement\" ]) fileEW = Path ( kwargs [ \"EW_displacement\" ]) try : fileCORR = Path ( kwargs [ \"CORR\" ]) except Exception as e : fileCORR = None timestamp = kwargs [ \"timestamp\" ] reftime = self . config_last_before ( timestamp )[ 'timestamp' ] ncpath = self . ncfile_path ( reftime ) frames = { \"ns\" : None , \"ew\" : None , \"corr\" : None } frames [ \"ns\" ] = read_csv ( fileNS , header = None ) frames [ \"ew\" ] = read_csv ( fileEW , header = None ) if fileCORR is None : frames [ \"corr\" ] = DataFrame ( full (( frames [ \"ns\" ]. shape ), 0 . 99 )) else : frames [ \"corr\" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , ** frames ). close () return kwargs getActionSchema def getActionSchema ( self , action ) View Source def getActionSchema ( self , action ): return getActionSchema ( self . module , action ) getActionValues def getActionValues ( self , action = None , timestamp = None ) View Source def getActionValues ( self , action = None , timestamp = None ) : if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self.uid,action,timestamp ] if not isinstance ( out , list ) : out = [ out ] except KeyError : return [] return out hasher def hasher ( self , * args , ** kwargs ) View Source def hasher ( self , * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return re . sub ( \"[^\\d]\" , \"\" , h )[ 0 : 14 ] mapfile_path def mapfile_path ( self , timestamp ) View Source def mapfile_path ( self , timestamp ): return self . mapcache / f \"{self.hasher(timestamp)}.map\" masterimg_path def masterimg_path ( self , timestamp ) View Source def masterimg_path ( self , timestamp ): return self . mapcache / f \"{self.hasher(timestamp)}.tiff\" ncfile_path def ncfile_path ( self , timestamp ) View Source def ncfile_path ( self , timestamp ): return self . filecache / f \"{self.hasher(timestamp)}.nc\"","title":"Index"},{"location":"reference/hielen2/ext/source_photomonitoring/#module-hielen2extsource_photomonitoring","text":"View Source # coding=utf-8 __name__ = \"Source_Photomonitoring\" __version__ = \"0.0.1\" __author__ = \"Alessandro Modesti\" __email__ = \"it@img-srl.com\" __description__ = \"HielenSource extensione\" __license__ = \"MIT\" __uri__ = \"\" from .phm import Source , ConfigSchema , FeedSchema , map __all__ = [ \"Source\" , \"ConfigSchema\" , \"FeedSchema\" , \"map\" ]","title":"Module hielen2.ext.source_photomonitoring"},{"location":"reference/hielen2/ext/source_photomonitoring/#sub-modules","text":"hielen2.ext.source_photomonitoring.phm hielen2.ext.source_photomonitoring.phm_ok hielen2.ext.source_photomonitoring.rendering hielen2.ext.source_photomonitoring.struct hielen2.ext.source_photomonitoring.struct_ok","title":"Sub-modules"},{"location":"reference/hielen2/ext/source_photomonitoring/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/ext/source_photomonitoring/#map","text":"def map ( feature , times = None , timeref = None , output = 'RV' ) View Source def map ( feature , times = None , timeref = None , output = \"RV\" ) : feature = sourceFactory ( feature ) timestamp = None if isinstance ( times , slice ) : timestamp = times . stop else : timestamp = times conf = feature . config_last_before ( timestamp ) reftimestamp = timeref or conf [ 'timestamp' ] ncfile = feature . ncfile_path ( conf [ 'timestamp' ] ) mapfile = feature . mapfile_path ( conf [ 'timestamp' ] ) conf = conf [ 'value' ] h = conf [ 'meta' ][ 'height' ] w = conf [ 'meta' ][ 'width' ] wsc = int ( conf [ 'window_size_change' ] ) imgout = zeros ( [ h,w,4 ] ) timestamp , imagearray = generate_map ( ncfile , timestamp = timestamp , timeref = timeref , param = output , step_size = conf [ 'step_size' ] ) imgout [ wsc:,wsc: ]= imagearray [ :h-wsc,:w-wsc ] imgname = f \"{feature.hasher(timestamp)}_{feature.hasher(reftimestamp)}_{output}.tiff\" path_image = feature . mapcache / imgname conf [ 'meta' ][ 'count' ]= 3 conf [ 'meta' ][ 'compress' ]= 'LZW' conf [ 'meta' ][ 'driver' ]= 'GTiff' conf [ 'meta' ][ 'dtype' ]= 'uint8' imagearray = imagearray [ :h-wsc,:w-wsc,0:conf['meta' ][ 'count' ] ] with rasterio . open ( path_image , 'w' , ** conf [ 'meta' ] ) as dst : for i in range ( 0 , conf [ 'meta' ][ 'count' ] ) : dst . write ( imagearray [ :,:,i ] , i + 1 ) url =[ \"http://localhost:8081/maps/mapserv\", \"?map=\"+ str(mapfile), \"&SERVICE=WMS&VERSION=1.1.1\", \"&imgfile=\"+ str(imgname), \"&layers=imglyr\", \"&transparent=true\", \"&format=image/png\", \"&mode=tile\", \"&tilemode=gmap\", \"&tile=364+214+10\", ] url = \"\" . join ( url ) ser = Series ( [ url ] , index = DatetimeIndex ( [ timestamp ] )) return ser","title":"map"},{"location":"reference/hielen2/ext/source_photomonitoring/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/ext/source_photomonitoring/#configschema","text":"class ConfigSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) 'master_image' (required): the base image used as reference grid for elaboration. It can be any image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent elaboration images. It can be a standard world file (six lines text file) according to http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the 'geo_regerence_file' and/or embeded into the 'master_image' View Source class ConfigSchema ( ActionSchema ): \"\"\"'master_image' (required): the base image used as reference grid for elaboration. It can be any \\ image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based \\ on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. \\ (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected \\ for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent \\ elaboration images. It can be a standard world file (six lines text file) according to \\ http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to \\ https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm \\ (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones \\ possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' \\ (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the \\ 'geo_regerence_file' and/or embeded into the 'master_image' \"\"\" master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Number ( required = False , default = 1 , allow_none = True , as_string = False ) window_size_change = fields . Number ( required = False , default = 0 , allow_none = True , as_string = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True )","title":"ConfigSchema"},{"location":"reference/hielen2/ext/source_photomonitoring/#ancestors-in-mro","text":"hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/ext/source_photomonitoring/#class-variables","text":"Meta OPTIONS_CLASS TYPE_MAPPING crs error_messages geo_reference_file master_image opts step_size window_size_change","title":"Class variables"},{"location":"reference/hielen2/ext/source_photomonitoring/#static-methods","text":"","title":"Static methods"},{"location":"reference/hielen2/ext/source_photomonitoring/#from_dict","text":"def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls","title":"from_dict"},{"location":"reference/hielen2/ext/source_photomonitoring/#instance-variables","text":"dict_class set_class","title":"Instance variables"},{"location":"reference/hielen2/ext/source_photomonitoring/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_photomonitoring/#dump","text":"def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result","title":"dump"},{"location":"reference/hielen2/ext/source_photomonitoring/#dumps","text":"def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs )","title":"dumps"},{"location":"reference/hielen2/ext/source_photomonitoring/#get_attribute","text":"def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default )","title":"get_attribute"},{"location":"reference/hielen2/ext/source_photomonitoring/#handle_error","text":"def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass","title":"handle_error"},{"location":"reference/hielen2/ext/source_photomonitoring/#load","text":"def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True )","title":"load"},{"location":"reference/hielen2/ext/source_photomonitoring/#loads","text":"def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown )","title":"loads"},{"location":"reference/hielen2/ext/source_photomonitoring/#on_bind_field","text":"def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None","title":"on_bind_field"},{"location":"reference/hielen2/ext/source_photomonitoring/#validate","text":"def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"validate"},{"location":"reference/hielen2/ext/source_photomonitoring/#feedschema","text":"class FeedSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) reference_time: timestamp of the reference \"master_image\". If Null assumes last \"master_image\" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info View Source class FeedSchema ( ActionSchema ): \"\"\"reference_time: timestamp of the reference \" master_image \". If Null assumes last \\ \" master_image \" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info \"\"\" NS_displacement = LocalFile ( required = True , allow_none = False ) EW_displacement = LocalFile ( required = True , allow_none = False ) CORR = LocalFile ( required = False , allow_none = True )","title":"FeedSchema"},{"location":"reference/hielen2/ext/source_photomonitoring/#ancestors-in-mro_1","text":"hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/ext/source_photomonitoring/#class-variables_1","text":"CORR EW_displacement Meta NS_displacement OPTIONS_CLASS TYPE_MAPPING error_messages opts","title":"Class variables"},{"location":"reference/hielen2/ext/source_photomonitoring/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/hielen2/ext/source_photomonitoring/#from_dict_1","text":"def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls","title":"from_dict"},{"location":"reference/hielen2/ext/source_photomonitoring/#instance-variables_1","text":"dict_class set_class","title":"Instance variables"},{"location":"reference/hielen2/ext/source_photomonitoring/#methods_1","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_photomonitoring/#dump_1","text":"def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result","title":"dump"},{"location":"reference/hielen2/ext/source_photomonitoring/#dumps_1","text":"def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs )","title":"dumps"},{"location":"reference/hielen2/ext/source_photomonitoring/#get_attribute_1","text":"def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default )","title":"get_attribute"},{"location":"reference/hielen2/ext/source_photomonitoring/#handle_error_1","text":"def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass","title":"handle_error"},{"location":"reference/hielen2/ext/source_photomonitoring/#load_1","text":"def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True )","title":"load"},{"location":"reference/hielen2/ext/source_photomonitoring/#loads_1","text":"def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown )","title":"loads"},{"location":"reference/hielen2/ext/source_photomonitoring/#on_bind_field_1","text":"def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None","title":"on_bind_field"},{"location":"reference/hielen2/ext/source_photomonitoring/#validate_1","text":"def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"validate"},{"location":"reference/hielen2/ext/source_photomonitoring/#source","text":"class Source ( feature ) PhotoMonitoring source manager View Source class Source ( HielenSource ) : ''' PhotoMonitoring source manager ''' def hasher ( self , * args , **kwargs ) : h = [ * args ] h . extend ( list ( kwargs . values ())) h='' . join ([ str ( a ) for a in h ]) return re . sub ( \" [ ^\\d ] \",\"\",h)[0:14] def config_last_before(self,timestamp): c=self.getActionValues('config',slice(None,timestamp)) try: return c[-1] except Exception as e: return None def ncfile_path(self,timestamp): return self.filecache / f\" { self . hasher ( timestamp )}. nc \" def masterimg_path(self,timestamp): return self.mapcache / f\" { self . hasher ( timestamp )}. tiff \" def mapfile_path(self,timestamp): return self.mapcache / f\" { self . hasher ( timestamp )}. map \" def config(self, **kwargs): out={} timestamp=kwargs['timestamp'] #Temporary image path path_temp_image=Path(kwargs[\" master_image \"]) self.filecache.mkdir() self.mapcache.mkdir() path_masterimg=self.masterimg_path(timestamp) path_mapfile=self.mapfile_path(timestamp) path_geo_ref=None try: name=str(path_temp_image).split(\" . \") #Temporary georef file path_temp_ref=Path(kwargs[\" geo_reference_file \"]) with open(path_temp_ref) as trf: ''' trying to match reference file type (wld or aux.wml) if exists ''' try: float(trf.readline()) path_geo_ref=Path(\" . \".join([*name[0:-1],\" wld \"])) except Exception as e: path_geo_ref=Path(\" . \".join([*name,\" aux \",\" xml \"])) path_temp_ref.replace(path_geo_ref) except Exception as e: pass # traceback.print_exc() try: ''' trying to define crs from income parameters ''' crs=rasterio.crs.CRS.from_string(kwargs['crs']) except Exception: crs=None try: with rasterio.open(path_temp_image) as src: meta = src.meta.copy() if crs is not None: meta['crs']=crs meta['transform']=list(meta['transform'])[0:6] try: meta['crs']=meta['crs'].to_string() except AttributeError: meta['crs']=None meta['count']=3 meta['compress']='lzw' meta['dtype']='uint8' if src.count == 1: #trasformare da gray scale a rgb rgb = src.read(1).copy() rgb = (rgb/2**16)*255 rgb = rgb.astype('uint8') rgb = [rgb,rgb,rgb] else: rgb = src.read() with rasterio.open(path_masterimg, 'w', **meta) as dst: for i in range(0, rgb.__len__()): dst.write(rgb[i],i+1) bands=dst.meta['count'] outcrs=dst.meta['crs'] outum=outcrs.linear_units #Master_image is ok. Macking mapfile setMFparams(path_mapfile, bands=bands, crs=outcrs, um=outum) except Exception as e: raise ValueError(e) x_offset=y_offset=kwargs['window_size_change'] or 0 step_size=kwargs['step_size'] or 1 out['master_image']=magic.from_file(str(path_masterimg)) out['timestamp']=timestamp out['step_size']=step_size out['window_size_change']=x_offset out['meta']=meta x_values=arange(y_offset,meta['width'],step_size)*meta['transform'][0]+meta['transform'][2] y_values=arange(x_offset,meta['height'],step_size)*meta['transform'][4]+meta['transform'][5] ncpath=self.ncfile_path(timestamp) config_NC(ncpath,timestamp,x_values,y_values).close() return out def cleanConfig(self,timestamp): os.unlink(self.ncfile_path(timestamp)) os.unlink(self.masterimg_path(timestamp)) os.unlink(self.mapfile_path(timestamp)) def feed(self, **kwargs): fileNS=Path(kwargs[\" NS_displacement \"]) fileEW=Path(kwargs[\" EW_displacement \"]) try: fileCORR=Path(kwargs[\" CORR \"]) except Exception as e: fileCORR=None timestamp=kwargs[\" timestamp \"] reftime=self.config_last_before(timestamp)['timestamp'] ncpath=self.ncfile_path(reftime) frames={\" ns \":None,\" ew \":None,\" corr \":None} frames[\" ns \"] = read_csv(fileNS,header=None) frames[\" ew \"] = read_csv(fileEW,header=None) if fileCORR is None: frames[\" corr \"] = DataFrame(full((frames[\" ns \"].shape),0.99)) else: frames[\" corr \" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , **frames ). close () return kwargs def cleanFeed ( timestamp ) : pass def data ( feat , timefrom = None , timeto = None , geom = None , **kwargs ) : return kwargs","title":"Source"},{"location":"reference/hielen2/ext/source_photomonitoring/#ancestors-in-mro_2","text":"hielen2.source.HielenSource abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/ext/source_photomonitoring/#methods_2","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_photomonitoring/#cleanconfig","text":"def cleanConfig ( self , timestamp ) View Source def cleanConfig ( self , timestamp ): os . unlink ( self . ncfile_path ( timestamp )) os . unlink ( self . masterimg_path ( timestamp )) os . unlink ( self . mapfile_path ( timestamp ))","title":"cleanConfig"},{"location":"reference/hielen2/ext/source_photomonitoring/#cleanfeed","text":"def cleanFeed ( timestamp ) View Source def cleanFeed ( timestamp ): pass","title":"cleanFeed"},{"location":"reference/hielen2/ext/source_photomonitoring/#config","text":"def config ( self , ** kwargs ) View Source def config ( self , **kwargs ) : out= {} timestamp = kwargs [ 'timestamp' ] # Temporary image path path_temp_image = Path ( kwargs [ \"master_image\" ]) self . filecache . mkdir () self . mapcache . mkdir () path_masterimg = self . masterimg_path ( timestamp ) path_mapfile = self . mapfile_path ( timestamp ) path_geo_ref = None try : name = str ( path_temp_image ). split ( \".\" ) # Temporary georef file path_temp_ref = Path ( kwargs [ \"geo_reference_file\" ]) with open ( path_temp_ref ) as trf : ''' trying to match reference file type (wld or aux.wml) if exists ''' try : float ( trf . readline ()) path_geo_ref = Path ( \".\" . join ([ * name [ 0 :- 1 ], \"wld\" ])) except Exception as e : path_geo_ref = Path ( \".\" . join ([ * name , \"aux\" , \"xml\" ])) path_temp_ref . replace ( path_geo_ref ) except Exception as e : pass # traceback . print_exc () try : ''' trying to define crs from income parameters ''' crs = rasterio . crs . CRS . from_string ( kwargs [ 'crs' ]) except Exception : crs = None try : with rasterio . open ( path_temp_image ) as src : meta = src . meta . copy () if crs is not None : meta [ 'crs' ] = crs meta [ 'transform' ] = list ( meta [ 'transform' ])[ 0 : 6 ] try : meta [ 'crs' ] = meta [ 'crs' ]. to_string () except AttributeError : meta [ 'crs' ] = None meta [ 'count' ] = 3 meta [ 'compress' ] ='lzw' meta [ 'dtype' ] ='uint8' if src . count == 1 : #trasformare da gray scale a rgb rgb = src . read ( 1 ). copy () rgb = ( rgb / 2 ** 16 ) * 255 rgb = rgb . astype ( 'uint8' ) rgb = [ rgb , rgb , rgb ] else : rgb = src . read () with rasterio . open ( path_masterimg , 'w' , **meta ) as dst : for i in range ( 0 , rgb . __ len__ ()) : dst . write ( rgb [ i ], i + 1 ) bands = dst . meta [ 'count' ] outcrs = dst . meta [ 'crs' ] outum = outcrs . linear_units # Master_image is ok . Macking mapfile setMFparams ( path_mapfile , bands = bands , crs = outcrs , um = outum ) except Exception as e : raise ValueError ( e ) x_offset = y_offset = kwargs [ 'window_size_change' ] or 0 step_size = kwargs [ 'step_size' ] or 1 out [ 'master_image' ] = magic . from_file ( str ( path_masterimg )) out [ 'timestamp' ] = timestamp out [ 'step_size' ] = step_size out [ 'window_size_change' ] = x_offset out [ 'meta' ] = meta x_values = arange ( y_offset , meta [ 'width' ], step_size ) * meta [ 'transform' ][ 0 ] + meta [ 'transform' ][ 2 ] y_values = arange ( x_offset , meta [ 'height' ], step_size ) * meta [ 'transform' ][ 4 ] + meta [ 'transform' ][ 5 ] ncpath = self . ncfile_path ( timestamp ) config_NC ( ncpath , timestamp , x_values , y_values ). close () return out","title":"config"},{"location":"reference/hielen2/ext/source_photomonitoring/#config_last_before","text":"def config_last_before ( self , timestamp ) View Source def config_last_before ( self , timestamp ): c = self . getActionValues ( 'config' , slice ( None , timestamp )) try : return c [ - 1 ] except Exception as e : return None","title":"config_last_before"},{"location":"reference/hielen2/ext/source_photomonitoring/#data","text":"def data ( feat , timefrom = None , timeto = None , geom = None , ** kwargs ) View Source def data ( feat , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs","title":"data"},{"location":"reference/hielen2/ext/source_photomonitoring/#deleteactionvalues","text":"def deleteActionValues ( self , action = None , timestamp = None ) View Source def deleteActionValues ( self , action = None , timestamp = None ) : out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ) : out =[ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \"{a.capitalize()}Schema\" self . __getattribute__ ( f \"clean{a.capitalize()}\" )( t ) except Exception as e : pass try : db [ 'actions' ][ self.uid,a,t ]= None except Exception as e : raise ValueError ( e ) return out","title":"deleteActionValues"},{"location":"reference/hielen2/ext/source_photomonitoring/#execaction","text":"def execAction ( self , action , ** kwargs ) View Source def execAction ( self , action , ** kwargs ): aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass (). load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e )","title":"execAction"},{"location":"reference/hielen2/ext/source_photomonitoring/#feed","text":"def feed ( self , ** kwargs ) View Source def feed ( self , ** kwargs ): fileNS = Path ( kwargs [ \"NS_displacement\" ]) fileEW = Path ( kwargs [ \"EW_displacement\" ]) try : fileCORR = Path ( kwargs [ \"CORR\" ]) except Exception as e : fileCORR = None timestamp = kwargs [ \"timestamp\" ] reftime = self . config_last_before ( timestamp )[ 'timestamp' ] ncpath = self . ncfile_path ( reftime ) frames = { \"ns\" : None , \"ew\" : None , \"corr\" : None } frames [ \"ns\" ] = read_csv ( fileNS , header = None ) frames [ \"ew\" ] = read_csv ( fileEW , header = None ) if fileCORR is None : frames [ \"corr\" ] = DataFrame ( full (( frames [ \"ns\" ]. shape ), 0 . 99 )) else : frames [ \"corr\" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , ** frames ). close () return kwargs","title":"feed"},{"location":"reference/hielen2/ext/source_photomonitoring/#getactionschema","text":"def getActionSchema ( self , action ) View Source def getActionSchema ( self , action ): return getActionSchema ( self . module , action )","title":"getActionSchema"},{"location":"reference/hielen2/ext/source_photomonitoring/#getactionvalues","text":"def getActionValues ( self , action = None , timestamp = None ) View Source def getActionValues ( self , action = None , timestamp = None ) : if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self.uid,action,timestamp ] if not isinstance ( out , list ) : out = [ out ] except KeyError : return [] return out","title":"getActionValues"},{"location":"reference/hielen2/ext/source_photomonitoring/#hasher","text":"def hasher ( self , * args , ** kwargs ) View Source def hasher ( self , * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return re . sub ( \"[^\\d]\" , \"\" , h )[ 0 : 14 ]","title":"hasher"},{"location":"reference/hielen2/ext/source_photomonitoring/#mapfile_path","text":"def mapfile_path ( self , timestamp ) View Source def mapfile_path ( self , timestamp ): return self . mapcache / f \"{self.hasher(timestamp)}.map\"","title":"mapfile_path"},{"location":"reference/hielen2/ext/source_photomonitoring/#masterimg_path","text":"def masterimg_path ( self , timestamp ) View Source def masterimg_path ( self , timestamp ): return self . mapcache / f \"{self.hasher(timestamp)}.tiff\"","title":"masterimg_path"},{"location":"reference/hielen2/ext/source_photomonitoring/#ncfile_path","text":"def ncfile_path ( self , timestamp ) View Source def ncfile_path ( self , timestamp ): return self . filecache / f \"{self.hasher(timestamp)}.nc\"","title":"ncfile_path"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/","text":"Module hielen2.ext.source_photomonitoring.phm View Source # coding=utf-8 from hielen2.source import HielenSource , ActionSchema , StringTime , sourceFactory from hielen2.utils import LocalFile from hielen2.maps.mapper import setMFparams import rasterio import magic import os import re from pathlib import Path from .struct import config_NC , feed_NC , generate_map from marshmallow import fields from shutil import rmtree from numpy import arange , full , zeros from pandas import read_csv , DataFrame , Series , DatetimeIndex import traceback class ConfigSchema ( ActionSchema ): \"\"\"'master_image' (required): the base image used as reference grid for elaboration. It can be any \\ image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based \\ on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. \\ (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected \\ for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent \\ elaboration images. It can be a standard world file (six lines text file) according to \\ http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to \\ https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm \\ (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones \\ possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' \\ (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the \\ 'geo_regerence_file' and/or embeded into the 'master_image' \"\"\" master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Number ( required = False , default = 1 , allow_none = True , as_string = False ) window_size_change = fields . Number ( required = False , default = 0 , allow_none = True , as_string = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True ) class FeedSchema ( ActionSchema ): \"\"\"reference_time: timestamp of the reference \"master_image\". If Null assumes last \\ \"master_image\" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info \"\"\" NS_displacement = LocalFile ( required = True , allow_none = False ) EW_displacement = LocalFile ( required = True , allow_none = False ) CORR = LocalFile ( required = False , allow_none = True ) class Source ( HielenSource ): ''' PhotoMonitoring source manager ''' def hasher ( self , * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return re . sub ( \"[^\\d]\" , \"\" , h )[ 0 : 14 ] def config_last_before ( self , timestamp ): c = self . getActionValues ( 'config' , slice ( None , timestamp )) try : return c [ - 1 ] except Exception as e : return None def ncfile_path ( self , timestamp ): return self . filecache / f \" { self . hasher ( timestamp ) } .nc\" def masterimg_path ( self , timestamp ): return self . mapcache / f \" { self . hasher ( timestamp ) } .tiff\" def mapfile_path ( self , timestamp ): return self . mapcache / f \" { self . hasher ( timestamp ) } .map\" def config ( self , ** kwargs ): out = {} timestamp = kwargs [ 'timestamp' ] #Temporary image path path_temp_image = Path ( kwargs [ \"master_image\" ]) self . filecache . mkdir () self . mapcache . mkdir () path_masterimg = self . masterimg_path ( timestamp ) path_mapfile = self . mapfile_path ( timestamp ) path_geo_ref = None try : name = str ( path_temp_image ) . split ( \".\" ) #Temporary georef file path_temp_ref = Path ( kwargs [ \"geo_reference_file\" ]) with open ( path_temp_ref ) as trf : ''' trying to match reference file type (wld or aux.wml) if exists ''' try : float ( trf . readline ()) path_geo_ref = Path ( \".\" . join ([ * name [ 0 : - 1 ], \"wld\" ])) except Exception as e : path_geo_ref = Path ( \".\" . join ([ * name , \"aux\" , \"xml\" ])) path_temp_ref . replace ( path_geo_ref ) except Exception as e : pass # traceback.print_exc() try : ''' trying to define crs from income parameters ''' crs = rasterio . crs . CRS . from_string ( kwargs [ 'crs' ]) except Exception : crs = None try : with rasterio . open ( path_temp_image ) as src : meta = src . meta . copy () if crs is not None : meta [ 'crs' ] = crs meta [ 'transform' ] = list ( meta [ 'transform' ])[ 0 : 6 ] try : meta [ 'crs' ] = meta [ 'crs' ] . to_string () except AttributeError : meta [ 'crs' ] = None meta [ 'count' ] = 3 meta [ 'compress' ] = 'lzw' meta [ 'dtype' ] = 'uint8' if src . count == 1 : #trasformare da gray scale a rgb rgb = src . read ( 1 ) . copy () rgb = ( rgb / 2 ** 16 ) * 255 rgb = rgb . astype ( 'uint8' ) rgb = [ rgb , rgb , rgb ] else : rgb = src . read () with rasterio . open ( path_masterimg , 'w' , ** meta ) as dst : for i in range ( 0 , rgb . __len__ ()): dst . write ( rgb [ i ], i + 1 ) bands = dst . meta [ 'count' ] outcrs = dst . meta [ 'crs' ] outum = outcrs . linear_units #Master_image is ok. Macking mapfile setMFparams ( path_mapfile , bands = bands , crs = outcrs , um = outum ) except Exception as e : raise ValueError ( e ) x_offset = y_offset = kwargs [ 'window_size_change' ] or 0 step_size = kwargs [ 'step_size' ] or 1 out [ 'master_image' ] = magic . from_file ( str ( path_masterimg )) out [ 'timestamp' ] = timestamp out [ 'step_size' ] = step_size out [ 'window_size_change' ] = x_offset out [ 'meta' ] = meta x_values = arange ( y_offset , meta [ 'width' ], step_size ) * meta [ 'transform' ][ 0 ] + meta [ 'transform' ][ 2 ] y_values = arange ( x_offset , meta [ 'height' ], step_size ) * meta [ 'transform' ][ 4 ] + meta [ 'transform' ][ 5 ] ncpath = self . ncfile_path ( timestamp ) config_NC ( ncpath , timestamp , x_values , y_values ) . close () return out def cleanConfig ( self , timestamp ): os . unlink ( self . ncfile_path ( timestamp )) os . unlink ( self . masterimg_path ( timestamp )) os . unlink ( self . mapfile_path ( timestamp )) def feed ( self , ** kwargs ): fileNS = Path ( kwargs [ \"NS_displacement\" ]) fileEW = Path ( kwargs [ \"EW_displacement\" ]) try : fileCORR = Path ( kwargs [ \"CORR\" ]) except Exception as e : fileCORR = None timestamp = kwargs [ \"timestamp\" ] reftime = self . config_last_before ( timestamp )[ 'timestamp' ] ncpath = self . ncfile_path ( reftime ) frames = { \"ns\" : None , \"ew\" : None , \"corr\" : None } frames [ \"ns\" ] = read_csv ( fileNS , header = None ) frames [ \"ew\" ] = read_csv ( fileEW , header = None ) if fileCORR is None : frames [ \"corr\" ] = DataFrame ( full (( frames [ \"ns\" ] . shape ), 0.99 )) else : frames [ \"corr\" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , ** frames ) . close () return kwargs def cleanFeed ( timestamp ): pass def data ( feat , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs def map ( feature , times = None , timeref = None , output = \"RV\" ): feature = sourceFactory ( feature ) timestamp = None if isinstance ( times , slice ): timestamp = times . stop else : timestamp = times conf = feature . config_last_before ( timestamp ) reftimestamp = timeref or conf [ 'timestamp' ] ncfile = feature . ncfile_path ( conf [ 'timestamp' ]) mapfile = feature . mapfile_path ( conf [ 'timestamp' ]) conf = conf [ 'value' ] h = conf [ 'meta' ][ 'height' ] w = conf [ 'meta' ][ 'width' ] wsc = int ( conf [ 'window_size_change' ]) imgout = zeros ([ h , w , 4 ]) timestamp , imagearray = generate_map ( ncfile , timestamp = timestamp , timeref = timeref , param = output , step_size = conf [ 'step_size' ]) imgout [ wsc :, wsc :] = imagearray [: h - wsc ,: w - wsc ] imgname = f \" { feature . hasher ( timestamp ) } _ { feature . hasher ( reftimestamp ) } _ { output } .tiff\" path_image = feature . mapcache / imgname conf [ 'meta' ][ 'count' ] = 3 conf [ 'meta' ][ 'compress' ] = 'LZW' conf [ 'meta' ][ 'driver' ] = 'GTiff' conf [ 'meta' ][ 'dtype' ] = 'uint8' imagearray = imagearray [: h - wsc ,: w - wsc , 0 : conf [ 'meta' ][ 'count' ]] with rasterio . open ( path_image , 'w' , ** conf [ 'meta' ]) as dst : for i in range ( 0 , conf [ 'meta' ][ 'count' ]): dst . write ( imagearray [:,:, i ], i + 1 ) url = [ \"http://localhost:8081/maps/mapserv\" , \"?map=\" + str ( mapfile ), \"&SERVICE=WMS&VERSION=1.1.1\" , \"&imgfile=\" + str ( imgname ), \"&layers=imglyr\" , \"&transparent=true\" , \"&format=image/png\" , \"&mode=tile\" , \"&tilemode=gmap\" , \"&tile=364+214+10\" , ] url = \"\" . join ( url ) ser = Series ([ url ], index = DatetimeIndex ([ timestamp ])) return ser Functions map def map ( feature , times = None , timeref = None , output = 'RV' ) View Source def map ( feature , times = None , timeref = None , output = \"RV\" ) : feature = sourceFactory ( feature ) timestamp = None if isinstance ( times , slice ) : timestamp = times . stop else : timestamp = times conf = feature . config_last_before ( timestamp ) reftimestamp = timeref or conf [ 'timestamp' ] ncfile = feature . ncfile_path ( conf [ 'timestamp' ] ) mapfile = feature . mapfile_path ( conf [ 'timestamp' ] ) conf = conf [ 'value' ] h = conf [ 'meta' ][ 'height' ] w = conf [ 'meta' ][ 'width' ] wsc = int ( conf [ 'window_size_change' ] ) imgout = zeros ( [ h,w,4 ] ) timestamp , imagearray = generate_map ( ncfile , timestamp = timestamp , timeref = timeref , param = output , step_size = conf [ 'step_size' ] ) imgout [ wsc:,wsc: ]= imagearray [ :h-wsc,:w-wsc ] imgname = f \"{feature.hasher(timestamp)}_{feature.hasher(reftimestamp)}_{output}.tiff\" path_image = feature . mapcache / imgname conf [ 'meta' ][ 'count' ]= 3 conf [ 'meta' ][ 'compress' ]= 'LZW' conf [ 'meta' ][ 'driver' ]= 'GTiff' conf [ 'meta' ][ 'dtype' ]= 'uint8' imagearray = imagearray [ :h-wsc,:w-wsc,0:conf['meta' ][ 'count' ] ] with rasterio . open ( path_image , 'w' , ** conf [ 'meta' ] ) as dst : for i in range ( 0 , conf [ 'meta' ][ 'count' ] ) : dst . write ( imagearray [ :,:,i ] , i + 1 ) url =[ \"http://localhost:8081/maps/mapserv\", \"?map=\"+ str(mapfile), \"&SERVICE=WMS&VERSION=1.1.1\", \"&imgfile=\"+ str(imgname), \"&layers=imglyr\", \"&transparent=true\", \"&format=image/png\", \"&mode=tile\", \"&tilemode=gmap\", \"&tile=364+214+10\", ] url = \"\" . join ( url ) ser = Series ( [ url ] , index = DatetimeIndex ( [ timestamp ] )) return ser Classes ConfigSchema class ConfigSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) 'master_image' (required): the base image used as reference grid for elaboration. It can be any image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent elaboration images. It can be a standard world file (six lines text file) according to http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the 'geo_regerence_file' and/or embeded into the 'master_image' View Source class ConfigSchema ( ActionSchema ): \"\"\"'master_image' (required): the base image used as reference grid for elaboration. It can be any \\ image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based \\ on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. \\ (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected \\ for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent \\ elaboration images. It can be a standard world file (six lines text file) according to \\ http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to \\ https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm \\ (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones \\ possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' \\ (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the \\ 'geo_regerence_file' and/or embeded into the 'master_image' \"\"\" master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Number ( required = False , default = 1 , allow_none = True , as_string = False ) window_size_change = fields . Number ( required = False , default = 0 , allow_none = True , as_string = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True ) Ancestors (in MRO) hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC Class variables Meta OPTIONS_CLASS TYPE_MAPPING crs error_messages geo_reference_file master_image opts step_size window_size_change Static methods from_dict def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls Instance variables dict_class set_class Methods dump def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result dumps def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs ) get_attribute def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default ) handle_error def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass load def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True ) loads def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown ) on_bind_field def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None validate def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {} FeedSchema class FeedSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) reference_time: timestamp of the reference \"master_image\". If Null assumes last \"master_image\" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info View Source class FeedSchema ( ActionSchema ): \"\"\"reference_time: timestamp of the reference \" master_image \". If Null assumes last \\ \" master_image \" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info \"\"\" NS_displacement = LocalFile ( required = True , allow_none = False ) EW_displacement = LocalFile ( required = True , allow_none = False ) CORR = LocalFile ( required = False , allow_none = True ) Ancestors (in MRO) hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC Class variables CORR EW_displacement Meta NS_displacement OPTIONS_CLASS TYPE_MAPPING error_messages opts Static methods from_dict def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls Instance variables dict_class set_class Methods dump def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result dumps def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs ) get_attribute def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default ) handle_error def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass load def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True ) loads def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown ) on_bind_field def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None validate def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {} Source class Source ( feature ) PhotoMonitoring source manager View Source class Source ( HielenSource ) : ''' PhotoMonitoring source manager ''' def hasher ( self , * args , **kwargs ) : h = [ * args ] h . extend ( list ( kwargs . values ())) h='' . join ([ str ( a ) for a in h ]) return re . sub ( \" [ ^\\d ] \",\"\",h)[0:14] def config_last_before(self,timestamp): c=self.getActionValues('config',slice(None,timestamp)) try: return c[-1] except Exception as e: return None def ncfile_path(self,timestamp): return self.filecache / f\" { self . hasher ( timestamp )}. nc \" def masterimg_path(self,timestamp): return self.mapcache / f\" { self . hasher ( timestamp )}. tiff \" def mapfile_path(self,timestamp): return self.mapcache / f\" { self . hasher ( timestamp )}. map \" def config(self, **kwargs): out={} timestamp=kwargs['timestamp'] #Temporary image path path_temp_image=Path(kwargs[\" master_image \"]) self.filecache.mkdir() self.mapcache.mkdir() path_masterimg=self.masterimg_path(timestamp) path_mapfile=self.mapfile_path(timestamp) path_geo_ref=None try: name=str(path_temp_image).split(\" . \") #Temporary georef file path_temp_ref=Path(kwargs[\" geo_reference_file \"]) with open(path_temp_ref) as trf: ''' trying to match reference file type (wld or aux.wml) if exists ''' try: float(trf.readline()) path_geo_ref=Path(\" . \".join([*name[0:-1],\" wld \"])) except Exception as e: path_geo_ref=Path(\" . \".join([*name,\" aux \",\" xml \"])) path_temp_ref.replace(path_geo_ref) except Exception as e: pass # traceback.print_exc() try: ''' trying to define crs from income parameters ''' crs=rasterio.crs.CRS.from_string(kwargs['crs']) except Exception: crs=None try: with rasterio.open(path_temp_image) as src: meta = src.meta.copy() if crs is not None: meta['crs']=crs meta['transform']=list(meta['transform'])[0:6] try: meta['crs']=meta['crs'].to_string() except AttributeError: meta['crs']=None meta['count']=3 meta['compress']='lzw' meta['dtype']='uint8' if src.count == 1: #trasformare da gray scale a rgb rgb = src.read(1).copy() rgb = (rgb/2**16)*255 rgb = rgb.astype('uint8') rgb = [rgb,rgb,rgb] else: rgb = src.read() with rasterio.open(path_masterimg, 'w', **meta) as dst: for i in range(0, rgb.__len__()): dst.write(rgb[i],i+1) bands=dst.meta['count'] outcrs=dst.meta['crs'] outum=outcrs.linear_units #Master_image is ok. Macking mapfile setMFparams(path_mapfile, bands=bands, crs=outcrs, um=outum) except Exception as e: raise ValueError(e) x_offset=y_offset=kwargs['window_size_change'] or 0 step_size=kwargs['step_size'] or 1 out['master_image']=magic.from_file(str(path_masterimg)) out['timestamp']=timestamp out['step_size']=step_size out['window_size_change']=x_offset out['meta']=meta x_values=arange(y_offset,meta['width'],step_size)*meta['transform'][0]+meta['transform'][2] y_values=arange(x_offset,meta['height'],step_size)*meta['transform'][4]+meta['transform'][5] ncpath=self.ncfile_path(timestamp) config_NC(ncpath,timestamp,x_values,y_values).close() return out def cleanConfig(self,timestamp): os.unlink(self.ncfile_path(timestamp)) os.unlink(self.masterimg_path(timestamp)) os.unlink(self.mapfile_path(timestamp)) def feed(self, **kwargs): fileNS=Path(kwargs[\" NS_displacement \"]) fileEW=Path(kwargs[\" EW_displacement \"]) try: fileCORR=Path(kwargs[\" CORR \"]) except Exception as e: fileCORR=None timestamp=kwargs[\" timestamp \"] reftime=self.config_last_before(timestamp)['timestamp'] ncpath=self.ncfile_path(reftime) frames={\" ns \":None,\" ew \":None,\" corr \":None} frames[\" ns \"] = read_csv(fileNS,header=None) frames[\" ew \"] = read_csv(fileEW,header=None) if fileCORR is None: frames[\" corr \"] = DataFrame(full((frames[\" ns \"].shape),0.99)) else: frames[\" corr \" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , **frames ). close () return kwargs def cleanFeed ( timestamp ) : pass def data ( feat , timefrom = None , timeto = None , geom = None , **kwargs ) : return kwargs Ancestors (in MRO) hielen2.source.HielenSource abc.ABC Methods cleanConfig def cleanConfig ( self , timestamp ) View Source def cleanConfig ( self , timestamp ): os . unlink ( self . ncfile_path ( timestamp )) os . unlink ( self . masterimg_path ( timestamp )) os . unlink ( self . mapfile_path ( timestamp )) cleanFeed def cleanFeed ( timestamp ) View Source def cleanFeed ( timestamp ): pass config def config ( self , ** kwargs ) View Source def config ( self , **kwargs ) : out= {} timestamp = kwargs [ 'timestamp' ] # Temporary image path path_temp_image = Path ( kwargs [ \"master_image\" ]) self . filecache . mkdir () self . mapcache . mkdir () path_masterimg = self . masterimg_path ( timestamp ) path_mapfile = self . mapfile_path ( timestamp ) path_geo_ref = None try : name = str ( path_temp_image ). split ( \".\" ) # Temporary georef file path_temp_ref = Path ( kwargs [ \"geo_reference_file\" ]) with open ( path_temp_ref ) as trf : ''' trying to match reference file type (wld or aux.wml) if exists ''' try : float ( trf . readline ()) path_geo_ref = Path ( \".\" . join ([ * name [ 0 :- 1 ], \"wld\" ])) except Exception as e : path_geo_ref = Path ( \".\" . join ([ * name , \"aux\" , \"xml\" ])) path_temp_ref . replace ( path_geo_ref ) except Exception as e : pass # traceback . print_exc () try : ''' trying to define crs from income parameters ''' crs = rasterio . crs . CRS . from_string ( kwargs [ 'crs' ]) except Exception : crs = None try : with rasterio . open ( path_temp_image ) as src : meta = src . meta . copy () if crs is not None : meta [ 'crs' ] = crs meta [ 'transform' ] = list ( meta [ 'transform' ])[ 0 : 6 ] try : meta [ 'crs' ] = meta [ 'crs' ]. to_string () except AttributeError : meta [ 'crs' ] = None meta [ 'count' ] = 3 meta [ 'compress' ] ='lzw' meta [ 'dtype' ] ='uint8' if src . count == 1 : #trasformare da gray scale a rgb rgb = src . read ( 1 ). copy () rgb = ( rgb / 2 ** 16 ) * 255 rgb = rgb . astype ( 'uint8' ) rgb = [ rgb , rgb , rgb ] else : rgb = src . read () with rasterio . open ( path_masterimg , 'w' , **meta ) as dst : for i in range ( 0 , rgb . __ len__ ()) : dst . write ( rgb [ i ], i + 1 ) bands = dst . meta [ 'count' ] outcrs = dst . meta [ 'crs' ] outum = outcrs . linear_units # Master_image is ok . Macking mapfile setMFparams ( path_mapfile , bands = bands , crs = outcrs , um = outum ) except Exception as e : raise ValueError ( e ) x_offset = y_offset = kwargs [ 'window_size_change' ] or 0 step_size = kwargs [ 'step_size' ] or 1 out [ 'master_image' ] = magic . from_file ( str ( path_masterimg )) out [ 'timestamp' ] = timestamp out [ 'step_size' ] = step_size out [ 'window_size_change' ] = x_offset out [ 'meta' ] = meta x_values = arange ( y_offset , meta [ 'width' ], step_size ) * meta [ 'transform' ][ 0 ] + meta [ 'transform' ][ 2 ] y_values = arange ( x_offset , meta [ 'height' ], step_size ) * meta [ 'transform' ][ 4 ] + meta [ 'transform' ][ 5 ] ncpath = self . ncfile_path ( timestamp ) config_NC ( ncpath , timestamp , x_values , y_values ). close () return out config_last_before def config_last_before ( self , timestamp ) View Source def config_last_before ( self , timestamp ): c = self . getActionValues ( 'config' , slice ( None , timestamp )) try : return c [ - 1 ] except Exception as e : return None data def data ( feat , timefrom = None , timeto = None , geom = None , ** kwargs ) View Source def data ( feat , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs deleteActionValues def deleteActionValues ( self , action = None , timestamp = None ) View Source def deleteActionValues ( self , action = None , timestamp = None ) : out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ) : out =[ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \"{a.capitalize()}Schema\" self . __getattribute__ ( f \"clean{a.capitalize()}\" )( t ) except Exception as e : pass try : db [ 'actions' ][ self.uid,a,t ]= None except Exception as e : raise ValueError ( e ) return out execAction def execAction ( self , action , ** kwargs ) View Source def execAction ( self , action , ** kwargs ): aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass (). load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e ) feed def feed ( self , ** kwargs ) View Source def feed ( self , ** kwargs ): fileNS = Path ( kwargs [ \"NS_displacement\" ]) fileEW = Path ( kwargs [ \"EW_displacement\" ]) try : fileCORR = Path ( kwargs [ \"CORR\" ]) except Exception as e : fileCORR = None timestamp = kwargs [ \"timestamp\" ] reftime = self . config_last_before ( timestamp )[ 'timestamp' ] ncpath = self . ncfile_path ( reftime ) frames = { \"ns\" : None , \"ew\" : None , \"corr\" : None } frames [ \"ns\" ] = read_csv ( fileNS , header = None ) frames [ \"ew\" ] = read_csv ( fileEW , header = None ) if fileCORR is None : frames [ \"corr\" ] = DataFrame ( full (( frames [ \"ns\" ]. shape ), 0 . 99 )) else : frames [ \"corr\" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , ** frames ). close () return kwargs getActionSchema def getActionSchema ( self , action ) View Source def getActionSchema ( self , action ): return getActionSchema ( self . module , action ) getActionValues def getActionValues ( self , action = None , timestamp = None ) View Source def getActionValues ( self , action = None , timestamp = None ) : if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self.uid,action,timestamp ] if not isinstance ( out , list ) : out = [ out ] except KeyError : return [] return out hasher def hasher ( self , * args , ** kwargs ) View Source def hasher ( self , * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return re . sub ( \"[^\\d]\" , \"\" , h )[ 0 : 14 ] mapfile_path def mapfile_path ( self , timestamp ) View Source def mapfile_path ( self , timestamp ): return self . mapcache / f \"{self.hasher(timestamp)}.map\" masterimg_path def masterimg_path ( self , timestamp ) View Source def masterimg_path ( self , timestamp ): return self . mapcache / f \"{self.hasher(timestamp)}.tiff\" ncfile_path def ncfile_path ( self , timestamp ) View Source def ncfile_path ( self , timestamp ): return self . filecache / f \"{self.hasher(timestamp)}.nc\"","title":"Phm"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#module-hielen2extsource_photomonitoringphm","text":"View Source # coding=utf-8 from hielen2.source import HielenSource , ActionSchema , StringTime , sourceFactory from hielen2.utils import LocalFile from hielen2.maps.mapper import setMFparams import rasterio import magic import os import re from pathlib import Path from .struct import config_NC , feed_NC , generate_map from marshmallow import fields from shutil import rmtree from numpy import arange , full , zeros from pandas import read_csv , DataFrame , Series , DatetimeIndex import traceback class ConfigSchema ( ActionSchema ): \"\"\"'master_image' (required): the base image used as reference grid for elaboration. It can be any \\ image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based \\ on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. \\ (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected \\ for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent \\ elaboration images. It can be a standard world file (six lines text file) according to \\ http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to \\ https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm \\ (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones \\ possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' \\ (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the \\ 'geo_regerence_file' and/or embeded into the 'master_image' \"\"\" master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Number ( required = False , default = 1 , allow_none = True , as_string = False ) window_size_change = fields . Number ( required = False , default = 0 , allow_none = True , as_string = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True ) class FeedSchema ( ActionSchema ): \"\"\"reference_time: timestamp of the reference \"master_image\". If Null assumes last \\ \"master_image\" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info \"\"\" NS_displacement = LocalFile ( required = True , allow_none = False ) EW_displacement = LocalFile ( required = True , allow_none = False ) CORR = LocalFile ( required = False , allow_none = True ) class Source ( HielenSource ): ''' PhotoMonitoring source manager ''' def hasher ( self , * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return re . sub ( \"[^\\d]\" , \"\" , h )[ 0 : 14 ] def config_last_before ( self , timestamp ): c = self . getActionValues ( 'config' , slice ( None , timestamp )) try : return c [ - 1 ] except Exception as e : return None def ncfile_path ( self , timestamp ): return self . filecache / f \" { self . hasher ( timestamp ) } .nc\" def masterimg_path ( self , timestamp ): return self . mapcache / f \" { self . hasher ( timestamp ) } .tiff\" def mapfile_path ( self , timestamp ): return self . mapcache / f \" { self . hasher ( timestamp ) } .map\" def config ( self , ** kwargs ): out = {} timestamp = kwargs [ 'timestamp' ] #Temporary image path path_temp_image = Path ( kwargs [ \"master_image\" ]) self . filecache . mkdir () self . mapcache . mkdir () path_masterimg = self . masterimg_path ( timestamp ) path_mapfile = self . mapfile_path ( timestamp ) path_geo_ref = None try : name = str ( path_temp_image ) . split ( \".\" ) #Temporary georef file path_temp_ref = Path ( kwargs [ \"geo_reference_file\" ]) with open ( path_temp_ref ) as trf : ''' trying to match reference file type (wld or aux.wml) if exists ''' try : float ( trf . readline ()) path_geo_ref = Path ( \".\" . join ([ * name [ 0 : - 1 ], \"wld\" ])) except Exception as e : path_geo_ref = Path ( \".\" . join ([ * name , \"aux\" , \"xml\" ])) path_temp_ref . replace ( path_geo_ref ) except Exception as e : pass # traceback.print_exc() try : ''' trying to define crs from income parameters ''' crs = rasterio . crs . CRS . from_string ( kwargs [ 'crs' ]) except Exception : crs = None try : with rasterio . open ( path_temp_image ) as src : meta = src . meta . copy () if crs is not None : meta [ 'crs' ] = crs meta [ 'transform' ] = list ( meta [ 'transform' ])[ 0 : 6 ] try : meta [ 'crs' ] = meta [ 'crs' ] . to_string () except AttributeError : meta [ 'crs' ] = None meta [ 'count' ] = 3 meta [ 'compress' ] = 'lzw' meta [ 'dtype' ] = 'uint8' if src . count == 1 : #trasformare da gray scale a rgb rgb = src . read ( 1 ) . copy () rgb = ( rgb / 2 ** 16 ) * 255 rgb = rgb . astype ( 'uint8' ) rgb = [ rgb , rgb , rgb ] else : rgb = src . read () with rasterio . open ( path_masterimg , 'w' , ** meta ) as dst : for i in range ( 0 , rgb . __len__ ()): dst . write ( rgb [ i ], i + 1 ) bands = dst . meta [ 'count' ] outcrs = dst . meta [ 'crs' ] outum = outcrs . linear_units #Master_image is ok. Macking mapfile setMFparams ( path_mapfile , bands = bands , crs = outcrs , um = outum ) except Exception as e : raise ValueError ( e ) x_offset = y_offset = kwargs [ 'window_size_change' ] or 0 step_size = kwargs [ 'step_size' ] or 1 out [ 'master_image' ] = magic . from_file ( str ( path_masterimg )) out [ 'timestamp' ] = timestamp out [ 'step_size' ] = step_size out [ 'window_size_change' ] = x_offset out [ 'meta' ] = meta x_values = arange ( y_offset , meta [ 'width' ], step_size ) * meta [ 'transform' ][ 0 ] + meta [ 'transform' ][ 2 ] y_values = arange ( x_offset , meta [ 'height' ], step_size ) * meta [ 'transform' ][ 4 ] + meta [ 'transform' ][ 5 ] ncpath = self . ncfile_path ( timestamp ) config_NC ( ncpath , timestamp , x_values , y_values ) . close () return out def cleanConfig ( self , timestamp ): os . unlink ( self . ncfile_path ( timestamp )) os . unlink ( self . masterimg_path ( timestamp )) os . unlink ( self . mapfile_path ( timestamp )) def feed ( self , ** kwargs ): fileNS = Path ( kwargs [ \"NS_displacement\" ]) fileEW = Path ( kwargs [ \"EW_displacement\" ]) try : fileCORR = Path ( kwargs [ \"CORR\" ]) except Exception as e : fileCORR = None timestamp = kwargs [ \"timestamp\" ] reftime = self . config_last_before ( timestamp )[ 'timestamp' ] ncpath = self . ncfile_path ( reftime ) frames = { \"ns\" : None , \"ew\" : None , \"corr\" : None } frames [ \"ns\" ] = read_csv ( fileNS , header = None ) frames [ \"ew\" ] = read_csv ( fileEW , header = None ) if fileCORR is None : frames [ \"corr\" ] = DataFrame ( full (( frames [ \"ns\" ] . shape ), 0.99 )) else : frames [ \"corr\" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , ** frames ) . close () return kwargs def cleanFeed ( timestamp ): pass def data ( feat , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs def map ( feature , times = None , timeref = None , output = \"RV\" ): feature = sourceFactory ( feature ) timestamp = None if isinstance ( times , slice ): timestamp = times . stop else : timestamp = times conf = feature . config_last_before ( timestamp ) reftimestamp = timeref or conf [ 'timestamp' ] ncfile = feature . ncfile_path ( conf [ 'timestamp' ]) mapfile = feature . mapfile_path ( conf [ 'timestamp' ]) conf = conf [ 'value' ] h = conf [ 'meta' ][ 'height' ] w = conf [ 'meta' ][ 'width' ] wsc = int ( conf [ 'window_size_change' ]) imgout = zeros ([ h , w , 4 ]) timestamp , imagearray = generate_map ( ncfile , timestamp = timestamp , timeref = timeref , param = output , step_size = conf [ 'step_size' ]) imgout [ wsc :, wsc :] = imagearray [: h - wsc ,: w - wsc ] imgname = f \" { feature . hasher ( timestamp ) } _ { feature . hasher ( reftimestamp ) } _ { output } .tiff\" path_image = feature . mapcache / imgname conf [ 'meta' ][ 'count' ] = 3 conf [ 'meta' ][ 'compress' ] = 'LZW' conf [ 'meta' ][ 'driver' ] = 'GTiff' conf [ 'meta' ][ 'dtype' ] = 'uint8' imagearray = imagearray [: h - wsc ,: w - wsc , 0 : conf [ 'meta' ][ 'count' ]] with rasterio . open ( path_image , 'w' , ** conf [ 'meta' ]) as dst : for i in range ( 0 , conf [ 'meta' ][ 'count' ]): dst . write ( imagearray [:,:, i ], i + 1 ) url = [ \"http://localhost:8081/maps/mapserv\" , \"?map=\" + str ( mapfile ), \"&SERVICE=WMS&VERSION=1.1.1\" , \"&imgfile=\" + str ( imgname ), \"&layers=imglyr\" , \"&transparent=true\" , \"&format=image/png\" , \"&mode=tile\" , \"&tilemode=gmap\" , \"&tile=364+214+10\" , ] url = \"\" . join ( url ) ser = Series ([ url ], index = DatetimeIndex ([ timestamp ])) return ser","title":"Module hielen2.ext.source_photomonitoring.phm"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#map","text":"def map ( feature , times = None , timeref = None , output = 'RV' ) View Source def map ( feature , times = None , timeref = None , output = \"RV\" ) : feature = sourceFactory ( feature ) timestamp = None if isinstance ( times , slice ) : timestamp = times . stop else : timestamp = times conf = feature . config_last_before ( timestamp ) reftimestamp = timeref or conf [ 'timestamp' ] ncfile = feature . ncfile_path ( conf [ 'timestamp' ] ) mapfile = feature . mapfile_path ( conf [ 'timestamp' ] ) conf = conf [ 'value' ] h = conf [ 'meta' ][ 'height' ] w = conf [ 'meta' ][ 'width' ] wsc = int ( conf [ 'window_size_change' ] ) imgout = zeros ( [ h,w,4 ] ) timestamp , imagearray = generate_map ( ncfile , timestamp = timestamp , timeref = timeref , param = output , step_size = conf [ 'step_size' ] ) imgout [ wsc:,wsc: ]= imagearray [ :h-wsc,:w-wsc ] imgname = f \"{feature.hasher(timestamp)}_{feature.hasher(reftimestamp)}_{output}.tiff\" path_image = feature . mapcache / imgname conf [ 'meta' ][ 'count' ]= 3 conf [ 'meta' ][ 'compress' ]= 'LZW' conf [ 'meta' ][ 'driver' ]= 'GTiff' conf [ 'meta' ][ 'dtype' ]= 'uint8' imagearray = imagearray [ :h-wsc,:w-wsc,0:conf['meta' ][ 'count' ] ] with rasterio . open ( path_image , 'w' , ** conf [ 'meta' ] ) as dst : for i in range ( 0 , conf [ 'meta' ][ 'count' ] ) : dst . write ( imagearray [ :,:,i ] , i + 1 ) url =[ \"http://localhost:8081/maps/mapserv\", \"?map=\"+ str(mapfile), \"&SERVICE=WMS&VERSION=1.1.1\", \"&imgfile=\"+ str(imgname), \"&layers=imglyr\", \"&transparent=true\", \"&format=image/png\", \"&mode=tile\", \"&tilemode=gmap\", \"&tile=364+214+10\", ] url = \"\" . join ( url ) ser = Series ( [ url ] , index = DatetimeIndex ( [ timestamp ] )) return ser","title":"map"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#configschema","text":"class ConfigSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) 'master_image' (required): the base image used as reference grid for elaboration. It can be any image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent elaboration images. It can be a standard world file (six lines text file) according to http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the 'geo_regerence_file' and/or embeded into the 'master_image' View Source class ConfigSchema ( ActionSchema ): \"\"\"'master_image' (required): the base image used as reference grid for elaboration. It can be any \\ image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based \\ on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. \\ (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected \\ for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent \\ elaboration images. It can be a standard world file (six lines text file) according to \\ http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to \\ https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm \\ (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones \\ possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' \\ (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the \\ 'geo_regerence_file' and/or embeded into the 'master_image' \"\"\" master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Number ( required = False , default = 1 , allow_none = True , as_string = False ) window_size_change = fields . Number ( required = False , default = 0 , allow_none = True , as_string = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True )","title":"ConfigSchema"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#ancestors-in-mro","text":"hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#class-variables","text":"Meta OPTIONS_CLASS TYPE_MAPPING crs error_messages geo_reference_file master_image opts step_size window_size_change","title":"Class variables"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#static-methods","text":"","title":"Static methods"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#from_dict","text":"def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls","title":"from_dict"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#instance-variables","text":"dict_class set_class","title":"Instance variables"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#dump","text":"def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result","title":"dump"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#dumps","text":"def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs )","title":"dumps"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#get_attribute","text":"def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default )","title":"get_attribute"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#handle_error","text":"def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass","title":"handle_error"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#load","text":"def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True )","title":"load"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#loads","text":"def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown )","title":"loads"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#on_bind_field","text":"def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None","title":"on_bind_field"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#validate","text":"def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"validate"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#feedschema","text":"class FeedSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) reference_time: timestamp of the reference \"master_image\". If Null assumes last \"master_image\" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info View Source class FeedSchema ( ActionSchema ): \"\"\"reference_time: timestamp of the reference \" master_image \". If Null assumes last \\ \" master_image \" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info \"\"\" NS_displacement = LocalFile ( required = True , allow_none = False ) EW_displacement = LocalFile ( required = True , allow_none = False ) CORR = LocalFile ( required = False , allow_none = True )","title":"FeedSchema"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#ancestors-in-mro_1","text":"hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#class-variables_1","text":"CORR EW_displacement Meta NS_displacement OPTIONS_CLASS TYPE_MAPPING error_messages opts","title":"Class variables"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#from_dict_1","text":"def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls","title":"from_dict"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#instance-variables_1","text":"dict_class set_class","title":"Instance variables"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#methods_1","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#dump_1","text":"def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result","title":"dump"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#dumps_1","text":"def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs )","title":"dumps"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#get_attribute_1","text":"def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default )","title":"get_attribute"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#handle_error_1","text":"def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass","title":"handle_error"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#load_1","text":"def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True )","title":"load"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#loads_1","text":"def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown )","title":"loads"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#on_bind_field_1","text":"def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None","title":"on_bind_field"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#validate_1","text":"def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"validate"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#source","text":"class Source ( feature ) PhotoMonitoring source manager View Source class Source ( HielenSource ) : ''' PhotoMonitoring source manager ''' def hasher ( self , * args , **kwargs ) : h = [ * args ] h . extend ( list ( kwargs . values ())) h='' . join ([ str ( a ) for a in h ]) return re . sub ( \" [ ^\\d ] \",\"\",h)[0:14] def config_last_before(self,timestamp): c=self.getActionValues('config',slice(None,timestamp)) try: return c[-1] except Exception as e: return None def ncfile_path(self,timestamp): return self.filecache / f\" { self . hasher ( timestamp )}. nc \" def masterimg_path(self,timestamp): return self.mapcache / f\" { self . hasher ( timestamp )}. tiff \" def mapfile_path(self,timestamp): return self.mapcache / f\" { self . hasher ( timestamp )}. map \" def config(self, **kwargs): out={} timestamp=kwargs['timestamp'] #Temporary image path path_temp_image=Path(kwargs[\" master_image \"]) self.filecache.mkdir() self.mapcache.mkdir() path_masterimg=self.masterimg_path(timestamp) path_mapfile=self.mapfile_path(timestamp) path_geo_ref=None try: name=str(path_temp_image).split(\" . \") #Temporary georef file path_temp_ref=Path(kwargs[\" geo_reference_file \"]) with open(path_temp_ref) as trf: ''' trying to match reference file type (wld or aux.wml) if exists ''' try: float(trf.readline()) path_geo_ref=Path(\" . \".join([*name[0:-1],\" wld \"])) except Exception as e: path_geo_ref=Path(\" . \".join([*name,\" aux \",\" xml \"])) path_temp_ref.replace(path_geo_ref) except Exception as e: pass # traceback.print_exc() try: ''' trying to define crs from income parameters ''' crs=rasterio.crs.CRS.from_string(kwargs['crs']) except Exception: crs=None try: with rasterio.open(path_temp_image) as src: meta = src.meta.copy() if crs is not None: meta['crs']=crs meta['transform']=list(meta['transform'])[0:6] try: meta['crs']=meta['crs'].to_string() except AttributeError: meta['crs']=None meta['count']=3 meta['compress']='lzw' meta['dtype']='uint8' if src.count == 1: #trasformare da gray scale a rgb rgb = src.read(1).copy() rgb = (rgb/2**16)*255 rgb = rgb.astype('uint8') rgb = [rgb,rgb,rgb] else: rgb = src.read() with rasterio.open(path_masterimg, 'w', **meta) as dst: for i in range(0, rgb.__len__()): dst.write(rgb[i],i+1) bands=dst.meta['count'] outcrs=dst.meta['crs'] outum=outcrs.linear_units #Master_image is ok. Macking mapfile setMFparams(path_mapfile, bands=bands, crs=outcrs, um=outum) except Exception as e: raise ValueError(e) x_offset=y_offset=kwargs['window_size_change'] or 0 step_size=kwargs['step_size'] or 1 out['master_image']=magic.from_file(str(path_masterimg)) out['timestamp']=timestamp out['step_size']=step_size out['window_size_change']=x_offset out['meta']=meta x_values=arange(y_offset,meta['width'],step_size)*meta['transform'][0]+meta['transform'][2] y_values=arange(x_offset,meta['height'],step_size)*meta['transform'][4]+meta['transform'][5] ncpath=self.ncfile_path(timestamp) config_NC(ncpath,timestamp,x_values,y_values).close() return out def cleanConfig(self,timestamp): os.unlink(self.ncfile_path(timestamp)) os.unlink(self.masterimg_path(timestamp)) os.unlink(self.mapfile_path(timestamp)) def feed(self, **kwargs): fileNS=Path(kwargs[\" NS_displacement \"]) fileEW=Path(kwargs[\" EW_displacement \"]) try: fileCORR=Path(kwargs[\" CORR \"]) except Exception as e: fileCORR=None timestamp=kwargs[\" timestamp \"] reftime=self.config_last_before(timestamp)['timestamp'] ncpath=self.ncfile_path(reftime) frames={\" ns \":None,\" ew \":None,\" corr \":None} frames[\" ns \"] = read_csv(fileNS,header=None) frames[\" ew \"] = read_csv(fileEW,header=None) if fileCORR is None: frames[\" corr \"] = DataFrame(full((frames[\" ns \"].shape),0.99)) else: frames[\" corr \" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , **frames ). close () return kwargs def cleanFeed ( timestamp ) : pass def data ( feat , timefrom = None , timeto = None , geom = None , **kwargs ) : return kwargs","title":"Source"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#ancestors-in-mro_2","text":"hielen2.source.HielenSource abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#methods_2","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#cleanconfig","text":"def cleanConfig ( self , timestamp ) View Source def cleanConfig ( self , timestamp ): os . unlink ( self . ncfile_path ( timestamp )) os . unlink ( self . masterimg_path ( timestamp )) os . unlink ( self . mapfile_path ( timestamp ))","title":"cleanConfig"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#cleanfeed","text":"def cleanFeed ( timestamp ) View Source def cleanFeed ( timestamp ): pass","title":"cleanFeed"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#config","text":"def config ( self , ** kwargs ) View Source def config ( self , **kwargs ) : out= {} timestamp = kwargs [ 'timestamp' ] # Temporary image path path_temp_image = Path ( kwargs [ \"master_image\" ]) self . filecache . mkdir () self . mapcache . mkdir () path_masterimg = self . masterimg_path ( timestamp ) path_mapfile = self . mapfile_path ( timestamp ) path_geo_ref = None try : name = str ( path_temp_image ). split ( \".\" ) # Temporary georef file path_temp_ref = Path ( kwargs [ \"geo_reference_file\" ]) with open ( path_temp_ref ) as trf : ''' trying to match reference file type (wld or aux.wml) if exists ''' try : float ( trf . readline ()) path_geo_ref = Path ( \".\" . join ([ * name [ 0 :- 1 ], \"wld\" ])) except Exception as e : path_geo_ref = Path ( \".\" . join ([ * name , \"aux\" , \"xml\" ])) path_temp_ref . replace ( path_geo_ref ) except Exception as e : pass # traceback . print_exc () try : ''' trying to define crs from income parameters ''' crs = rasterio . crs . CRS . from_string ( kwargs [ 'crs' ]) except Exception : crs = None try : with rasterio . open ( path_temp_image ) as src : meta = src . meta . copy () if crs is not None : meta [ 'crs' ] = crs meta [ 'transform' ] = list ( meta [ 'transform' ])[ 0 : 6 ] try : meta [ 'crs' ] = meta [ 'crs' ]. to_string () except AttributeError : meta [ 'crs' ] = None meta [ 'count' ] = 3 meta [ 'compress' ] ='lzw' meta [ 'dtype' ] ='uint8' if src . count == 1 : #trasformare da gray scale a rgb rgb = src . read ( 1 ). copy () rgb = ( rgb / 2 ** 16 ) * 255 rgb = rgb . astype ( 'uint8' ) rgb = [ rgb , rgb , rgb ] else : rgb = src . read () with rasterio . open ( path_masterimg , 'w' , **meta ) as dst : for i in range ( 0 , rgb . __ len__ ()) : dst . write ( rgb [ i ], i + 1 ) bands = dst . meta [ 'count' ] outcrs = dst . meta [ 'crs' ] outum = outcrs . linear_units # Master_image is ok . Macking mapfile setMFparams ( path_mapfile , bands = bands , crs = outcrs , um = outum ) except Exception as e : raise ValueError ( e ) x_offset = y_offset = kwargs [ 'window_size_change' ] or 0 step_size = kwargs [ 'step_size' ] or 1 out [ 'master_image' ] = magic . from_file ( str ( path_masterimg )) out [ 'timestamp' ] = timestamp out [ 'step_size' ] = step_size out [ 'window_size_change' ] = x_offset out [ 'meta' ] = meta x_values = arange ( y_offset , meta [ 'width' ], step_size ) * meta [ 'transform' ][ 0 ] + meta [ 'transform' ][ 2 ] y_values = arange ( x_offset , meta [ 'height' ], step_size ) * meta [ 'transform' ][ 4 ] + meta [ 'transform' ][ 5 ] ncpath = self . ncfile_path ( timestamp ) config_NC ( ncpath , timestamp , x_values , y_values ). close () return out","title":"config"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#config_last_before","text":"def config_last_before ( self , timestamp ) View Source def config_last_before ( self , timestamp ): c = self . getActionValues ( 'config' , slice ( None , timestamp )) try : return c [ - 1 ] except Exception as e : return None","title":"config_last_before"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#data","text":"def data ( feat , timefrom = None , timeto = None , geom = None , ** kwargs ) View Source def data ( feat , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs","title":"data"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#deleteactionvalues","text":"def deleteActionValues ( self , action = None , timestamp = None ) View Source def deleteActionValues ( self , action = None , timestamp = None ) : out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ) : out =[ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \"{a.capitalize()}Schema\" self . __getattribute__ ( f \"clean{a.capitalize()}\" )( t ) except Exception as e : pass try : db [ 'actions' ][ self.uid,a,t ]= None except Exception as e : raise ValueError ( e ) return out","title":"deleteActionValues"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#execaction","text":"def execAction ( self , action , ** kwargs ) View Source def execAction ( self , action , ** kwargs ): aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass (). load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e )","title":"execAction"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#feed","text":"def feed ( self , ** kwargs ) View Source def feed ( self , ** kwargs ): fileNS = Path ( kwargs [ \"NS_displacement\" ]) fileEW = Path ( kwargs [ \"EW_displacement\" ]) try : fileCORR = Path ( kwargs [ \"CORR\" ]) except Exception as e : fileCORR = None timestamp = kwargs [ \"timestamp\" ] reftime = self . config_last_before ( timestamp )[ 'timestamp' ] ncpath = self . ncfile_path ( reftime ) frames = { \"ns\" : None , \"ew\" : None , \"corr\" : None } frames [ \"ns\" ] = read_csv ( fileNS , header = None ) frames [ \"ew\" ] = read_csv ( fileEW , header = None ) if fileCORR is None : frames [ \"corr\" ] = DataFrame ( full (( frames [ \"ns\" ]. shape ), 0 . 99 )) else : frames [ \"corr\" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , ** frames ). close () return kwargs","title":"feed"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#getactionschema","text":"def getActionSchema ( self , action ) View Source def getActionSchema ( self , action ): return getActionSchema ( self . module , action )","title":"getActionSchema"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#getactionvalues","text":"def getActionValues ( self , action = None , timestamp = None ) View Source def getActionValues ( self , action = None , timestamp = None ) : if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self.uid,action,timestamp ] if not isinstance ( out , list ) : out = [ out ] except KeyError : return [] return out","title":"getActionValues"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#hasher","text":"def hasher ( self , * args , ** kwargs ) View Source def hasher ( self , * args , ** kwargs ): h = [ * args ] h . extend ( list ( kwargs . values ())) h = '' . join ([ str ( a ) for a in h ]) return re . sub ( \"[^\\d]\" , \"\" , h )[ 0 : 14 ]","title":"hasher"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#mapfile_path","text":"def mapfile_path ( self , timestamp ) View Source def mapfile_path ( self , timestamp ): return self . mapcache / f \"{self.hasher(timestamp)}.map\"","title":"mapfile_path"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#masterimg_path","text":"def masterimg_path ( self , timestamp ) View Source def masterimg_path ( self , timestamp ): return self . mapcache / f \"{self.hasher(timestamp)}.tiff\"","title":"masterimg_path"},{"location":"reference/hielen2/ext/source_photomonitoring/phm/#ncfile_path","text":"def ncfile_path ( self , timestamp ) View Source def ncfile_path ( self , timestamp ): return self . filecache / f \"{self.hasher(timestamp)}.nc\"","title":"ncfile_path"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/","text":"Module hielen2.ext.source_photomonitoring.phm_ok View Source # coding=utf-8 from hielen2.source import HielenSource , ActionSchema , StringTime from hielen2.utils import LocalFile import rasterio import magic import os import re from pathlib import Path from .struct import config_NC , feed_NC , generate_map from marshmallow import fields from shutil import rmtree from numpy import arange , full , zeros from pandas import read_csv , DataFrame import traceback class ConfigSchema ( ActionSchema ): \"\"\"'master_image' (required): the base image used as reference grid for elaboration. It can be any \\ image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based \\ on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. \\ (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected \\ for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent \\ elaboration images. It can be a standard world file (six lines text file) according to \\ http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to \\ https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm \\ (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones \\ possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' \\ (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the \\ 'geo_regerence_file' and/or embeded into the 'master_image' \"\"\" master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Number ( required = False , default = 1 , allow_none = True , as_string = False ) window_size_change = fields . Number ( required = False , default = 0 , allow_none = True , as_string = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True ) class FeedSchema ( ActionSchema ): \"\"\"reference_time: timestamp of the reference \"master_image\". If Null assumes last \\ \"master_image\" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info \"\"\" NS_displacement = LocalFile ( required = True , allow_none = False ) EW_displacement = LocalFile ( required = True , allow_none = False ) CORR = LocalFile ( required = False , allow_none = True ) class Source ( HielenSource ): ''' PhotoMonitoring source manager ''' def config_cache_path ( self , timestamp ): return self . filecache / self . filecache . hasher ( timestamp ) def config_last_before ( self , timestamp ): c = self . getActionValues ( 'config' , slice ( None , timestamp )) try : return c [ - 1 ] except Exception as e : return None def config_nc_file_path ( self , timestamp ): return self . config_cache_path ( timestamp ) / \"master.nc\" def config_master_image_path ( self , timestamp ): return self . config_cache_path ( timestamp ) / \"master.img\" def config ( self , ** kwargs ): out = {} timestamp = kwargs [ 'timestamp' ] #Temporary image path path_temp_image = Path ( kwargs [ \"master_image\" ]) self . filecache . mkdir ( self . config_cache_path ( timestamp ) ) path_master_image = self . config_master_image_path ( timestamp ) path_geo_ref = None try : name = str ( path_temp_image ) . split ( \".\" ) #Temporary georef file path_temp_ref = Path ( kwargs [ \"geo_reference_file\" ]) with open ( path_temp_ref ) as trf : ''' trying to match reference file type (wld or aux.wml) if exists ''' try : float ( trf . readline ()) path_geo_ref = Path ( \".\" . join ([ * name [ 0 : - 1 ], \"wld\" ])) except Exception as e : path_geo_ref = Path ( \".\" . join ([ * name , \"aux\" , \"xml\" ])) path_temp_ref . replace ( path_geo_ref ) except Exception as e : pass # traceback.print_exc() try : ''' trying to define crs from income parameters ''' crs = rasterio . crs . CRS . from_string ( kwargs [ 'crs' ]) except Exception : crs = None try : with rasterio . open ( path_temp_image ) as src : meta = src . meta . copy () if crs is not None : meta [ 'crs' ] = crs meta [ 'transform' ] = list ( meta [ 'transform' ])[ 0 : 6 ] try : meta [ 'crs' ] = meta [ 'crs' ] . to_string () except AttributeError : meta [ 'crs' ] = None meta [ 'count' ] = 3 meta [ 'compress' ] = 'lzw' meta [ 'dtype' ] = 'uint8' if src . count == 1 : #trasformare da gray scale a rgb rgb = src . read ( 1 ) . copy () rgb = ( rgb / 2 ** 16 ) * 255 rgb = rgb . astype ( 'uint8' ) rgb = [ rgb , rgb , rgb ] else : rgb = src . read () with rasterio . open ( path_master_image , 'w' , ** meta ) as dst : for i in range ( 0 , rgb . __len__ ()): dst . write ( rgb [ i ], i + 1 ) except Exception as e : raise ValueError ( e ) out [ 'master_image' ] = magic . from_file ( str ( path_master_image )) out [ 'timestamp' ] = timestamp out [ 'step_size' ] = kwargs [ 'step_size' ] out [ 'window_size_change' ] = kwargs [ 'window_size_change' ] out [ 'meta' ] = meta x_offset = kwargs [ 'window_size_change' ] y_offset = kwargs [ 'window_size_change' ] step_size = kwargs [ 'step_size' ] x_values = arange ( y_offset , meta [ 'width' ], step_size ) * meta [ 'transform' ][ 0 ] + meta [ 'transform' ][ 2 ] y_values = arange ( x_offset , meta [ 'height' ], step_size ) * meta [ 'transform' ][ 4 ] + meta [ 'transform' ][ 5 ] ncpath = self . config_nc_file_path ( timestamp ) config_NC ( ncpath , timestamp , x_values , y_values ) . close () return out def cleanConfig ( self , timestamp ): p = self . config_cache_path ( timestamp ) #TODO delete sub actions rmtree ( p ) def feed ( self , ** kwargs ): fileNS = Path ( kwargs [ \"NS_displacement\" ]) fileEW = Path ( kwargs [ \"EW_displacement\" ]) try : fileCORR = Path ( kwargs [ \"CORR\" ]) except Exception as e : fileCORR = None timestamp = kwargs [ \"timestamp\" ] reftime = self . config_last_before ( timestamp )[ 'timestamp' ] ncpath = self . config_nc_file_path ( reftime ) frames = { \"ns\" : None , \"ew\" : None , \"corr\" : None } frames [ \"ns\" ] = read_csv ( fileNS , header = None ) frames [ \"ew\" ] = read_csv ( fileEW , header = None ) if fileCORR is None : frames [ \"corr\" ] = DataFrame ( full (( frames [ \"ns\" ] . shape ), 0.99 )) else : frames [ \"corr\" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , ** frames ) . close () return kwargs def cleanFeed ( timestamp ): pass def map ( self , timestamp = None , timeref = None , param = \"RV\" ): conf = self . config_last_before ( timestamp ) ncfile = self . config_nc_file_path ( conf [ 'timestamp' ]) conf = conf [ 'value' ] path_image = self . config_cache_path ( conf [ 'timestamp' ]) / 'img.tiff' h = conf [ 'meta' ][ 'height' ] w = conf [ 'meta' ][ 'width' ] wsc = int ( conf [ 'window_size_change' ]) imgout = zeros ([ h , w , 4 ]) imagearray = generate_map ( ncfile , timestamp , timeref , param = param , step_size = conf [ 'step_size' ]) imgout [ wsc :, wsc :] = imagearray [: h - wsc ,: w - wsc ] conf [ 'meta' ][ 'count' ] = 3 conf [ 'meta' ][ 'compress' ] = 'LZW' conf [ 'meta' ][ 'driver' ] = 'GTiff' conf [ 'meta' ][ 'dtype' ] = 'uint8' imagearray = imagearray [: h - wsc ,: w - wsc , 0 : conf [ 'meta' ][ 'count' ]] with rasterio . open ( path_image , 'w' , ** conf [ 'meta' ]) as dst : for i in range ( 0 , conf [ 'meta' ][ 'count' ]): dst . write ( imagearray [:,:, i ], i + 1 ) def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs Classes ConfigSchema class ConfigSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) 'master_image' (required): the base image used as reference grid for elaboration. It can be any image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent elaboration images. It can be a standard world file (six lines text file) according to http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the 'geo_regerence_file' and/or embeded into the 'master_image' View Source class ConfigSchema ( ActionSchema ): \"\"\"'master_image' (required): the base image used as reference grid for elaboration. It can be any \\ image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based \\ on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. \\ (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected \\ for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent \\ elaboration images. It can be a standard world file (six lines text file) according to \\ http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to \\ https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm \\ (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones \\ possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' \\ (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the \\ 'geo_regerence_file' and/or embeded into the 'master_image' \"\"\" master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Number ( required = False , default = 1 , allow_none = True , as_string = False ) window_size_change = fields . Number ( required = False , default = 0 , allow_none = True , as_string = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True ) Ancestors (in MRO) hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC Class variables Meta OPTIONS_CLASS TYPE_MAPPING crs error_messages geo_reference_file master_image opts step_size window_size_change Static methods from_dict def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls Instance variables dict_class set_class Methods dump def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result dumps def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs ) get_attribute def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default ) handle_error def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass load def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True ) loads def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown ) on_bind_field def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None validate def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {} FeedSchema class FeedSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) reference_time: timestamp of the reference \"master_image\". If Null assumes last \"master_image\" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info View Source class FeedSchema ( ActionSchema ): \"\"\"reference_time: timestamp of the reference \" master_image \". If Null assumes last \\ \" master_image \" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info \"\"\" NS_displacement = LocalFile ( required = True , allow_none = False ) EW_displacement = LocalFile ( required = True , allow_none = False ) CORR = LocalFile ( required = False , allow_none = True ) Ancestors (in MRO) hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC Class variables CORR EW_displacement Meta NS_displacement OPTIONS_CLASS TYPE_MAPPING error_messages opts Static methods from_dict def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls Instance variables dict_class set_class Methods dump def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result dumps def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs ) get_attribute def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default ) handle_error def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass load def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True ) loads def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown ) on_bind_field def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None validate def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {} Source class Source ( feature ) PhotoMonitoring source manager View Source class Source ( HielenSource ) : ''' PhotoMonitoring source manager ''' def config_cache_path ( self , timestamp ) : return self . filecache / self . filecache . hasher ( timestamp ) def config_last_before ( self , timestamp ) : c = self . getActionValues ( 'config' , slice ( None , timestamp )) try : return c [ - 1 ] except Exception as e : return None def config_nc_file_path ( self , timestamp ) : return self . config_cache_path ( timestamp ) / \"master.nc\" def config_master_image_path ( self , timestamp ) : return self . config_cache_path ( timestamp ) / \"master.img\" def config ( self , **kwargs ) : out= {} timestamp = kwargs [ 'timestamp' ] # Temporary image path path_temp_image = Path ( kwargs [ \"master_image\" ]) self . filecache . mkdir ( self . config_cache_path ( timestamp ) ) path_master_image = self . config_master_image_path ( timestamp ) path_geo_ref = None try : name = str ( path_temp_image ). split ( \".\" ) # Temporary georef file path_temp_ref = Path ( kwargs [ \"geo_reference_file\" ]) with open ( path_temp_ref ) as trf : ''' trying to match reference file type (wld or aux.wml) if exists ''' try : float ( trf . readline ()) path_geo_ref = Path ( \".\" . join ([ * name [ 0 :- 1 ], \"wld\" ])) except Exception as e : path_geo_ref = Path ( \".\" . join ([ * name , \"aux\" , \"xml\" ])) path_temp_ref . replace ( path_geo_ref ) except Exception as e : pass # traceback . print_exc () try : ''' trying to define crs from income parameters ''' crs = rasterio . crs . CRS . from_string ( kwargs [ 'crs' ]) except Exception : crs = None try : with rasterio . open ( path_temp_image ) as src : meta = src . meta . copy () if crs is not None : meta [ 'crs' ] = crs meta [ 'transform' ] = list ( meta [ 'transform' ])[ 0 : 6 ] try : meta [ 'crs' ] = meta [ 'crs' ]. to_string () except AttributeError : meta [ 'crs' ] = None meta [ 'count' ] = 3 meta [ 'compress' ] ='lzw' meta [ 'dtype' ] ='uint8' if src . count == 1 : #trasformare da gray scale a rgb rgb = src . read ( 1 ). copy () rgb = ( rgb / 2 ** 16 ) * 255 rgb = rgb . astype ( 'uint8' ) rgb = [ rgb , rgb , rgb ] else : rgb = src . read () with rasterio . open ( path_master_image , 'w' , **meta ) as dst : for i in range ( 0 , rgb . __ len__ ()) : dst . write ( rgb [ i ], i + 1 ) except Exception as e : raise ValueError ( e ) out [ 'master_image' ] = magic . from_file ( str ( path_master_image )) out [ 'timestamp' ] = timestamp out [ 'step_size' ] = kwargs [ 'step_size' ] out [ 'window_size_change' ] = kwargs [ 'window_size_change' ] out [ 'meta' ] = meta x_offset = kwargs [ 'window_size_change' ] y_offset = kwargs [ 'window_size_change' ] step_size = kwargs [ 'step_size' ] x_values = arange ( y_offset , meta [ 'width' ], step_size ) * meta [ 'transform' ][ 0 ] + meta [ 'transform' ][ 2 ] y_values = arange ( x_offset , meta [ 'height' ], step_size ) * meta [ 'transform' ][ 4 ] + meta [ 'transform' ][ 5 ] ncpath = self . config_nc_file_path ( timestamp ) config_NC ( ncpath , timestamp , x_values , y_values ). close () return out def cleanConfig ( self , timestamp ) : p = self . config_cache_path ( timestamp ) # TODO delete sub actions rmtree ( p ) def feed ( self , **kwargs ) : fileNS = Path ( kwargs [ \"NS_displacement\" ]) fileEW = Path ( kwargs [ \"EW_displacement\" ]) try : fileCORR = Path ( kwargs [ \"CORR\" ]) except Exception as e : fileCORR = None timestamp = kwargs [ \"timestamp\" ] reftime = self . config_last_before ( timestamp )[ 'timestamp' ] ncpath = self . config_nc_file_path ( reftime ) frames= { \"ns\" : None , \"ew\" : None , \"corr\" : None } frames [ \"ns\" ] = read_csv ( fileNS , header = None ) frames [ \"ew\" ] = read_csv ( fileEW , header = None ) if fileCORR is None : frames [ \"corr\" ] = DataFrame ( full (( frames [ \"ns\" ]. shape ), 0.99 )) else : frames [ \"corr\" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , **frames ). close () return kwargs def cleanFeed ( timestamp ) : pass def map ( self , timestamp = None , timeref = None , param= \"RV\" ) : conf = self . config_last_before ( timestamp ) ncfile = self . config_nc_file_path ( conf [ 'timestamp' ]) conf = conf [ 'value' ] path_image = self . config_cache_path ( conf [ 'timestamp' ]) /'img.tiff' h = conf [ 'meta' ][ 'height' ] w = conf [ 'meta' ][ 'width' ] wsc = int ( conf [ 'window_size_change' ]) imgout = zeros ([ h , w , 4 ]) imagearray = generate_map ( ncfile , timestamp , timeref , param = param , step_size = conf [ 'step_size' ]) imgout [ wsc :, wsc :] = imagearray [ :h - wsc , :w - wsc ] conf [ 'meta' ][ 'count' ] = 3 conf [ 'meta' ][ 'compress' ] ='LZW' conf [ 'meta' ][ 'driver' ] ='GTiff' conf [ 'meta' ][ 'dtype' ] ='uint8' imagearray = imagearray [ :h - wsc , :w - wsc , 0 :conf [ 'meta' ][ 'count' ]] with rasterio . open ( path_image , 'w' , **conf [ 'meta' ]) as dst : for i in range ( 0 , conf [ 'meta' ][ 'count' ]) : dst . write ( imagearray [ : , : , i ], i + 1 ) def data ( self , timefrom = None , timeto = None , geom = None , **kwargs ) : return kwargs Ancestors (in MRO) hielen2.source.HielenSource abc.ABC Methods cleanConfig def cleanConfig ( self , timestamp ) View Source def cleanConfig ( self , timestamp ): p = self . config_cache_path ( timestamp ) # TODO delete sub actions rmtree ( p ) cleanFeed def cleanFeed ( timestamp ) View Source def cleanFeed ( timestamp ): pass config def config ( self , ** kwargs ) View Source def config ( self , **kwargs ) : out= {} timestamp = kwargs [ 'timestamp' ] # Temporary image path path_temp_image = Path ( kwargs [ \"master_image\" ]) self . filecache . mkdir ( self . config_cache_path ( timestamp ) ) path_master_image = self . config_master_image_path ( timestamp ) path_geo_ref = None try : name = str ( path_temp_image ). split ( \".\" ) # Temporary georef file path_temp_ref = Path ( kwargs [ \"geo_reference_file\" ]) with open ( path_temp_ref ) as trf : ''' trying to match reference file type (wld or aux.wml) if exists ''' try : float ( trf . readline ()) path_geo_ref = Path ( \".\" . join ([ * name [ 0 :- 1 ], \"wld\" ])) except Exception as e : path_geo_ref = Path ( \".\" . join ([ * name , \"aux\" , \"xml\" ])) path_temp_ref . replace ( path_geo_ref ) except Exception as e : pass # traceback . print_exc () try : ''' trying to define crs from income parameters ''' crs = rasterio . crs . CRS . from_string ( kwargs [ 'crs' ]) except Exception : crs = None try : with rasterio . open ( path_temp_image ) as src : meta = src . meta . copy () if crs is not None : meta [ 'crs' ] = crs meta [ 'transform' ] = list ( meta [ 'transform' ])[ 0 : 6 ] try : meta [ 'crs' ] = meta [ 'crs' ]. to_string () except AttributeError : meta [ 'crs' ] = None meta [ 'count' ] = 3 meta [ 'compress' ] ='lzw' meta [ 'dtype' ] ='uint8' if src . count == 1 : #trasformare da gray scale a rgb rgb = src . read ( 1 ). copy () rgb = ( rgb / 2 ** 16 ) * 255 rgb = rgb . astype ( 'uint8' ) rgb = [ rgb , rgb , rgb ] else : rgb = src . read () with rasterio . open ( path_master_image , 'w' , **meta ) as dst : for i in range ( 0 , rgb . __ len__ ()) : dst . write ( rgb [ i ], i + 1 ) except Exception as e : raise ValueError ( e ) out [ 'master_image' ] = magic . from_file ( str ( path_master_image )) out [ 'timestamp' ] = timestamp out [ 'step_size' ] = kwargs [ 'step_size' ] out [ 'window_size_change' ] = kwargs [ 'window_size_change' ] out [ 'meta' ] = meta x_offset = kwargs [ 'window_size_change' ] y_offset = kwargs [ 'window_size_change' ] step_size = kwargs [ 'step_size' ] x_values = arange ( y_offset , meta [ 'width' ], step_size ) * meta [ 'transform' ][ 0 ] + meta [ 'transform' ][ 2 ] y_values = arange ( x_offset , meta [ 'height' ], step_size ) * meta [ 'transform' ][ 4 ] + meta [ 'transform' ][ 5 ] ncpath = self . config_nc_file_path ( timestamp ) config_NC ( ncpath , timestamp , x_values , y_values ). close () return out config_cache_path def config_cache_path ( self , timestamp ) View Source def config_cache_path ( self , timestamp ): return self . filecache / self . filecache . hasher ( timestamp ) config_last_before def config_last_before ( self , timestamp ) View Source def config_last_before ( self , timestamp ): c = self . getActionValues ( 'config' , slice ( None , timestamp )) try : return c [ - 1 ] except Exception as e : return None config_master_image_path def config_master_image_path ( self , timestamp ) View Source def config_master_image_path ( self , timestamp ): return self . config_cache_path ( timestamp ) / \"master.img\" config_nc_file_path def config_nc_file_path ( self , timestamp ) View Source def config_nc_file_path ( self , timestamp ): return self . config_cache_path ( timestamp ) / \"master.nc\" data def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ) View Source def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs deleteActionValues def deleteActionValues ( self , action = None , timestamp = None ) View Source def deleteActionValues ( self , action = None , timestamp = None ) : out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ) : out =[ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \"{a.capitalize()}Schema\" self . __getattribute__ ( f \"clean{a.capitalize()}\" )( t ) except Exception as e : pass try : db [ 'actions' ][ self.uid,a,t ]= None except Exception as e : raise ValueError ( e ) return out execAction def execAction ( self , action , ** kwargs ) View Source def execAction ( self , action , ** kwargs ): aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass (). load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e ) feed def feed ( self , ** kwargs ) View Source def feed ( self , ** kwargs ): fileNS = Path ( kwargs [ \"NS_displacement\" ]) fileEW = Path ( kwargs [ \"EW_displacement\" ]) try : fileCORR = Path ( kwargs [ \"CORR\" ]) except Exception as e : fileCORR = None timestamp = kwargs [ \"timestamp\" ] reftime = self . config_last_before ( timestamp )[ 'timestamp' ] ncpath = self . config_nc_file_path ( reftime ) frames = { \"ns\" : None , \"ew\" : None , \"corr\" : None } frames [ \"ns\" ] = read_csv ( fileNS , header = None ) frames [ \"ew\" ] = read_csv ( fileEW , header = None ) if fileCORR is None : frames [ \"corr\" ] = DataFrame ( full (( frames [ \"ns\" ]. shape ), 0 . 99 )) else : frames [ \"corr\" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , ** frames ). close () return kwargs getActionSchema def getActionSchema ( self , action ) View Source def getActionSchema ( self , action ): return getActionSchema ( self . module , action ) getActionValues def getActionValues ( self , action = None , timestamp = None ) View Source def getActionValues ( self , action = None , timestamp = None ) : if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self.uid,action,timestamp ] if not isinstance ( out , list ) : out = [ out ] except KeyError : return [] return out map def map ( self , timestamp = None , timeref = None , param = 'RV' ) View Source def map ( self , timestamp = None , timeref = None , param = \"RV\" ): conf = self . config_last_before ( timestamp ) ncfile = self . config_nc_file_path ( conf [ 'timestamp' ]) conf = conf [ 'value' ] path_image = self . config_cache_path ( conf [ 'timestamp' ]) / 'img.tiff' h = conf [ 'meta' ][ 'height' ] w = conf [ 'meta' ][ 'width' ] wsc = int ( conf [ 'window_size_change' ]) imgout = zeros ([ h , w , 4 ]) imagearray = generate_map ( ncfile , timestamp , timeref , param = param , step_size = conf [ 'step_size' ]) imgout [ wsc :, wsc :] = imagearray [: h - wsc ,: w - wsc ] conf [ 'meta' ][ 'count' ] = 3 conf [ 'meta' ][ 'compress' ] = 'LZW' conf [ 'meta' ][ 'driver' ] = 'GTiff' conf [ 'meta' ][ 'dtype' ] = 'uint8' imagearray = imagearray [: h - wsc ,: w - wsc , 0 : conf [ 'meta' ][ 'count' ]] with rasterio . open ( path_image , 'w' , ** conf [ 'meta' ]) as dst : for i in range ( 0 , conf [ 'meta' ][ 'count' ]): dst . write ( imagearray [:,:, i ], i + 1 )","title":"Phm Ok"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#module-hielen2extsource_photomonitoringphm_ok","text":"View Source # coding=utf-8 from hielen2.source import HielenSource , ActionSchema , StringTime from hielen2.utils import LocalFile import rasterio import magic import os import re from pathlib import Path from .struct import config_NC , feed_NC , generate_map from marshmallow import fields from shutil import rmtree from numpy import arange , full , zeros from pandas import read_csv , DataFrame import traceback class ConfigSchema ( ActionSchema ): \"\"\"'master_image' (required): the base image used as reference grid for elaboration. It can be any \\ image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based \\ on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. \\ (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected \\ for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent \\ elaboration images. It can be a standard world file (six lines text file) according to \\ http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to \\ https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm \\ (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones \\ possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' \\ (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the \\ 'geo_regerence_file' and/or embeded into the 'master_image' \"\"\" master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Number ( required = False , default = 1 , allow_none = True , as_string = False ) window_size_change = fields . Number ( required = False , default = 0 , allow_none = True , as_string = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True ) class FeedSchema ( ActionSchema ): \"\"\"reference_time: timestamp of the reference \"master_image\". If Null assumes last \\ \"master_image\" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info \"\"\" NS_displacement = LocalFile ( required = True , allow_none = False ) EW_displacement = LocalFile ( required = True , allow_none = False ) CORR = LocalFile ( required = False , allow_none = True ) class Source ( HielenSource ): ''' PhotoMonitoring source manager ''' def config_cache_path ( self , timestamp ): return self . filecache / self . filecache . hasher ( timestamp ) def config_last_before ( self , timestamp ): c = self . getActionValues ( 'config' , slice ( None , timestamp )) try : return c [ - 1 ] except Exception as e : return None def config_nc_file_path ( self , timestamp ): return self . config_cache_path ( timestamp ) / \"master.nc\" def config_master_image_path ( self , timestamp ): return self . config_cache_path ( timestamp ) / \"master.img\" def config ( self , ** kwargs ): out = {} timestamp = kwargs [ 'timestamp' ] #Temporary image path path_temp_image = Path ( kwargs [ \"master_image\" ]) self . filecache . mkdir ( self . config_cache_path ( timestamp ) ) path_master_image = self . config_master_image_path ( timestamp ) path_geo_ref = None try : name = str ( path_temp_image ) . split ( \".\" ) #Temporary georef file path_temp_ref = Path ( kwargs [ \"geo_reference_file\" ]) with open ( path_temp_ref ) as trf : ''' trying to match reference file type (wld or aux.wml) if exists ''' try : float ( trf . readline ()) path_geo_ref = Path ( \".\" . join ([ * name [ 0 : - 1 ], \"wld\" ])) except Exception as e : path_geo_ref = Path ( \".\" . join ([ * name , \"aux\" , \"xml\" ])) path_temp_ref . replace ( path_geo_ref ) except Exception as e : pass # traceback.print_exc() try : ''' trying to define crs from income parameters ''' crs = rasterio . crs . CRS . from_string ( kwargs [ 'crs' ]) except Exception : crs = None try : with rasterio . open ( path_temp_image ) as src : meta = src . meta . copy () if crs is not None : meta [ 'crs' ] = crs meta [ 'transform' ] = list ( meta [ 'transform' ])[ 0 : 6 ] try : meta [ 'crs' ] = meta [ 'crs' ] . to_string () except AttributeError : meta [ 'crs' ] = None meta [ 'count' ] = 3 meta [ 'compress' ] = 'lzw' meta [ 'dtype' ] = 'uint8' if src . count == 1 : #trasformare da gray scale a rgb rgb = src . read ( 1 ) . copy () rgb = ( rgb / 2 ** 16 ) * 255 rgb = rgb . astype ( 'uint8' ) rgb = [ rgb , rgb , rgb ] else : rgb = src . read () with rasterio . open ( path_master_image , 'w' , ** meta ) as dst : for i in range ( 0 , rgb . __len__ ()): dst . write ( rgb [ i ], i + 1 ) except Exception as e : raise ValueError ( e ) out [ 'master_image' ] = magic . from_file ( str ( path_master_image )) out [ 'timestamp' ] = timestamp out [ 'step_size' ] = kwargs [ 'step_size' ] out [ 'window_size_change' ] = kwargs [ 'window_size_change' ] out [ 'meta' ] = meta x_offset = kwargs [ 'window_size_change' ] y_offset = kwargs [ 'window_size_change' ] step_size = kwargs [ 'step_size' ] x_values = arange ( y_offset , meta [ 'width' ], step_size ) * meta [ 'transform' ][ 0 ] + meta [ 'transform' ][ 2 ] y_values = arange ( x_offset , meta [ 'height' ], step_size ) * meta [ 'transform' ][ 4 ] + meta [ 'transform' ][ 5 ] ncpath = self . config_nc_file_path ( timestamp ) config_NC ( ncpath , timestamp , x_values , y_values ) . close () return out def cleanConfig ( self , timestamp ): p = self . config_cache_path ( timestamp ) #TODO delete sub actions rmtree ( p ) def feed ( self , ** kwargs ): fileNS = Path ( kwargs [ \"NS_displacement\" ]) fileEW = Path ( kwargs [ \"EW_displacement\" ]) try : fileCORR = Path ( kwargs [ \"CORR\" ]) except Exception as e : fileCORR = None timestamp = kwargs [ \"timestamp\" ] reftime = self . config_last_before ( timestamp )[ 'timestamp' ] ncpath = self . config_nc_file_path ( reftime ) frames = { \"ns\" : None , \"ew\" : None , \"corr\" : None } frames [ \"ns\" ] = read_csv ( fileNS , header = None ) frames [ \"ew\" ] = read_csv ( fileEW , header = None ) if fileCORR is None : frames [ \"corr\" ] = DataFrame ( full (( frames [ \"ns\" ] . shape ), 0.99 )) else : frames [ \"corr\" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , ** frames ) . close () return kwargs def cleanFeed ( timestamp ): pass def map ( self , timestamp = None , timeref = None , param = \"RV\" ): conf = self . config_last_before ( timestamp ) ncfile = self . config_nc_file_path ( conf [ 'timestamp' ]) conf = conf [ 'value' ] path_image = self . config_cache_path ( conf [ 'timestamp' ]) / 'img.tiff' h = conf [ 'meta' ][ 'height' ] w = conf [ 'meta' ][ 'width' ] wsc = int ( conf [ 'window_size_change' ]) imgout = zeros ([ h , w , 4 ]) imagearray = generate_map ( ncfile , timestamp , timeref , param = param , step_size = conf [ 'step_size' ]) imgout [ wsc :, wsc :] = imagearray [: h - wsc ,: w - wsc ] conf [ 'meta' ][ 'count' ] = 3 conf [ 'meta' ][ 'compress' ] = 'LZW' conf [ 'meta' ][ 'driver' ] = 'GTiff' conf [ 'meta' ][ 'dtype' ] = 'uint8' imagearray = imagearray [: h - wsc ,: w - wsc , 0 : conf [ 'meta' ][ 'count' ]] with rasterio . open ( path_image , 'w' , ** conf [ 'meta' ]) as dst : for i in range ( 0 , conf [ 'meta' ][ 'count' ]): dst . write ( imagearray [:,:, i ], i + 1 ) def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs","title":"Module hielen2.ext.source_photomonitoring.phm_ok"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#configschema","text":"class ConfigSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) 'master_image' (required): the base image used as reference grid for elaboration. It can be any image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent elaboration images. It can be a standard world file (six lines text file) according to http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the 'geo_regerence_file' and/or embeded into the 'master_image' View Source class ConfigSchema ( ActionSchema ): \"\"\"'master_image' (required): the base image used as reference grid for elaboration. It can be any \\ image format managed by rasterio pyhton library (GeoTIFF, jpeg, ...). Any elaboration image based \\ on the 'master_image' will share geometry and reference system with it. 'step_size': Pixels sub samplig ratio expected for the elaboration grids compared to 'master_image'. \\ (see Feed Action) 'windows_size_change': Pixel expressed, upper-left corner position (both vertical and orizontal) expected \\ for the overlaying elaboration grids starting from the upper-left corner of the 'master_image' 'geo_reference_file': Reference file for the geolocalization of the 'master_image' and all the dependent \\ elaboration images. It can be a standard world file (six lines text file) according to \\ http://www.kralidis.ca/gis/worldfile.htm, as well an '.aux.xml' file according to \\ https://desktop.arcgis.com/en/arcmap/10.3/manage-data/raster-and-images/auxiliary-files.htm \\ (just the Coordinate system, the Transformation and the Projection informations are here managed). IMPORTANT: When a valid 'geo_regerence_file' is provided, therein informations overwrite the ones \\ possibly embedded into the 'master_image' 'crs': the Coordinate Reference System of the master_image in the string form 'autority:code' \\ (i.e.: 'EPSG:3857'). IMPORTANT: If a valid 'crs' is provided, this value overwrites the ones possibly provided with the \\ 'geo_regerence_file' and/or embeded into the 'master_image' \"\"\" master_image = LocalFile ( required = True , allow_none = False ) step_size = fields . Number ( required = False , default = 1 , allow_none = True , as_string = False ) window_size_change = fields . Number ( required = False , default = 0 , allow_none = True , as_string = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True )","title":"ConfigSchema"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#ancestors-in-mro","text":"hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#class-variables","text":"Meta OPTIONS_CLASS TYPE_MAPPING crs error_messages geo_reference_file master_image opts step_size window_size_change","title":"Class variables"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#static-methods","text":"","title":"Static methods"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#from_dict","text":"def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls","title":"from_dict"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#instance-variables","text":"dict_class set_class","title":"Instance variables"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#dump","text":"def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result","title":"dump"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#dumps","text":"def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs )","title":"dumps"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#get_attribute","text":"def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default )","title":"get_attribute"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#handle_error","text":"def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass","title":"handle_error"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#load","text":"def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True )","title":"load"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#loads","text":"def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown )","title":"loads"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#on_bind_field","text":"def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None","title":"on_bind_field"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#validate","text":"def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"validate"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#feedschema","text":"class FeedSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) reference_time: timestamp of the reference \"master_image\". If Null assumes last \"master_image\" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info View Source class FeedSchema ( ActionSchema ): \"\"\"reference_time: timestamp of the reference \" master_image \". If Null assumes last \\ \" master_image \" configured. NS_displacement: textfile containing the grid of the North-South displacement. EW_displacement: textfile containing the grid of the East-Weast displacement. CORR: textfile containing the grid of the coerence info \"\"\" NS_displacement = LocalFile ( required = True , allow_none = False ) EW_displacement = LocalFile ( required = True , allow_none = False ) CORR = LocalFile ( required = False , allow_none = True )","title":"FeedSchema"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#ancestors-in-mro_1","text":"hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#class-variables_1","text":"CORR EW_displacement Meta NS_displacement OPTIONS_CLASS TYPE_MAPPING error_messages opts","title":"Class variables"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#from_dict_1","text":"def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls","title":"from_dict"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#instance-variables_1","text":"dict_class set_class","title":"Instance variables"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#methods_1","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#dump_1","text":"def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result","title":"dump"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#dumps_1","text":"def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs )","title":"dumps"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#get_attribute_1","text":"def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default )","title":"get_attribute"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#handle_error_1","text":"def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass","title":"handle_error"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#load_1","text":"def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True )","title":"load"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#loads_1","text":"def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown )","title":"loads"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#on_bind_field_1","text":"def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None","title":"on_bind_field"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#validate_1","text":"def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"validate"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#source","text":"class Source ( feature ) PhotoMonitoring source manager View Source class Source ( HielenSource ) : ''' PhotoMonitoring source manager ''' def config_cache_path ( self , timestamp ) : return self . filecache / self . filecache . hasher ( timestamp ) def config_last_before ( self , timestamp ) : c = self . getActionValues ( 'config' , slice ( None , timestamp )) try : return c [ - 1 ] except Exception as e : return None def config_nc_file_path ( self , timestamp ) : return self . config_cache_path ( timestamp ) / \"master.nc\" def config_master_image_path ( self , timestamp ) : return self . config_cache_path ( timestamp ) / \"master.img\" def config ( self , **kwargs ) : out= {} timestamp = kwargs [ 'timestamp' ] # Temporary image path path_temp_image = Path ( kwargs [ \"master_image\" ]) self . filecache . mkdir ( self . config_cache_path ( timestamp ) ) path_master_image = self . config_master_image_path ( timestamp ) path_geo_ref = None try : name = str ( path_temp_image ). split ( \".\" ) # Temporary georef file path_temp_ref = Path ( kwargs [ \"geo_reference_file\" ]) with open ( path_temp_ref ) as trf : ''' trying to match reference file type (wld or aux.wml) if exists ''' try : float ( trf . readline ()) path_geo_ref = Path ( \".\" . join ([ * name [ 0 :- 1 ], \"wld\" ])) except Exception as e : path_geo_ref = Path ( \".\" . join ([ * name , \"aux\" , \"xml\" ])) path_temp_ref . replace ( path_geo_ref ) except Exception as e : pass # traceback . print_exc () try : ''' trying to define crs from income parameters ''' crs = rasterio . crs . CRS . from_string ( kwargs [ 'crs' ]) except Exception : crs = None try : with rasterio . open ( path_temp_image ) as src : meta = src . meta . copy () if crs is not None : meta [ 'crs' ] = crs meta [ 'transform' ] = list ( meta [ 'transform' ])[ 0 : 6 ] try : meta [ 'crs' ] = meta [ 'crs' ]. to_string () except AttributeError : meta [ 'crs' ] = None meta [ 'count' ] = 3 meta [ 'compress' ] ='lzw' meta [ 'dtype' ] ='uint8' if src . count == 1 : #trasformare da gray scale a rgb rgb = src . read ( 1 ). copy () rgb = ( rgb / 2 ** 16 ) * 255 rgb = rgb . astype ( 'uint8' ) rgb = [ rgb , rgb , rgb ] else : rgb = src . read () with rasterio . open ( path_master_image , 'w' , **meta ) as dst : for i in range ( 0 , rgb . __ len__ ()) : dst . write ( rgb [ i ], i + 1 ) except Exception as e : raise ValueError ( e ) out [ 'master_image' ] = magic . from_file ( str ( path_master_image )) out [ 'timestamp' ] = timestamp out [ 'step_size' ] = kwargs [ 'step_size' ] out [ 'window_size_change' ] = kwargs [ 'window_size_change' ] out [ 'meta' ] = meta x_offset = kwargs [ 'window_size_change' ] y_offset = kwargs [ 'window_size_change' ] step_size = kwargs [ 'step_size' ] x_values = arange ( y_offset , meta [ 'width' ], step_size ) * meta [ 'transform' ][ 0 ] + meta [ 'transform' ][ 2 ] y_values = arange ( x_offset , meta [ 'height' ], step_size ) * meta [ 'transform' ][ 4 ] + meta [ 'transform' ][ 5 ] ncpath = self . config_nc_file_path ( timestamp ) config_NC ( ncpath , timestamp , x_values , y_values ). close () return out def cleanConfig ( self , timestamp ) : p = self . config_cache_path ( timestamp ) # TODO delete sub actions rmtree ( p ) def feed ( self , **kwargs ) : fileNS = Path ( kwargs [ \"NS_displacement\" ]) fileEW = Path ( kwargs [ \"EW_displacement\" ]) try : fileCORR = Path ( kwargs [ \"CORR\" ]) except Exception as e : fileCORR = None timestamp = kwargs [ \"timestamp\" ] reftime = self . config_last_before ( timestamp )[ 'timestamp' ] ncpath = self . config_nc_file_path ( reftime ) frames= { \"ns\" : None , \"ew\" : None , \"corr\" : None } frames [ \"ns\" ] = read_csv ( fileNS , header = None ) frames [ \"ew\" ] = read_csv ( fileEW , header = None ) if fileCORR is None : frames [ \"corr\" ] = DataFrame ( full (( frames [ \"ns\" ]. shape ), 0.99 )) else : frames [ \"corr\" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , **frames ). close () return kwargs def cleanFeed ( timestamp ) : pass def map ( self , timestamp = None , timeref = None , param= \"RV\" ) : conf = self . config_last_before ( timestamp ) ncfile = self . config_nc_file_path ( conf [ 'timestamp' ]) conf = conf [ 'value' ] path_image = self . config_cache_path ( conf [ 'timestamp' ]) /'img.tiff' h = conf [ 'meta' ][ 'height' ] w = conf [ 'meta' ][ 'width' ] wsc = int ( conf [ 'window_size_change' ]) imgout = zeros ([ h , w , 4 ]) imagearray = generate_map ( ncfile , timestamp , timeref , param = param , step_size = conf [ 'step_size' ]) imgout [ wsc :, wsc :] = imagearray [ :h - wsc , :w - wsc ] conf [ 'meta' ][ 'count' ] = 3 conf [ 'meta' ][ 'compress' ] ='LZW' conf [ 'meta' ][ 'driver' ] ='GTiff' conf [ 'meta' ][ 'dtype' ] ='uint8' imagearray = imagearray [ :h - wsc , :w - wsc , 0 :conf [ 'meta' ][ 'count' ]] with rasterio . open ( path_image , 'w' , **conf [ 'meta' ]) as dst : for i in range ( 0 , conf [ 'meta' ][ 'count' ]) : dst . write ( imagearray [ : , : , i ], i + 1 ) def data ( self , timefrom = None , timeto = None , geom = None , **kwargs ) : return kwargs","title":"Source"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#ancestors-in-mro_2","text":"hielen2.source.HielenSource abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#methods_2","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#cleanconfig","text":"def cleanConfig ( self , timestamp ) View Source def cleanConfig ( self , timestamp ): p = self . config_cache_path ( timestamp ) # TODO delete sub actions rmtree ( p )","title":"cleanConfig"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#cleanfeed","text":"def cleanFeed ( timestamp ) View Source def cleanFeed ( timestamp ): pass","title":"cleanFeed"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#config","text":"def config ( self , ** kwargs ) View Source def config ( self , **kwargs ) : out= {} timestamp = kwargs [ 'timestamp' ] # Temporary image path path_temp_image = Path ( kwargs [ \"master_image\" ]) self . filecache . mkdir ( self . config_cache_path ( timestamp ) ) path_master_image = self . config_master_image_path ( timestamp ) path_geo_ref = None try : name = str ( path_temp_image ). split ( \".\" ) # Temporary georef file path_temp_ref = Path ( kwargs [ \"geo_reference_file\" ]) with open ( path_temp_ref ) as trf : ''' trying to match reference file type (wld or aux.wml) if exists ''' try : float ( trf . readline ()) path_geo_ref = Path ( \".\" . join ([ * name [ 0 :- 1 ], \"wld\" ])) except Exception as e : path_geo_ref = Path ( \".\" . join ([ * name , \"aux\" , \"xml\" ])) path_temp_ref . replace ( path_geo_ref ) except Exception as e : pass # traceback . print_exc () try : ''' trying to define crs from income parameters ''' crs = rasterio . crs . CRS . from_string ( kwargs [ 'crs' ]) except Exception : crs = None try : with rasterio . open ( path_temp_image ) as src : meta = src . meta . copy () if crs is not None : meta [ 'crs' ] = crs meta [ 'transform' ] = list ( meta [ 'transform' ])[ 0 : 6 ] try : meta [ 'crs' ] = meta [ 'crs' ]. to_string () except AttributeError : meta [ 'crs' ] = None meta [ 'count' ] = 3 meta [ 'compress' ] ='lzw' meta [ 'dtype' ] ='uint8' if src . count == 1 : #trasformare da gray scale a rgb rgb = src . read ( 1 ). copy () rgb = ( rgb / 2 ** 16 ) * 255 rgb = rgb . astype ( 'uint8' ) rgb = [ rgb , rgb , rgb ] else : rgb = src . read () with rasterio . open ( path_master_image , 'w' , **meta ) as dst : for i in range ( 0 , rgb . __ len__ ()) : dst . write ( rgb [ i ], i + 1 ) except Exception as e : raise ValueError ( e ) out [ 'master_image' ] = magic . from_file ( str ( path_master_image )) out [ 'timestamp' ] = timestamp out [ 'step_size' ] = kwargs [ 'step_size' ] out [ 'window_size_change' ] = kwargs [ 'window_size_change' ] out [ 'meta' ] = meta x_offset = kwargs [ 'window_size_change' ] y_offset = kwargs [ 'window_size_change' ] step_size = kwargs [ 'step_size' ] x_values = arange ( y_offset , meta [ 'width' ], step_size ) * meta [ 'transform' ][ 0 ] + meta [ 'transform' ][ 2 ] y_values = arange ( x_offset , meta [ 'height' ], step_size ) * meta [ 'transform' ][ 4 ] + meta [ 'transform' ][ 5 ] ncpath = self . config_nc_file_path ( timestamp ) config_NC ( ncpath , timestamp , x_values , y_values ). close () return out","title":"config"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#config_cache_path","text":"def config_cache_path ( self , timestamp ) View Source def config_cache_path ( self , timestamp ): return self . filecache / self . filecache . hasher ( timestamp )","title":"config_cache_path"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#config_last_before","text":"def config_last_before ( self , timestamp ) View Source def config_last_before ( self , timestamp ): c = self . getActionValues ( 'config' , slice ( None , timestamp )) try : return c [ - 1 ] except Exception as e : return None","title":"config_last_before"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#config_master_image_path","text":"def config_master_image_path ( self , timestamp ) View Source def config_master_image_path ( self , timestamp ): return self . config_cache_path ( timestamp ) / \"master.img\"","title":"config_master_image_path"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#config_nc_file_path","text":"def config_nc_file_path ( self , timestamp ) View Source def config_nc_file_path ( self , timestamp ): return self . config_cache_path ( timestamp ) / \"master.nc\"","title":"config_nc_file_path"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#data","text":"def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ) View Source def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs","title":"data"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#deleteactionvalues","text":"def deleteActionValues ( self , action = None , timestamp = None ) View Source def deleteActionValues ( self , action = None , timestamp = None ) : out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ) : out =[ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \"{a.capitalize()}Schema\" self . __getattribute__ ( f \"clean{a.capitalize()}\" )( t ) except Exception as e : pass try : db [ 'actions' ][ self.uid,a,t ]= None except Exception as e : raise ValueError ( e ) return out","title":"deleteActionValues"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#execaction","text":"def execAction ( self , action , ** kwargs ) View Source def execAction ( self , action , ** kwargs ): aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass (). load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e )","title":"execAction"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#feed","text":"def feed ( self , ** kwargs ) View Source def feed ( self , ** kwargs ): fileNS = Path ( kwargs [ \"NS_displacement\" ]) fileEW = Path ( kwargs [ \"EW_displacement\" ]) try : fileCORR = Path ( kwargs [ \"CORR\" ]) except Exception as e : fileCORR = None timestamp = kwargs [ \"timestamp\" ] reftime = self . config_last_before ( timestamp )[ 'timestamp' ] ncpath = self . config_nc_file_path ( reftime ) frames = { \"ns\" : None , \"ew\" : None , \"corr\" : None } frames [ \"ns\" ] = read_csv ( fileNS , header = None ) frames [ \"ew\" ] = read_csv ( fileEW , header = None ) if fileCORR is None : frames [ \"corr\" ] = DataFrame ( full (( frames [ \"ns\" ]. shape ), 0 . 99 )) else : frames [ \"corr\" ] = read_csv ( fileCORR , header = None ) feed_NC ( ncpath , timestamp , ** frames ). close () return kwargs","title":"feed"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#getactionschema","text":"def getActionSchema ( self , action ) View Source def getActionSchema ( self , action ): return getActionSchema ( self . module , action )","title":"getActionSchema"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#getactionvalues","text":"def getActionValues ( self , action = None , timestamp = None ) View Source def getActionValues ( self , action = None , timestamp = None ) : if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self.uid,action,timestamp ] if not isinstance ( out , list ) : out = [ out ] except KeyError : return [] return out","title":"getActionValues"},{"location":"reference/hielen2/ext/source_photomonitoring/phm_ok/#map","text":"def map ( self , timestamp = None , timeref = None , param = 'RV' ) View Source def map ( self , timestamp = None , timeref = None , param = \"RV\" ): conf = self . config_last_before ( timestamp ) ncfile = self . config_nc_file_path ( conf [ 'timestamp' ]) conf = conf [ 'value' ] path_image = self . config_cache_path ( conf [ 'timestamp' ]) / 'img.tiff' h = conf [ 'meta' ][ 'height' ] w = conf [ 'meta' ][ 'width' ] wsc = int ( conf [ 'window_size_change' ]) imgout = zeros ([ h , w , 4 ]) imagearray = generate_map ( ncfile , timestamp , timeref , param = param , step_size = conf [ 'step_size' ]) imgout [ wsc :, wsc :] = imagearray [: h - wsc ,: w - wsc ] conf [ 'meta' ][ 'count' ] = 3 conf [ 'meta' ][ 'compress' ] = 'LZW' conf [ 'meta' ][ 'driver' ] = 'GTiff' conf [ 'meta' ][ 'dtype' ] = 'uint8' imagearray = imagearray [: h - wsc ,: w - wsc , 0 : conf [ 'meta' ][ 'count' ]] with rasterio . open ( path_image , 'w' , ** conf [ 'meta' ]) as dst : for i in range ( 0 , conf [ 'meta' ][ 'count' ]): dst . write ( imagearray [:,:, i ], i + 1 )","title":"map"},{"location":"reference/hielen2/ext/source_photomonitoring/rendering/","text":"Module hielen2.ext.source_photomonitoring.rendering View Source # coding: utf-8 import numpy as np import matplotlib.pyplot as plt import xarray as xr , datetime import scipy.ndimage as snd import pandas as pd from matplotlib.colors import LinearSegmentedColormap def agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t class Render (): def __init__ ( self , targetfile = './incomes/sag.nc' , gridratio = 8 , ** kwargs ): self . dataset = xr . open_dataset ( targetfile ) self . gridratio = gridratio #CASS METHOD def _open_matrix ( ns = None , ew = None , corr = None , output = \"RV\" , gridratio = 8 ): ''' params dataset: dataset at fixed time output: \"RV\" result + vector \"R\" results \"V\" vector \"NS\" North-South\" \"EW\" East-West ''' # filtro con maschera di coerenza e faccio zoom if corr is not None : ns = ns . where ( corr >= 0 ) ew = ew . where ( corr >= 0 ) if output in ( \"RV\" , \"R\" , \"V\" ): h = np . sqrt ( ns ** 2 + ew ** 2 ) if output in ( \"EW\" ): h = ew if output in ( \"NS\" ): h = ns # upsampling h = snd . zoom ( h , gridratio , order = 0 , mode = 'nearest' ) Y = np . arange ( 0 , h . shape [ 0 ]) X = np . arange ( 0 , h . shape [ 1 ]) heatmap = xr . DataArray ( h , coords = [( \"y\" , Y ), ( \"x\" , X )]) if output in ( \"R\" , \"NS\" , \"EW\" ): return dict ( heatmap = heatmap , vectors = None , dims = h . shape ) # A questo punto solo RV o V # Elaboro i versori # upsampling a = snd . zoom ( np . arctan2 ( ns , ew ), gridratio , order = 0 , mode = 'nearest' ) angle = xr . DataArray ( a , coords = [( \"y\" , Y ), ( \"x\" , X )]) #riduco il versore rollingside = 100 #CON MEDIA #https://stackoverflow.com/questions/52886703/xarray-multidimensional-binning-array-reduction-on-sample-dataset-of-4-x4-to coeff = heatmap . rolling ( x = rollingside ) . construct ( 'tmp' ) . isel ( x = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) coeff = heatmap . rolling ( y = rollingside ) . construct ( 'tmp' ) . isel ( y = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) angle = angle . rolling ( x = rollingside ) . construct ( 'tmp' ) . isel ( x = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) angle = angle . rolling ( y = rollingside ) . construct ( 'tmp' ) . isel ( y = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) #SENZA MEDIA ''' coeff=coeff[::rollingside,::rollingside] ''' # filtro **ARBITRARIAMENTE** quelli che superano il 2 sigma # coeff=coeff.where(coeff<coeff.mean()+coeff.std()*2) # normalizzo su un valore adeguato coeff = coeff / coeff . mean () # applico il logaritmo per enfatizzare gli spostamenti minimi e # ridurre l'impatto visivo degli outlayers # coeff=np.log(1+coeff) # filtro l'angolo in base al grid del coefficiente angle = angle . where ( coeff ) X , Y = np . meshgrid ( angle [ 'x' ], angle [ 'y' ]) dY = np . sin ( angle ) * coeff dX = np . cos ( angle ) * coeff if output in ( \"V\" ): heatmap = heatmap . where ( heatmap is np . nan ) return dict ( heatmap = heatmap , vectors = ( X , Y , dX , dY ), dims = h . shape ) # CLASS METHOD def _print_image ( heatmap = None , vectors = None , colors = [ \"green\" , \"red\" , \"blue\" ], vmin = 0 , vmax = 5 , dims = None ): cmap = LinearSegmentedColormap . from_list ( \"mycmap\" , colors ) W = dims [ 1 ] #+64 H = dims [ 0 ] #+64 dpi = 72 plt . tight_layout = dict ( pad = 0 ) fig = plt . figure () fig . tight_layout = dict ( pad = 0 ) fig . frameon = False fig . dpi = dpi fig . set_size_inches ( W / dpi , H / dpi ) fig . facecolor = \"None\" fig . linewidth = 0 ax = plt . Axes ( fig ,[ 0 , 0 , 1 , 1 ]) ax . set_axis_off () if heatmap is not None : ax . imshow ( heatmap , alpha = 0.20 , origin = 'upper' , cmap = cmap , norm = None , vmin = vmin , vmax = vmax ) if vectors is not None : ax . quiver ( * vectors , width = 0.0008 , color = 'white' , scale = 100 ) fig . add_axes ( ax ) fig . canvas . draw () data = np . frombuffer ( fig . canvas . tostring_argb (), dtype = np . uint8 ) data = data . reshape ( fig . canvas . get_width_height ()[:: - 1 ] + ( 4 ,))[:, :, [ 3 , 2 , 1 , 0 ]] return data @property def timeline ( self ): times = self . dataset . time . values return list ( map ( lambda x : str ( x ) . replace ( '.000000000' , '' ),( times ))) @property def reftime ( self ): return self . dataset . timestamp def isbasetime ( self , time ): try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False def generate_map ( self , timestamp = None , timeref = None , output = \"RV\" ): timestamp = agoodtime ( timestamp ) timeref = agoodtime ( timeref ) if timestamp is None : timestamp = str ( self . dataset . time [ - 1 ] . values ) dataset = self . dataset . sel ( time = timestamp ) corr = dataset . corr if timeref is not None and not self . isbasetime ( timeref ): dataref = self . dataset . sel ( time = timeref ) dataset = dataset - dataref corr = None colors = [ \"green\" , \"red\" , \"blue\" ] vmin = 0 vmax = 5 if output in ( \"NS\" , \"EW\" ): colors = [ \"blue\" , \"green\" , \"red\" ] vmin =- 2.5 vmax = 2.5 managed = Render . _open_matrix ( ns = dataset . ns , ew = dataset . ew , corr = corr , output = output , gridratio = self . gridratio ) return Render . _print_image ( ** managed , colors = colors , vmin = vmin , vmax = vmax ) def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = \"R\" , timeref = None ): ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method = 'nearest' ) . sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ): ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method = 'nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if output in ( \"R\" , \"RV\" ): out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if output in ( \"V\" ): out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if output in ( \"NS\" ): out = dst . ns if output in ( \"EW\" ): out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]) . dropna () return out Functions agoodtime def agoodtime ( t ) View Source def agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t Classes Render class Render ( targetfile = './incomes/sag.nc' , gridratio = 8 , ** kwargs ) View Source class Render () : def __ init__ ( self , targetfile='./incomes/sag.nc' , gridratio = 8 , **kwargs ) : self . dataset = xr . open_dataset ( targetfile ) self . gridratio = gridratio # CASS METHOD def _ open_matrix ( ns = None , ew = None , corr = None , output= \"RV\" , gridratio = 8 ) : ''' params dataset: dataset at fixed time output: \"RV\" result + vector \"R\" results \"V\" vector \"NS\" North-South\" \"EW\" East-West ''' # filtro con maschera di coerenza e faccio zoom if corr is not None : ns = ns . where ( corr >= 0 ) ew = ew . where ( corr >= 0 ) if output in ( \"RV\" , \"R\" , \"V\" ) : h = np . sqrt ( ns** 2 + ew** 2 ) if output in ( \"EW\" ) : h = ew if output in ( \"NS\" ) : h = ns # upsampling h = snd . zoom ( h , gridratio , order = 0 , mode='nearest' ) Y = np . arange ( 0 , h . shape [ 0 ]) X = np . arange ( 0 , h . shape [ 1 ]) heatmap = xr . DataArray ( h , coords = [( \"y\" , Y ), ( \"x\" , X )]) if output in ( \"R\" , \"NS\" , \"EW\" ) : return dict ( heatmap = heatmap , vectors = None , dims = h . shape ) # A questo punto solo RV o V # Elaboro i versori # upsampling a = snd . zoom ( np . arctan2 ( ns , ew ), gridratio , order = 0 , mode='nearest' ) angle = xr . DataArray ( a , coords = [( \"y\" , Y ), ( \"x\" , X )]) #riduco il versore rollingside = 100 # CON MEDIA # https : // stackoverflow . com / questions / 52886703 / xarray - multidimensional - binning - array - reduction - on - sample - dataset - of - 4 - x4 - to coeff = heatmap . rolling ( x = rollingside ). construct ( 'tmp' ). isel ( x = slice ( 1 , None , rollingside )). mean ( 'tmp' , skipna = False ) coeff = heatmap . rolling ( y = rollingside ). construct ( 'tmp' ). isel ( y = slice ( 1 , None , rollingside )). mean ( 'tmp' , skipna = False ) angle = angle . rolling ( x = rollingside ). construct ( 'tmp' ). isel ( x = slice ( 1 , None , rollingside )). mean ( 'tmp' , skipna = False ) angle = angle . rolling ( y = rollingside ). construct ( 'tmp' ). isel ( y = slice ( 1 , None , rollingside )). mean ( 'tmp' , skipna = False ) # SENZA MEDIA ''' coeff=coeff[::rollingside,::rollingside] ''' # filtro ** ARBITRARIAMENTE ** quelli che superano il 2 sigma # coeff = coeff . where ( coeff < coeff . mean () + coeff . std () * 2 ) # normalizzo su un valore adeguato coeff = coeff / coeff . mean () # applico il logaritmo per enfatizzare gli spostamenti minimi e # ridurre l'impatto visivo degli outlayers # coeff=np.log(1+coeff) # filtro l'angolo in base al grid del coefficiente angle = angle . where ( coeff ) X , Y = np . meshgrid ( angle [ 'x' ], angle [ 'y' ]) dY = np . sin ( angle ) * coeff dX = np . cos ( angle ) * coeff if output in ( \"V\" ) : heatmap = heatmap . where ( heatmap is np . nan ) return dict ( heatmap = heatmap , vectors= ( X , Y , dX , dY ), dims = h . shape ) # CLASS METHOD def _ print_image ( heatmap = None , vectors = None , colors = [ \"green\" , \"red\" , \"blue\" ], vmin = 0 , vmax = 5 , dims = None ) : cmap = LinearSegmentedColormap . from_list ( \"mycmap\" , colors ) W = dims [ 1 ] #+ 64 H = dims [ 0 ] #+ 64 dpi = 72 plt . tight_layout = dict ( pad = 0 ) fig = plt . figure () fig . tight_layout = dict ( pad = 0 ) fig . frameon = False fig . dpi = dpi fig . set_size_inches ( W / dpi , H / dpi ) fig . facecolor= \"None\" fig . linewidth = 0 ax = plt . Axes ( fig ,[ 0 , 0 , 1 , 1 ]) ax . set_axis_off () if heatmap is not None : ax . imshow ( heatmap , alpha = 0.20 , origin='upper' , cmap = cmap , norm = None , vmin = vmin , vmax = vmax ) if vectors is not None : ax . quiver ( * vectors , width = 0.0008 , color='white' , scale = 100 ) fig . add_axes ( ax ) fig . canvas . draw () data = np . frombuffer ( fig . canvas . tostring_argb (), dtype = np . uint8 ) data = data . reshape ( fig . canvas . get_width_height ()[ ::- 1 ] + ( 4 ,))[ : , : , [ 3 , 2 , 1 , 0 ]] return data @property def timeline ( self ) : times = self . dataset . time . values return list ( map ( lambda x : str ( x ). replace ( '.000000000' , '' ),( times ))) @property def reftime ( self ) : return self . dataset . timestamp def isbasetime ( self , time ) : try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False def generate_map ( self , timestamp = None , timeref = None , output= \"RV\" ) : timestamp = agoodtime ( timestamp ) timeref = agoodtime ( timeref ) if timestamp is None : timestamp = str ( self . dataset . time [ - 1 ]. values ) dataset = self . dataset . sel ( time = timestamp ) corr = dataset . corr if timeref is not None and not self . isbasetime ( timeref ) : dataref = self . dataset . sel ( time = timeref ) dataset = dataset - dataref corr = None colors = [ \"green\" , \"red\" , \"blue\" ] vmin = 0 vmax = 5 if output in ( \"NS\" , \"EW\" ) : colors = [ \"blue\" , \"green\" , \"red\" ] vmin=- 2.5 vmax = 2.5 managed = Render . _ open_matrix ( ns = dataset . ns , ew = dataset . ew , corr = corr , output = output , gridratio = self . gridratio ) return Render . _ print_image ( **managed , colors = colors , vmin = vmin , vmax = vmax ) def extract_data ( self , geom= ( 0 , 0 ), timefrom = None , timeto = None , output= \"R\" , timeref = None ) : ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method='nearest' ). sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ) : ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method='nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if output in ( \"R\" , \"RV\" ) : out = np . sqrt ( dst . ns** 2 + dst . ew** 2 ) if output in ( \"V\" ) : out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if output in ( \"NS\" ) : out = dst . ns if output in ( \"EW\" ) : out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]). dropna () return out Instance variables reftime timeline Methods extract_data def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = 'R' , timeref = None ) params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component View Source def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = \"R\" , timeref = None ): ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method = 'nearest' ). sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ): ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method = 'nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if output in ( \"R\" , \"RV\" ): out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if output in ( \"V\" ): out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if output in ( \"NS\" ): out = dst . ns if output in ( \"EW\" ): out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]). dropna () return out generate_map def generate_map ( self , timestamp = None , timeref = None , output = 'RV' ) View Source def generate_map ( self , timestamp = None , timeref = None , output = \"RV\" ): timestamp = agoodtime ( timestamp ) timeref = agoodtime ( timeref ) if timestamp is None : timestamp = str ( self . dataset . time [ - 1 ]. values ) dataset = self . dataset . sel ( time = timestamp ) corr = dataset . corr if timeref is not None and not self . isbasetime ( timeref ): dataref = self . dataset . sel ( time = timeref ) dataset = dataset - dataref corr = None colors = [ \"green\" , \"red\" , \"blue\" ] vmin = 0 vmax = 5 if output in ( \"NS\" , \"EW\" ): colors = [ \"blue\" , \"green\" , \"red\" ] vmin =- 2 . 5 vmax = 2 . 5 managed = Render . _open_matrix ( ns = dataset . ns , ew = dataset . ew , corr = corr , output = output , gridratio = self . gridratio ) return Render . _print_image ( ** managed , colors = colors , vmin = vmin , vmax = vmax ) isbasetime def isbasetime ( self , time ) View Source def isbasetime ( self , time ): try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False","title":"Rendering"},{"location":"reference/hielen2/ext/source_photomonitoring/rendering/#module-hielen2extsource_photomonitoringrendering","text":"View Source # coding: utf-8 import numpy as np import matplotlib.pyplot as plt import xarray as xr , datetime import scipy.ndimage as snd import pandas as pd from matplotlib.colors import LinearSegmentedColormap def agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t class Render (): def __init__ ( self , targetfile = './incomes/sag.nc' , gridratio = 8 , ** kwargs ): self . dataset = xr . open_dataset ( targetfile ) self . gridratio = gridratio #CASS METHOD def _open_matrix ( ns = None , ew = None , corr = None , output = \"RV\" , gridratio = 8 ): ''' params dataset: dataset at fixed time output: \"RV\" result + vector \"R\" results \"V\" vector \"NS\" North-South\" \"EW\" East-West ''' # filtro con maschera di coerenza e faccio zoom if corr is not None : ns = ns . where ( corr >= 0 ) ew = ew . where ( corr >= 0 ) if output in ( \"RV\" , \"R\" , \"V\" ): h = np . sqrt ( ns ** 2 + ew ** 2 ) if output in ( \"EW\" ): h = ew if output in ( \"NS\" ): h = ns # upsampling h = snd . zoom ( h , gridratio , order = 0 , mode = 'nearest' ) Y = np . arange ( 0 , h . shape [ 0 ]) X = np . arange ( 0 , h . shape [ 1 ]) heatmap = xr . DataArray ( h , coords = [( \"y\" , Y ), ( \"x\" , X )]) if output in ( \"R\" , \"NS\" , \"EW\" ): return dict ( heatmap = heatmap , vectors = None , dims = h . shape ) # A questo punto solo RV o V # Elaboro i versori # upsampling a = snd . zoom ( np . arctan2 ( ns , ew ), gridratio , order = 0 , mode = 'nearest' ) angle = xr . DataArray ( a , coords = [( \"y\" , Y ), ( \"x\" , X )]) #riduco il versore rollingside = 100 #CON MEDIA #https://stackoverflow.com/questions/52886703/xarray-multidimensional-binning-array-reduction-on-sample-dataset-of-4-x4-to coeff = heatmap . rolling ( x = rollingside ) . construct ( 'tmp' ) . isel ( x = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) coeff = heatmap . rolling ( y = rollingside ) . construct ( 'tmp' ) . isel ( y = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) angle = angle . rolling ( x = rollingside ) . construct ( 'tmp' ) . isel ( x = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) angle = angle . rolling ( y = rollingside ) . construct ( 'tmp' ) . isel ( y = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) #SENZA MEDIA ''' coeff=coeff[::rollingside,::rollingside] ''' # filtro **ARBITRARIAMENTE** quelli che superano il 2 sigma # coeff=coeff.where(coeff<coeff.mean()+coeff.std()*2) # normalizzo su un valore adeguato coeff = coeff / coeff . mean () # applico il logaritmo per enfatizzare gli spostamenti minimi e # ridurre l'impatto visivo degli outlayers # coeff=np.log(1+coeff) # filtro l'angolo in base al grid del coefficiente angle = angle . where ( coeff ) X , Y = np . meshgrid ( angle [ 'x' ], angle [ 'y' ]) dY = np . sin ( angle ) * coeff dX = np . cos ( angle ) * coeff if output in ( \"V\" ): heatmap = heatmap . where ( heatmap is np . nan ) return dict ( heatmap = heatmap , vectors = ( X , Y , dX , dY ), dims = h . shape ) # CLASS METHOD def _print_image ( heatmap = None , vectors = None , colors = [ \"green\" , \"red\" , \"blue\" ], vmin = 0 , vmax = 5 , dims = None ): cmap = LinearSegmentedColormap . from_list ( \"mycmap\" , colors ) W = dims [ 1 ] #+64 H = dims [ 0 ] #+64 dpi = 72 plt . tight_layout = dict ( pad = 0 ) fig = plt . figure () fig . tight_layout = dict ( pad = 0 ) fig . frameon = False fig . dpi = dpi fig . set_size_inches ( W / dpi , H / dpi ) fig . facecolor = \"None\" fig . linewidth = 0 ax = plt . Axes ( fig ,[ 0 , 0 , 1 , 1 ]) ax . set_axis_off () if heatmap is not None : ax . imshow ( heatmap , alpha = 0.20 , origin = 'upper' , cmap = cmap , norm = None , vmin = vmin , vmax = vmax ) if vectors is not None : ax . quiver ( * vectors , width = 0.0008 , color = 'white' , scale = 100 ) fig . add_axes ( ax ) fig . canvas . draw () data = np . frombuffer ( fig . canvas . tostring_argb (), dtype = np . uint8 ) data = data . reshape ( fig . canvas . get_width_height ()[:: - 1 ] + ( 4 ,))[:, :, [ 3 , 2 , 1 , 0 ]] return data @property def timeline ( self ): times = self . dataset . time . values return list ( map ( lambda x : str ( x ) . replace ( '.000000000' , '' ),( times ))) @property def reftime ( self ): return self . dataset . timestamp def isbasetime ( self , time ): try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False def generate_map ( self , timestamp = None , timeref = None , output = \"RV\" ): timestamp = agoodtime ( timestamp ) timeref = agoodtime ( timeref ) if timestamp is None : timestamp = str ( self . dataset . time [ - 1 ] . values ) dataset = self . dataset . sel ( time = timestamp ) corr = dataset . corr if timeref is not None and not self . isbasetime ( timeref ): dataref = self . dataset . sel ( time = timeref ) dataset = dataset - dataref corr = None colors = [ \"green\" , \"red\" , \"blue\" ] vmin = 0 vmax = 5 if output in ( \"NS\" , \"EW\" ): colors = [ \"blue\" , \"green\" , \"red\" ] vmin =- 2.5 vmax = 2.5 managed = Render . _open_matrix ( ns = dataset . ns , ew = dataset . ew , corr = corr , output = output , gridratio = self . gridratio ) return Render . _print_image ( ** managed , colors = colors , vmin = vmin , vmax = vmax ) def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = \"R\" , timeref = None ): ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method = 'nearest' ) . sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ): ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method = 'nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if output in ( \"R\" , \"RV\" ): out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if output in ( \"V\" ): out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if output in ( \"NS\" ): out = dst . ns if output in ( \"EW\" ): out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]) . dropna () return out","title":"Module hielen2.ext.source_photomonitoring.rendering"},{"location":"reference/hielen2/ext/source_photomonitoring/rendering/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/ext/source_photomonitoring/rendering/#agoodtime","text":"def agoodtime ( t ) View Source def agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t","title":"agoodtime"},{"location":"reference/hielen2/ext/source_photomonitoring/rendering/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/ext/source_photomonitoring/rendering/#render","text":"class Render ( targetfile = './incomes/sag.nc' , gridratio = 8 , ** kwargs ) View Source class Render () : def __ init__ ( self , targetfile='./incomes/sag.nc' , gridratio = 8 , **kwargs ) : self . dataset = xr . open_dataset ( targetfile ) self . gridratio = gridratio # CASS METHOD def _ open_matrix ( ns = None , ew = None , corr = None , output= \"RV\" , gridratio = 8 ) : ''' params dataset: dataset at fixed time output: \"RV\" result + vector \"R\" results \"V\" vector \"NS\" North-South\" \"EW\" East-West ''' # filtro con maschera di coerenza e faccio zoom if corr is not None : ns = ns . where ( corr >= 0 ) ew = ew . where ( corr >= 0 ) if output in ( \"RV\" , \"R\" , \"V\" ) : h = np . sqrt ( ns** 2 + ew** 2 ) if output in ( \"EW\" ) : h = ew if output in ( \"NS\" ) : h = ns # upsampling h = snd . zoom ( h , gridratio , order = 0 , mode='nearest' ) Y = np . arange ( 0 , h . shape [ 0 ]) X = np . arange ( 0 , h . shape [ 1 ]) heatmap = xr . DataArray ( h , coords = [( \"y\" , Y ), ( \"x\" , X )]) if output in ( \"R\" , \"NS\" , \"EW\" ) : return dict ( heatmap = heatmap , vectors = None , dims = h . shape ) # A questo punto solo RV o V # Elaboro i versori # upsampling a = snd . zoom ( np . arctan2 ( ns , ew ), gridratio , order = 0 , mode='nearest' ) angle = xr . DataArray ( a , coords = [( \"y\" , Y ), ( \"x\" , X )]) #riduco il versore rollingside = 100 # CON MEDIA # https : // stackoverflow . com / questions / 52886703 / xarray - multidimensional - binning - array - reduction - on - sample - dataset - of - 4 - x4 - to coeff = heatmap . rolling ( x = rollingside ). construct ( 'tmp' ). isel ( x = slice ( 1 , None , rollingside )). mean ( 'tmp' , skipna = False ) coeff = heatmap . rolling ( y = rollingside ). construct ( 'tmp' ). isel ( y = slice ( 1 , None , rollingside )). mean ( 'tmp' , skipna = False ) angle = angle . rolling ( x = rollingside ). construct ( 'tmp' ). isel ( x = slice ( 1 , None , rollingside )). mean ( 'tmp' , skipna = False ) angle = angle . rolling ( y = rollingside ). construct ( 'tmp' ). isel ( y = slice ( 1 , None , rollingside )). mean ( 'tmp' , skipna = False ) # SENZA MEDIA ''' coeff=coeff[::rollingside,::rollingside] ''' # filtro ** ARBITRARIAMENTE ** quelli che superano il 2 sigma # coeff = coeff . where ( coeff < coeff . mean () + coeff . std () * 2 ) # normalizzo su un valore adeguato coeff = coeff / coeff . mean () # applico il logaritmo per enfatizzare gli spostamenti minimi e # ridurre l'impatto visivo degli outlayers # coeff=np.log(1+coeff) # filtro l'angolo in base al grid del coefficiente angle = angle . where ( coeff ) X , Y = np . meshgrid ( angle [ 'x' ], angle [ 'y' ]) dY = np . sin ( angle ) * coeff dX = np . cos ( angle ) * coeff if output in ( \"V\" ) : heatmap = heatmap . where ( heatmap is np . nan ) return dict ( heatmap = heatmap , vectors= ( X , Y , dX , dY ), dims = h . shape ) # CLASS METHOD def _ print_image ( heatmap = None , vectors = None , colors = [ \"green\" , \"red\" , \"blue\" ], vmin = 0 , vmax = 5 , dims = None ) : cmap = LinearSegmentedColormap . from_list ( \"mycmap\" , colors ) W = dims [ 1 ] #+ 64 H = dims [ 0 ] #+ 64 dpi = 72 plt . tight_layout = dict ( pad = 0 ) fig = plt . figure () fig . tight_layout = dict ( pad = 0 ) fig . frameon = False fig . dpi = dpi fig . set_size_inches ( W / dpi , H / dpi ) fig . facecolor= \"None\" fig . linewidth = 0 ax = plt . Axes ( fig ,[ 0 , 0 , 1 , 1 ]) ax . set_axis_off () if heatmap is not None : ax . imshow ( heatmap , alpha = 0.20 , origin='upper' , cmap = cmap , norm = None , vmin = vmin , vmax = vmax ) if vectors is not None : ax . quiver ( * vectors , width = 0.0008 , color='white' , scale = 100 ) fig . add_axes ( ax ) fig . canvas . draw () data = np . frombuffer ( fig . canvas . tostring_argb (), dtype = np . uint8 ) data = data . reshape ( fig . canvas . get_width_height ()[ ::- 1 ] + ( 4 ,))[ : , : , [ 3 , 2 , 1 , 0 ]] return data @property def timeline ( self ) : times = self . dataset . time . values return list ( map ( lambda x : str ( x ). replace ( '.000000000' , '' ),( times ))) @property def reftime ( self ) : return self . dataset . timestamp def isbasetime ( self , time ) : try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False def generate_map ( self , timestamp = None , timeref = None , output= \"RV\" ) : timestamp = agoodtime ( timestamp ) timeref = agoodtime ( timeref ) if timestamp is None : timestamp = str ( self . dataset . time [ - 1 ]. values ) dataset = self . dataset . sel ( time = timestamp ) corr = dataset . corr if timeref is not None and not self . isbasetime ( timeref ) : dataref = self . dataset . sel ( time = timeref ) dataset = dataset - dataref corr = None colors = [ \"green\" , \"red\" , \"blue\" ] vmin = 0 vmax = 5 if output in ( \"NS\" , \"EW\" ) : colors = [ \"blue\" , \"green\" , \"red\" ] vmin=- 2.5 vmax = 2.5 managed = Render . _ open_matrix ( ns = dataset . ns , ew = dataset . ew , corr = corr , output = output , gridratio = self . gridratio ) return Render . _ print_image ( **managed , colors = colors , vmin = vmin , vmax = vmax ) def extract_data ( self , geom= ( 0 , 0 ), timefrom = None , timeto = None , output= \"R\" , timeref = None ) : ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method='nearest' ). sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ) : ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method='nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if output in ( \"R\" , \"RV\" ) : out = np . sqrt ( dst . ns** 2 + dst . ew** 2 ) if output in ( \"V\" ) : out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if output in ( \"NS\" ) : out = dst . ns if output in ( \"EW\" ) : out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]). dropna () return out","title":"Render"},{"location":"reference/hielen2/ext/source_photomonitoring/rendering/#instance-variables","text":"reftime timeline","title":"Instance variables"},{"location":"reference/hielen2/ext/source_photomonitoring/rendering/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_photomonitoring/rendering/#extract_data","text":"def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = 'R' , timeref = None ) params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component View Source def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = \"R\" , timeref = None ): ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method = 'nearest' ). sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ): ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method = 'nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if output in ( \"R\" , \"RV\" ): out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if output in ( \"V\" ): out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if output in ( \"NS\" ): out = dst . ns if output in ( \"EW\" ): out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]). dropna () return out","title":"extract_data"},{"location":"reference/hielen2/ext/source_photomonitoring/rendering/#generate_map","text":"def generate_map ( self , timestamp = None , timeref = None , output = 'RV' ) View Source def generate_map ( self , timestamp = None , timeref = None , output = \"RV\" ): timestamp = agoodtime ( timestamp ) timeref = agoodtime ( timeref ) if timestamp is None : timestamp = str ( self . dataset . time [ - 1 ]. values ) dataset = self . dataset . sel ( time = timestamp ) corr = dataset . corr if timeref is not None and not self . isbasetime ( timeref ): dataref = self . dataset . sel ( time = timeref ) dataset = dataset - dataref corr = None colors = [ \"green\" , \"red\" , \"blue\" ] vmin = 0 vmax = 5 if output in ( \"NS\" , \"EW\" ): colors = [ \"blue\" , \"green\" , \"red\" ] vmin =- 2 . 5 vmax = 2 . 5 managed = Render . _open_matrix ( ns = dataset . ns , ew = dataset . ew , corr = corr , output = output , gridratio = self . gridratio ) return Render . _print_image ( ** managed , colors = colors , vmin = vmin , vmax = vmax )","title":"generate_map"},{"location":"reference/hielen2/ext/source_photomonitoring/rendering/#isbasetime","text":"def isbasetime ( self , time ) View Source def isbasetime ( self , time ): try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False","title":"isbasetime"},{"location":"reference/hielen2/ext/source_photomonitoring/struct/","text":"Module hielen2.ext.source_photomonitoring.struct View Source # coding: utf-8 import numpy as np import pandas as pd import matplotlib matplotlib . use ( 'Agg' ) import matplotlib.pyplot as plt from matplotlib.colors import LinearSegmentedColormap import xarray as xr , datetime import scipy.ndimage as snd import re from netCDF4 import Dataset , date2num import os import PIL def config_NC ( target , timestamp , x_values , y_values ): \"\"\" Crea il file netCDF secondo un formato standard returns: la struttura dati del file NetCDF params tagetfile: nome del file refitime: tempo zero y_values: array delle y x_values: array delle x \"\"\" dataset = Dataset ( target , 'w' , format = \"NETCDF4\" ) dataset . Conventions = \"CF-1.7\" dataset . timestamp = agoodtime ( timestamp ) # time informations dataset . createDimension ( \"time\" , None ) dataset . createVariable ( \"time\" , \"f8\" ,( \"time\" ,)) dataset . variables [ \"time\" ] . units = f \"hours since { timestamp } \" dataset . variables [ \"time\" ] . calendar = \"standard\" dataset . variables [ \"time\" ] . long_name = \"observation_time\" # y informations dataset . createDimension ( \"y\" , y_values . __len__ ()) dataset . createVariable ( \"y\" , \"f4\" ,( \"y\" ,)) dataset . variables [ \"y\" ] . units = \"1\" dataset . variables [ \"y\" ] . long_name = \"projection_y_coordinate\" dataset . variables [ \"y\" ][:] = y_values # x informations dataset . createDimension ( \"x\" , x_values . __len__ ()) dataset . createVariable ( \"x\" , \"f4\" ,( \"x\" ,)) dataset . variables [ \"x\" ] . units = \"1\" dataset . variables [ \"x\" ] . long_name = \"projection_x_coordinate\" dataset . variables [ \"x\" ][:] = x_values # corr (correlation coefficient) informations dataset . createVariable ( \"corr\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = 1 , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"corr\" ] . units = \"1\" dataset . variables [ \"corr\" ] . long_name = \"correlation_coefficient\" # ns (north-south) informations dataset . createVariable ( \"ns\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ns\" ] . units = \"px\" dataset . variables [ \"ns\" ] . long_name = \"north_south_axis_displacement\" # ew (east-west) informations dataset . createVariable ( \"ew\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ew\" ] . units = \"px\" dataset . variables [ \"ew\" ] . long_name = \"east_west_axis_displacement\" #zf=np.zeros((y_values.__len__(),x_values.__len__())) #feed_NC(dataset,timestamp,ns=zf,ew=zf,corr=zf) return dataset def feed_NC ( target , time , ** kwargs ): \"\"\" Appende i grid al file netCDF target: netCDF4.Dataset or path to a valid .nc file time: timestamp del dato kwarg: dict nomevariabile:datagrid \"\"\" if isinstance ( target , Dataset ): dataset = target else : dataset = Dataset ( target , 'a' , format = \"NETCDF4\" ) timevar = dataset . variables [ 'time' ] time = date2num ( np . datetime64 ( time ) . tolist (), timevar . units ) # If exists substitute position = np . where ( timevar == time ) try : position = int ( position [ 0 ]) except TypeError : position = timevar . shape [ 0 ] timevar [ position ] = time for k , w in kwargs . items (): if w is not None : y_slice = slice ( 0 , w . shape [ 0 ]) x_slice = slice ( 0 , w . shape [ 1 ]) try : dataset . variables [ k ][ position , y_slice , x_slice ] = w except KeyError as e : raise ( e ) pass return dataset def agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t #CASS METHOD def _open_matrix ( dataset , step_size = 1 , param = \"RV\" ): ''' params dataset: dataset at fixed time param: \"RV\" result + vector \"R\" results \"V\" vector \"NS\" North-South\" \"EW\" East-West ''' # filtro con maschera di correlazione # se tutte le celle di corr sono null il minimo \u00e8 TRUE altrimenti FALSE. # Inverto la condizione per sapere se c'\u00e8 qualcosa ns = dataset . ns ew = dataset . ew if param in ( \"D\" , \"V\" ): h = np . sqrt ( ns ** 2 + ew ** 2 ) if param in ( \"EW\" ): h = ew if param in ( \"NS\" ): h = ns # upsampling h = snd . zoom ( h , step_size , order = 0 , mode = 'nearest' ) Y = np . arange ( 0 , h . shape [ 0 ]) X = np . arange ( 0 , h . shape [ 1 ]) heatmap = xr . DataArray ( h , coords = [( \"y\" , Y ), ( \"x\" , X )]) if param in ( \"R\" , \"NS\" , \"EW\" ): return dict ( heatmap = heatmap , vectors = None ) # A questo punto solo RV o V # Elaboro i versori # upsampling a = snd . zoom ( np . arctan2 ( ns , ew ), step_size , order = 0 , mode = 'nearest' ) angle = xr . DataArray ( a , coords = [( \"y\" , Y ), ( \"x\" , X )]) #riduco il versore rollingside = int ( step_size * 6 ) #CON MEDIA #https://stackoverflow.com/questions/52886703/xarray-multidimensional-binning-array-reduction-on-sample-dataset-of-4-x4-to coeff = heatmap . rolling ( x = rollingside ) . construct ( 'tmp' ) . isel ( x = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) coeff = heatmap . rolling ( y = rollingside ) . construct ( 'tmp' ) . isel ( y = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) angle = angle . rolling ( x = rollingside ) . construct ( 'tmp' ) . isel ( x = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) angle = angle . rolling ( y = rollingside ) . construct ( 'tmp' ) . isel ( y = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) #SENZA MEDIA ''' coeff=coeff[::rollingside,::rollingside] ''' # filtro **ARBITRARIAMENTE** quelli che superano il 2 sigma # coeff=coeff.where(coeff<coeff.mean()+coeff.std()*2) # normalizzo su un valore adeguato coeff = coeff / ( coeff . mean () * 4 ) # applico il logaritmo per enfatizzare gli spostamenti minimi e # ridurre l'impatto visivo degli outlayers # coeff=np.log(1+coeff) # filtro l'angolo in base al grid del coefficiente angle = angle . where ( np . abs ( coeff ) > np . abs ( coeff . mean ())) X , Y = np . meshgrid ( angle [ 'x' ], angle [ 'y' ]) dY = np . sin ( angle ) * coeff dX = np . cos ( angle ) * coeff if param in ( \"V\" ): heatmap = heatmap . where ( heatmap is np . nan ) return dict ( heatmap = heatmap , vectors = ( X , Y , dX , dY )) # CLASS METHOD def _render ( heatmap , vectors = None , colors = [ \"green\" , \"red\" , \"blue\" ], vmin = 0 , vmax = 5 ): cmap = LinearSegmentedColormap . from_list ( \"mycmap\" , colors ) W = heatmap . shape [ 1 ] H = heatmap . shape [ 0 ] plt . tight_layout = dict ( pad = 0 ) fig = plt . figure () fig . tight_layout = dict ( pad = 0 ) fig . frameon = False fig . dpi = 72 fig . set_size_inches ( W / fig . dpi , H / fig . dpi ) fig . facecolor = \"None\" fig . linewidth = 0 ax = plt . Axes ( fig ,[ 0 , 0 , 1 , 1 ]) ax . set_axis_off () ax . imshow ( heatmap , alpha = 0.20 , origin = 'upper' , cmap = cmap , norm = None , vmin = vmin , vmax = vmax ) if vectors is not None : ax . quiver ( * vectors , width = 0.0008 , color = 'white' , scale = 100 ) fig . add_axes ( ax ) fig . canvas . draw () data = np . frombuffer ( fig . canvas . tostring_argb (), dtype = np . uint8 ) data = data . reshape ( fig . canvas . get_width_height ()[:: - 1 ] + ( 4 ,))[:, :, [ 3 , 2 , 1 , 0 ]] plt . close () return data def generate_map ( targetfile , timestamp = None , timeref = None , param = None , step_size = None , colors = None , vmin = None , vmax = None ): if timestamp is None : pass if param is None : param = \"R\" if step_size is None : stap_size = 1 if colors is None : colors = [ \"red\" , \"green\" , \"blue\" ] if vmin is None : vmin =- 150 if vmax is None : vmax = 150 dataset = xr . open_dataset ( targetfile ) if timestamp is None : timestamp = str ( dataset . time [ - 1 ] . values ) else : timestamp = agoodtime ( timestamp ) ds1 = dataset . sel ( time = timestamp ) ds1 = ds1 . where ( ds1 . corr > 0 ) if timeref is not None : timeref = agoodtime ( timeref ) ds2 = dataset . sel ( time = timeref ) ds2 = ds2 . where ( ds2 . corr > 0 ) ds1 = ds1 - ds2 ds1 . attrs = dataset . attrs managed = _open_matrix ( dataset = ds1 , param = param , step_size = step_size ) return [ timestamp , _render ( ** managed , colors = colors , vmin = vmin , vmax = vmax )] class Render (): @property def timeline ( self ): times = self . dataset . time . values return list ( map ( lambda x : str ( x ) . replace ( '.000000000' , '' ),( times ))) @property def reftime ( self ): return self . dataset . timestamp def isbasetime ( self , time ): try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , param = \"R\" , timeref = None ): ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit param: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method = 'nearest' ) . sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ): ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method = 'nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if param in ( \"R\" , \"RV\" ): out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if param in ( \"V\" ): out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if param in ( \"NS\" ): out = dst . ns if param in ( \"EW\" ): out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]) . dropna () return out Functions agoodtime def agoodtime ( t ) View Source def agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t config_NC def config_NC ( target , timestamp , x_values , y_values ) Crea il file netCDF secondo un formato standard returns: la struttura dati del file NetCDF params tagetfile: nome del file refitime: tempo zero y_values: array delle y x_values: array delle x View Source def config_NC ( target , timestamp , x_values , y_values ): \"\"\" Crea il file netCDF secondo un formato standard returns: la struttura dati del file NetCDF params tagetfile: nome del file refitime: tempo zero y_values: array delle y x_values: array delle x \"\"\" dataset = Dataset ( target , 'w' , format = \"NETCDF4\" ) dataset . Conventions = \"CF-1.7\" dataset . timestamp = agoodtime ( timestamp ) # time informations dataset . createDimension ( \"time\" , None ) dataset . createVariable ( \"time\" , \"f8\" ,( \"time\" ,)) dataset . variables [ \"time\" ]. units = f \"hours since {timestamp}\" dataset . variables [ \"time\" ]. calendar = \"standard\" dataset . variables [ \"time\" ]. long_name = \"observation_time\" # y informations dataset . createDimension ( \"y\" , y_values . __len__ ()) dataset . createVariable ( \"y\" , \"f4\" ,( \"y\" ,)) dataset . variables [ \"y\" ]. units = \"1\" dataset . variables [ \"y\" ]. long_name = \"projection_y_coordinate\" dataset . variables [ \"y\" ][:] = y_values # x informations dataset . createDimension ( \"x\" , x_values . __len__ ()) dataset . createVariable ( \"x\" , \"f4\" ,( \"x\" ,)) dataset . variables [ \"x\" ]. units = \"1\" dataset . variables [ \"x\" ]. long_name = \"projection_x_coordinate\" dataset . variables [ \"x\" ][:] = x_values # corr ( correlation coefficient ) informations dataset . createVariable ( \"corr\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = 1 , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"corr\" ]. units = \"1\" dataset . variables [ \"corr\" ]. long_name = \"correlation_coefficient\" # ns ( north - south ) informations dataset . createVariable ( \"ns\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ns\" ]. units = \"px\" dataset . variables [ \"ns\" ]. long_name = \"north_south_axis_displacement\" # ew ( east - west ) informations dataset . createVariable ( \"ew\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ew\" ]. units = \"px\" dataset . variables [ \"ew\" ]. long_name = \"east_west_axis_displacement\" # zf = np . zeros (( y_values . __len__ (), x_values . __len__ ())) # feed_NC ( dataset , timestamp , ns = zf , ew = zf , corr = zf ) return dataset feed_NC def feed_NC ( target , time , ** kwargs ) Appende i grid al file netCDF target: netCDF4.Dataset or path to a valid .nc file time: timestamp del dato kwarg: dict nomevariabile:datagrid View Source def feed_NC ( target , time , ** kwargs ) : \"\"\" Appende i grid al file netCDF target: netCDF4.Dataset or path to a valid .nc file time: timestamp del dato kwarg: dict nomevariabile:datagrid \"\"\" if isinstance ( target , Dataset ) : dataset = target else : dataset = Dataset ( target , 'a' , format = \"NETCDF4\" ) timevar = dataset . variables [ 'time' ] time = date2num ( np . datetime64 ( time ). tolist (), timevar . units ) # If exists substitute position = np . where ( timevar == time ) try : position = int ( position [ 0 ] ) except TypeError : position = timevar . shape [ 0 ] timevar [ position ]= time for k , w in kwargs . items () : if w is not None : y_slice = slice ( 0 , w . shape [ 0 ] ) x_slice = slice ( 0 , w . shape [ 1 ] ) try : dataset . variables [ k ][ position,y_slice,x_slice ] = w except KeyError as e : raise ( e ) pass return dataset generate_map def generate_map ( targetfile , timestamp = None , timeref = None , param = None , step_size = None , colors = None , vmin = None , vmax = None ) View Source def generate_map ( targetfile , timestamp = None , timeref = None , param = None , step_size = None , colors = None , vmin = None , vmax = None ): if timestamp is None : pass if param is None : param = \"R\" if step_size is None : stap_size = 1 if colors is None : colors = [ \"red\" , \"green\" , \"blue\" ] if vmin is None : vmin =- 150 if vmax is None : vmax = 150 dataset = xr . open_dataset ( targetfile ) if timestamp is None : timestamp = str ( dataset . time [ - 1 ]. values ) else : timestamp = agoodtime ( timestamp ) ds1 = dataset . sel ( time = timestamp ) ds1 = ds1 . where ( ds1 . corr > 0 ) if timeref is not None : timeref = agoodtime ( timeref ) ds2 = dataset . sel ( time = timeref ) ds2 = ds2 . where ( ds2 . corr > 0 ) ds1 = ds1 - ds2 ds1 . attrs = dataset . attrs managed = _open_matrix ( dataset = ds1 , param = param , step_size = step_size ) return [ timestamp , _render ( ** managed , colors = colors , vmin = vmin , vmax = vmax )] Classes Render class Render ( / , * args , ** kwargs ) View Source class Render () : @property def timeline ( self ) : times = self . dataset . time . values return list ( map ( lambda x : str ( x ). replace ( '.000000000' , '' ),( times ))) @property def reftime ( self ) : return self . dataset . timestamp def isbasetime ( self , time ) : try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , param = \"R\" , timeref = None ) : ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit param: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ] , y = geom [ 1 ] , method = 'nearest' ). sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ) : ref = self . dataset . sel ( x = geom [ 0 ] , y = geom [ 1 ] , time = timeref , method = 'nearest' ) dst [ 'ns' ]= dst . ns - ref . ns dst [ 'ew' ]= dst . ew - ref . ew if param in ( \"R\" , \"RV\" ) : out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if param in ( \"V\" ) : out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if param in ( \"NS\" ) : out = dst . ns if param in ( \"EW\" ) : out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns =[ 'xxx' ] ). dropna () return out Instance variables reftime timeline Methods extract_data def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , param = 'R' , timeref = None ) params geom: point to extract timefrom: time lower limit timeto: time upper limit param: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component View Source def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , param = \"R\" , timeref = None ): ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit param: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method = 'nearest' ). sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ): ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method = 'nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if param in ( \"R\" , \"RV\" ): out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if param in ( \"V\" ): out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if param in ( \"NS\" ): out = dst . ns if param in ( \"EW\" ): out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]). dropna () return out isbasetime def isbasetime ( self , time ) View Source def isbasetime ( self , time ): try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False","title":"Struct"},{"location":"reference/hielen2/ext/source_photomonitoring/struct/#module-hielen2extsource_photomonitoringstruct","text":"View Source # coding: utf-8 import numpy as np import pandas as pd import matplotlib matplotlib . use ( 'Agg' ) import matplotlib.pyplot as plt from matplotlib.colors import LinearSegmentedColormap import xarray as xr , datetime import scipy.ndimage as snd import re from netCDF4 import Dataset , date2num import os import PIL def config_NC ( target , timestamp , x_values , y_values ): \"\"\" Crea il file netCDF secondo un formato standard returns: la struttura dati del file NetCDF params tagetfile: nome del file refitime: tempo zero y_values: array delle y x_values: array delle x \"\"\" dataset = Dataset ( target , 'w' , format = \"NETCDF4\" ) dataset . Conventions = \"CF-1.7\" dataset . timestamp = agoodtime ( timestamp ) # time informations dataset . createDimension ( \"time\" , None ) dataset . createVariable ( \"time\" , \"f8\" ,( \"time\" ,)) dataset . variables [ \"time\" ] . units = f \"hours since { timestamp } \" dataset . variables [ \"time\" ] . calendar = \"standard\" dataset . variables [ \"time\" ] . long_name = \"observation_time\" # y informations dataset . createDimension ( \"y\" , y_values . __len__ ()) dataset . createVariable ( \"y\" , \"f4\" ,( \"y\" ,)) dataset . variables [ \"y\" ] . units = \"1\" dataset . variables [ \"y\" ] . long_name = \"projection_y_coordinate\" dataset . variables [ \"y\" ][:] = y_values # x informations dataset . createDimension ( \"x\" , x_values . __len__ ()) dataset . createVariable ( \"x\" , \"f4\" ,( \"x\" ,)) dataset . variables [ \"x\" ] . units = \"1\" dataset . variables [ \"x\" ] . long_name = \"projection_x_coordinate\" dataset . variables [ \"x\" ][:] = x_values # corr (correlation coefficient) informations dataset . createVariable ( \"corr\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = 1 , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"corr\" ] . units = \"1\" dataset . variables [ \"corr\" ] . long_name = \"correlation_coefficient\" # ns (north-south) informations dataset . createVariable ( \"ns\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ns\" ] . units = \"px\" dataset . variables [ \"ns\" ] . long_name = \"north_south_axis_displacement\" # ew (east-west) informations dataset . createVariable ( \"ew\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ew\" ] . units = \"px\" dataset . variables [ \"ew\" ] . long_name = \"east_west_axis_displacement\" #zf=np.zeros((y_values.__len__(),x_values.__len__())) #feed_NC(dataset,timestamp,ns=zf,ew=zf,corr=zf) return dataset def feed_NC ( target , time , ** kwargs ): \"\"\" Appende i grid al file netCDF target: netCDF4.Dataset or path to a valid .nc file time: timestamp del dato kwarg: dict nomevariabile:datagrid \"\"\" if isinstance ( target , Dataset ): dataset = target else : dataset = Dataset ( target , 'a' , format = \"NETCDF4\" ) timevar = dataset . variables [ 'time' ] time = date2num ( np . datetime64 ( time ) . tolist (), timevar . units ) # If exists substitute position = np . where ( timevar == time ) try : position = int ( position [ 0 ]) except TypeError : position = timevar . shape [ 0 ] timevar [ position ] = time for k , w in kwargs . items (): if w is not None : y_slice = slice ( 0 , w . shape [ 0 ]) x_slice = slice ( 0 , w . shape [ 1 ]) try : dataset . variables [ k ][ position , y_slice , x_slice ] = w except KeyError as e : raise ( e ) pass return dataset def agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t #CASS METHOD def _open_matrix ( dataset , step_size = 1 , param = \"RV\" ): ''' params dataset: dataset at fixed time param: \"RV\" result + vector \"R\" results \"V\" vector \"NS\" North-South\" \"EW\" East-West ''' # filtro con maschera di correlazione # se tutte le celle di corr sono null il minimo \u00e8 TRUE altrimenti FALSE. # Inverto la condizione per sapere se c'\u00e8 qualcosa ns = dataset . ns ew = dataset . ew if param in ( \"D\" , \"V\" ): h = np . sqrt ( ns ** 2 + ew ** 2 ) if param in ( \"EW\" ): h = ew if param in ( \"NS\" ): h = ns # upsampling h = snd . zoom ( h , step_size , order = 0 , mode = 'nearest' ) Y = np . arange ( 0 , h . shape [ 0 ]) X = np . arange ( 0 , h . shape [ 1 ]) heatmap = xr . DataArray ( h , coords = [( \"y\" , Y ), ( \"x\" , X )]) if param in ( \"R\" , \"NS\" , \"EW\" ): return dict ( heatmap = heatmap , vectors = None ) # A questo punto solo RV o V # Elaboro i versori # upsampling a = snd . zoom ( np . arctan2 ( ns , ew ), step_size , order = 0 , mode = 'nearest' ) angle = xr . DataArray ( a , coords = [( \"y\" , Y ), ( \"x\" , X )]) #riduco il versore rollingside = int ( step_size * 6 ) #CON MEDIA #https://stackoverflow.com/questions/52886703/xarray-multidimensional-binning-array-reduction-on-sample-dataset-of-4-x4-to coeff = heatmap . rolling ( x = rollingside ) . construct ( 'tmp' ) . isel ( x = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) coeff = heatmap . rolling ( y = rollingside ) . construct ( 'tmp' ) . isel ( y = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) angle = angle . rolling ( x = rollingside ) . construct ( 'tmp' ) . isel ( x = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) angle = angle . rolling ( y = rollingside ) . construct ( 'tmp' ) . isel ( y = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) #SENZA MEDIA ''' coeff=coeff[::rollingside,::rollingside] ''' # filtro **ARBITRARIAMENTE** quelli che superano il 2 sigma # coeff=coeff.where(coeff<coeff.mean()+coeff.std()*2) # normalizzo su un valore adeguato coeff = coeff / ( coeff . mean () * 4 ) # applico il logaritmo per enfatizzare gli spostamenti minimi e # ridurre l'impatto visivo degli outlayers # coeff=np.log(1+coeff) # filtro l'angolo in base al grid del coefficiente angle = angle . where ( np . abs ( coeff ) > np . abs ( coeff . mean ())) X , Y = np . meshgrid ( angle [ 'x' ], angle [ 'y' ]) dY = np . sin ( angle ) * coeff dX = np . cos ( angle ) * coeff if param in ( \"V\" ): heatmap = heatmap . where ( heatmap is np . nan ) return dict ( heatmap = heatmap , vectors = ( X , Y , dX , dY )) # CLASS METHOD def _render ( heatmap , vectors = None , colors = [ \"green\" , \"red\" , \"blue\" ], vmin = 0 , vmax = 5 ): cmap = LinearSegmentedColormap . from_list ( \"mycmap\" , colors ) W = heatmap . shape [ 1 ] H = heatmap . shape [ 0 ] plt . tight_layout = dict ( pad = 0 ) fig = plt . figure () fig . tight_layout = dict ( pad = 0 ) fig . frameon = False fig . dpi = 72 fig . set_size_inches ( W / fig . dpi , H / fig . dpi ) fig . facecolor = \"None\" fig . linewidth = 0 ax = plt . Axes ( fig ,[ 0 , 0 , 1 , 1 ]) ax . set_axis_off () ax . imshow ( heatmap , alpha = 0.20 , origin = 'upper' , cmap = cmap , norm = None , vmin = vmin , vmax = vmax ) if vectors is not None : ax . quiver ( * vectors , width = 0.0008 , color = 'white' , scale = 100 ) fig . add_axes ( ax ) fig . canvas . draw () data = np . frombuffer ( fig . canvas . tostring_argb (), dtype = np . uint8 ) data = data . reshape ( fig . canvas . get_width_height ()[:: - 1 ] + ( 4 ,))[:, :, [ 3 , 2 , 1 , 0 ]] plt . close () return data def generate_map ( targetfile , timestamp = None , timeref = None , param = None , step_size = None , colors = None , vmin = None , vmax = None ): if timestamp is None : pass if param is None : param = \"R\" if step_size is None : stap_size = 1 if colors is None : colors = [ \"red\" , \"green\" , \"blue\" ] if vmin is None : vmin =- 150 if vmax is None : vmax = 150 dataset = xr . open_dataset ( targetfile ) if timestamp is None : timestamp = str ( dataset . time [ - 1 ] . values ) else : timestamp = agoodtime ( timestamp ) ds1 = dataset . sel ( time = timestamp ) ds1 = ds1 . where ( ds1 . corr > 0 ) if timeref is not None : timeref = agoodtime ( timeref ) ds2 = dataset . sel ( time = timeref ) ds2 = ds2 . where ( ds2 . corr > 0 ) ds1 = ds1 - ds2 ds1 . attrs = dataset . attrs managed = _open_matrix ( dataset = ds1 , param = param , step_size = step_size ) return [ timestamp , _render ( ** managed , colors = colors , vmin = vmin , vmax = vmax )] class Render (): @property def timeline ( self ): times = self . dataset . time . values return list ( map ( lambda x : str ( x ) . replace ( '.000000000' , '' ),( times ))) @property def reftime ( self ): return self . dataset . timestamp def isbasetime ( self , time ): try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , param = \"R\" , timeref = None ): ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit param: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method = 'nearest' ) . sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ): ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method = 'nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if param in ( \"R\" , \"RV\" ): out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if param in ( \"V\" ): out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if param in ( \"NS\" ): out = dst . ns if param in ( \"EW\" ): out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]) . dropna () return out","title":"Module hielen2.ext.source_photomonitoring.struct"},{"location":"reference/hielen2/ext/source_photomonitoring/struct/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/ext/source_photomonitoring/struct/#agoodtime","text":"def agoodtime ( t ) View Source def agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t","title":"agoodtime"},{"location":"reference/hielen2/ext/source_photomonitoring/struct/#config_nc","text":"def config_NC ( target , timestamp , x_values , y_values ) Crea il file netCDF secondo un formato standard returns: la struttura dati del file NetCDF params tagetfile: nome del file refitime: tempo zero y_values: array delle y x_values: array delle x View Source def config_NC ( target , timestamp , x_values , y_values ): \"\"\" Crea il file netCDF secondo un formato standard returns: la struttura dati del file NetCDF params tagetfile: nome del file refitime: tempo zero y_values: array delle y x_values: array delle x \"\"\" dataset = Dataset ( target , 'w' , format = \"NETCDF4\" ) dataset . Conventions = \"CF-1.7\" dataset . timestamp = agoodtime ( timestamp ) # time informations dataset . createDimension ( \"time\" , None ) dataset . createVariable ( \"time\" , \"f8\" ,( \"time\" ,)) dataset . variables [ \"time\" ]. units = f \"hours since {timestamp}\" dataset . variables [ \"time\" ]. calendar = \"standard\" dataset . variables [ \"time\" ]. long_name = \"observation_time\" # y informations dataset . createDimension ( \"y\" , y_values . __len__ ()) dataset . createVariable ( \"y\" , \"f4\" ,( \"y\" ,)) dataset . variables [ \"y\" ]. units = \"1\" dataset . variables [ \"y\" ]. long_name = \"projection_y_coordinate\" dataset . variables [ \"y\" ][:] = y_values # x informations dataset . createDimension ( \"x\" , x_values . __len__ ()) dataset . createVariable ( \"x\" , \"f4\" ,( \"x\" ,)) dataset . variables [ \"x\" ]. units = \"1\" dataset . variables [ \"x\" ]. long_name = \"projection_x_coordinate\" dataset . variables [ \"x\" ][:] = x_values # corr ( correlation coefficient ) informations dataset . createVariable ( \"corr\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = 1 , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"corr\" ]. units = \"1\" dataset . variables [ \"corr\" ]. long_name = \"correlation_coefficient\" # ns ( north - south ) informations dataset . createVariable ( \"ns\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ns\" ]. units = \"px\" dataset . variables [ \"ns\" ]. long_name = \"north_south_axis_displacement\" # ew ( east - west ) informations dataset . createVariable ( \"ew\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ew\" ]. units = \"px\" dataset . variables [ \"ew\" ]. long_name = \"east_west_axis_displacement\" # zf = np . zeros (( y_values . __len__ (), x_values . __len__ ())) # feed_NC ( dataset , timestamp , ns = zf , ew = zf , corr = zf ) return dataset","title":"config_NC"},{"location":"reference/hielen2/ext/source_photomonitoring/struct/#feed_nc","text":"def feed_NC ( target , time , ** kwargs ) Appende i grid al file netCDF target: netCDF4.Dataset or path to a valid .nc file time: timestamp del dato kwarg: dict nomevariabile:datagrid View Source def feed_NC ( target , time , ** kwargs ) : \"\"\" Appende i grid al file netCDF target: netCDF4.Dataset or path to a valid .nc file time: timestamp del dato kwarg: dict nomevariabile:datagrid \"\"\" if isinstance ( target , Dataset ) : dataset = target else : dataset = Dataset ( target , 'a' , format = \"NETCDF4\" ) timevar = dataset . variables [ 'time' ] time = date2num ( np . datetime64 ( time ). tolist (), timevar . units ) # If exists substitute position = np . where ( timevar == time ) try : position = int ( position [ 0 ] ) except TypeError : position = timevar . shape [ 0 ] timevar [ position ]= time for k , w in kwargs . items () : if w is not None : y_slice = slice ( 0 , w . shape [ 0 ] ) x_slice = slice ( 0 , w . shape [ 1 ] ) try : dataset . variables [ k ][ position,y_slice,x_slice ] = w except KeyError as e : raise ( e ) pass return dataset","title":"feed_NC"},{"location":"reference/hielen2/ext/source_photomonitoring/struct/#generate_map","text":"def generate_map ( targetfile , timestamp = None , timeref = None , param = None , step_size = None , colors = None , vmin = None , vmax = None ) View Source def generate_map ( targetfile , timestamp = None , timeref = None , param = None , step_size = None , colors = None , vmin = None , vmax = None ): if timestamp is None : pass if param is None : param = \"R\" if step_size is None : stap_size = 1 if colors is None : colors = [ \"red\" , \"green\" , \"blue\" ] if vmin is None : vmin =- 150 if vmax is None : vmax = 150 dataset = xr . open_dataset ( targetfile ) if timestamp is None : timestamp = str ( dataset . time [ - 1 ]. values ) else : timestamp = agoodtime ( timestamp ) ds1 = dataset . sel ( time = timestamp ) ds1 = ds1 . where ( ds1 . corr > 0 ) if timeref is not None : timeref = agoodtime ( timeref ) ds2 = dataset . sel ( time = timeref ) ds2 = ds2 . where ( ds2 . corr > 0 ) ds1 = ds1 - ds2 ds1 . attrs = dataset . attrs managed = _open_matrix ( dataset = ds1 , param = param , step_size = step_size ) return [ timestamp , _render ( ** managed , colors = colors , vmin = vmin , vmax = vmax )]","title":"generate_map"},{"location":"reference/hielen2/ext/source_photomonitoring/struct/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/ext/source_photomonitoring/struct/#render","text":"class Render ( / , * args , ** kwargs ) View Source class Render () : @property def timeline ( self ) : times = self . dataset . time . values return list ( map ( lambda x : str ( x ). replace ( '.000000000' , '' ),( times ))) @property def reftime ( self ) : return self . dataset . timestamp def isbasetime ( self , time ) : try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , param = \"R\" , timeref = None ) : ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit param: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ] , y = geom [ 1 ] , method = 'nearest' ). sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ) : ref = self . dataset . sel ( x = geom [ 0 ] , y = geom [ 1 ] , time = timeref , method = 'nearest' ) dst [ 'ns' ]= dst . ns - ref . ns dst [ 'ew' ]= dst . ew - ref . ew if param in ( \"R\" , \"RV\" ) : out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if param in ( \"V\" ) : out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if param in ( \"NS\" ) : out = dst . ns if param in ( \"EW\" ) : out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns =[ 'xxx' ] ). dropna () return out","title":"Render"},{"location":"reference/hielen2/ext/source_photomonitoring/struct/#instance-variables","text":"reftime timeline","title":"Instance variables"},{"location":"reference/hielen2/ext/source_photomonitoring/struct/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_photomonitoring/struct/#extract_data","text":"def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , param = 'R' , timeref = None ) params geom: point to extract timefrom: time lower limit timeto: time upper limit param: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component View Source def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , param = \"R\" , timeref = None ): ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit param: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method = 'nearest' ). sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ): ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method = 'nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if param in ( \"R\" , \"RV\" ): out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if param in ( \"V\" ): out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if param in ( \"NS\" ): out = dst . ns if param in ( \"EW\" ): out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]). dropna () return out","title":"extract_data"},{"location":"reference/hielen2/ext/source_photomonitoring/struct/#isbasetime","text":"def isbasetime ( self , time ) View Source def isbasetime ( self , time ): try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False","title":"isbasetime"},{"location":"reference/hielen2/ext/source_photomonitoring/struct_ok/","text":"Module hielen2.ext.source_photomonitoring.struct_ok View Source # coding: utf-8 import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib.colors import LinearSegmentedColormap import xarray as xr , datetime import scipy.ndimage as snd import re from netCDF4 import Dataset , date2num import os def config_NC ( target , timestamp , x_values , y_values , step_size = None , x_offset = None , y_offset = None ): \"\"\" Crea il file netCDF secondo un formato standard returns: la struttura dati del file NetCDF params tagetfile: nome del file refitime: tempo zero y_values: array delle y x_values: array delle x \"\"\" try : step_size = int ( step_size ) except Exception as e : step_size = 1 try : x_offset = int ( step_size ) except Exception as e : x_offset = 0 try : y_offset = int ( step_size ) except Exception as e : y_offset = 0 y_values = y_values [ y_offset :: step_size ] x_values = x_values [ x_offset :: step_size ] dataset = Dataset ( target , 'w' , format = \"NETCDF4\" ) dataset . Conventions = \"CF-1.7\" dataset . timestamp = agoodtime ( timestamp ) dataset . step_size = step_size # time informations dataset . createDimension ( \"time\" , None ) dataset . createVariable ( \"time\" , \"f8\" ,( \"time\" ,)) dataset . variables [ \"time\" ] . units = f \"hours since { timestamp } \" dataset . variables [ \"time\" ] . calendar = \"standard\" dataset . variables [ \"time\" ] . long_name = \"observation_time\" # y informations dataset . createDimension ( \"y\" , y_values . __len__ ()) dataset . createVariable ( \"y\" , \"f4\" ,( \"y\" ,)) dataset . variables [ \"y\" ] . units = \"1\" dataset . variables [ \"y\" ] . long_name = \"projection_y_coordinate\" dataset . variables [ \"y\" ] . pixel_offset = y_offset dataset . variables [ \"y\" ][:] = y_values # x informations dataset . createDimension ( \"x\" , x_values . __len__ ()) dataset . createVariable ( \"x\" , \"f4\" ,( \"x\" ,)) dataset . variables [ \"x\" ] . units = \"1\" dataset . variables [ \"x\" ] . long_name = \"projection_x_coordinate\" dataset . variables [ \"x\" ] . pixel_offset = x_offset dataset . variables [ \"x\" ][:] = x_values # ns (north-south) informations dataset . createVariable ( \"ns\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ns\" ] . units = \"px\" dataset . variables [ \"ns\" ] . long_name = \"north_south_axis_displacement\" # ew (east-west) informations dataset . createVariable ( \"ew\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ew\" ] . units = \"px\" dataset . variables [ \"ew\" ] . long_name = \"east_west_axis_displacement\" # corr (correlation coefficient) informations dataset . createVariable ( \"corr\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"corr\" ] . units = \"1\" dataset . variables [ \"corr\" ] . long_name = \"correlation_coefficient\" zf = np . zeros (( y_values . __len__ (), x_values . __len__ ())) feed_NC ( dataset , timestamp , ns = zf , ew = zf , corr = zf ) return dataset def feed_NC ( target , time , ** kwargs ): \"\"\" Appende i grid al file netCDF target: netCDF4.Dataset or path to a valid .nc file time: timestamp del dato kwarg: dict nomevariabile:datagrid \"\"\" if isinstance ( target , Dataset ): dataset = target else : dataset = Dataset ( target , 'a' , format = \"NETCDF4\" ) timevar = dataset . variables [ 'time' ] time = date2num ( np . datetime64 ( time ) . tolist (), timevar . units ) # If exists substitute position = np . where ( timevar == time ) try : position = int ( position [ 0 ]) except TypeError : position = timevar . shape [ 0 ] timevar [ position ] = time for k , w in kwargs . items (): if w is not None : y_slice = slice ( 0 , w . shape [ 0 ]) x_slice = slice ( 0 , w . shape [ 1 ]) try : dataset . variables [ k ][ position , y_slice , x_slice ] = w except KeyError as e : pass return dataset def agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t #CASS METHOD def _open_matrix ( dataset , output = \"RV\" , correlation = 0 ): ''' params dataset: dataset at fixed time output: \"RV\" result + vector \"R\" results \"V\" vector \"NS\" North-South\" \"EW\" East-West ''' # filtro con maschera di correlazione # se tutte le celle di corr sono null il minimo \u00e8 TRUE altrimenti FALSE. # Inverto la condizione per sapere se c'\u00e8 qualcosa if correlation and not ns . corr . isnull () . min (): ns = dataset . ns . where ( dataset . corr >= correlation ) ew = dataset . ew . where ( dataset . corr >= correlation ) else : ns = dataset . ns ew = dataset . ew step_size = dataset . step_size if output in ( \"RV\" , \"R\" , \"V\" ): h = np . sqrt ( ns ** 2 + ew ** 2 ) if output in ( \"EW\" ): h = ew if output in ( \"NS\" ): h = ns # upsampling h = snd . zoom ( h , step_size , order = 0 , mode = 'nearest' ) Y = np . arange ( 0 , h . shape [ 0 ]) X = np . arange ( 0 , h . shape [ 1 ]) heatmap = xr . DataArray ( h , coords = [( \"y\" , Y ), ( \"x\" , X )]) if output in ( \"R\" , \"NS\" , \"EW\" ): return dict ( heatmap = heatmap , vectors = None , dims = h . shape ) # A questo punto solo RV o V # Elaboro i versori # upsampling a = snd . zoom ( np . arctan2 ( ns , ew ), step_size , order = 0 , mode = 'nearest' ) angle = xr . DataArray ( a , coords = [( \"y\" , Y ), ( \"x\" , X )]) #riduco il versore rollingside = 100 #CON MEDIA #https://stackoverflow.com/questions/52886703/xarray-multidimensional-binning-array-reduction-on-sample-dataset-of-4-x4-to coeff = heatmap . rolling ( x = rollingside ) . construct ( 'tmp' ) . isel ( x = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) coeff = heatmap . rolling ( y = rollingside ) . construct ( 'tmp' ) . isel ( y = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) angle = angle . rolling ( x = rollingside ) . construct ( 'tmp' ) . isel ( x = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) angle = angle . rolling ( y = rollingside ) . construct ( 'tmp' ) . isel ( y = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) #SENZA MEDIA ''' coeff=coeff[::rollingside,::rollingside] ''' # filtro **ARBITRARIAMENTE** quelli che superano il 2 sigma # coeff=coeff.where(coeff<coeff.mean()+coeff.std()*2) # normalizzo su un valore adeguato coeff = coeff / coeff . mean () # applico il logaritmo per enfatizzare gli spostamenti minimi e # ridurre l'impatto visivo degli outlayers # coeff=np.log(1+coeff) # filtro l'angolo in base al grid del coefficiente angle = angle . where ( coeff ) X , Y = np . meshgrid ( angle [ 'x' ], angle [ 'y' ]) dY = np . sin ( angle ) * coeff dX = np . cos ( angle ) * coeff if output in ( \"V\" ): heatmap = heatmap . where ( heatmap is np . nan ) return dict ( heatmap = heatmap , vectors = ( X , Y , dX , dY ), dims = h . shape ) # CLASS METHOD def _render ( heatmap , vectors = None , colors = [ \"green\" , \"red\" , \"blue\" ], vmin = 0 , vmax = 5 ): cmap = LinearSegmentedColormap . from_list ( \"mycmap\" , colors ) W = heatmap . shape [ 1 ] H = heatmap . shape [ 1 ] plt . tight_layout = dict ( pad = 0 ) fig = plt . figure () fig . tight_layout = dict ( pad = 0 ) fig . frameon = False fig . dpi = 72 fig . set_size_inches ( W / dpi , H / dpi ) fig . facecolor = \"None\" fig . linewidth = 0 ax = plt . Axes ( fig ,[ 0 , 0 , 1 , 1 ]) ax . set_axis_off () ax . imshow ( heatmap , alpha = 0.20 , origin = 'upper' , cmap = cmap , norm = None , vmin = vmin , vmax = vmax ) if vectors is not None : ax . quiver ( * vectors , width = 0.0008 , color = 'white' , scale = 100 ) fig . add_axes ( ax ) fig . canvas . draw () data = np . frombuffer ( fig . canvas . tostring_argb (), dtype = np . uint8 ) data = data . reshape ( fig . canvas . get_width_height ()[:: - 1 ] + ( 4 ,))[:, :, [ 3 , 2 , 1 , 0 ]] return data def generate_map ( targetfile , timestamp = None , timeref = None , output = \"RV\" , cmap = [ \"green\" , \"red\" , \"blue\" ], vmin =- 2.5 , vmax = 2 - 5 ): dataset = xr . open_dataset ( targetfile ) if timestamp is None : timestamp = str ( dataset . time [ - 1 ] . values ) else : timestamp = agoodtime ( timestamp ) if timeref is None : timeref = str ( dataset . time [ 0 ] . values ) correlation = 0.01 else : timeref = agoodtime ( timeref ) correlation = 0 print ( \"a\" ) #Nel caso di ref zero il risultato \u00e8 dato dal dataset in \"timestamp\" d1 = dataset . sel ( time = timestamp ) . compute () d2 = dataset . sel ( time = timeref ) . compute () d3 = ( d1 - d2 ) . compute () print ( d2 ) managed = _open_matrix ( dataset = dataset , output = output , correlation = correlation ) print ( \"c\" ) imgarray = _render ( ** managed , colors = colors , vmin = vmin , vmax = vmax ) print ( \"\" ) # void=xr.DataArray(h, coords=[(\"y\", Y), (\"x\", X)]) # result = PIL.Image.fromarray(imgarray) # img = PIL.Image.new(result.mode, (width, height), (0,0,0,0)) # img.paste(result, dataset.x.pixels_offset,dataset.y.pixels_offset) # filename=f\"{re.sub('[^\\d]','',timeref)}_{re.sub('[^\\d]','',timestamp)}.tiff\" class Render (): def __init__ ( self , targetfile = './incomes/sag.nc' , gridratio = 8 , ** kwargs ): self . gridratio = gridratio @property def timeline ( self ): times = self . dataset . time . values return list ( map ( lambda x : str ( x ) . replace ( '.000000000' , '' ),( times ))) @property def reftime ( self ): return self . dataset . timestamp def isbasetime ( self , time ): try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = \"R\" , timeref = None ): ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method = 'nearest' ) . sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ): ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method = 'nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if output in ( \"R\" , \"RV\" ): out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if output in ( \"V\" ): out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if output in ( \"NS\" ): out = dst . ns if output in ( \"EW\" ): out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]) . dropna () return out Functions agoodtime def agoodtime ( t ) View Source def agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t config_NC def config_NC ( target , timestamp , x_values , y_values , step_size = None , x_offset = None , y_offset = None ) Crea il file netCDF secondo un formato standard returns: la struttura dati del file NetCDF params tagetfile: nome del file refitime: tempo zero y_values: array delle y x_values: array delle x View Source def config_NC ( target , timestamp , x_values , y_values , step_size = None , x_offset = None , y_offset = None ): \"\"\" Crea il file netCDF secondo un formato standard returns: la struttura dati del file NetCDF params tagetfile: nome del file refitime: tempo zero y_values: array delle y x_values: array delle x \"\"\" try : step_size = int ( step_size ) except Exception as e : step_size = 1 try : x_offset = int ( step_size ) except Exception as e : x_offset = 0 try : y_offset = int ( step_size ) except Exception as e : y_offset = 0 y_values = y_values [ y_offset :: step_size ] x_values = x_values [ x_offset :: step_size ] dataset = Dataset ( target , 'w' , format = \"NETCDF4\" ) dataset . Conventions = \"CF-1.7\" dataset . timestamp = agoodtime ( timestamp ) dataset . step_size = step_size # time informations dataset . createDimension ( \"time\" , None ) dataset . createVariable ( \"time\" , \"f8\" ,( \"time\" ,)) dataset . variables [ \"time\" ]. units = f \"hours since {timestamp}\" dataset . variables [ \"time\" ]. calendar = \"standard\" dataset . variables [ \"time\" ]. long_name = \"observation_time\" # y informations dataset . createDimension ( \"y\" , y_values . __len__ ()) dataset . createVariable ( \"y\" , \"f4\" ,( \"y\" ,)) dataset . variables [ \"y\" ]. units = \"1\" dataset . variables [ \"y\" ]. long_name = \"projection_y_coordinate\" dataset . variables [ \"y\" ]. pixel_offset = y_offset dataset . variables [ \"y\" ][:] = y_values # x informations dataset . createDimension ( \"x\" , x_values . __len__ ()) dataset . createVariable ( \"x\" , \"f4\" ,( \"x\" ,)) dataset . variables [ \"x\" ]. units = \"1\" dataset . variables [ \"x\" ]. long_name = \"projection_x_coordinate\" dataset . variables [ \"x\" ]. pixel_offset = x_offset dataset . variables [ \"x\" ][:] = x_values # ns ( north - south ) informations dataset . createVariable ( \"ns\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ns\" ]. units = \"px\" dataset . variables [ \"ns\" ]. long_name = \"north_south_axis_displacement\" # ew ( east - west ) informations dataset . createVariable ( \"ew\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ew\" ]. units = \"px\" dataset . variables [ \"ew\" ]. long_name = \"east_west_axis_displacement\" # corr ( correlation coefficient ) informations dataset . createVariable ( \"corr\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"corr\" ]. units = \"1\" dataset . variables [ \"corr\" ]. long_name = \"correlation_coefficient\" zf = np . zeros (( y_values . __len__ (), x_values . __len__ ())) feed_NC ( dataset , timestamp , ns = zf , ew = zf , corr = zf ) return dataset feed_NC def feed_NC ( target , time , ** kwargs ) Appende i grid al file netCDF target: netCDF4.Dataset or path to a valid .nc file time: timestamp del dato kwarg: dict nomevariabile:datagrid View Source def feed_NC ( target , time , ** kwargs ) : \"\"\" Appende i grid al file netCDF target: netCDF4.Dataset or path to a valid .nc file time: timestamp del dato kwarg: dict nomevariabile:datagrid \"\"\" if isinstance ( target , Dataset ) : dataset = target else : dataset = Dataset ( target , 'a' , format = \"NETCDF4\" ) timevar = dataset . variables [ 'time' ] time = date2num ( np . datetime64 ( time ). tolist (), timevar . units ) # If exists substitute position = np . where ( timevar == time ) try : position = int ( position [ 0 ] ) except TypeError : position = timevar . shape [ 0 ] timevar [ position ]= time for k , w in kwargs . items () : if w is not None : y_slice = slice ( 0 , w . shape [ 0 ] ) x_slice = slice ( 0 , w . shape [ 1 ] ) try : dataset . variables [ k ][ position,y_slice,x_slice ] = w except KeyError as e : pass return dataset generate_map def generate_map ( targetfile , timestamp = None , timeref = None , output = 'RV' , cmap = [ 'green' , 'red' , 'blue' ], vmin =- 2.5 , vmax =- 3 ) View Source def generate_map ( targetfile , timestamp = None , timeref = None , output = \"RV\" , cmap = [ \"green\" , \"red\" , \"blue\" ], vmin =- 2 . 5 , vmax = 2 - 5 ): dataset = xr . open_dataset ( targetfile ) if timestamp is None : timestamp = str ( dataset . time [ - 1 ]. values ) else : timestamp = agoodtime ( timestamp ) if timeref is None : timeref = str ( dataset . time [ 0 ]. values ) correlation = 0 . 01 else : timeref = agoodtime ( timeref ) correlation = 0 print ( \"a\" ) # Nel caso di ref zero il risultato \u00e8 dato dal dataset in \"timestamp\" d1 = dataset . sel ( time = timestamp ). compute () d2 = dataset . sel ( time = timeref ). compute () d3 = ( d1 - d2 ). compute () print ( d2 ) managed = _open_matrix ( dataset = dataset , output = output , correlation = correlation ) print ( \"c\" ) imgarray = _render ( ** managed , colors = colors , vmin = vmin , vmax = vmax ) print ( \"\" ) Classes Render class Render ( targetfile = './incomes/sag.nc' , gridratio = 8 , ** kwargs ) View Source class Render () : def __init__ ( self , targetfile = './incomes/sag.nc' , gridratio = 8 , ** kwargs ) : self . gridratio = gridratio @property def timeline ( self ) : times = self . dataset . time . values return list ( map ( lambda x : str ( x ). replace ( '.000000000' , '' ),( times ))) @property def reftime ( self ) : return self . dataset . timestamp def isbasetime ( self , time ) : try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = \"R\" , timeref = None ) : ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ] , y = geom [ 1 ] , method = 'nearest' ). sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ) : ref = self . dataset . sel ( x = geom [ 0 ] , y = geom [ 1 ] , time = timeref , method = 'nearest' ) dst [ 'ns' ]= dst . ns - ref . ns dst [ 'ew' ]= dst . ew - ref . ew if output in ( \"R\" , \"RV\" ) : out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if output in ( \"V\" ) : out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if output in ( \"NS\" ) : out = dst . ns if output in ( \"EW\" ) : out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns =[ 'xxx' ] ). dropna () return out Instance variables reftime timeline Methods extract_data def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = 'R' , timeref = None ) params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component View Source def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = \"R\" , timeref = None ): ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method = 'nearest' ). sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ): ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method = 'nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if output in ( \"R\" , \"RV\" ): out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if output in ( \"V\" ): out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if output in ( \"NS\" ): out = dst . ns if output in ( \"EW\" ): out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]). dropna () return out isbasetime def isbasetime ( self , time ) View Source def isbasetime ( self , time ): try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False","title":"Struct Ok"},{"location":"reference/hielen2/ext/source_photomonitoring/struct_ok/#module-hielen2extsource_photomonitoringstruct_ok","text":"View Source # coding: utf-8 import numpy as np import pandas as pd import matplotlib.pyplot as plt from matplotlib.colors import LinearSegmentedColormap import xarray as xr , datetime import scipy.ndimage as snd import re from netCDF4 import Dataset , date2num import os def config_NC ( target , timestamp , x_values , y_values , step_size = None , x_offset = None , y_offset = None ): \"\"\" Crea il file netCDF secondo un formato standard returns: la struttura dati del file NetCDF params tagetfile: nome del file refitime: tempo zero y_values: array delle y x_values: array delle x \"\"\" try : step_size = int ( step_size ) except Exception as e : step_size = 1 try : x_offset = int ( step_size ) except Exception as e : x_offset = 0 try : y_offset = int ( step_size ) except Exception as e : y_offset = 0 y_values = y_values [ y_offset :: step_size ] x_values = x_values [ x_offset :: step_size ] dataset = Dataset ( target , 'w' , format = \"NETCDF4\" ) dataset . Conventions = \"CF-1.7\" dataset . timestamp = agoodtime ( timestamp ) dataset . step_size = step_size # time informations dataset . createDimension ( \"time\" , None ) dataset . createVariable ( \"time\" , \"f8\" ,( \"time\" ,)) dataset . variables [ \"time\" ] . units = f \"hours since { timestamp } \" dataset . variables [ \"time\" ] . calendar = \"standard\" dataset . variables [ \"time\" ] . long_name = \"observation_time\" # y informations dataset . createDimension ( \"y\" , y_values . __len__ ()) dataset . createVariable ( \"y\" , \"f4\" ,( \"y\" ,)) dataset . variables [ \"y\" ] . units = \"1\" dataset . variables [ \"y\" ] . long_name = \"projection_y_coordinate\" dataset . variables [ \"y\" ] . pixel_offset = y_offset dataset . variables [ \"y\" ][:] = y_values # x informations dataset . createDimension ( \"x\" , x_values . __len__ ()) dataset . createVariable ( \"x\" , \"f4\" ,( \"x\" ,)) dataset . variables [ \"x\" ] . units = \"1\" dataset . variables [ \"x\" ] . long_name = \"projection_x_coordinate\" dataset . variables [ \"x\" ] . pixel_offset = x_offset dataset . variables [ \"x\" ][:] = x_values # ns (north-south) informations dataset . createVariable ( \"ns\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ns\" ] . units = \"px\" dataset . variables [ \"ns\" ] . long_name = \"north_south_axis_displacement\" # ew (east-west) informations dataset . createVariable ( \"ew\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ew\" ] . units = \"px\" dataset . variables [ \"ew\" ] . long_name = \"east_west_axis_displacement\" # corr (correlation coefficient) informations dataset . createVariable ( \"corr\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"corr\" ] . units = \"1\" dataset . variables [ \"corr\" ] . long_name = \"correlation_coefficient\" zf = np . zeros (( y_values . __len__ (), x_values . __len__ ())) feed_NC ( dataset , timestamp , ns = zf , ew = zf , corr = zf ) return dataset def feed_NC ( target , time , ** kwargs ): \"\"\" Appende i grid al file netCDF target: netCDF4.Dataset or path to a valid .nc file time: timestamp del dato kwarg: dict nomevariabile:datagrid \"\"\" if isinstance ( target , Dataset ): dataset = target else : dataset = Dataset ( target , 'a' , format = \"NETCDF4\" ) timevar = dataset . variables [ 'time' ] time = date2num ( np . datetime64 ( time ) . tolist (), timevar . units ) # If exists substitute position = np . where ( timevar == time ) try : position = int ( position [ 0 ]) except TypeError : position = timevar . shape [ 0 ] timevar [ position ] = time for k , w in kwargs . items (): if w is not None : y_slice = slice ( 0 , w . shape [ 0 ]) x_slice = slice ( 0 , w . shape [ 1 ]) try : dataset . variables [ k ][ position , y_slice , x_slice ] = w except KeyError as e : pass return dataset def agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t #CASS METHOD def _open_matrix ( dataset , output = \"RV\" , correlation = 0 ): ''' params dataset: dataset at fixed time output: \"RV\" result + vector \"R\" results \"V\" vector \"NS\" North-South\" \"EW\" East-West ''' # filtro con maschera di correlazione # se tutte le celle di corr sono null il minimo \u00e8 TRUE altrimenti FALSE. # Inverto la condizione per sapere se c'\u00e8 qualcosa if correlation and not ns . corr . isnull () . min (): ns = dataset . ns . where ( dataset . corr >= correlation ) ew = dataset . ew . where ( dataset . corr >= correlation ) else : ns = dataset . ns ew = dataset . ew step_size = dataset . step_size if output in ( \"RV\" , \"R\" , \"V\" ): h = np . sqrt ( ns ** 2 + ew ** 2 ) if output in ( \"EW\" ): h = ew if output in ( \"NS\" ): h = ns # upsampling h = snd . zoom ( h , step_size , order = 0 , mode = 'nearest' ) Y = np . arange ( 0 , h . shape [ 0 ]) X = np . arange ( 0 , h . shape [ 1 ]) heatmap = xr . DataArray ( h , coords = [( \"y\" , Y ), ( \"x\" , X )]) if output in ( \"R\" , \"NS\" , \"EW\" ): return dict ( heatmap = heatmap , vectors = None , dims = h . shape ) # A questo punto solo RV o V # Elaboro i versori # upsampling a = snd . zoom ( np . arctan2 ( ns , ew ), step_size , order = 0 , mode = 'nearest' ) angle = xr . DataArray ( a , coords = [( \"y\" , Y ), ( \"x\" , X )]) #riduco il versore rollingside = 100 #CON MEDIA #https://stackoverflow.com/questions/52886703/xarray-multidimensional-binning-array-reduction-on-sample-dataset-of-4-x4-to coeff = heatmap . rolling ( x = rollingside ) . construct ( 'tmp' ) . isel ( x = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) coeff = heatmap . rolling ( y = rollingside ) . construct ( 'tmp' ) . isel ( y = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) angle = angle . rolling ( x = rollingside ) . construct ( 'tmp' ) . isel ( x = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) angle = angle . rolling ( y = rollingside ) . construct ( 'tmp' ) . isel ( y = slice ( 1 , None , rollingside )) . mean ( 'tmp' , skipna = False ) #SENZA MEDIA ''' coeff=coeff[::rollingside,::rollingside] ''' # filtro **ARBITRARIAMENTE** quelli che superano il 2 sigma # coeff=coeff.where(coeff<coeff.mean()+coeff.std()*2) # normalizzo su un valore adeguato coeff = coeff / coeff . mean () # applico il logaritmo per enfatizzare gli spostamenti minimi e # ridurre l'impatto visivo degli outlayers # coeff=np.log(1+coeff) # filtro l'angolo in base al grid del coefficiente angle = angle . where ( coeff ) X , Y = np . meshgrid ( angle [ 'x' ], angle [ 'y' ]) dY = np . sin ( angle ) * coeff dX = np . cos ( angle ) * coeff if output in ( \"V\" ): heatmap = heatmap . where ( heatmap is np . nan ) return dict ( heatmap = heatmap , vectors = ( X , Y , dX , dY ), dims = h . shape ) # CLASS METHOD def _render ( heatmap , vectors = None , colors = [ \"green\" , \"red\" , \"blue\" ], vmin = 0 , vmax = 5 ): cmap = LinearSegmentedColormap . from_list ( \"mycmap\" , colors ) W = heatmap . shape [ 1 ] H = heatmap . shape [ 1 ] plt . tight_layout = dict ( pad = 0 ) fig = plt . figure () fig . tight_layout = dict ( pad = 0 ) fig . frameon = False fig . dpi = 72 fig . set_size_inches ( W / dpi , H / dpi ) fig . facecolor = \"None\" fig . linewidth = 0 ax = plt . Axes ( fig ,[ 0 , 0 , 1 , 1 ]) ax . set_axis_off () ax . imshow ( heatmap , alpha = 0.20 , origin = 'upper' , cmap = cmap , norm = None , vmin = vmin , vmax = vmax ) if vectors is not None : ax . quiver ( * vectors , width = 0.0008 , color = 'white' , scale = 100 ) fig . add_axes ( ax ) fig . canvas . draw () data = np . frombuffer ( fig . canvas . tostring_argb (), dtype = np . uint8 ) data = data . reshape ( fig . canvas . get_width_height ()[:: - 1 ] + ( 4 ,))[:, :, [ 3 , 2 , 1 , 0 ]] return data def generate_map ( targetfile , timestamp = None , timeref = None , output = \"RV\" , cmap = [ \"green\" , \"red\" , \"blue\" ], vmin =- 2.5 , vmax = 2 - 5 ): dataset = xr . open_dataset ( targetfile ) if timestamp is None : timestamp = str ( dataset . time [ - 1 ] . values ) else : timestamp = agoodtime ( timestamp ) if timeref is None : timeref = str ( dataset . time [ 0 ] . values ) correlation = 0.01 else : timeref = agoodtime ( timeref ) correlation = 0 print ( \"a\" ) #Nel caso di ref zero il risultato \u00e8 dato dal dataset in \"timestamp\" d1 = dataset . sel ( time = timestamp ) . compute () d2 = dataset . sel ( time = timeref ) . compute () d3 = ( d1 - d2 ) . compute () print ( d2 ) managed = _open_matrix ( dataset = dataset , output = output , correlation = correlation ) print ( \"c\" ) imgarray = _render ( ** managed , colors = colors , vmin = vmin , vmax = vmax ) print ( \"\" ) # void=xr.DataArray(h, coords=[(\"y\", Y), (\"x\", X)]) # result = PIL.Image.fromarray(imgarray) # img = PIL.Image.new(result.mode, (width, height), (0,0,0,0)) # img.paste(result, dataset.x.pixels_offset,dataset.y.pixels_offset) # filename=f\"{re.sub('[^\\d]','',timeref)}_{re.sub('[^\\d]','',timestamp)}.tiff\" class Render (): def __init__ ( self , targetfile = './incomes/sag.nc' , gridratio = 8 , ** kwargs ): self . gridratio = gridratio @property def timeline ( self ): times = self . dataset . time . values return list ( map ( lambda x : str ( x ) . replace ( '.000000000' , '' ),( times ))) @property def reftime ( self ): return self . dataset . timestamp def isbasetime ( self , time ): try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = \"R\" , timeref = None ): ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method = 'nearest' ) . sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ): ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method = 'nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if output in ( \"R\" , \"RV\" ): out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if output in ( \"V\" ): out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if output in ( \"NS\" ): out = dst . ns if output in ( \"EW\" ): out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]) . dropna () return out","title":"Module hielen2.ext.source_photomonitoring.struct_ok"},{"location":"reference/hielen2/ext/source_photomonitoring/struct_ok/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/ext/source_photomonitoring/struct_ok/#agoodtime","text":"def agoodtime ( t ) View Source def agoodtime ( t ): try : t = np . datetime64 ( t ) assert not np . isnat ( t ) t = str ( t ) except Exception : t = None return t","title":"agoodtime"},{"location":"reference/hielen2/ext/source_photomonitoring/struct_ok/#config_nc","text":"def config_NC ( target , timestamp , x_values , y_values , step_size = None , x_offset = None , y_offset = None ) Crea il file netCDF secondo un formato standard returns: la struttura dati del file NetCDF params tagetfile: nome del file refitime: tempo zero y_values: array delle y x_values: array delle x View Source def config_NC ( target , timestamp , x_values , y_values , step_size = None , x_offset = None , y_offset = None ): \"\"\" Crea il file netCDF secondo un formato standard returns: la struttura dati del file NetCDF params tagetfile: nome del file refitime: tempo zero y_values: array delle y x_values: array delle x \"\"\" try : step_size = int ( step_size ) except Exception as e : step_size = 1 try : x_offset = int ( step_size ) except Exception as e : x_offset = 0 try : y_offset = int ( step_size ) except Exception as e : y_offset = 0 y_values = y_values [ y_offset :: step_size ] x_values = x_values [ x_offset :: step_size ] dataset = Dataset ( target , 'w' , format = \"NETCDF4\" ) dataset . Conventions = \"CF-1.7\" dataset . timestamp = agoodtime ( timestamp ) dataset . step_size = step_size # time informations dataset . createDimension ( \"time\" , None ) dataset . createVariable ( \"time\" , \"f8\" ,( \"time\" ,)) dataset . variables [ \"time\" ]. units = f \"hours since {timestamp}\" dataset . variables [ \"time\" ]. calendar = \"standard\" dataset . variables [ \"time\" ]. long_name = \"observation_time\" # y informations dataset . createDimension ( \"y\" , y_values . __len__ ()) dataset . createVariable ( \"y\" , \"f4\" ,( \"y\" ,)) dataset . variables [ \"y\" ]. units = \"1\" dataset . variables [ \"y\" ]. long_name = \"projection_y_coordinate\" dataset . variables [ \"y\" ]. pixel_offset = y_offset dataset . variables [ \"y\" ][:] = y_values # x informations dataset . createDimension ( \"x\" , x_values . __len__ ()) dataset . createVariable ( \"x\" , \"f4\" ,( \"x\" ,)) dataset . variables [ \"x\" ]. units = \"1\" dataset . variables [ \"x\" ]. long_name = \"projection_x_coordinate\" dataset . variables [ \"x\" ]. pixel_offset = x_offset dataset . variables [ \"x\" ][:] = x_values # ns ( north - south ) informations dataset . createVariable ( \"ns\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ns\" ]. units = \"px\" dataset . variables [ \"ns\" ]. long_name = \"north_south_axis_displacement\" # ew ( east - west ) informations dataset . createVariable ( \"ew\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"ew\" ]. units = \"px\" dataset . variables [ \"ew\" ]. long_name = \"east_west_axis_displacement\" # corr ( correlation coefficient ) informations dataset . createVariable ( \"corr\" , \"f4\" ,( \"time\" , \"y\" , \"x\" ,), fill_value = np . nan , zlib = True , least_significant_digit = 3 ) dataset . variables [ \"corr\" ]. units = \"1\" dataset . variables [ \"corr\" ]. long_name = \"correlation_coefficient\" zf = np . zeros (( y_values . __len__ (), x_values . __len__ ())) feed_NC ( dataset , timestamp , ns = zf , ew = zf , corr = zf ) return dataset","title":"config_NC"},{"location":"reference/hielen2/ext/source_photomonitoring/struct_ok/#feed_nc","text":"def feed_NC ( target , time , ** kwargs ) Appende i grid al file netCDF target: netCDF4.Dataset or path to a valid .nc file time: timestamp del dato kwarg: dict nomevariabile:datagrid View Source def feed_NC ( target , time , ** kwargs ) : \"\"\" Appende i grid al file netCDF target: netCDF4.Dataset or path to a valid .nc file time: timestamp del dato kwarg: dict nomevariabile:datagrid \"\"\" if isinstance ( target , Dataset ) : dataset = target else : dataset = Dataset ( target , 'a' , format = \"NETCDF4\" ) timevar = dataset . variables [ 'time' ] time = date2num ( np . datetime64 ( time ). tolist (), timevar . units ) # If exists substitute position = np . where ( timevar == time ) try : position = int ( position [ 0 ] ) except TypeError : position = timevar . shape [ 0 ] timevar [ position ]= time for k , w in kwargs . items () : if w is not None : y_slice = slice ( 0 , w . shape [ 0 ] ) x_slice = slice ( 0 , w . shape [ 1 ] ) try : dataset . variables [ k ][ position,y_slice,x_slice ] = w except KeyError as e : pass return dataset","title":"feed_NC"},{"location":"reference/hielen2/ext/source_photomonitoring/struct_ok/#generate_map","text":"def generate_map ( targetfile , timestamp = None , timeref = None , output = 'RV' , cmap = [ 'green' , 'red' , 'blue' ], vmin =- 2.5 , vmax =- 3 ) View Source def generate_map ( targetfile , timestamp = None , timeref = None , output = \"RV\" , cmap = [ \"green\" , \"red\" , \"blue\" ], vmin =- 2 . 5 , vmax = 2 - 5 ): dataset = xr . open_dataset ( targetfile ) if timestamp is None : timestamp = str ( dataset . time [ - 1 ]. values ) else : timestamp = agoodtime ( timestamp ) if timeref is None : timeref = str ( dataset . time [ 0 ]. values ) correlation = 0 . 01 else : timeref = agoodtime ( timeref ) correlation = 0 print ( \"a\" ) # Nel caso di ref zero il risultato \u00e8 dato dal dataset in \"timestamp\" d1 = dataset . sel ( time = timestamp ). compute () d2 = dataset . sel ( time = timeref ). compute () d3 = ( d1 - d2 ). compute () print ( d2 ) managed = _open_matrix ( dataset = dataset , output = output , correlation = correlation ) print ( \"c\" ) imgarray = _render ( ** managed , colors = colors , vmin = vmin , vmax = vmax ) print ( \"\" )","title":"generate_map"},{"location":"reference/hielen2/ext/source_photomonitoring/struct_ok/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/ext/source_photomonitoring/struct_ok/#render","text":"class Render ( targetfile = './incomes/sag.nc' , gridratio = 8 , ** kwargs ) View Source class Render () : def __init__ ( self , targetfile = './incomes/sag.nc' , gridratio = 8 , ** kwargs ) : self . gridratio = gridratio @property def timeline ( self ) : times = self . dataset . time . values return list ( map ( lambda x : str ( x ). replace ( '.000000000' , '' ),( times ))) @property def reftime ( self ) : return self . dataset . timestamp def isbasetime ( self , time ) : try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = \"R\" , timeref = None ) : ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ] , y = geom [ 1 ] , method = 'nearest' ). sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ) : ref = self . dataset . sel ( x = geom [ 0 ] , y = geom [ 1 ] , time = timeref , method = 'nearest' ) dst [ 'ns' ]= dst . ns - ref . ns dst [ 'ew' ]= dst . ew - ref . ew if output in ( \"R\" , \"RV\" ) : out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if output in ( \"V\" ) : out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if output in ( \"NS\" ) : out = dst . ns if output in ( \"EW\" ) : out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns =[ 'xxx' ] ). dropna () return out","title":"Render"},{"location":"reference/hielen2/ext/source_photomonitoring/struct_ok/#instance-variables","text":"reftime timeline","title":"Instance variables"},{"location":"reference/hielen2/ext/source_photomonitoring/struct_ok/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_photomonitoring/struct_ok/#extract_data","text":"def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = 'R' , timeref = None ) params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component View Source def extract_data ( self , geom = ( 0 , 0 ), timefrom = None , timeto = None , output = \"R\" , timeref = None ): ''' params geom: point to extract timefrom: time lower limit timeto: time upper limit output: \"R\" result (the module itself) \"NS\" North-South component \"EW\" Eeast-West component ''' timefrom = agoodtime ( timefrom ) timeto = agoodtime ( timeto ) timeref = agoodtime ( timeref ) dst = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], method = 'nearest' ). sel ( time = slice ( timefrom , timeto )) if timeref is not None and not self . isbasetime ( timeref ): ref = self . dataset . sel ( x = geom [ 0 ], y = geom [ 1 ], time = timeref , method = 'nearest' ) dst [ 'ns' ] = dst . ns - ref . ns dst [ 'ew' ] = dst . ew - ref . ew if output in ( \"R\" , \"RV\" ): out = np . sqrt ( dst . ns ** 2 + dst . ew ** 2 ) if output in ( \"V\" ): out = np . rad2deg ( np . arctan2 ( dst . ns , dst . ew )) if output in ( \"NS\" ): out = dst . ns if output in ( \"EW\" ): out = dst . ew out = pd . DataFrame ( out , index = out . time . values , columns = [ 'xxx' ]). dropna () return out","title":"extract_data"},{"location":"reference/hielen2/ext/source_photomonitoring/struct_ok/#isbasetime","text":"def isbasetime ( self , time ) View Source def isbasetime ( self , time ): try : return np . datetime64 ( time ) == np . datetime64 ( self . reftime ) except Exception : return False","title":"isbasetime"},{"location":"reference/hielen2/ext/source_tinsar/","text":"Module hielen2.ext.source_tinsar View Source # coding=utf-8 from hielen2.source import HielenSource from hielen2.source import HielenSource , ActionSchema from hielen2.utils import LocalFile from marshmallow import fields class ConfigSchema ( ActionSchema ): master_cloud = LocalFile ( required = True , allow_none = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True ) class FeedSchema ( ActionSchema ): scanner_file = LocalFile ( required = True , allow_none = False ) class Source ( HielenSource ): def config ( self , ** kwargs ): return kwargs def feed ( self , ** kwargs ): return kwargs def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs Sub-modules hielen2.ext.source_tinsar.cloudpainter Classes ConfigSchema class ConfigSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) Minimal ActionSchema object. Used to define at least a timestamp View Source class ConfigSchema ( ActionSchema ): master_cloud = LocalFile ( required = True , allow_none = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True ) Ancestors (in MRO) hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC Class variables Meta OPTIONS_CLASS TYPE_MAPPING crs error_messages geo_reference_file master_cloud opts Static methods from_dict def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls Instance variables dict_class set_class Methods dump def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result dumps def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs ) get_attribute def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default ) handle_error def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass load def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True ) loads def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown ) on_bind_field def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None validate def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {} FeedSchema class FeedSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) Minimal ActionSchema object. Used to define at least a timestamp View Source class FeedSchema ( ActionSchema ): scanner_file = LocalFile ( required = True , allow_none = False ) Ancestors (in MRO) hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC Class variables Meta OPTIONS_CLASS TYPE_MAPPING error_messages opts scanner_file Static methods from_dict def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls Instance variables dict_class set_class Methods dump def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result dumps def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs ) get_attribute def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default ) handle_error def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass load def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True ) loads def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown ) on_bind_field def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None validate def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {} Source class Source ( feature ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Source ( HielenSource ): def config ( self , ** kwargs ): return kwargs def feed ( self , ** kwargs ): return kwargs def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs Ancestors (in MRO) hielen2.source.HielenSource abc.ABC Methods config def config ( self , ** kwargs ) View Source def config ( self , ** kwargs ): return kwargs data def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ) View Source def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs deleteActionValues def deleteActionValues ( self , action = None , timestamp = None ) View Source def deleteActionValues ( self , action = None , timestamp = None ) : out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ) : out =[ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \"{a.capitalize()}Schema\" self . __getattribute__ ( f \"clean{a.capitalize()}\" )( t ) except Exception as e : pass try : db [ 'actions' ][ self.uid,a,t ]= None except Exception as e : raise ValueError ( e ) return out execAction def execAction ( self , action , ** kwargs ) View Source def execAction ( self , action , ** kwargs ): aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass (). load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e ) feed def feed ( self , ** kwargs ) View Source def feed ( self , ** kwargs ): return kwargs getActionSchema def getActionSchema ( self , action ) View Source def getActionSchema ( self , action ): return getActionSchema ( self . module , action ) getActionValues def getActionValues ( self , action = None , timestamp = None ) View Source def getActionValues ( self , action = None , timestamp = None ) : if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self.uid,action,timestamp ] if not isinstance ( out , list ) : out = [ out ] except KeyError : return [] return out","title":"Index"},{"location":"reference/hielen2/ext/source_tinsar/#module-hielen2extsource_tinsar","text":"View Source # coding=utf-8 from hielen2.source import HielenSource from hielen2.source import HielenSource , ActionSchema from hielen2.utils import LocalFile from marshmallow import fields class ConfigSchema ( ActionSchema ): master_cloud = LocalFile ( required = True , allow_none = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True ) class FeedSchema ( ActionSchema ): scanner_file = LocalFile ( required = True , allow_none = False ) class Source ( HielenSource ): def config ( self , ** kwargs ): return kwargs def feed ( self , ** kwargs ): return kwargs def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs","title":"Module hielen2.ext.source_tinsar"},{"location":"reference/hielen2/ext/source_tinsar/#sub-modules","text":"hielen2.ext.source_tinsar.cloudpainter","title":"Sub-modules"},{"location":"reference/hielen2/ext/source_tinsar/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/ext/source_tinsar/#configschema","text":"class ConfigSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) Minimal ActionSchema object. Used to define at least a timestamp View Source class ConfigSchema ( ActionSchema ): master_cloud = LocalFile ( required = True , allow_none = False ) geo_reference_file = LocalFile ( required = False , default = None , allow_none = True ) crs = fields . Str ( required = False , default = None , allow_none = True )","title":"ConfigSchema"},{"location":"reference/hielen2/ext/source_tinsar/#ancestors-in-mro","text":"hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/ext/source_tinsar/#class-variables","text":"Meta OPTIONS_CLASS TYPE_MAPPING crs error_messages geo_reference_file master_cloud opts","title":"Class variables"},{"location":"reference/hielen2/ext/source_tinsar/#static-methods","text":"","title":"Static methods"},{"location":"reference/hielen2/ext/source_tinsar/#from_dict","text":"def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls","title":"from_dict"},{"location":"reference/hielen2/ext/source_tinsar/#instance-variables","text":"dict_class set_class","title":"Instance variables"},{"location":"reference/hielen2/ext/source_tinsar/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_tinsar/#dump","text":"def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result","title":"dump"},{"location":"reference/hielen2/ext/source_tinsar/#dumps","text":"def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs )","title":"dumps"},{"location":"reference/hielen2/ext/source_tinsar/#get_attribute","text":"def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default )","title":"get_attribute"},{"location":"reference/hielen2/ext/source_tinsar/#handle_error","text":"def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass","title":"handle_error"},{"location":"reference/hielen2/ext/source_tinsar/#load","text":"def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True )","title":"load"},{"location":"reference/hielen2/ext/source_tinsar/#loads","text":"def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown )","title":"loads"},{"location":"reference/hielen2/ext/source_tinsar/#on_bind_field","text":"def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None","title":"on_bind_field"},{"location":"reference/hielen2/ext/source_tinsar/#validate","text":"def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"validate"},{"location":"reference/hielen2/ext/source_tinsar/#feedschema","text":"class FeedSchema ( * , only : Union [ Sequence [ str ], Set [ str ], NoneType ] = None , exclude : Union [ Sequence [ str ], Set [ str ]] = (), many : bool = False , context : Union [ Dict , NoneType ] = None , load_only : Union [ Sequence [ str ], Set [ str ]] = (), dump_only : Union [ Sequence [ str ], Set [ str ]] = (), partial : Union [ bool , Sequence [ str ], Set [ str ]] = False , unknown : Union [ str , NoneType ] = None ) Minimal ActionSchema object. Used to define at least a timestamp View Source class FeedSchema ( ActionSchema ): scanner_file = LocalFile ( required = True , allow_none = False )","title":"FeedSchema"},{"location":"reference/hielen2/ext/source_tinsar/#ancestors-in-mro_1","text":"hielen2.source.ActionSchema marshmallow.schema.Schema marshmallow.base.SchemaABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/ext/source_tinsar/#class-variables_1","text":"Meta OPTIONS_CLASS TYPE_MAPPING error_messages opts scanner_file","title":"Class variables"},{"location":"reference/hielen2/ext/source_tinsar/#static-methods_1","text":"","title":"Static methods"},{"location":"reference/hielen2/ext/source_tinsar/#from_dict_1","text":"def from_dict ( fields : Dict [ str , Union [ marshmallow . fields . Field , type ]], * , name : str = 'GeneratedSchema' ) -> type Generate a Schema class given a dictionary of fields. .. code-block:: python from marshmallow import Schema , fields PersonSchema = Schema . from_dict ({ \"name\" : fields . Str ()}) print ( PersonSchema () . load ({ \"name\" : \"David\" })) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in Nested fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the repr for the class. .. versionadded:: 3.0.0 View Source @classmethod def from_dict ( cls , fields : typing . Dict [ str , typing . Union [ ma_fields . Field , type ]], * , name : str = \"GeneratedSchema\" ) -> type : \"\"\"Generate a `Schema` class given a dictionary of fields. .. code-block:: python from marshmallow import Schema, fields PersonSchema = Schema.from_dict({\"name\": fields.Str()}) print(PersonSchema().load({\"name\": \"David\"})) # => {'name': 'David'} Generated schemas are not added to the class registry and therefore cannot be referred to by name in `Nested` fields. :param dict fields: Dictionary mapping field names to field instances. :param str name: Optional name for the class, which will appear in the ``repr`` for the class. .. versionadded:: 3.0.0 \"\"\" attrs = fields . copy () attrs [ \"Meta\" ] = type ( \"GeneratedMeta\" , ( getattr ( cls , \"Meta\" , object ),), { \"register\" : False } ) schema_cls = type ( name , ( cls ,), attrs ) return schema_cls","title":"from_dict"},{"location":"reference/hielen2/ext/source_tinsar/#instance-variables_1","text":"dict_class set_class","title":"Instance variables"},{"location":"reference/hielen2/ext/source_tinsar/#methods_1","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_tinsar/#dump_1","text":"def dump ( self , obj : Any , * , many : Union [ bool , NoneType ] = None ) Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. View Source def dump ( self , obj : typing . Any , * , many : typing . Optional [ bool ] = None ): \"\"\"Serialize an object to native Python data types according to this Schema's fields. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A dict of serialized data :rtype: dict .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. .. versionchanged:: 3.0.0rc9 Validation no longer occurs upon serialization. \"\"\" many = self . many if many is None else bool ( many ) if many and is_iterable_but_not_string ( obj ): obj = list ( obj ) if self . _has_processors ( PRE_DUMP ): processed_obj = self . _invoke_dump_processors ( PRE_DUMP , obj , many = many , original_data = obj ) else : processed_obj = obj result = self . _serialize ( processed_obj , many = many ) if self . _has_processors ( POST_DUMP ): result = self . _invoke_dump_processors ( POST_DUMP , result , many = many , original_data = obj ) return result","title":"dump"},{"location":"reference/hielen2/ext/source_tinsar/#dumps_1","text":"def dumps ( self , obj : Any , * args , many : Union [ bool , NoneType ] = None , ** kwargs ) Same as :meth: dump , except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize obj as a collection. If None , the value for self.many is used. :return: A json string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if obj is invalid. View Source def dumps ( self , obj : typing . Any , * args , many : typing . Optional [ bool ] = None , ** kwargs ): \"\"\"Same as :meth:`dump`, except return a JSON-encoded string. :param obj: The object to serialize. :param many: Whether to serialize `obj` as a collection. If `None`, the value for `self.many` is used. :return: A ``json`` string :rtype: str .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the serialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if ``obj`` is invalid. \"\"\" serialized = self . dump ( obj , many = many ) return self . opts . render_module . dumps ( serialized , * args , ** kwargs )","title":"dumps"},{"location":"reference/hielen2/ext/source_tinsar/#get_attribute_1","text":"def get_attribute ( self , obj : Any , attr : str , default : Any ) Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of obj and attr . View Source def get_attribute ( self , obj : typing . Any , attr : str , default : typing . Any ): \"\"\"Defines how to pull values from an object to serialize. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0a1 Changed position of ``obj`` and ``attr``. \"\"\" return get_value ( obj , attr , default )","title":"get_attribute"},{"location":"reference/hielen2/ext/source_tinsar/#handle_error_1","text":"def handle_error ( self , error : marshmallow . exceptions . ValidationError , data : Any , * , many : bool , ** kwargs ) Custom error handler function for the schema. :param error: The ValidationError raised during (de)serialization. :param data: The original input data. :param many: Value of many on dump or load. :param partial: Value of partial on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives many and partial (on deserialization) as keyword arguments. View Source def handle_error ( self , error : ValidationError , data : typing . Any , * , many : bool , ** kwargs ): \"\"\"Custom error handler function for the schema. :param error: The `ValidationError` raised during (de)serialization. :param data: The original input data. :param many: Value of ``many`` on dump or load. :param partial: Value of ``partial`` on load. .. versionadded:: 2.0.0 .. versionchanged:: 3.0.0rc9 Receives `many` and `partial` (on deserialization) as keyword arguments. \"\"\" pass","title":"handle_error"},{"location":"reference/hielen2/ext/source_tinsar/#load_1","text":"def load ( self , data : Union [ Mapping [ str , Any ], Iterable [ Mapping [ str , Any ]]], * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None ) Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def load ( self , data : typing . Union [ typing . Mapping [ str , typing . Any ], typing . Iterable [ typing . Mapping [ str , typing . Any ]], ], * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None ): \"\"\"Deserialize a data structure to an object defined by this Schema's fields. :param data: The data to deserialize. :param many: Whether to deserialize `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" return self . _do_load ( data , many = many , partial = partial , unknown = unknown , postprocess = True )","title":"load"},{"location":"reference/hielen2/ext/source_tinsar/#loads_1","text":"def loads ( self , json_data : str , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None , unknown : Union [ str , NoneType ] = None , ** kwargs ) Same as :meth: load , except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize obj as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use EXCLUDE , INCLUDE or RAISE . If None , the value for self.unknown is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a (data, errors) duple. A :exc: ValidationError <marshmallow.exceptions.ValidationError> is raised if invalid data are passed. View Source def loads ( self , json_data : str , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None , unknown : typing . Optional [ str ] = None , ** kwargs ): \"\"\"Same as :meth:`load`, except it takes a JSON string as input. :param json_data: A JSON string of the data to deserialize. :param many: Whether to deserialize `obj` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :param unknown: Whether to exclude, include, or raise an error for unknown fields in the data. Use `EXCLUDE`, `INCLUDE` or `RAISE`. If `None`, the value for `self.unknown` is used. :return: Deserialized data .. versionadded:: 1.0.0 .. versionchanged:: 3.0.0b7 This method returns the deserialized data rather than a ``(data, errors)`` duple. A :exc:`ValidationError <marshmallow.exceptions.ValidationError>` is raised if invalid data are passed. \"\"\" data = self . opts . render_module . loads ( json_data , ** kwargs ) return self . load ( data , many = many , partial = partial , unknown = unknown )","title":"loads"},{"location":"reference/hielen2/ext/source_tinsar/#on_bind_field_1","text":"def on_bind_field ( self , field_name : str , field_obj : marshmallow . fields . Field ) -> None Hook to modify a field when it is bound to the Schema . No-op by default. View Source def on_bind_field ( self , field_name : str , field_obj : ma_fields . Field ) -> None : \"\"\"Hook to modify a field when it is bound to the `Schema`. No-op by default. \"\"\" return None","title":"on_bind_field"},{"location":"reference/hielen2/ext/source_tinsar/#validate_1","text":"def validate ( self , data : Mapping , * , many : Union [ bool , NoneType ] = None , partial : Union [ bool , Sequence [ str ], Set [ str ], NoneType ] = None ) -> Dict [ str , List [ str ]] Validate data against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate data as a collection. If None , the value for self.many is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to Nested fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 View Source def validate ( self , data : typing . Mapping , * , many : typing . Optional [ bool ] = None , partial : typing . Optional [ typing . Union [ bool , types . StrSequenceOrSet ]] = None ) -> typing . Dict [ str , typing . List [ str ]]: \"\"\"Validate `data` against the schema, returning a dictionary of validation errors. :param data: The data to validate. :param many: Whether to validate `data` as a collection. If `None`, the value for `self.many` is used. :param partial: Whether to ignore missing fields and not require any fields declared. Propagates down to ``Nested`` fields as well. If its value is an iterable, only missing fields listed in that iterable will be ignored. Use dot delimiters to specify nested fields. :return: A dictionary of validation errors. .. versionadded:: 1.1.0 \"\"\" try : self . _do_load ( data , many = many , partial = partial , postprocess = False ) except ValidationError as exc : return typing . cast ( typing . Dict [ str , typing . List [ str ]], exc . messages ) return {}","title":"validate"},{"location":"reference/hielen2/ext/source_tinsar/#source","text":"class Source ( feature ) Helper class that provides a standard way to create an ABC using inheritance. View Source class Source ( HielenSource ): def config ( self , ** kwargs ): return kwargs def feed ( self , ** kwargs ): return kwargs def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs","title":"Source"},{"location":"reference/hielen2/ext/source_tinsar/#ancestors-in-mro_2","text":"hielen2.source.HielenSource abc.ABC","title":"Ancestors (in MRO)"},{"location":"reference/hielen2/ext/source_tinsar/#methods_2","text":"","title":"Methods"},{"location":"reference/hielen2/ext/source_tinsar/#config","text":"def config ( self , ** kwargs ) View Source def config ( self , ** kwargs ): return kwargs","title":"config"},{"location":"reference/hielen2/ext/source_tinsar/#data","text":"def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ) View Source def data ( self , timefrom = None , timeto = None , geom = None , ** kwargs ): return kwargs","title":"data"},{"location":"reference/hielen2/ext/source_tinsar/#deleteactionvalues","text":"def deleteActionValues ( self , action = None , timestamp = None ) View Source def deleteActionValues ( self , action = None , timestamp = None ) : out = self . getActionValues ( action , timestamp ) if not isinstance ( out , list ) : out =[ out ] for act in out : a = act [ 'action' ] t = act [ 'timestamp' ] try : f \"{a.capitalize()}Schema\" self . __getattribute__ ( f \"clean{a.capitalize()}\" )( t ) except Exception as e : pass try : db [ 'actions' ][ self.uid,a,t ]= None except Exception as e : raise ValueError ( e ) return out","title":"deleteActionValues"},{"location":"reference/hielen2/ext/source_tinsar/#execaction","text":"def execAction ( self , action , ** kwargs ) View Source def execAction ( self , action , ** kwargs ): aclass = getActionSchemaClass ( self . module , action ) try : kwargs = aclass (). load ( kwargs ) return self . __getattribute__ ( action )( ** kwargs ) except Exception as e : raise ValueError ( e )","title":"execAction"},{"location":"reference/hielen2/ext/source_tinsar/#feed","text":"def feed ( self , ** kwargs ) View Source def feed ( self , ** kwargs ): return kwargs","title":"feed"},{"location":"reference/hielen2/ext/source_tinsar/#getactionschema","text":"def getActionSchema ( self , action ) View Source def getActionSchema ( self , action ): return getActionSchema ( self . module , action )","title":"getActionSchema"},{"location":"reference/hielen2/ext/source_tinsar/#getactionvalues","text":"def getActionValues ( self , action = None , timestamp = None ) View Source def getActionValues ( self , action = None , timestamp = None ) : if action is None : action = slice ( None , None ) if timestamp is None : timestamp = slice ( None , None ) try : out = db [ 'actions' ][ self.uid,action,timestamp ] if not isinstance ( out , list ) : out = [ out ] except KeyError : return [] return out","title":"getActionValues"},{"location":"reference/hielen2/ext/source_tinsar/cloudpainter/","text":"Module hielen2.ext.source_tinsar.cloudpainter View Source # coding: utf-8 import pandas as pd import numpy as np import laspy from scipy.spatial import KDTree from matplotlib.colors import Normalize , Colormap , LinearSegmentedColormap from matplotlib.cm import ScalarMappable import getopt import sys import open3d as o3d def valorize ( basecld , valcld , distance = 10 , group = 1 , degree = 0 ): \"\"\" Assigns values to input base cloud points, overlapping the input values cloud and calculating, for each point in base cloud, the [distance based, weighted] mean of the values of N nearest neighbours, taken from the input values cloud. returns: pandas.Series containing calculated values for all the points in the base cloud which has at least one neighbour within the fixed distance params: basecld - pandas.DataFrame or numpy.ndarray with shape (n,3), representing the coordinates of the points of the base cloud. ['x','y','z'] valcld - pandas.DataFrame or numpy.ndarray with shape (n,4), representing the coordinates of the points of the value cloud and the value of each of them. ['x','y','z','v'] distance - maximum search range for neighbours group - maximum number of neighbours to search for degree - degree of contribution loss over the distance for each neighbour value. Each value of the neighbours is weighted as: 1/d**degree where d is the distance between the neighbour and the base cloud point. Degree 0 implies the arithmetic mean of all the neighbour values found, regardless the relative distance. i.e: N1----------P----N2 Given two neighbours, N1 and N2, for a fixed point P where: d(N1)=10; v(N1)=-15 d(N2)=4; v(N2)=3 v(P) = (-15/10**d+3/4**d)/(1/10**d+1/4**d) being d the degree, we have: d=0 : v(P) = -6 <-- arimetic mean d=1 : v(P) = -2.14 d=2 : v(P) = 0.51 d=3 : v(P) = 1.91 d=4 : v(P) = 2.55 d=5 : v(P) = 2.81 .. d=9 : v(P) = 2.99 .. d(x): v(P) = 3 <-- convergence to the closest's value Note: with group=1 and degree=0, each base cloud point assumes the exact value of the unique closest neighbour, if it exists. \"\"\" print ( \"CHECK: Enter Valorize\" ) basecld = pd . DataFrame ( basecld ) basecld . columns = [ \"x\" , \"y\" , \"z\" ] valcld = pd . DataFrame ( valcld ) valcld . columns = [ \"x\" , \"y\" , \"z\" , \"v\" ] ## Calculating the Series of distance (dist) and value clouds relative ## ids (idsv). the position of each cell in the arrays reflects the cells ## in the base cloud. k = KDTree ( valcld [[ \"x\" , \"y\" , \"z\" ]] . values ) dist , idsv = k . query ( basecld . values , group , distance_upper_bound = distance ) dist = pd . DataFrame ( dist ) . stack () dist . name = \"dist\" idsv = pd . DataFrame ( idsv ) . stack () idsv . name = \"idsv\" print ( \"CHECK: Distance calculation done\" ) ## Here we construct the DataFrame contains relation beetween each point ## in base cloud (base cloud index) and his neighbous (value cloud ids, ## distance and neighbour group progressive). Then we clean the infinite ## distances (points with no neighbours) values = dist . to_frame () . join ( idsv ) values = values [ values [ \"dist\" ] != np . inf ] # Indexind on 'idsv', allows to join with the value cloud and add the 'v' # info values = values . reset_index () . set_index ( \"idsv\" ) . sort_index () values = values . join ( valcld [ \"v\" ], how = \"left\" ) values . columns = [ \"cldid\" , \"instance\" , \"dist\" , \"displ\" ] ## Indexing on 'cldid' and 'instance', allow to make DataFrame comparable ## with base cloud values = values . set_index ([ \"cldid\" , \"instance\" ]) . sort_index () ## Here we calculte the [distance based, weighted] mean values [ \"weight\" ] = 1 / np . power ( values [ \"dist\" ], degree ) values [ \"contrib\" ] = values [ \"displ\" ] * values [ \"weight\" ] values = values . groupby ( \"cldid\" ) . apply ( lambda x : sum ( x [ \"contrib\" ]) / sum ( x [ \"weight\" ]) ) print ( \"CHECK: Weight calculation done\" ) return values def colorize ( vals , cmap = [ \"red\" , \"green\" , \"blue\" ], vmin = None , vmax = None ): \"\"\" Maps each value in the input array with the appropriate RGBA color. See matplotlib.colors.Colormap returns: ndarray with shape (n,4) containing the color tuples ['r','g','b','a'] color channels values are normalized in range (0,1) params: vals - array of values. cmap - See matplotlib.colors.Colormap object or colour list. default ['red','green','blue']. vmax - max limit value for colormap. If None is passed, vals.max(). will be assumed, otherwise vals exceding this parameter will be clipped vmin - min limit value for colormap. If None is passed, vals.min() will be assumed, otherwise vals under this parameter will be clipped \"\"\" print ( \"CHECK: Enter Colorize\" ) vmin = vmin or vals . min () vmax = vmax or vals . max () norm = Normalize ( vmin = vmin , vmax = vmax , clip = True ) if not isinstance ( cmap , Colormap ): cmap = LinearSegmentedColormap . from_list ( \"mycmap\" , cmap ) mapper = ScalarMappable ( norm = norm , cmap = cmap ) cols = mapper . to_rgba ( vals ) return cols def paint ( valcld , basecld = None , distance = 10 , group = 1 , degree = 0 , cmap = [ \"red\" , \"green\" , \"blue\" ], vmin = None , vmax = None , ): \"\"\" Assigns a color to each point of a base cloud, merging the information of a second valorized cloud overlapping the first one. See cloudpainter.valorize and cloudpainter.colorize for futher informations result: pandas.DataFrame containing ['x','y','z','v','r','g','b'] tuples for each point in the base cloud. Points with no neighbours will be assigned with [0.9,0.9,0.9] for ['r','g','b'] and np.nan for ['v'] params: basecld - pandas.DataFrame or numpy.ndarray with shape (n,3), representing the coordinates of the points of the base cloud. ['x','y','z'] valcld - pandas.DataFrame or numpy.ndarray with shape (n,4), representing the coordinates of the points of the value cloud and the value of each of them. ['x','y','z','v'] distance - maximum search range for neighbours group - maximum number of neighbours to search for degree - degree of contribution loss over the distance for each neighbour value. Each value of the neighbours is weighted as: 1/d**degree where d is the distance between the neighbour and the base cloud point. Degree 0 implies the arithmetic mean of all the neighbour values found, regardless the relative distance. cmap - matplotlib.colors.Colormap object or colour list. default ['red','green','blue']. vmax - max limit value for colormap. If None is passed, vals.max(). will be assumed, otherwise vals exceding this parameter will be clipped vmin - min limit value for colormap. If None is passed, vals.min() will be assumed, otherwise vals under this parameter will be clipped \"\"\" valcld = pd . DataFrame ( valcld ) valcld . columns = [ \"x\" , \"y\" , \"z\" , \"v\" ] values = None if basecld is not None : basecld = pd . DataFrame ( basecld ) basecld . columns = [ \"x\" , \"y\" , \"z\" ] values = valorize ( basecld , valcld , distance = distance , group = group , degree = degree ) values . name = \"v\" else : values = valcld [ \"v\" ] colors = pd . DataFrame ( colorize ( values , cmap = cmap , vmin = vmin , vmax = vmax ), columns = [ \"r\" , \"g\" , \"b\" , \"a\" ], index = values . index , )[[ \"r\" , \"g\" , \"b\" ]] if basecld is not None : result = basecld . join ( colors , how = \"left\" ) . replace ( np . nan , 0.9 ) result = result . join ( values , how = \"left\" ) else : result = valcld . join ( colors , how = \"left\" ) return result [[ \"x\" , \"y\" , \"z\" , \"v\" , \"r\" , \"g\" , \"b\" ]] #65536 def makelaz ( frame , filetowrite , coordmult = 1 , colormult = 1 , scale = [ 1 , 1 , 1 ], framevalues = None ): hdr = laspy . header . Header ( point_format = 2 ) frame . columns = [ 'x' , 'y' , 'z' , 'r' , 'g' , 'b' ] outfile = laspy . file . File ( filetowrite , mode = 'w' , header = hdr ) x = frame [ 'x' ] * coordmult y = frame [ 'y' ] * coordmult z = frame [ 'z' ] * coordmult r = frame [ 'r' ] * colormult g = frame [ 'g' ] * colormult b = frame [ 'b' ] * colormult #outfile.header.min=[ np.floor(np.min(x)), np.floor(np.min(y)), np.floor(np.min(z)) ] #outfile.header.max=[ np.ceil(np.max(x)), np.ceil(np.max(y)), np.ceil(np.max(z)) ] #outfile.header.offset=outfile.header.min outfile . header . scale = scale outfile . x = x . values outfile . y = y . values outfile . z = z . values outfile . Red = r . values outfile . Green = g . values outfile . Blue = b . values outfile . header . offset = outfile . header . min outfile . close () def openpcl ( res ): #QUI USO open3d perch\u00e8 \u00e8 molto comodo pcl = o3d . geometry . PointCloud () pcl . points = o3d . utility . Vector3dVector ( res [[ \"x\" , \"y\" , \"z\" ]] . values ) pcl . colors = o3d . utility . Vector3dVector ( res [[ \"r\" , \"g\" , \"b\" ]] . values ) o3d . visualization . draw_geometries ([ pcl ]) def usage (): helptext = r \"\"\"usage: cloudainter.py [option] path/to/radar/result.csv parmeters: path/to/radar/result.csv : path for csv based radar cloud in the format X,Y,Z,V,... options: -b path --basecloud=path : path for csv based reference cloud in the format X,Y,Z,... -g number --group=number : max number of neighbours to interpolate -d number --distance=number : nearest neighbour max radius -D number --degree=number : neighbour contribute attenuation degree along distance -c csv --colors=csv : colormap as comma separated colors names -o path --outfile=path : output file name. Ignored when --output=view -O type --output=type : output types: [laz|csv|view]. Default laz -v number --vmin=number : min value for the colormap range -V number --vmax=numner : max value for the colormap range\"\"\" print ( helptext ) if __name__ == \"__main__\" : #DEFAULTS datacloud = None basecloud = None group = 1 distance = 5 degree = 0 colors = [ 'violet' , 'blue' , 'cyan' , 'green' , 'yellow' , 'orange' , 'red' ] outfile = None output = 'las' vmin =- 500 vmax = 500 try : opts , args = getopt . getopt ( sys . argv [ 1 :], \"b:g:d:D:c:o:O:v:V:\" , [ \"basecloud=\" , \"group=\" , \"distance=\" , \"degree=\" , \"colors=\" , \"outfile=\" , \"output=\" , \"vmin=\" , \"vmax=\" ]) for o , a in opts : if o in ( \"-b\" , \"--basecloud\" ): basecloud = a elif o in ( \"-g\" , \"--group\" ): group = a elif o in ( \"-d\" , \"--distance\" ): distance = a elif o in ( \"-D\" , \"--degree\" ): degree = a elif o in ( \"-c\" , \"--colors\" ): colors = a . split ( \",\" ) elif o in ( \"-o\" , \"--outfile\" ): outfile = a elif o in ( \"-O\" , \"--output\" ): output = a elif o in ( \"-v\" , \"--vmin\" ): vmin = a elif o in ( \"-V\" , \"--vmax\" ): vmax = a else : assert False , \"unhandled option\" try : infile = args [ 0 ] except Exception : raise Exception ( f \"No file\" ) datacloud = pd . read_csv ( infile ) datacloud = datacloud [ datacloud . columns [ 0 : 4 ]] datacloud . columns = [ \"X\" , \"Y\" , \"Z\" , \"V\" ] if basecloud is not None : basecloud = pd . read_csv ( basecloud ) basecloud = basecloud [ basecloud . columns [ 0 : 3 ]] basecloud . columns = [ \"X\" , \"Y\" , \"Z\" ] result = paint ( valcld = datacloud , basecld = basecloud , distance = distance , degree = degree , group = group , cmap = colors , vmin = vmin , vmax = vmax ) print ( output ) if outfile is None : outfile = '.' . join ([ infile , output ]) if output == 'las' : makelaz ( result [[ \"x\" , \"y\" , \"z\" , \"r\" , \"g\" , \"b\" ]], outfile ) elif output == 'csv' : result . to_csv ( outfile , index = False ) elif output == 'view' : openpcl ( result ) else : raise Exception ( f 'unkwnown output type: { output } ' ) except Exception as err : # print help information and exit: print ( err ) usage () sys . exit ( 2 ) Functions colorize def colorize ( vals , cmap = [ 'red' , 'green' , 'blue' ], vmin = None , vmax = None ) Maps each value in the input array with the appropriate RGBA color. See matplotlib.colors.Colormap returns: ndarray with shape (n,4) containing the color tuples ['r','g','b','a'] color channels values are normalized in range (0,1) params: vals - array of values. cmap - See matplotlib.colors.Colormap object or colour list. default ['red','green','blue']. vmax - max limit value for colormap. If None is passed, vals.max(). will be assumed, otherwise vals exceding this parameter will be clipped vmin - min limit value for colormap. If None is passed, vals.min() will be assumed, otherwise vals under this parameter will be clipped View Source def colorize ( vals , cmap = [ \"red\" , \"green\" , \"blue\" ], vmin = None , vmax = None ): \"\"\" Maps each value in the input array with the appropriate RGBA color. See matplotlib.colors.Colormap returns: ndarray with shape (n,4) containing the color tuples ['r','g','b','a'] color channels values are normalized in range (0,1) params: vals - array of values. cmap - See matplotlib.colors.Colormap object or colour list. default ['red','green','blue']. vmax - max limit value for colormap. If None is passed, vals.max(). will be assumed, otherwise vals exceding this parameter will be clipped vmin - min limit value for colormap. If None is passed, vals.min() will be assumed, otherwise vals under this parameter will be clipped \"\"\" print ( \"CHECK: Enter Colorize\" ) vmin = vmin or vals . min () vmax = vmax or vals . max () norm = Normalize ( vmin = vmin , vmax = vmax , clip = True ) if not isinstance ( cmap , Colormap ): cmap = LinearSegmentedColormap . from_list ( \"mycmap\" , cmap ) mapper = ScalarMappable ( norm = norm , cmap = cmap ) cols = mapper . to_rgba ( vals ) return cols makelaz def makelaz ( frame , filetowrite , coordmult = 1 , colormult = 1 , scale = [ 1 , 1 , 1 ], framevalues = None ) View Source def makelaz ( frame , filetowrite , coordmult = 1 , colormult = 1 , scale = [ 1 , 1 , 1 ], framevalues = None ): hdr = laspy . header . Header ( point_format = 2 ) frame . columns = [ 'x' , 'y' , 'z' , 'r' , 'g' , 'b' ] outfile = laspy . file . File ( filetowrite , mode = 'w' , header = hdr ) x = frame [ 'x' ] * coordmult y = frame [ 'y' ] * coordmult z = frame [ 'z' ] * coordmult r = frame [ 'r' ] * colormult g = frame [ 'g' ] * colormult b = frame [ 'b' ] * colormult # outfile . header . min = [ np . floor ( np . min ( x )), np . floor ( np . min ( y )), np . floor ( np . min ( z )) ] # outfile . header . max = [ np . ceil ( np . max ( x )), np . ceil ( np . max ( y )), np . ceil ( np . max ( z )) ] # outfile . header . offset = outfile . header . min outfile . header . scale = scale outfile . x = x . values outfile . y = y . values outfile . z = z . values outfile . Red = r . values outfile . Green = g . values outfile . Blue = b . values outfile . header . offset = outfile . header . min outfile . close () openpcl def openpcl ( res ) View Source def openpcl ( res ) : #QUI USO open3d perch\u00e8 \u00e8 molto comodo pcl = o3d . geometry . PointCloud () pcl . points = o3d . utility . Vector3dVector ( res [ [\"x\", \"y\", \"z\" ] ] . values ) pcl . colors = o3d . utility . Vector3dVector ( res [ [\"r\", \"g\", \"b\" ] ] . values ) o3d . visualization . draw_geometries ( [ pcl ] ) paint def paint ( valcld , basecld = None , distance = 10 , group = 1 , degree = 0 , cmap = [ 'red' , 'green' , 'blue' ], vmin = None , vmax = None ) Assigns a color to each point of a base cloud, merging the information of a second valorized cloud overlapping the first one. See cloudpainter.valorize and cloudpainter.colorize for futher informations result: pandas.DataFrame containing ['x','y','z','v','r','g','b'] tuples for each point in the base cloud. Points with no neighbours will be assigned with [0.9,0.9,0.9] for ['r','g','b'] and np.nan for ['v'] params: basecld - pandas.DataFrame or numpy.ndarray with shape (n,3), representing the coordinates of the points of the base cloud. ['x','y','z'] valcld - pandas.DataFrame or numpy.ndarray with shape (n,4), representing the coordinates of the points of the value cloud and the value of each of them. ['x','y','z','v'] distance - maximum search range for neighbours group - maximum number of neighbours to search for degree - degree of contribution loss over the distance for each neighbour value. Each value of the neighbours is weighted as: 1 / d ** degree where d is the distance between the neighbour and the base cloud point . Degree 0 implies the arithmetic mean of all the neighbour values found , regardless the relative distance . cmap - matplotlib.colors.Colormap object or colour list. default ['red','green','blue']. vmax - max limit value for colormap. If None is passed, vals.max(). will be assumed, otherwise vals exceding this parameter will be clipped vmin - min limit value for colormap. If None is passed, vals.min() will be assumed, otherwise vals under this parameter will be clipped View Source def paint ( valcld , basecld = None , distance = 10 , group = 1 , degree = 0 , cmap = [ \"red\" , \"green\" , \"blue\" ], vmin = None , vmax = None , ): \"\"\" Assigns a color to each point of a base cloud, merging the information of a second valorized cloud overlapping the first one. See cloudpainter.valorize and cloudpainter.colorize for futher informations result: pandas.DataFrame containing ['x','y','z','v','r','g','b'] tuples for each point in the base cloud. Points with no neighbours will be assigned with [0.9,0.9,0.9] for ['r','g','b'] and np.nan for ['v'] params: basecld - pandas.DataFrame or numpy.ndarray with shape (n,3), representing the coordinates of the points of the base cloud. ['x','y','z'] valcld - pandas.DataFrame or numpy.ndarray with shape (n,4), representing the coordinates of the points of the value cloud and the value of each of them. ['x','y','z','v'] distance - maximum search range for neighbours group - maximum number of neighbours to search for degree - degree of contribution loss over the distance for each neighbour value. Each value of the neighbours is weighted as: 1/d**degree where d is the distance between the neighbour and the base cloud point. Degree 0 implies the arithmetic mean of all the neighbour values found, regardless the relative distance. cmap - matplotlib.colors.Colormap object or colour list. default ['red','green','blue']. vmax - max limit value for colormap. If None is passed, vals.max(). will be assumed, otherwise vals exceding this parameter will be clipped vmin - min limit value for colormap. If None is passed, vals.min() will be assumed, otherwise vals under this parameter will be clipped \"\"\" valcld = pd . DataFrame ( valcld ) valcld . columns = [ \"x\" , \"y\" , \"z\" , \"v\" ] values = None if basecld is not None : basecld = pd . DataFrame ( basecld ) basecld . columns = [ \"x\" , \"y\" , \"z\" ] values = valorize ( basecld , valcld , distance = distance , group = group , degree = degree ) values . name = \"v\" else : values = valcld [ \"v\" ] colors = pd . DataFrame ( colorize ( values , cmap = cmap , vmin = vmin , vmax = vmax ), columns = [ \"r\" , \"g\" , \"b\" , \"a\" ], index = values . index , )[[ \"r\" , \"g\" , \"b\" ]] if basecld is not None : result = basecld . join ( colors , how = \"left\" ). replace ( np . nan , 0 . 9 ) result = result . join ( values , how = \"left\" ) else : result = valcld . join ( colors , how = \"left\" ) return result [[ \"x\" , \"y\" , \"z\" , \"v\" , \"r\" , \"g\" , \"b\" ]] usage def usage ( ) View Source def usage () : helptext = r \"\"\"usage: cloudainter.py [option] path/to/radar/result.csv parmeters: path/to/radar/result.csv : path for csv based radar cloud in the format X,Y,Z,V,... options: -b path --basecloud=path : path for csv based reference cloud in the format X,Y,Z,... -g number --group=number : max number of neighbours to interpolate -d number --distance=number : nearest neighbour max radius -D number --degree=number : neighbour contribute attenuation degree along distance -c csv --colors=csv : colormap as comma separated colors names -o path --outfile=path : output file name. Ignored when --output=view -O type --output=type : output types: [laz|csv|view]. Default laz -v number --vmin=number : min value for the colormap range -V number --vmax=numner : max value for the colormap range\"\"\" print ( helptext ) valorize def valorize ( basecld , valcld , distance = 10 , group = 1 , degree = 0 ) Assigns values to input base cloud points, overlapping the input values cloud and calculating, for each point in base cloud, the [distance based, weighted] mean of the values of N nearest neighbours, taken from the input values cloud. returns: pandas.Series containing calculated values for all the points in the base cloud which has at least one neighbour within the fixed distance params: basecld - pandas.DataFrame or numpy.ndarray with shape (n,3), representing the coordinates of the points of the base cloud. ['x','y','z'] valcld - pandas.DataFrame or numpy.ndarray with shape (n,4), representing the coordinates of the points of the value cloud and the value of each of them. ['x','y','z','v'] distance - maximum search range for neighbours group - maximum number of neighbours to search for degree - degree of contribution loss over the distance for each neighbour value. Each value of the neighbours is weighted as: 1 / d ** degree where d is the distance between the neighbour and the base cloud point . Degree 0 implies the arithmetic mean of all the neighbour values found , regardless the relative distance . i . e : N1 ----------P----N2 Given two neighbours , N1 and N2 , for a fixed point P where : d ( N1 ) = 10 ; v ( N1 ) =- 15 d ( N2 ) = 4 ; v ( N2 ) = 3 v ( P ) = ( - 15 / 10 ** d + 3 / 4 ** d ) / ( 1 / 10 ** d + 1 / 4 ** d ) being d the degree , we have : d = 0 : v ( P ) = - 6 < -- arimetic mean d = 1 : v ( P ) = - 2 . 14 d = 2 : v ( P ) = 0 . 51 d = 3 : v ( P ) = 1 . 91 d = 4 : v ( P ) = 2 . 55 d = 5 : v ( P ) = 2 . 81 .. d = 9 : v ( P ) = 2 . 99 .. d ( x ): v ( P ) = 3 < -- convergence to the closest's value Note: with group=1 and degree=0, each base cloud point assumes the exact value of the unique closest neighbour, if it exists. View Source def valorize ( basecld , valcld , distance = 10 , group = 1 , degree = 0 ): \"\"\" Assigns values to input base cloud points, overlapping the input values cloud and calculating, for each point in base cloud, the [distance based, weighted] mean of the values of N nearest neighbours, taken from the input values cloud. returns: pandas.Series containing calculated values for all the points in the base cloud which has at least one neighbour within the fixed distance params: basecld - pandas.DataFrame or numpy.ndarray with shape (n,3), representing the coordinates of the points of the base cloud. ['x','y','z'] valcld - pandas.DataFrame or numpy.ndarray with shape (n,4), representing the coordinates of the points of the value cloud and the value of each of them. ['x','y','z','v'] distance - maximum search range for neighbours group - maximum number of neighbours to search for degree - degree of contribution loss over the distance for each neighbour value. Each value of the neighbours is weighted as: 1/d**degree where d is the distance between the neighbour and the base cloud point. Degree 0 implies the arithmetic mean of all the neighbour values found, regardless the relative distance. i.e: N1----------P----N2 Given two neighbours, N1 and N2, for a fixed point P where: d(N1)=10; v(N1)=-15 d(N2)=4; v(N2)=3 v(P) = (-15/10**d+3/4**d)/(1/10**d+1/4**d) being d the degree, we have: d=0 : v(P) = -6 <-- arimetic mean d=1 : v(P) = -2.14 d=2 : v(P) = 0.51 d=3 : v(P) = 1.91 d=4 : v(P) = 2.55 d=5 : v(P) = 2.81 .. d=9 : v(P) = 2.99 .. d(x): v(P) = 3 <-- convergence to the closest's value Note: with group=1 and degree=0, each base cloud point assumes the exact value of the unique closest neighbour, if it exists. \"\"\" print ( \"CHECK: Enter Valorize\" ) basecld = pd . DataFrame ( basecld ) basecld . columns = [ \"x\" , \"y\" , \"z\" ] valcld = pd . DataFrame ( valcld ) valcld . columns = [ \"x\" , \"y\" , \"z\" , \"v\" ] ## Calculating the Series of distance ( dist ) and value clouds relative ## ids ( idsv ). the position of each cell in the arrays reflects the cells ## in the base cloud . k = KDTree ( valcld [[ \"x\" , \"y\" , \"z\" ]]. values ) dist , idsv = k . query ( basecld . values , group , distance_upper_bound = distance ) dist = pd . DataFrame ( dist ). stack () dist . name = \"dist\" idsv = pd . DataFrame ( idsv ). stack () idsv . name = \"idsv\" print ( \"CHECK: Distance calculation done\" ) ## Here we construct the DataFrame contains relation beetween each point ## in base cloud ( base cloud index ) and his neighbous ( value cloud ids , ## distance and neighbour group progressive ). Then we clean the infinite ## distances ( points with no neighbours ) values = dist . to_frame (). join ( idsv ) values = values [ values [ \"dist\" ] != np . inf ] # Indexind on 'idsv' , allows to join with the value cloud and add the 'v' # info values = values . reset_index (). set_index ( \"idsv\" ). sort_index () values = values . join ( valcld [ \"v\" ], how = \"left\" ) values . columns = [ \"cldid\" , \"instance\" , \"dist\" , \"displ\" ] ## Indexing on 'cldid' and 'instance' , allow to make DataFrame comparable ## with base cloud values = values . set_index ([ \"cldid\" , \"instance\" ]). sort_index () ## Here we calculte the [ distance based , weighted ] mean values [ \"weight\" ] = 1 / np . power ( values [ \"dist\" ], degree ) values [ \"contrib\" ] = values [ \"displ\" ] * values [ \"weight\" ] values = values . groupby ( \"cldid\" ). apply ( lambda x : sum ( x [ \"contrib\" ]) / sum ( x [ \"weight\" ]) ) print ( \"CHECK: Weight calculation done\" ) return values","title":"Cloudpainter"},{"location":"reference/hielen2/ext/source_tinsar/cloudpainter/#module-hielen2extsource_tinsarcloudpainter","text":"View Source # coding: utf-8 import pandas as pd import numpy as np import laspy from scipy.spatial import KDTree from matplotlib.colors import Normalize , Colormap , LinearSegmentedColormap from matplotlib.cm import ScalarMappable import getopt import sys import open3d as o3d def valorize ( basecld , valcld , distance = 10 , group = 1 , degree = 0 ): \"\"\" Assigns values to input base cloud points, overlapping the input values cloud and calculating, for each point in base cloud, the [distance based, weighted] mean of the values of N nearest neighbours, taken from the input values cloud. returns: pandas.Series containing calculated values for all the points in the base cloud which has at least one neighbour within the fixed distance params: basecld - pandas.DataFrame or numpy.ndarray with shape (n,3), representing the coordinates of the points of the base cloud. ['x','y','z'] valcld - pandas.DataFrame or numpy.ndarray with shape (n,4), representing the coordinates of the points of the value cloud and the value of each of them. ['x','y','z','v'] distance - maximum search range for neighbours group - maximum number of neighbours to search for degree - degree of contribution loss over the distance for each neighbour value. Each value of the neighbours is weighted as: 1/d**degree where d is the distance between the neighbour and the base cloud point. Degree 0 implies the arithmetic mean of all the neighbour values found, regardless the relative distance. i.e: N1----------P----N2 Given two neighbours, N1 and N2, for a fixed point P where: d(N1)=10; v(N1)=-15 d(N2)=4; v(N2)=3 v(P) = (-15/10**d+3/4**d)/(1/10**d+1/4**d) being d the degree, we have: d=0 : v(P) = -6 <-- arimetic mean d=1 : v(P) = -2.14 d=2 : v(P) = 0.51 d=3 : v(P) = 1.91 d=4 : v(P) = 2.55 d=5 : v(P) = 2.81 .. d=9 : v(P) = 2.99 .. d(x): v(P) = 3 <-- convergence to the closest's value Note: with group=1 and degree=0, each base cloud point assumes the exact value of the unique closest neighbour, if it exists. \"\"\" print ( \"CHECK: Enter Valorize\" ) basecld = pd . DataFrame ( basecld ) basecld . columns = [ \"x\" , \"y\" , \"z\" ] valcld = pd . DataFrame ( valcld ) valcld . columns = [ \"x\" , \"y\" , \"z\" , \"v\" ] ## Calculating the Series of distance (dist) and value clouds relative ## ids (idsv). the position of each cell in the arrays reflects the cells ## in the base cloud. k = KDTree ( valcld [[ \"x\" , \"y\" , \"z\" ]] . values ) dist , idsv = k . query ( basecld . values , group , distance_upper_bound = distance ) dist = pd . DataFrame ( dist ) . stack () dist . name = \"dist\" idsv = pd . DataFrame ( idsv ) . stack () idsv . name = \"idsv\" print ( \"CHECK: Distance calculation done\" ) ## Here we construct the DataFrame contains relation beetween each point ## in base cloud (base cloud index) and his neighbous (value cloud ids, ## distance and neighbour group progressive). Then we clean the infinite ## distances (points with no neighbours) values = dist . to_frame () . join ( idsv ) values = values [ values [ \"dist\" ] != np . inf ] # Indexind on 'idsv', allows to join with the value cloud and add the 'v' # info values = values . reset_index () . set_index ( \"idsv\" ) . sort_index () values = values . join ( valcld [ \"v\" ], how = \"left\" ) values . columns = [ \"cldid\" , \"instance\" , \"dist\" , \"displ\" ] ## Indexing on 'cldid' and 'instance', allow to make DataFrame comparable ## with base cloud values = values . set_index ([ \"cldid\" , \"instance\" ]) . sort_index () ## Here we calculte the [distance based, weighted] mean values [ \"weight\" ] = 1 / np . power ( values [ \"dist\" ], degree ) values [ \"contrib\" ] = values [ \"displ\" ] * values [ \"weight\" ] values = values . groupby ( \"cldid\" ) . apply ( lambda x : sum ( x [ \"contrib\" ]) / sum ( x [ \"weight\" ]) ) print ( \"CHECK: Weight calculation done\" ) return values def colorize ( vals , cmap = [ \"red\" , \"green\" , \"blue\" ], vmin = None , vmax = None ): \"\"\" Maps each value in the input array with the appropriate RGBA color. See matplotlib.colors.Colormap returns: ndarray with shape (n,4) containing the color tuples ['r','g','b','a'] color channels values are normalized in range (0,1) params: vals - array of values. cmap - See matplotlib.colors.Colormap object or colour list. default ['red','green','blue']. vmax - max limit value for colormap. If None is passed, vals.max(). will be assumed, otherwise vals exceding this parameter will be clipped vmin - min limit value for colormap. If None is passed, vals.min() will be assumed, otherwise vals under this parameter will be clipped \"\"\" print ( \"CHECK: Enter Colorize\" ) vmin = vmin or vals . min () vmax = vmax or vals . max () norm = Normalize ( vmin = vmin , vmax = vmax , clip = True ) if not isinstance ( cmap , Colormap ): cmap = LinearSegmentedColormap . from_list ( \"mycmap\" , cmap ) mapper = ScalarMappable ( norm = norm , cmap = cmap ) cols = mapper . to_rgba ( vals ) return cols def paint ( valcld , basecld = None , distance = 10 , group = 1 , degree = 0 , cmap = [ \"red\" , \"green\" , \"blue\" ], vmin = None , vmax = None , ): \"\"\" Assigns a color to each point of a base cloud, merging the information of a second valorized cloud overlapping the first one. See cloudpainter.valorize and cloudpainter.colorize for futher informations result: pandas.DataFrame containing ['x','y','z','v','r','g','b'] tuples for each point in the base cloud. Points with no neighbours will be assigned with [0.9,0.9,0.9] for ['r','g','b'] and np.nan for ['v'] params: basecld - pandas.DataFrame or numpy.ndarray with shape (n,3), representing the coordinates of the points of the base cloud. ['x','y','z'] valcld - pandas.DataFrame or numpy.ndarray with shape (n,4), representing the coordinates of the points of the value cloud and the value of each of them. ['x','y','z','v'] distance - maximum search range for neighbours group - maximum number of neighbours to search for degree - degree of contribution loss over the distance for each neighbour value. Each value of the neighbours is weighted as: 1/d**degree where d is the distance between the neighbour and the base cloud point. Degree 0 implies the arithmetic mean of all the neighbour values found, regardless the relative distance. cmap - matplotlib.colors.Colormap object or colour list. default ['red','green','blue']. vmax - max limit value for colormap. If None is passed, vals.max(). will be assumed, otherwise vals exceding this parameter will be clipped vmin - min limit value for colormap. If None is passed, vals.min() will be assumed, otherwise vals under this parameter will be clipped \"\"\" valcld = pd . DataFrame ( valcld ) valcld . columns = [ \"x\" , \"y\" , \"z\" , \"v\" ] values = None if basecld is not None : basecld = pd . DataFrame ( basecld ) basecld . columns = [ \"x\" , \"y\" , \"z\" ] values = valorize ( basecld , valcld , distance = distance , group = group , degree = degree ) values . name = \"v\" else : values = valcld [ \"v\" ] colors = pd . DataFrame ( colorize ( values , cmap = cmap , vmin = vmin , vmax = vmax ), columns = [ \"r\" , \"g\" , \"b\" , \"a\" ], index = values . index , )[[ \"r\" , \"g\" , \"b\" ]] if basecld is not None : result = basecld . join ( colors , how = \"left\" ) . replace ( np . nan , 0.9 ) result = result . join ( values , how = \"left\" ) else : result = valcld . join ( colors , how = \"left\" ) return result [[ \"x\" , \"y\" , \"z\" , \"v\" , \"r\" , \"g\" , \"b\" ]] #65536 def makelaz ( frame , filetowrite , coordmult = 1 , colormult = 1 , scale = [ 1 , 1 , 1 ], framevalues = None ): hdr = laspy . header . Header ( point_format = 2 ) frame . columns = [ 'x' , 'y' , 'z' , 'r' , 'g' , 'b' ] outfile = laspy . file . File ( filetowrite , mode = 'w' , header = hdr ) x = frame [ 'x' ] * coordmult y = frame [ 'y' ] * coordmult z = frame [ 'z' ] * coordmult r = frame [ 'r' ] * colormult g = frame [ 'g' ] * colormult b = frame [ 'b' ] * colormult #outfile.header.min=[ np.floor(np.min(x)), np.floor(np.min(y)), np.floor(np.min(z)) ] #outfile.header.max=[ np.ceil(np.max(x)), np.ceil(np.max(y)), np.ceil(np.max(z)) ] #outfile.header.offset=outfile.header.min outfile . header . scale = scale outfile . x = x . values outfile . y = y . values outfile . z = z . values outfile . Red = r . values outfile . Green = g . values outfile . Blue = b . values outfile . header . offset = outfile . header . min outfile . close () def openpcl ( res ): #QUI USO open3d perch\u00e8 \u00e8 molto comodo pcl = o3d . geometry . PointCloud () pcl . points = o3d . utility . Vector3dVector ( res [[ \"x\" , \"y\" , \"z\" ]] . values ) pcl . colors = o3d . utility . Vector3dVector ( res [[ \"r\" , \"g\" , \"b\" ]] . values ) o3d . visualization . draw_geometries ([ pcl ]) def usage (): helptext = r \"\"\"usage: cloudainter.py [option] path/to/radar/result.csv parmeters: path/to/radar/result.csv : path for csv based radar cloud in the format X,Y,Z,V,... options: -b path --basecloud=path : path for csv based reference cloud in the format X,Y,Z,... -g number --group=number : max number of neighbours to interpolate -d number --distance=number : nearest neighbour max radius -D number --degree=number : neighbour contribute attenuation degree along distance -c csv --colors=csv : colormap as comma separated colors names -o path --outfile=path : output file name. Ignored when --output=view -O type --output=type : output types: [laz|csv|view]. Default laz -v number --vmin=number : min value for the colormap range -V number --vmax=numner : max value for the colormap range\"\"\" print ( helptext ) if __name__ == \"__main__\" : #DEFAULTS datacloud = None basecloud = None group = 1 distance = 5 degree = 0 colors = [ 'violet' , 'blue' , 'cyan' , 'green' , 'yellow' , 'orange' , 'red' ] outfile = None output = 'las' vmin =- 500 vmax = 500 try : opts , args = getopt . getopt ( sys . argv [ 1 :], \"b:g:d:D:c:o:O:v:V:\" , [ \"basecloud=\" , \"group=\" , \"distance=\" , \"degree=\" , \"colors=\" , \"outfile=\" , \"output=\" , \"vmin=\" , \"vmax=\" ]) for o , a in opts : if o in ( \"-b\" , \"--basecloud\" ): basecloud = a elif o in ( \"-g\" , \"--group\" ): group = a elif o in ( \"-d\" , \"--distance\" ): distance = a elif o in ( \"-D\" , \"--degree\" ): degree = a elif o in ( \"-c\" , \"--colors\" ): colors = a . split ( \",\" ) elif o in ( \"-o\" , \"--outfile\" ): outfile = a elif o in ( \"-O\" , \"--output\" ): output = a elif o in ( \"-v\" , \"--vmin\" ): vmin = a elif o in ( \"-V\" , \"--vmax\" ): vmax = a else : assert False , \"unhandled option\" try : infile = args [ 0 ] except Exception : raise Exception ( f \"No file\" ) datacloud = pd . read_csv ( infile ) datacloud = datacloud [ datacloud . columns [ 0 : 4 ]] datacloud . columns = [ \"X\" , \"Y\" , \"Z\" , \"V\" ] if basecloud is not None : basecloud = pd . read_csv ( basecloud ) basecloud = basecloud [ basecloud . columns [ 0 : 3 ]] basecloud . columns = [ \"X\" , \"Y\" , \"Z\" ] result = paint ( valcld = datacloud , basecld = basecloud , distance = distance , degree = degree , group = group , cmap = colors , vmin = vmin , vmax = vmax ) print ( output ) if outfile is None : outfile = '.' . join ([ infile , output ]) if output == 'las' : makelaz ( result [[ \"x\" , \"y\" , \"z\" , \"r\" , \"g\" , \"b\" ]], outfile ) elif output == 'csv' : result . to_csv ( outfile , index = False ) elif output == 'view' : openpcl ( result ) else : raise Exception ( f 'unkwnown output type: { output } ' ) except Exception as err : # print help information and exit: print ( err ) usage () sys . exit ( 2 )","title":"Module hielen2.ext.source_tinsar.cloudpainter"},{"location":"reference/hielen2/ext/source_tinsar/cloudpainter/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/ext/source_tinsar/cloudpainter/#colorize","text":"def colorize ( vals , cmap = [ 'red' , 'green' , 'blue' ], vmin = None , vmax = None ) Maps each value in the input array with the appropriate RGBA color. See matplotlib.colors.Colormap returns: ndarray with shape (n,4) containing the color tuples ['r','g','b','a'] color channels values are normalized in range (0,1) params: vals - array of values. cmap - See matplotlib.colors.Colormap object or colour list. default ['red','green','blue']. vmax - max limit value for colormap. If None is passed, vals.max(). will be assumed, otherwise vals exceding this parameter will be clipped vmin - min limit value for colormap. If None is passed, vals.min() will be assumed, otherwise vals under this parameter will be clipped View Source def colorize ( vals , cmap = [ \"red\" , \"green\" , \"blue\" ], vmin = None , vmax = None ): \"\"\" Maps each value in the input array with the appropriate RGBA color. See matplotlib.colors.Colormap returns: ndarray with shape (n,4) containing the color tuples ['r','g','b','a'] color channels values are normalized in range (0,1) params: vals - array of values. cmap - See matplotlib.colors.Colormap object or colour list. default ['red','green','blue']. vmax - max limit value for colormap. If None is passed, vals.max(). will be assumed, otherwise vals exceding this parameter will be clipped vmin - min limit value for colormap. If None is passed, vals.min() will be assumed, otherwise vals under this parameter will be clipped \"\"\" print ( \"CHECK: Enter Colorize\" ) vmin = vmin or vals . min () vmax = vmax or vals . max () norm = Normalize ( vmin = vmin , vmax = vmax , clip = True ) if not isinstance ( cmap , Colormap ): cmap = LinearSegmentedColormap . from_list ( \"mycmap\" , cmap ) mapper = ScalarMappable ( norm = norm , cmap = cmap ) cols = mapper . to_rgba ( vals ) return cols","title":"colorize"},{"location":"reference/hielen2/ext/source_tinsar/cloudpainter/#makelaz","text":"def makelaz ( frame , filetowrite , coordmult = 1 , colormult = 1 , scale = [ 1 , 1 , 1 ], framevalues = None ) View Source def makelaz ( frame , filetowrite , coordmult = 1 , colormult = 1 , scale = [ 1 , 1 , 1 ], framevalues = None ): hdr = laspy . header . Header ( point_format = 2 ) frame . columns = [ 'x' , 'y' , 'z' , 'r' , 'g' , 'b' ] outfile = laspy . file . File ( filetowrite , mode = 'w' , header = hdr ) x = frame [ 'x' ] * coordmult y = frame [ 'y' ] * coordmult z = frame [ 'z' ] * coordmult r = frame [ 'r' ] * colormult g = frame [ 'g' ] * colormult b = frame [ 'b' ] * colormult # outfile . header . min = [ np . floor ( np . min ( x )), np . floor ( np . min ( y )), np . floor ( np . min ( z )) ] # outfile . header . max = [ np . ceil ( np . max ( x )), np . ceil ( np . max ( y )), np . ceil ( np . max ( z )) ] # outfile . header . offset = outfile . header . min outfile . header . scale = scale outfile . x = x . values outfile . y = y . values outfile . z = z . values outfile . Red = r . values outfile . Green = g . values outfile . Blue = b . values outfile . header . offset = outfile . header . min outfile . close ()","title":"makelaz"},{"location":"reference/hielen2/ext/source_tinsar/cloudpainter/#openpcl","text":"def openpcl ( res ) View Source def openpcl ( res ) : #QUI USO open3d perch\u00e8 \u00e8 molto comodo pcl = o3d . geometry . PointCloud () pcl . points = o3d . utility . Vector3dVector ( res [ [\"x\", \"y\", \"z\" ] ] . values ) pcl . colors = o3d . utility . Vector3dVector ( res [ [\"r\", \"g\", \"b\" ] ] . values ) o3d . visualization . draw_geometries ( [ pcl ] )","title":"openpcl"},{"location":"reference/hielen2/ext/source_tinsar/cloudpainter/#paint","text":"def paint ( valcld , basecld = None , distance = 10 , group = 1 , degree = 0 , cmap = [ 'red' , 'green' , 'blue' ], vmin = None , vmax = None ) Assigns a color to each point of a base cloud, merging the information of a second valorized cloud overlapping the first one. See cloudpainter.valorize and cloudpainter.colorize for futher informations result: pandas.DataFrame containing ['x','y','z','v','r','g','b'] tuples for each point in the base cloud. Points with no neighbours will be assigned with [0.9,0.9,0.9] for ['r','g','b'] and np.nan for ['v'] params: basecld - pandas.DataFrame or numpy.ndarray with shape (n,3), representing the coordinates of the points of the base cloud. ['x','y','z'] valcld - pandas.DataFrame or numpy.ndarray with shape (n,4), representing the coordinates of the points of the value cloud and the value of each of them. ['x','y','z','v'] distance - maximum search range for neighbours group - maximum number of neighbours to search for degree - degree of contribution loss over the distance for each neighbour value. Each value of the neighbours is weighted as: 1 / d ** degree where d is the distance between the neighbour and the base cloud point . Degree 0 implies the arithmetic mean of all the neighbour values found , regardless the relative distance . cmap - matplotlib.colors.Colormap object or colour list. default ['red','green','blue']. vmax - max limit value for colormap. If None is passed, vals.max(). will be assumed, otherwise vals exceding this parameter will be clipped vmin - min limit value for colormap. If None is passed, vals.min() will be assumed, otherwise vals under this parameter will be clipped View Source def paint ( valcld , basecld = None , distance = 10 , group = 1 , degree = 0 , cmap = [ \"red\" , \"green\" , \"blue\" ], vmin = None , vmax = None , ): \"\"\" Assigns a color to each point of a base cloud, merging the information of a second valorized cloud overlapping the first one. See cloudpainter.valorize and cloudpainter.colorize for futher informations result: pandas.DataFrame containing ['x','y','z','v','r','g','b'] tuples for each point in the base cloud. Points with no neighbours will be assigned with [0.9,0.9,0.9] for ['r','g','b'] and np.nan for ['v'] params: basecld - pandas.DataFrame or numpy.ndarray with shape (n,3), representing the coordinates of the points of the base cloud. ['x','y','z'] valcld - pandas.DataFrame or numpy.ndarray with shape (n,4), representing the coordinates of the points of the value cloud and the value of each of them. ['x','y','z','v'] distance - maximum search range for neighbours group - maximum number of neighbours to search for degree - degree of contribution loss over the distance for each neighbour value. Each value of the neighbours is weighted as: 1/d**degree where d is the distance between the neighbour and the base cloud point. Degree 0 implies the arithmetic mean of all the neighbour values found, regardless the relative distance. cmap - matplotlib.colors.Colormap object or colour list. default ['red','green','blue']. vmax - max limit value for colormap. If None is passed, vals.max(). will be assumed, otherwise vals exceding this parameter will be clipped vmin - min limit value for colormap. If None is passed, vals.min() will be assumed, otherwise vals under this parameter will be clipped \"\"\" valcld = pd . DataFrame ( valcld ) valcld . columns = [ \"x\" , \"y\" , \"z\" , \"v\" ] values = None if basecld is not None : basecld = pd . DataFrame ( basecld ) basecld . columns = [ \"x\" , \"y\" , \"z\" ] values = valorize ( basecld , valcld , distance = distance , group = group , degree = degree ) values . name = \"v\" else : values = valcld [ \"v\" ] colors = pd . DataFrame ( colorize ( values , cmap = cmap , vmin = vmin , vmax = vmax ), columns = [ \"r\" , \"g\" , \"b\" , \"a\" ], index = values . index , )[[ \"r\" , \"g\" , \"b\" ]] if basecld is not None : result = basecld . join ( colors , how = \"left\" ). replace ( np . nan , 0 . 9 ) result = result . join ( values , how = \"left\" ) else : result = valcld . join ( colors , how = \"left\" ) return result [[ \"x\" , \"y\" , \"z\" , \"v\" , \"r\" , \"g\" , \"b\" ]]","title":"paint"},{"location":"reference/hielen2/ext/source_tinsar/cloudpainter/#usage","text":"def usage ( ) View Source def usage () : helptext = r \"\"\"usage: cloudainter.py [option] path/to/radar/result.csv parmeters: path/to/radar/result.csv : path for csv based radar cloud in the format X,Y,Z,V,... options: -b path --basecloud=path : path for csv based reference cloud in the format X,Y,Z,... -g number --group=number : max number of neighbours to interpolate -d number --distance=number : nearest neighbour max radius -D number --degree=number : neighbour contribute attenuation degree along distance -c csv --colors=csv : colormap as comma separated colors names -o path --outfile=path : output file name. Ignored when --output=view -O type --output=type : output types: [laz|csv|view]. Default laz -v number --vmin=number : min value for the colormap range -V number --vmax=numner : max value for the colormap range\"\"\" print ( helptext )","title":"usage"},{"location":"reference/hielen2/ext/source_tinsar/cloudpainter/#valorize","text":"def valorize ( basecld , valcld , distance = 10 , group = 1 , degree = 0 ) Assigns values to input base cloud points, overlapping the input values cloud and calculating, for each point in base cloud, the [distance based, weighted] mean of the values of N nearest neighbours, taken from the input values cloud. returns: pandas.Series containing calculated values for all the points in the base cloud which has at least one neighbour within the fixed distance params: basecld - pandas.DataFrame or numpy.ndarray with shape (n,3), representing the coordinates of the points of the base cloud. ['x','y','z'] valcld - pandas.DataFrame or numpy.ndarray with shape (n,4), representing the coordinates of the points of the value cloud and the value of each of them. ['x','y','z','v'] distance - maximum search range for neighbours group - maximum number of neighbours to search for degree - degree of contribution loss over the distance for each neighbour value. Each value of the neighbours is weighted as: 1 / d ** degree where d is the distance between the neighbour and the base cloud point . Degree 0 implies the arithmetic mean of all the neighbour values found , regardless the relative distance . i . e : N1 ----------P----N2 Given two neighbours , N1 and N2 , for a fixed point P where : d ( N1 ) = 10 ; v ( N1 ) =- 15 d ( N2 ) = 4 ; v ( N2 ) = 3 v ( P ) = ( - 15 / 10 ** d + 3 / 4 ** d ) / ( 1 / 10 ** d + 1 / 4 ** d ) being d the degree , we have : d = 0 : v ( P ) = - 6 < -- arimetic mean d = 1 : v ( P ) = - 2 . 14 d = 2 : v ( P ) = 0 . 51 d = 3 : v ( P ) = 1 . 91 d = 4 : v ( P ) = 2 . 55 d = 5 : v ( P ) = 2 . 81 .. d = 9 : v ( P ) = 2 . 99 .. d ( x ): v ( P ) = 3 < -- convergence to the closest's value Note: with group=1 and degree=0, each base cloud point assumes the exact value of the unique closest neighbour, if it exists. View Source def valorize ( basecld , valcld , distance = 10 , group = 1 , degree = 0 ): \"\"\" Assigns values to input base cloud points, overlapping the input values cloud and calculating, for each point in base cloud, the [distance based, weighted] mean of the values of N nearest neighbours, taken from the input values cloud. returns: pandas.Series containing calculated values for all the points in the base cloud which has at least one neighbour within the fixed distance params: basecld - pandas.DataFrame or numpy.ndarray with shape (n,3), representing the coordinates of the points of the base cloud. ['x','y','z'] valcld - pandas.DataFrame or numpy.ndarray with shape (n,4), representing the coordinates of the points of the value cloud and the value of each of them. ['x','y','z','v'] distance - maximum search range for neighbours group - maximum number of neighbours to search for degree - degree of contribution loss over the distance for each neighbour value. Each value of the neighbours is weighted as: 1/d**degree where d is the distance between the neighbour and the base cloud point. Degree 0 implies the arithmetic mean of all the neighbour values found, regardless the relative distance. i.e: N1----------P----N2 Given two neighbours, N1 and N2, for a fixed point P where: d(N1)=10; v(N1)=-15 d(N2)=4; v(N2)=3 v(P) = (-15/10**d+3/4**d)/(1/10**d+1/4**d) being d the degree, we have: d=0 : v(P) = -6 <-- arimetic mean d=1 : v(P) = -2.14 d=2 : v(P) = 0.51 d=3 : v(P) = 1.91 d=4 : v(P) = 2.55 d=5 : v(P) = 2.81 .. d=9 : v(P) = 2.99 .. d(x): v(P) = 3 <-- convergence to the closest's value Note: with group=1 and degree=0, each base cloud point assumes the exact value of the unique closest neighbour, if it exists. \"\"\" print ( \"CHECK: Enter Valorize\" ) basecld = pd . DataFrame ( basecld ) basecld . columns = [ \"x\" , \"y\" , \"z\" ] valcld = pd . DataFrame ( valcld ) valcld . columns = [ \"x\" , \"y\" , \"z\" , \"v\" ] ## Calculating the Series of distance ( dist ) and value clouds relative ## ids ( idsv ). the position of each cell in the arrays reflects the cells ## in the base cloud . k = KDTree ( valcld [[ \"x\" , \"y\" , \"z\" ]]. values ) dist , idsv = k . query ( basecld . values , group , distance_upper_bound = distance ) dist = pd . DataFrame ( dist ). stack () dist . name = \"dist\" idsv = pd . DataFrame ( idsv ). stack () idsv . name = \"idsv\" print ( \"CHECK: Distance calculation done\" ) ## Here we construct the DataFrame contains relation beetween each point ## in base cloud ( base cloud index ) and his neighbous ( value cloud ids , ## distance and neighbour group progressive ). Then we clean the infinite ## distances ( points with no neighbours ) values = dist . to_frame (). join ( idsv ) values = values [ values [ \"dist\" ] != np . inf ] # Indexind on 'idsv' , allows to join with the value cloud and add the 'v' # info values = values . reset_index (). set_index ( \"idsv\" ). sort_index () values = values . join ( valcld [ \"v\" ], how = \"left\" ) values . columns = [ \"cldid\" , \"instance\" , \"dist\" , \"displ\" ] ## Indexing on 'cldid' and 'instance' , allow to make DataFrame comparable ## with base cloud values = values . set_index ([ \"cldid\" , \"instance\" ]). sort_index () ## Here we calculte the [ distance based , weighted ] mean values [ \"weight\" ] = 1 / np . power ( values [ \"dist\" ], degree ) values [ \"contrib\" ] = values [ \"displ\" ] * values [ \"weight\" ] values = values . groupby ( \"cldid\" ). apply ( lambda x : sum ( x [ \"contrib\" ]) / sum ( x [ \"weight\" ]) ) print ( \"CHECK: Weight calculation done\" ) return values","title":"valorize"},{"location":"reference/hielen2/maps/","text":"Module hielen2.maps Sub-modules hielen2.maps.data_access_layer hielen2.maps.mapper","title":"Index"},{"location":"reference/hielen2/maps/#module-hielen2maps","text":"","title":"Module hielen2.maps"},{"location":"reference/hielen2/maps/#sub-modules","text":"hielen2.maps.data_access_layer hielen2.maps.mapper","title":"Sub-modules"},{"location":"reference/hielen2/maps/data_access_layer/","text":"Module hielen2.maps.data_access_layer View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 #!/usr/bin/env python # coding=utf-8 from pandas import DataFrame , Series from time import time from concurrent.futures import ThreadPoolExecutor from functools import wraps from numpy import nan , unique from importlib import import_module from hielen2 import db from hielen2.utils import isot2ut , ut2isot def _threadpool ( f ): @wraps ( f ) def wrap ( * args , ** kwargs ): return ThreadPoolExecutor () . submit ( f , * args , ** kwargs ) return wrap class Series : def __init__ ( self , uid ): series_info = db [ \"series\" ][ uid ] self . __dict__ . update ( series_info ) geninfo = dict ( ( k , w ) for k , w in series_info . items () if k in ( \"modules\" , \"operator\" , \"operands\" ) ) self . generator = Generator ( ** geninfo ) @_threadpool def thdata ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ): return self . data ( times , timeref , refresh ) def data ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ): try : if refresh is not None and refresh : raise KeyError out = db [ \"datacache\" ][ self . uid ][ times ] except KeyError : out = self . generator . _generate ( times , timeref = timeref , ** kwargs ) try : out = out [ out . columns [ 0 ]] except AttributeError : pass try : db [ \"datacache\" ] . update ( self . uid , out ) except KeyError as e : db [ \"datacache\" ][ self . uid ] = out out . index . name = \"timestamp\" return out class Generator : def __init__ ( self , modules = None , operator = None , operands = None ): self . operator = operator or \"mod.map(**operands)\" self . modules = {} if not modules is None : for k , m in modules . items (): self . operator = self . operator . replace ( k , f \"self.modules[ { k !r} ]\" ) self . modules [ k ] = import_module ( m ) self . operands = {} if operands is not None : self . operands = dict ( Generator . _parse_operand ( * op ) for op in operands . items () ) def _parse_operand ( key , value ): \"\"\" trying to extract a series \"\"\" try : return ( key , Series ( value )) except KeyError : pass \"\"\" trying to extract element attribute \"\"\" #TODO modificare per utilizare configurazione try : v = value . split ( \".\" ) assert v . __len__ () == 2 return ( key , db [ \"features\" ][ v [ 0 ]][ \"properties\" ][ v [ 1 ]]) except Exception : pass \"\"\" giving up. It should be a scalar. return it \"\"\" return ( key , value ) def _generate ( self , times , timeref , ** kwargs ): operands = dict ( times = times , timeref = timeref , ** kwargs ) operands . update ( { k : w for k , w in self . operands . items () if not isinstance ( w , Series )} ) runners = { k : w . thdata ( times , timeref ) for k , w in self . operands . items () if isinstance ( w , Series ) } operands . update ({ k : w . result () for k , w in runners . items ()}) #operands.update( { k:w.data(times, timeref, **kwargs) for k,w in self.operands.items() if isinstance(w,Series) } ) #print('OPERANDI',operands) #print('OPERATORE', self.operator) return eval ( self . operator ) Variables db nan Classes Generator class Generator ( modules = None , operator = None , operands = None ) View Source class Generator : def __init__ ( self , modules = None , operator = None , operands = None ) : self . operator = operator or \"mod.map(**operands)\" self . modules = {} if not modules is None : for k , m in modules . items () : self . operator = self . operator . replace ( k , f \"self.modules[{k!r}]\" ) self . modules [ k ] = import_module ( m ) self . operands = {} if operands is not None : self . operands = dict ( Generator . _parse_operand ( * op ) for op in operands . items () ) def _parse_operand ( key , value ) : \"\"\" trying to extract a series \"\"\" try : return ( key , Series ( value )) except KeyError : pass \"\"\" trying to extract element attribute \"\"\" #TODO modificare per utilizare configurazione try : v = value . split ( \".\" ) assert v . __len__ () == 2 return ( key , db [ \"features\" ][ v[0 ] ] [ \"properties\" ][ v[1 ] ] ) except Exception : pass \"\"\" giving up. It should be a scalar. return it \"\"\" return ( key , value ) def _generate ( self , times , timeref , ** kwargs ) : operands = dict ( times = times , timeref = timeref , ** kwargs ) operands . update ( { k : w for k , w in self . operands . items () if not isinstance ( w , Series ) } ) runners = { k : w . thdata ( times , timeref ) for k , w in self . operands . items () if isinstance ( w , Series ) } operands . update ( { k : w . result () for k , w in runners . items () } ) #operands . update ( { k : w . data ( times , timeref , ** kwargs ) for k , w in self . operands . items () if isinstance ( w , Series ) } ) #print ( 'OPERANDI' , operands ) #print ( 'OPERATORE' , self . operator ) return eval ( self . operator ) Series class Series ( uid ) View Source class Series : def __init__ ( self , uid ) : series_info = db [ \"series\" ][ uid ] self . __dict__ . update ( series_info ) geninfo = dict ( ( k , w ) for k , w in series_info . items () if k in ( \"modules\" , \"operator\" , \"operands\" ) ) self . generator = Generator ( ** geninfo ) @_threadpool def thdata ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ) : return self . data ( times , timeref , refresh ) def data ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ) : try : if refresh is not None and refresh : raise KeyError out = db [ \"datacache\" ][ self.uid ][ times ] except KeyError : out = self . generator . _generate ( times , timeref = timeref , ** kwargs ) try : out = out [ out.columns[0 ] ] except AttributeError : pass try : db [ \"datacache\" ] . update ( self . uid , out ) except KeyError as e : db [ \"datacache\" ][ self.uid ]= out out . index . name = \"timestamp\" return out Methods data def data ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ) View Source def data ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ) : try : if refresh is not None and refresh : raise KeyError out = db [ \"datacache\" ][ self.uid ][ times ] except KeyError : out = self . generator . _generate ( times , timeref = timeref , ** kwargs ) try : out = out [ out.columns[0 ] ] except AttributeError : pass try : db [ \"datacache\" ] . update ( self . uid , out ) except KeyError as e : db [ \"datacache\" ][ self.uid ]= out out . index . name = \"timestamp\" return out thdata def thdata ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ) View Source @_threadpool def thdata ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ) : return self . data ( times , timeref , refresh )","title":"Data Access Layer"},{"location":"reference/hielen2/maps/data_access_layer/#module-hielen2mapsdata_access_layer","text":"View Source 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 #!/usr/bin/env python # coding=utf-8 from pandas import DataFrame , Series from time import time from concurrent.futures import ThreadPoolExecutor from functools import wraps from numpy import nan , unique from importlib import import_module from hielen2 import db from hielen2.utils import isot2ut , ut2isot def _threadpool ( f ): @wraps ( f ) def wrap ( * args , ** kwargs ): return ThreadPoolExecutor () . submit ( f , * args , ** kwargs ) return wrap class Series : def __init__ ( self , uid ): series_info = db [ \"series\" ][ uid ] self . __dict__ . update ( series_info ) geninfo = dict ( ( k , w ) for k , w in series_info . items () if k in ( \"modules\" , \"operator\" , \"operands\" ) ) self . generator = Generator ( ** geninfo ) @_threadpool def thdata ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ): return self . data ( times , timeref , refresh ) def data ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ): try : if refresh is not None and refresh : raise KeyError out = db [ \"datacache\" ][ self . uid ][ times ] except KeyError : out = self . generator . _generate ( times , timeref = timeref , ** kwargs ) try : out = out [ out . columns [ 0 ]] except AttributeError : pass try : db [ \"datacache\" ] . update ( self . uid , out ) except KeyError as e : db [ \"datacache\" ][ self . uid ] = out out . index . name = \"timestamp\" return out class Generator : def __init__ ( self , modules = None , operator = None , operands = None ): self . operator = operator or \"mod.map(**operands)\" self . modules = {} if not modules is None : for k , m in modules . items (): self . operator = self . operator . replace ( k , f \"self.modules[ { k !r} ]\" ) self . modules [ k ] = import_module ( m ) self . operands = {} if operands is not None : self . operands = dict ( Generator . _parse_operand ( * op ) for op in operands . items () ) def _parse_operand ( key , value ): \"\"\" trying to extract a series \"\"\" try : return ( key , Series ( value )) except KeyError : pass \"\"\" trying to extract element attribute \"\"\" #TODO modificare per utilizare configurazione try : v = value . split ( \".\" ) assert v . __len__ () == 2 return ( key , db [ \"features\" ][ v [ 0 ]][ \"properties\" ][ v [ 1 ]]) except Exception : pass \"\"\" giving up. It should be a scalar. return it \"\"\" return ( key , value ) def _generate ( self , times , timeref , ** kwargs ): operands = dict ( times = times , timeref = timeref , ** kwargs ) operands . update ( { k : w for k , w in self . operands . items () if not isinstance ( w , Series )} ) runners = { k : w . thdata ( times , timeref ) for k , w in self . operands . items () if isinstance ( w , Series ) } operands . update ({ k : w . result () for k , w in runners . items ()}) #operands.update( { k:w.data(times, timeref, **kwargs) for k,w in self.operands.items() if isinstance(w,Series) } ) #print('OPERANDI',operands) #print('OPERATORE', self.operator) return eval ( self . operator )","title":"Module hielen2.maps.data_access_layer"},{"location":"reference/hielen2/maps/data_access_layer/#variables","text":"db nan","title":"Variables"},{"location":"reference/hielen2/maps/data_access_layer/#classes","text":"","title":"Classes"},{"location":"reference/hielen2/maps/data_access_layer/#generator","text":"class Generator ( modules = None , operator = None , operands = None ) View Source class Generator : def __init__ ( self , modules = None , operator = None , operands = None ) : self . operator = operator or \"mod.map(**operands)\" self . modules = {} if not modules is None : for k , m in modules . items () : self . operator = self . operator . replace ( k , f \"self.modules[{k!r}]\" ) self . modules [ k ] = import_module ( m ) self . operands = {} if operands is not None : self . operands = dict ( Generator . _parse_operand ( * op ) for op in operands . items () ) def _parse_operand ( key , value ) : \"\"\" trying to extract a series \"\"\" try : return ( key , Series ( value )) except KeyError : pass \"\"\" trying to extract element attribute \"\"\" #TODO modificare per utilizare configurazione try : v = value . split ( \".\" ) assert v . __len__ () == 2 return ( key , db [ \"features\" ][ v[0 ] ] [ \"properties\" ][ v[1 ] ] ) except Exception : pass \"\"\" giving up. It should be a scalar. return it \"\"\" return ( key , value ) def _generate ( self , times , timeref , ** kwargs ) : operands = dict ( times = times , timeref = timeref , ** kwargs ) operands . update ( { k : w for k , w in self . operands . items () if not isinstance ( w , Series ) } ) runners = { k : w . thdata ( times , timeref ) for k , w in self . operands . items () if isinstance ( w , Series ) } operands . update ( { k : w . result () for k , w in runners . items () } ) #operands . update ( { k : w . data ( times , timeref , ** kwargs ) for k , w in self . operands . items () if isinstance ( w , Series ) } ) #print ( 'OPERANDI' , operands ) #print ( 'OPERATORE' , self . operator ) return eval ( self . operator )","title":"Generator"},{"location":"reference/hielen2/maps/data_access_layer/#series","text":"class Series ( uid ) View Source class Series : def __init__ ( self , uid ) : series_info = db [ \"series\" ][ uid ] self . __dict__ . update ( series_info ) geninfo = dict ( ( k , w ) for k , w in series_info . items () if k in ( \"modules\" , \"operator\" , \"operands\" ) ) self . generator = Generator ( ** geninfo ) @_threadpool def thdata ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ) : return self . data ( times , timeref , refresh ) def data ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ) : try : if refresh is not None and refresh : raise KeyError out = db [ \"datacache\" ][ self.uid ][ times ] except KeyError : out = self . generator . _generate ( times , timeref = timeref , ** kwargs ) try : out = out [ out.columns[0 ] ] except AttributeError : pass try : db [ \"datacache\" ] . update ( self . uid , out ) except KeyError as e : db [ \"datacache\" ][ self.uid ]= out out . index . name = \"timestamp\" return out","title":"Series"},{"location":"reference/hielen2/maps/data_access_layer/#methods","text":"","title":"Methods"},{"location":"reference/hielen2/maps/data_access_layer/#data","text":"def data ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ) View Source def data ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ) : try : if refresh is not None and refresh : raise KeyError out = db [ \"datacache\" ][ self.uid ][ times ] except KeyError : out = self . generator . _generate ( times , timeref = timeref , ** kwargs ) try : out = out [ out.columns[0 ] ] except AttributeError : pass try : db [ \"datacache\" ] . update ( self . uid , out ) except KeyError as e : db [ \"datacache\" ][ self.uid ]= out out . index . name = \"timestamp\" return out","title":"data"},{"location":"reference/hielen2/maps/data_access_layer/#thdata","text":"def thdata ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ) View Source @_threadpool def thdata ( self , times = None , timeref = None , refresh = None , * args , ** kwargs ) : return self . data ( times , timeref , refresh )","title":"thdata"},{"location":"reference/hielen2/maps/mapper/","text":"Module hielen2.maps.mapper View Source # outmapf < output mapfile , absolute or relative ( to the running script ) path and filename : MANDATORY > # bands < image processing bands ; defaults to '1, 2, 3' > # scale1 < image first band ( red or B / W ) processing scale ( min , max ); defaults to '0,255' in case of RGB and to '0,65536' in case of B / W > # scale2 < image second band ( green ) processing scale ( min , max ); defaults to '0,255' > # scale3 < image third band ( blue ) processing scale ( min , max ); defaults to '0,255' > # crs < image CRS projection ( EPSG number ); defaults to '3857' > # lyrext < image extent ( minX , minY , maxX , maxY ); defaults to '-20026376.39 -20048966.10 20026376.39 20048966.10' ( = fullext EPSG : 3857 ) > # datadir < raster image data directory , relative to mapfile location ; defaults to '../data' > # inmapf < input mapfile , absolute or relative ( to the running script ) path and filename ; defaults to 'SAGraster.map' > # um < unit of measurement ( 'METERS' , 'DD' ...); defaults to 'METERS' > import mappyfile import re from hielen2 import conf def setMFparams ( outmapf , bands = 3 , scale =[ '0,255','0,255','0,255' ] , crs = 'EPSG:3857' , lyrext = '-20026376.39 -20048966.10 20026376.39 20048966.10' , datadir = '' , inmapf = conf [ 'maptemplate' ] , um = 'METERS' , ows_onlineresources = \"http://pippo&\" ) : mapfile = mappyfile . open ( inmapf ) mapfile [ \"shapepath\" ] = datadir mapfile [ \"web\" ][ \"metadata\" ][ \"ows_onlineresource\" ] = ows_onlineresources if ( bands == 1 ) : scale =[ '0,65536' ] layer = mapfile [ \"layers\" ][ 0 ] tbands = ',' . join ( map ( str ,( range ( 1 , bands + 1 )))) layer [ 'processing' ]=[ f'BANDS={tbands}' ] for i in range ( 0 , bands ) : layer [ 'processing' ] . append ( f \"SCALE_{i+1}={scale[i]}\" ) layer [ \"projection\" ] = f \"init={str(crs).lower()}\" layer [ \"units\" ] = re . sub ( 'METRES?' , 'METERS' , um . upper ()) layer [ \"metadata\" ][ \"ows_srs\" ] = str ( crs ). upper () layer [ \"metadata\" ][ \"ows_extent\" ] = lyrext mappyfile . save ( mapfile , outmapf , 2 ) Variables conf Functions setMFparams def setMFparams ( outmapf , bands = 3 , scale = [ '0,255' , '0,255' , '0,255' ], crs = 'EPSG:3857' , lyrext = '-20026376.39 -20048966.10 20026376.39 20048966.10' , datadir = '' , inmapf = '/home/fantamodeman/SVILUPPOandTESTING/python_sandbox/dev/modules/hielen2/src/var/template.map' , um = 'METERS' , ows_onlineresources = 'http://pippo&' ) View Source def setMFparams ( outmapf , bands = 3 , scale =[ '0,255','0,255','0,255' ] , crs = 'EPSG:3857' , lyrext = '-20026376.39 -20048966.10 20026376.39 20048966.10' , datadir = '' , inmapf = conf [ 'maptemplate' ] , um = 'METERS' , ows_onlineresources = \"http://pippo&\" ) : mapfile = mappyfile . open ( inmapf ) mapfile [ \"shapepath\" ] = datadir mapfile [ \"web\" ][ \"metadata\" ][ \"ows_onlineresource\" ] = ows_onlineresources if ( bands == 1 ) : scale =[ '0,65536' ] layer = mapfile [ \"layers\" ][ 0 ] tbands = ',' . join ( map ( str ,( range ( 1 , bands + 1 )))) layer [ 'processing' ]=[ f'BANDS={tbands}' ] for i in range ( 0 , bands ) : layer [ 'processing' ] . append ( f \"SCALE_{i+1}={scale[i]}\" ) layer [ \"projection\" ] = f \"init={str(crs).lower()}\" layer [ \"units\" ] = re . sub ( 'METRES?' , 'METERS' , um . upper ()) layer [ \"metadata\" ][ \"ows_srs\" ] = str ( crs ). upper () layer [ \"metadata\" ][ \"ows_extent\" ] = lyrext mappyfile . save ( mapfile , outmapf , 2 )","title":"Mapper"},{"location":"reference/hielen2/maps/mapper/#module-hielen2mapsmapper","text":"View Source # outmapf < output mapfile , absolute or relative ( to the running script ) path and filename : MANDATORY > # bands < image processing bands ; defaults to '1, 2, 3' > # scale1 < image first band ( red or B / W ) processing scale ( min , max ); defaults to '0,255' in case of RGB and to '0,65536' in case of B / W > # scale2 < image second band ( green ) processing scale ( min , max ); defaults to '0,255' > # scale3 < image third band ( blue ) processing scale ( min , max ); defaults to '0,255' > # crs < image CRS projection ( EPSG number ); defaults to '3857' > # lyrext < image extent ( minX , minY , maxX , maxY ); defaults to '-20026376.39 -20048966.10 20026376.39 20048966.10' ( = fullext EPSG : 3857 ) > # datadir < raster image data directory , relative to mapfile location ; defaults to '../data' > # inmapf < input mapfile , absolute or relative ( to the running script ) path and filename ; defaults to 'SAGraster.map' > # um < unit of measurement ( 'METERS' , 'DD' ...); defaults to 'METERS' > import mappyfile import re from hielen2 import conf def setMFparams ( outmapf , bands = 3 , scale =[ '0,255','0,255','0,255' ] , crs = 'EPSG:3857' , lyrext = '-20026376.39 -20048966.10 20026376.39 20048966.10' , datadir = '' , inmapf = conf [ 'maptemplate' ] , um = 'METERS' , ows_onlineresources = \"http://pippo&\" ) : mapfile = mappyfile . open ( inmapf ) mapfile [ \"shapepath\" ] = datadir mapfile [ \"web\" ][ \"metadata\" ][ \"ows_onlineresource\" ] = ows_onlineresources if ( bands == 1 ) : scale =[ '0,65536' ] layer = mapfile [ \"layers\" ][ 0 ] tbands = ',' . join ( map ( str ,( range ( 1 , bands + 1 )))) layer [ 'processing' ]=[ f'BANDS={tbands}' ] for i in range ( 0 , bands ) : layer [ 'processing' ] . append ( f \"SCALE_{i+1}={scale[i]}\" ) layer [ \"projection\" ] = f \"init={str(crs).lower()}\" layer [ \"units\" ] = re . sub ( 'METRES?' , 'METERS' , um . upper ()) layer [ \"metadata\" ][ \"ows_srs\" ] = str ( crs ). upper () layer [ \"metadata\" ][ \"ows_extent\" ] = lyrext mappyfile . save ( mapfile , outmapf , 2 )","title":"Module hielen2.maps.mapper"},{"location":"reference/hielen2/maps/mapper/#variables","text":"conf","title":"Variables"},{"location":"reference/hielen2/maps/mapper/#functions","text":"","title":"Functions"},{"location":"reference/hielen2/maps/mapper/#setmfparams","text":"def setMFparams ( outmapf , bands = 3 , scale = [ '0,255' , '0,255' , '0,255' ], crs = 'EPSG:3857' , lyrext = '-20026376.39 -20048966.10 20026376.39 20048966.10' , datadir = '' , inmapf = '/home/fantamodeman/SVILUPPOandTESTING/python_sandbox/dev/modules/hielen2/src/var/template.map' , um = 'METERS' , ows_onlineresources = 'http://pippo&' ) View Source def setMFparams ( outmapf , bands = 3 , scale =[ '0,255','0,255','0,255' ] , crs = 'EPSG:3857' , lyrext = '-20026376.39 -20048966.10 20026376.39 20048966.10' , datadir = '' , inmapf = conf [ 'maptemplate' ] , um = 'METERS' , ows_onlineresources = \"http://pippo&\" ) : mapfile = mappyfile . open ( inmapf ) mapfile [ \"shapepath\" ] = datadir mapfile [ \"web\" ][ \"metadata\" ][ \"ows_onlineresource\" ] = ows_onlineresources if ( bands == 1 ) : scale =[ '0,65536' ] layer = mapfile [ \"layers\" ][ 0 ] tbands = ',' . join ( map ( str ,( range ( 1 , bands + 1 )))) layer [ 'processing' ]=[ f'BANDS={tbands}' ] for i in range ( 0 , bands ) : layer [ 'processing' ] . append ( f \"SCALE_{i+1}={scale[i]}\" ) layer [ \"projection\" ] = f \"init={str(crs).lower()}\" layer [ \"units\" ] = re . sub ( 'METRES?' , 'METERS' , um . upper ()) layer [ \"metadata\" ][ \"ows_srs\" ] = str ( crs ). upper () layer [ \"metadata\" ][ \"ows_extent\" ] = lyrext mappyfile . save ( mapfile , outmapf , 2 )","title":"setMFparams"}]}